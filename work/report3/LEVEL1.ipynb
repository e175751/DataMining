{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データマイニング Report3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 全体の流れ\n",
    "    + NLTKの解説本の0章〜12章まで、計13個のHTMLファイルをダウンロードせよ。\n",
    "    + BoWベースの特徴ベクトル（Level 1 もしくは Level 2）を生成せよ。\n",
    "    + 共起行列ベースの特徴ベクトル（Level3）を生成せよ。\n",
    "    + ラベル付き文書に対して分類タスク（Level4）を実行せよ。\n",
    "+ Level 1: 文書ファイル毎に、``Bag-of-Words``で特徴ベクトルを生成せよ。\n",
    "+ Level 2: ``BoW``に``TF-IDF``で重み調整した特徴ベクトルを生成せよ。\n",
    "+ Level 3: 単語の``共起行列``から特徴ベクトルを生成せよ。\n",
    "+ Level 4: ``文書分類``せよ。\n",
    "+ オプション例\n",
    "    + 相互情報量から``特徴ベクトル``を生成してみよう。\n",
    "    + 共起行列に基づいた特徴ベクトル、もしくは相互特徴量に基づいた特徴ベクトルを``SVD``により``次元削減``してみよう。\n",
    "    + SVDによる次元削減時に``2次元``とせよ。気になる単語1つを選び、上位10件と下位10件を2次元空間にマッピングせよ。マッピング結果、どのように散らばっているか観察し、想定とどのぐらい似通っているか考察してみよう。\n",
    "    + ``日本語文書``について自然言語処理してみよう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import wordpunct_tokenize, sent_tokenize\n",
    "import numpy as np\n",
    "import glob\n",
    "import scipy.spatial.distance as distance\n",
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LEVEL1:文書ファイル毎に、Bag-of-Wordsで特徴ベクトルを生成せよ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ collect_words_eng(): 英文書集合から単語コードブック作成\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nltkのdownloadするべきmoudle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/e175751/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/e175751/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/e175751/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag-of-Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文書集合からターム素性集合（コードブック）を作る"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def collect_words_eng(docs):\n",
    "    '''\n",
    "    英文書集合から単語コードブック作成。\n",
    "    シンプルに文書集合を予め決めうちした方式で処理する。\n",
    "    必要に応じて指定できるようにしていた方が使い易いかも。\n",
    "\n",
    "    :param docs(list): 1文書1文字列で保存。複数文書をリストとして並べたもの。\n",
    "    :return (list): 文分割、単語分割、基本形、ストップワード除去した、ユニークな単語一覧。\n",
    "    '''\n",
    "    \n",
    "    codebook = []\n",
    "    stopwords = nltk.corpus.stopwords.words('english') \n",
    "    \n",
    "    #stopwords.append('.')   # ピリオドを追加。\n",
    "    #stopwords.append(',')   # カンマを追加。\n",
    "    #stopwords.append('')    # 空文字を追加。\n",
    "    \n",
    "    symbol = [\"'\", '\"', ':', ';', '.', ',', '-', '!', '?', \"'s\",\"<\",\">\",\"_\"]\n",
    "    '''\n",
    "    SWList = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\n",
    "              \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \n",
    "              'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', \n",
    "              'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', \n",
    "              'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', \n",
    "              'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', \n",
    "              'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', \n",
    "              'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on',\n",
    "              'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', \n",
    "              'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', \n",
    "              'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will',\n",
    "              'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain',\n",
    "              'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \n",
    "              \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\",\n",
    "              'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn',\n",
    "              \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
    "    '''\n",
    "    \n",
    "    clean_frequency = nltk.FreqDist(w.lower() for w in docs if w.lower() not in stopwords + symbol)\n",
    "    \n",
    "    wnl = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "    \n",
    "    for doc in docs:\n",
    "        for sent in sent_tokenize(doc):\n",
    "            for word in wordpunct_tokenize(sent):\n",
    "                this_word = wnl.lemmatize(word.lower())\n",
    "                if this_word not in codebook and this_word not in clean_frequency:\n",
    "                    codebook.append(this_word)\n",
    "    return codebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_words_eng1(docs):\n",
    "    \n",
    "    codebook = []\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    stopwords.append('.')   # ピリオドを追加。\n",
    "    stopwords.append(',')   # カンマを追加。\n",
    "    stopwords.append('')    # 空文字を追加。\n",
    "    wnl = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "    for doc in docs:\n",
    "        for sent in sent_tokenize(doc):\n",
    "            for word in wordpunct_tokenize(sent):\n",
    "                this_word = wnl.lemmatize(word.lower())\n",
    "                if this_word not in codebook and this_word not in stopwords:\n",
    "                    codebook.append(this_word)\n",
    "    return codebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "サンプル(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs3 = []\n",
    "docs3.append(\"This is test.\")\n",
    "docs3.append(\"That is test too.\")\n",
    "docs3.append(\"There are so many many tests.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``clean_frequencya``を使った場合\n",
    "これにより、vector数が10個になる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "codebook =  ['test', 'many']\n"
     ]
    }
   ],
   "source": [
    "codebook = collect_words_eng1(docs3)\n",
    "print('codebook = ',codebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``stopwords``のままの場合\n",
    "これにより、vector数が2個となる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "codebook =  ['this', 'is', 'test', '.', 'that', 'too', 'there', 'are', 'so', 'many']\n"
     ]
    }
   ],
   "source": [
    "codebook = collect_words_eng(docs3)\n",
    "print('codebook = ',codebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## コードブックを素性とする文書ベクトルを作る (直接ベクトル生成)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_vectors_eng(docs, codebook):\n",
    "    '''コードブックを素性とする文書ベクトルを作る（直接ベクトル生成）\n",
    "\n",
    "    :param docs(list): 1文書1文字列で保存。複数文書をリストとして並べたもの。\n",
    "    :param codebook(list): ユニークな単語一覧。\n",
    "    :return (list): コードブックを元に、出現回数を特徴量とするベクトルを返す。\n",
    "    '''\n",
    "    vectors = []\n",
    "    wnl = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "    for doc in docs:\n",
    "        this_vector = []\n",
    "        fdist = nltk.FreqDist()\n",
    "        for sent in sent_tokenize(doc):\n",
    "            for word in wordpunct_tokenize(sent):\n",
    "                this_word = wnl.lemmatize(word.lower())\n",
    "                fdist[this_word] += 1\n",
    "        for word in codebook:\n",
    "            this_vector.append(fdist[word])\n",
    "        vectors.append(this_vector)\n",
    "    return vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docs[0] = This is test.\n",
      "vectors[0] = [1, 1, 1, 1, 0, 0, 0, 0, 0, 0]\n",
      "----\n",
      "docs[1] = That is test too.\n",
      "vectors[1] = [0, 1, 1, 1, 1, 1, 0, 0, 0, 0]\n",
      "----\n",
      "docs[2] = There are so many many tests.\n",
      "vectors[2] = [0, 0, 1, 1, 0, 0, 1, 1, 1, 2]\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "vectors = make_vectors_eng(docs3, codebook)\n",
    "for index in range(len(docs3)):\n",
    "    print('docs[{}] = {}'.format(index,docs3[index]))\n",
    "    print('vectors[{}] = {}'.format(index,vectors[index]))\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ユークリッド距離"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(vectors):\n",
    "    vectors = np.array(vectors)\n",
    "    distances = []\n",
    "    for i in range(len(vectors)):\n",
    "        temp = []\n",
    "        for j in range(len(vectors)):\n",
    "            temp.append(np.linalg.norm(vectors[i] - vectors[j]))\n",
    "        distances.append(temp)\n",
    "    return distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# euclidean_distance\n",
      "[0.0, 1.7320508075688772, 3.0]\n",
      "[1.7320508075688772, 0.0, 3.1622776601683795]\n",
      "[3.0, 3.1622776601683795, 0.0]\n"
     ]
    }
   ],
   "source": [
    "distances = euclidean_distance(vectors)\n",
    "print('# euclidean_distance')\n",
    "for index in range(len(distances)):\n",
    "    print(distances[index])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## コサイン類似度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vectors):\n",
    "    vectors = np.array(vectors)\n",
    "    distances = []\n",
    "    for i in range(len(vectors)):\n",
    "        temp = []\n",
    "        for j in range(len(vectors)):\n",
    "            temp.append(distance.cosine(vectors[i], vectors[j]))\n",
    "        distances.append(temp)\n",
    "    return distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## コサイン類似度(こっちが本物)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_sim(vector):\n",
    "    vectors = np.array(vector)\n",
    "    ListVector=[]\n",
    "    for i in range(len(vectors)):\n",
    "        temp=[]\n",
    "        for j in range(len(vectors)):\n",
    "            temp.append(np.dot(vectors[i], vectors[j]) / (np.linalg.norm(vectors[i]) * np.linalg.norm(vectors[j])))\n",
    "        ListVector.append(temp)\n",
    "    return ListVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# cosine_similarity\n",
      "[1.0, 0.6708203932499369, 0.3333333333333333]\n",
      "[0.6708203932499369, 0.9999999999999998, 0.29814239699997197]\n",
      "[0.3333333333333333, 0.29814239699997197, 1.0]\n"
     ]
    }
   ],
   "source": [
    "hoge = cos_sim(vectors)\n",
    "print('# cosine_similarity')\n",
    "for index in range(len(hoge)):\n",
    "    print(hoge[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# cosine_similarity\n",
      "[0.0, 0.3291796067500631, 0.6666666666666667]\n",
      "[0.3291796067500631, 0.0, 0.7018576030000281]\n",
      "[0.6666666666666667, 0.7018576030000281, 0.0]\n"
     ]
    }
   ],
   "source": [
    "similarities = cosine_similarity(vectors)\n",
    "print('# cosine_similarity')\n",
    "for index in range(len(similarities)):\n",
    "    print(similarities[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## それでは実際に文章を分類する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fileのpathを配列に格納する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "List_Data_NL=[]\n",
    "for i in range(1,14):\n",
    "    List_Data_NL = glob.glob( \"./data/*.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./data/kadai1.html',\n",
       " './data/kadai6.html',\n",
       " './data/kadai10.html',\n",
       " './data/kadai11.html',\n",
       " './data/kadai7.html',\n",
       " './data/kadai4.html',\n",
       " './data/kadai12.html',\n",
       " './data/kadai8.html',\n",
       " './data/kadai9.html',\n",
       " './data/kadai13.html',\n",
       " './data/kadai5.html',\n",
       " './data/kadai2.html',\n",
       " './data/kadai3.html']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "List_Data_NL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataPath = \"./data/kadai\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanhtml(raw_html):\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, '', raw_html)\n",
    "    return cleantext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sentence = []\n",
    "for i in range(1,len(List_Data_NL)+1):\n",
    "    with open(DataPath +str(i) + \".html\" ) as f:\n",
    "        r = f.read()\n",
    "        text = cleanhtml(r)\n",
    "        sentence.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n\\n\\n\\nfunction astext(node)\\n{\\n    return node.innerHTML.replace(/(]+)>)/ig,\"\")\\n                         .replace(/&gt;/ig, \">\")\\n                         .replace(/&lt;/ig, \"<\")\\n                         .replace(/&quot;/ig, \\'\"\\')\\n                         .replace(/&amp;/ig, \"&\");\\n}\\n\\nfunction copy_notify(node, bar_color, data)\\n{\\n    // The outer box: relative + inline positioning.\\n    var box1 = document.createElement(\"div\");\\n    box1.style.position = \"relative\";\\n    box1.style.display = \"inline\";\\n    box1.style.top = \"2em\";\\n    box1.style.left = \"1em\";\\n  \\n    // A shadow for fun\\n    var shadow = document.createElement(\"div\");\\n    shadow.style.position = \"absolute\";\\n    shadow.style.left = \"-1.3em\";\\n    shadow.style.top = \"-1.3em\";\\n    shadow.style.background = \"#404040\";\\n    \\n    // The inner box: absolute positioning.\\n    var box2 = document.createElement(\"div\");\\n    box2.style.position = \"relative\";\\n    box2.style.border = \"1px solid #a0a0a0\";\\n    box2.style.left = \"-.2em\";\\n    box2.style.top = \"-.2em\";\\n    box2.style.background = \"white\";\\n    box2.style.padding = \".3em .4em .3em .4em\";\\n    box2.style.fontStyle = \"normal\";\\n    box2.style.background = \"#f0e0e0\";\\n\\n    node.insertBefore(box1, node.childNodes.item(0));\\n    box1.appendChild(shadow);\\n    shadow.appendChild(box2);\\n    box2.innerHTML=\"Copied&nbsp;to&nbsp;the&nbsp;clipboard: \" +\\n                   \"\"+\\n                   data+\"\";\\n    setTimeout(function() { node.removeChild(box1); }, 1000);\\n\\n    var elt = node.parentNode.firstChild;\\n    elt.style.background = \"#ffc0c0\";\\n    setTimeout(function() { elt.style.background = bar_color; }, 200);\\n}\\n\\nfunction copy_codeblock_to_clipboard(node)\\n{\\n    var data = astext(node)+\"\\\\n\";\\n    if (copy_text_to_clipboard(data)) {\\n        copy_notify(node, \"#40a060\", data);\\n    }\\n}\\n\\nfunction copy_doctest_to_clipboard(node)\\n{\\n    var s = astext(node)+\"\\\\n   \";\\n    var data = \"\";\\n\\n    var start = 0;\\n    var end = s.indexOf(\"\\\\n\");\\n    while (end >= 0) {\\n        if (s.substring(start, start+4) == \">>> \") {\\n            data += s.substring(start+4, end+1);\\n        }\\n        else if (s.substring(start, start+4) == \"... \") {\\n            data += s.substring(start+4, end+1);\\n        }\\n        /*\\n        else if (end-start > 1) {\\n            data += \"# \" + s.substring(start, end+1);\\n        }*/\\n        // Grab the next line.\\n        start = end+1;\\n        end = s.indexOf(\"\\\\n\", start);\\n    }\\n    \\n    if (copy_text_to_clipboard(data)) {\\n        copy_notify(node, \"#4060a0\", data);\\n    }\\n}\\n    \\nfunction copy_text_to_clipboard(data)\\n{\\n    if (window.clipboardData) {\\n        window.clipboardData.setData(\"Text\", data);\\n        return true;\\n     }\\n    else if (window.netscape) {\\n        // w/ default firefox settings, permission will be denied for this:\\n        netscape.security.PrivilegeManager\\n                      .enablePrivilege(\"UniversalXPConnect\");\\n    \\n        var clip = Components.classes[\"@mozilla.org/widget/clipboard;1\"]\\n                      .createInstance(Components.interfaces.nsIClipboard);\\n        if (!clip) return;\\n    \\n        var trans = Components.classes[\"@mozilla.org/widget/transferable;1\"]\\n                       .createInstance(Components.interfaces.nsITransferable);\\n        if (!trans) return;\\n    \\n        trans.addDataFlavor(\"text/unicode\");\\n    \\n        var str = new Object();\\n        var len = new Object();\\n    \\n        var str = Components.classes[\"@mozilla.org/supports-string;1\"]\\n                     .createInstance(Components.interfaces.nsISupportsString);\\n        var datacopy=data;\\n        str.data=datacopy;\\n        trans.setTransferData(\"text/unicode\",str,datacopy.length*2);\\n        var clipid=Components.interfaces.nsIClipboard;\\n    \\n        if (!clip) return false;\\n    \\n        clip.setData(trans,null,clipid.kGlobalClipboard);\\n        return true;\\n    }\\n    return false;\\n}\\n//-->\\n\\n\\n\\n\\n\\n\\nPreface\\n\\n\\n/*\\n:Author: Edward Loper, James Curran\\n:Copyright: This stylesheet has been placed in the public domain.\\n\\nStylesheet for use with Docutils.\\n\\nThis stylesheet defines new css classes used by NLTK.\\n\\nIt uses a Python syntax highlighting scheme that matches\\nthe colour scheme used by IDLE, which makes it easier for\\nbeginners to check they are typing things in correctly.\\n*/\\n\\n/* Include the standard docutils stylesheet. */\\n@import url(default.css);\\n\\n/* Custom inline roles */\\nspan.placeholder    { font-style: italic; font-family: monospace; }\\nspan.example        { font-style: italic; }\\nspan.emphasis       { font-style: italic; }\\nspan.termdef        { font-weight: bold; }\\n/*span.term           { font-style: italic; }*/\\nspan.category       { font-variant: small-caps; }\\nspan.feature        { font-variant: small-caps; }\\nspan.fval           { font-style: italic; }\\nspan.math           { font-style: italic; }\\nspan.mathit         { font-style: italic; }\\nspan.lex            { font-variant: small-caps; }\\nspan.guide-linecount{ text-align: right; display: block;}\\n\\n/* Python souce code listings */\\nspan.pysrc-prompt   { color: #9b0000; }\\nspan.pysrc-more     { color: #9b00ff; }\\nspan.pysrc-keyword  { color: #e06000; }\\nspan.pysrc-builtin  { color: #940094; }\\nspan.pysrc-string   { color: #00aa00; }\\nspan.pysrc-comment  { color: #ff0000; }\\nspan.pysrc-output   { color: #0000ff; }\\nspan.pysrc-except   { color: #ff0000; }\\nspan.pysrc-defname  { color: #008080; }\\n\\n\\n/* Doctest blocks */\\npre.doctest         { margin: 0; padding: 0; font-weight: bold; }\\ndiv.doctest         { margin: 0 1em 1em 1em; padding: 0; }\\ntable.doctest       { margin: 0; padding: 0;\\n                      border-top: 1px solid gray;\\n                      border-bottom: 1px solid gray; }\\npre.copy-notify     { margin: 0; padding: 0.2em; font-weight: bold;\\n                      background-color: #ffffff; }\\n\\n/* Python source listings */\\ndiv.pylisting       { margin: 0 1em 1em 1em; padding: 0; }\\ntable.pylisting     { margin: 0; padding: 0;\\n                      border-top: 1px solid gray; }\\ntd.caption { border-top: 1px solid black; margin: 0; padding: 0; }\\n.caption-label { font-weight: bold;  }\\ntd.caption p { margin: 0; padding: 0; font-style: normal;}\\n\\ntable tr td.codeblock { \\n  padding: 0.2em ! important; margin: 0;\\n  border-left: 1px solid gray;\\n  border-right: 2px solid gray;\\n  border-top: 0px solid gray;\\n  border-bottom: 1px solid gray;\\n  font-weight: bold; background-color: #eeffee;\\n}\\ntable pre span {\\n  white-space: pre-wrap;\\n}\\ntable tr td.doctest  { \\n  padding: 0.2em; margin: 0;\\n  border-left: 1px solid gray;\\n  border-right: 2px solid gray;\\n  border-top: 0px solid gray;\\n  border-bottom: 1px solid gray;\\n  font-weight: bold; background-color: #eeeeff;\\n}\\n\\ntd.codeblock table tr td.copybar {\\n    background: #40a060; border: 1px solid gray;\\n    font-family: monospace; padding: 0; margin: 0; }\\ntd.doctest table tr td.copybar {\\n    background: #4060a0; border: 1px solid gray;\\n    font-family: monospace; padding: 0; margin: 0; }\\n\\ntd.pysrc { padding-left: 0.5em; }\\n\\nimg.callout { border-width: 0px; }\\n\\ntable.docutils {\\n    border-style: solid;\\n    border-width: 1px;\\n    margin-top: 6px;\\n    border-color: grey;\\n    border-collapse: collapse; }\\n\\ntable.docutils th {\\n    border-style: none;\\n    border-width: 1px;\\n    border-color: grey;\\n    padding: 0 .5em 0 .5em; }\\n\\ntable.docutils td {\\n    border-style: none;\\n    border-width: 1px;\\n    border-color: grey; \\n    padding: 0 .5em 0 .5em; }\\n\\ntable.footnote td { padding: 0; }\\ntable.footnote { border-width: 0; }\\ntable.footnote td { border-width: 0; }\\ntable.footnote th { border-width: 0; }\\n\\ntable.noborder { border-width: 0; }\\n\\ntable.example pre { margin-top: 4px; margin-bottom: 0; }\\n\\n/* For figures & tables */\\np.caption { margin-bottom: 0; }\\ndiv.figure { text-align: center; }\\n\\n/* The index */\\ndiv.index { border: 1px solid black;\\n            background-color: #eeeeee; }\\ndiv.index h1 { padding-left: 0.5em; margin-top: 0.5ex;\\n               border-bottom: 1px solid black; }\\nul.index { margin-left: 0.5em; padding-left: 0; }\\nli.index { list-style-type: none; }\\np.index-heading { font-size: 120%; font-style: italic; margin: 0; }\\nli.index ul { margin-left: 2em; padding-left: 0; }\\n\\n/* \\'Note\\' callouts */\\ndiv.note\\n{\\n  border-right:   #87ceeb 1px solid;\\n  padding-right: 4px;\\n  border-top: #87ceeb 1px solid;\\n  padding-left: 4px;\\n  padding-bottom: 4px;\\n  margin: 2px 5% 10px;\\n  border-left: #87ceeb 1px solid;\\n  padding-top: 4px;\\n  border-bottom: #87ceeb 1px solid;\\n  font-style: normal;\\n  font-family: verdana, arial;\\n  background-color: #b0c4de;\\n}\\n\\ntable.avm { border: 0px solid black; width: 0; }\\ntable.avm tbody tr {border: 0px solid black; }\\ntable.avm tbody tr td { padding: 2px; }\\ntable.avm tbody tr td.avm-key { padding: 5px; font-variant: small-caps; }\\ntable.avm tbody tr td.avm-eq { padding: 5px; }\\ntable.avm tbody tr td.avm-val { padding: 5px; font-style: italic; }\\np.avm-empty { font-style: normal; }\\ntable.avm colgroup col { border: 0px solid black; }\\ntable.avm tbody tr td.avm-topleft \\n    { border-left: 2px solid #000080; border-top: 2px solid #000080; }\\ntable.avm tbody tr td.avm-botleft \\n    { border-left: 2px solid #000080; border-bottom: 2px solid #000080; }\\ntable.avm tbody tr td.avm-topright\\n    { border-right: 2px solid #000080; border-top: 2px solid #000080; }\\ntable.avm tbody tr td.avm-botright\\n    { border-right: 2px solid #000080; border-bottom: 2px solid #000080; }\\ntable.avm tbody tr td.avm-left\\n    { border-left: 2px solid #000080; }\\ntable.avm tbody tr td.avm-right\\n    { border-right: 2px solid #000080; }\\ntable.avm tbody tr td.avm-topbotleft\\n    { border: 2px solid #000080; border-right: 0px solid black; }\\ntable.avm tbody tr td.avm-topbotright\\n    { border: 2px solid #000080; border-left: 0px solid black; }\\ntable.avm tbody tr td.avm-ident\\n    { font-size: 80%; padding: 0; padding-left: 2px; vertical-align: top; }\\n.avm-pointer\\n{ border: 1px solid #008000; padding: 1px; color: #008000; \\n  background: #c0ffc0; font-style: normal; }\\n\\ntable.gloss { border: 0px solid black; width: 0; }\\ntable.gloss tbody tr { border: 0px solid black; }\\ntable.gloss tbody tr td { border: 0px solid black; }\\ntable.gloss colgroup col { border: 0px solid black; }\\ntable.gloss p { margin: 0; padding: 0; }\\n\\ntable.rst-example { border: 1px solid black; }\\ntable.rst-example tbody tr td { background: #eeeeee; }\\ntable.rst-example thead tr th { background: #c0ffff; }\\ntd.rst-raw { width: 0; }\\n\\n/* Used by nltk.org/doc/test: */\\ndiv.doctest-list { text-align: center; }\\ntable.doctest-list { border: 1px solid black;\\n  margin-left: auto; margin-right: auto;\\n}\\ntable.doctest-list tbody tr td { background: #eeeeee;\\n  border: 1px solid #cccccc; text-align: left; }\\ntable.doctest-list thead tr th { background: #304050; color: #ffffff;\\n  border: 1px solid #000000;}\\ntable.doctest-list thead tr a { color: #ffffff; }\\nspan.doctest-passed { color: #008000; }\\nspan.doctest-failed { color: #800000; }\\n\\n\\n\\n\\n\\nPreface\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n<!-- Grammatical Category - e.g. NP and verb as technical terms\\n.. role:: gc\\n   :class: category -->\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nThis is a book about Natural Language Processing. By \"natural\\nlanguage\" we mean a language that is used for everyday\\ncommunication by humans; languages like English, Hindi or\\nPortuguese. In contrast to artificial languages such as\\nprogramming languages and mathematical notations, natural languages have\\nevolved as they pass from generation to generation, and are hard to\\npin down with explicit rules. We will take Natural Language Processing — or NLP\\nfor short — in a wide sense to cover any kind of computer manipulation\\nof natural language. At one extreme, it could be as simple as counting\\nword frequencies to compare different writing styles.\\nAt the other extreme, NLP involves \"understanding\" complete human\\nutterances, at least to the extent of being able to give useful\\nresponses to them.\\nTechnologies based on NLP\\nare becoming increasingly widespread. For example, phones and handheld computers\\nsupport predictive text and handwriting recognition;  web\\nsearch engines give access to information locked up in unstructured\\ntext; machine translation allows us to retrieve texts written in\\nChinese and read them in Spanish; text analysis enables us to\\ndetect sentiment in tweets and blogs.  By providing more natural human-machine interfaces, and more\\nsophisticated access to stored information, language processing has\\ncome to play a central role in the multilingual information society.\\nThis book provides a highly accessible introduction to the field of NLP.\\nIt can be used for individual study or as the textbook for a course\\non natural language processing or computational linguistics,\\nor as a supplement to courses in artificial intelligence,\\ntext mining, or corpus linguistics.\\nThe book is intensely practical, containing\\nhundreds of fully-worked examples and graded exercises.\\nThe book is based on the Python programming language together with an open source\\nlibrary called the Natural Language Toolkit (NLTK).\\nNLTK includes extensive software, data, and documentation, all freely downloadable from http://nltk.org/.\\nDistributions are provided for Windows, Macintosh and Unix platforms.\\nWe strongly encourage you to download Python and NLTK, and try out the\\nexamples and exercises along the way.\\n\\nAudience\\nNLP is important for scientific, economic, social, and cultural reasons.\\nNLP is experiencing rapid growth as its theories and methods are deployed in\\na variety of new language technologies.  For this reason it is\\nimportant for a wide range of people to have a working knowledge of NLP.\\nWithin industry, this includes people in\\nhuman-computer interaction, business information analysis,\\nand web software development.\\nWithin academia, it includes people in areas from\\nhumanities computing and corpus linguistics\\nthrough to computer science and artificial intelligence.\\n(To many people in academia, NLP is known by the name of\\n\"Computational Linguistics.\")\\nThis book is intended for a diverse range of people who want to\\nlearn how to write programs that analyze written language,\\nregardless of previous programming experience:\\n\\n\\n\\n\\nNew to programming?:\\n&nbsp;The early chapters of the book are suitable\\nfor readers with no prior knowledge of programming, so long as you\\naren\\'t afraid to tackle new concepts and develop new computing skills.\\nThe book is full of examples that you can copy and\\ntry for yourself, together with hundreds of graded exercises.\\nIf you need a more general introduction to Python, see the\\nlist of Python resources at http://docs.python.org/.\\n\\nNew to Python?:Experienced programmers can quickly learn enough\\nPython using this book to get immersed in natural language processing.\\nAll relevant Python features are carefully explained and exemplified,\\nand you will quickly come to appreciate Python\\'s suitability for this\\napplication area.  The language index will help you locate relevant\\ndiscussions in the book.\\n\\nAlready dreaming in Python?:\\n&nbsp;Skim the Python examples\\nand dig into the interesting language analysis\\nmaterial that starts in 1..\\nYou\\'ll soon be applying your skills to this fascinating domain.\\n\\n\\n\\n\\n\\nEmphasis\\nThis book is a practical introduction to NLP.  You will learn by\\nexample, write real programs, and grasp the value of being able to\\ntest an idea through implementation.  If you haven\\'t learnt already,\\nthis book will teach you programming.  Unlike other programming\\nbooks, we provide extensive illustrations and exercises from NLP.  The\\napproach we have taken is also principled, in that we cover the\\ntheoretical underpinnings and don\\'t shy away from careful linguistic\\nand computational analysis.  We have tried to be pragmatic in\\nstriking a balance between theory and application, identifying the\\nconnections and the tensions.  Finally, we recognize that you\\nwon\\'t get through this unless it is also pleasurable, so we have\\ntried to include many applications and examples that are interesting\\nand entertaining, sometimes whimsical.\\nNote that this book is not a reference work.  Its coverage of Python\\nand NLP is selective, and presented in a tutorial style.  For\\nreference material, please consult the substantial quantity of\\nsearchable resources available at http://python.org/ and http://nltk.org/.\\nThis book is not an advanced computer science text.\\nThe content ranges from introductory to intermediate, and is\\ndirected at readers who want to learn how to analyze\\ntext using Python and the Natural Language Toolkit.\\nTo learn about advanced algorithms implemented in NLTK,\\nyou can examine the Python code linked from http://nltk.org/,\\nand consult the other materials cited in this book.\\n\\n\\nWhat You Will Learn\\nBy digging into the material presented here, you will learn:\\n\\nHow simple programs can help you manipulate and analyze\\nlanguage data, and how to write these programs\\nHow key concepts from NLP and linguistics are used to describe and\\nanalyse language\\nHow data structures and algorithms are used in NLP\\nHow language data is stored in standard formats, and how data can\\nbe used to evaluate the performance of NLP techniques\\n\\nDepending on your background, and your motivation for being interested in NLP,\\nyou will gain different kinds of skills and knowledge from this book, as set out\\nin III.1.\\nTable III.1: Skills and knowledge to be gained from reading this book, depending on readers\\' goals and background\\n\\n\\n\\n\\n\\n\\nGoals\\nBackground in arts and humanities\\nBackground in science and engineering\\n\\n\\n\\nLanguage analysis\\nManipulating large corpora,\\nexploring linguistic models,\\nand testing empirical claims.\\nUsing techniques in data modeling,\\ndata mining, and knowledge discovery\\nto analyze natural language.\\n\\nLanguage technology\\nBuilding robust systems to\\nperform linguistic tasks\\nwith technological applications.\\nUsing linguistic algorithms and\\ndata structures in robust\\nlanguage processing software.\\n\\n\\n\\n\\n\\n\\n\\nOrganization\\nThe early chapters are organized in order of conceptual difficulty,\\nstarting with a practical introduction to language processing\\nthat shows how to explore interesting bodies of text using\\ntiny Python programs (Chapters 1-3).  This is followed by\\na chapter on structured programming (Chapter 4) that consolidates the\\nprogramming topics scattered across the preceding chapters.\\nAfter this, the pace picks up, and we move\\non to a series of chapters covering fundamental topics in\\nlanguage processing:\\ntagging, classification, and information extraction (Chapters 5-7).\\nThe next three chapters look at ways to parse a sentence, recognize\\nits syntactic structure, and construct representations of meaning (Chapters 8-10).\\nThe final chapter is devoted to linguistic data and how it can\\nbe managed effectively (Chapter 11).\\nThe book concludes with an Afterword, briefly discussing the past and future of the field.\\nWithin each chapter, we switch between different styles of presentation.\\nIn one style, natural language is the driver.  We analyze language,\\nexplore linguistic concepts, and use programming examples to support\\nthe discussion.  We often employ Python constructs that have\\nnot been introduced systematically, so you can see their purpose\\nbefore delving into the details of how and why they work.\\nThis is just like learning idiomatic expressions in a foreign language:\\nyou\\'re able to buy a nice pastry without first having learnt\\nthe intricacies of question formation.\\nIn the other style of presentation, the programming language will be the driver.\\nWe\\'ll analyze programs, explore algorithms, and the linguistic examples\\nwill play a supporting role.\\nEach chapter ends with a series of graded exercises,\\nwhich are useful for consolidating the material.\\nThe exercises are graded according to the following scheme:\\n☼ is for easy exercises that involve minor modifications\\nto supplied code samples or other simple activities;\\n◑ is for intermediate exercises that explore an aspect\\nof the material in more depth, requiring careful analysis and design;\\n★ is for difficult, open-ended tasks that will challenge your\\nunderstanding of the material and force you to think independently\\n(readers new to programming should skip these).\\nEach chapter has a further reading section and an online \"extras\"\\nsection at http://nltk.org/, with pointers to more advanced materials and\\nonline resources.  Online versions of all the code examples are also\\navailable there.\\n\\n\\nWhy Python?\\nPython is a simple yet powerful programming language with excellent\\nfunctionality for processing linguistic data.  Python can be\\ndownloaded for free from http://python.org/.\\nInstallers are available for all platforms.\\nHere is a five-line Python program that processes file.txt\\nand prints all the words ending in ing:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; for line in open(\"file.txt\"):\\n...     for word in line.split():\\n...         if word.endswith(\\'ing\\'):\\n...             print(word)\\n\\n\\n\\nThis program illustrates some of the main features of Python.  First,\\nwhitespace is used to nest lines of code, thus the line starting\\nwith if falls inside the scope of the previous line starting with\\nfor; this ensures that the ing test is performed for each\\nword.  Second, Python is object-oriented; each variable is an entity\\nthat has certain defined attributes and methods.  For example, the\\nvalue of the variable line is more than a sequence of characters.\\nIt is a string object that has a \"method\" (or operation) called\\nsplit() that we can use to break a line into its words.  To apply\\na method to an object, we write the object name, followed by a period,\\nfollowed by the method name, i.e. line.split().  Third, methods\\nhave arguments expressed inside parentheses.  For instance, in the\\nexample, word.endswith(\\'ing\\') had the argument \\'ing\\' to\\nindicate that we wanted words ending with ing and not something else.\\nFinally — and most importantly —\\nPython is highly readable, so much so that it is fairly easy to guess\\nwhat the program does even if you have never written a program\\nbefore.\\nWe chose Python because it has a shallow learning curve,\\nits syntax and semantics are transparent,\\nand it has good string-handling functionality.  As an interpreted\\nlanguage, Python facilitates interactive exploration.  As an\\nobject-oriented language, Python permits data and methods to be\\nencapsulated and re-used easily.  As a dynamic language, Python\\npermits attributes to be added to objects on the fly, and permits\\nvariables to be typed dynamically, facilitating rapid development.\\nPython comes with an extensive\\nstandard library, including components for graphical programming,\\nnumerical processing, and web connectivity.\\nPython is heavily used in industry, scientific research, and education\\naround the world.  Python is often praised for the way it facilitates\\nproductivity, quality, and maintainability of software.  A collection of\\nPython success stories is posted at http://python.org/about/success/.\\nNLTK defines an infrastructure that can be used to build NLP\\nprograms in Python.  It provides\\nbasic classes for representing data relevant to natural language processing;\\nstandard interfaces for performing tasks such as\\npart-of-speech tagging, syntactic parsing, and text classification;\\nand standard implementations for each task which can be combined to solve complex problems.\\nNLTK comes with extensive documentation. In addition to this\\nbook, the website at http://nltk.org/ provides API documentation\\nthat covers every module, class and function in the toolkit,\\nspecifying parameters and giving examples of usage.\\n\\n\\nPython 3 and NLTK 3\\nThis version of the book has been updated to support Python 3 and NLTK\\n3. Python 3 includes some significant changes:\\n\\nthe print statement is now a function requiring parentheses;\\nmany functions now return iterators instead of lists (to save memory usage);\\ninteger division returns a floating point number\\nall text is now Unicode\\nstrings are formatted using the format method\\n\\nFor a more detailed list of changes, please see\\nhttps://docs.python.org/dev/whatsnew/3.0.html.\\nThere is a utility called 2to3.py which can convert your Python 2\\ncode to Python 3; for details please see\\nhttps://docs.python.org/2/library/2to3.html.\\nNLTK also includes some pervasive changes:\\n\\nmany types are initialised from strings using a fromstring() method\\nmany functions now return iterators instead of lists\\nContextFreeGrammar is now called CFG and WeightedGrammar is now called PCFG\\nbatch_tokenize() is now called tokenize_sents(); there are corresponding changes for batch taggers, parsers, and classifiers\\nsome implementations have been removed in favour of external packages, or because they could not be maintained adequately\\n\\nFor a more detailed list of changes, please see\\nhttps://github.com/nltk/nltk/wiki/Porting-your-code-to-NLTK-3.0.\\n\\n\\nSoftware Requirements\\nTo get the most out of this book, you should install several free software packages.\\nCurrent download pointers and instructions are available at http://nltk.org/.\\n\\n\\n\\n\\nPython:The material presented in this book assumes that you are using Python version 3.2 or later.\\n(Note that NLTK 3.0 also works with Python 2.6 and 2.7.)\\n\\nNLTK:The code examples in this book use NLTK version 3.0.  Subsequent releases of NLTK\\nwill be backward-compatible with NLTK 3.0.\\n\\nNLTK-Data:This contains the linguistic corpora that are analyzed and processed in the book.\\n\\nNumPy:(recommended)\\nThis is a scientific computing library with support for multidimensional arrays and\\nlinear algebra, required for certain probability, tagging, clustering, and classification\\ntasks.\\n\\nMatplotlib:(recommended)\\nThis is a 2D plotting library for data visualization,\\nand is used in some of the book\\'s code samples that produce line graphs and bar charts.\\n\\nStanford NLP Tools:\\n&nbsp;(recommended)\\nNLTK includes interfaces to the Stanford NLP Tools which are useful for large scale\\nlanguage processing (see http://nlp.stanford.edu/software/).\\n\\nNetworkX:(optional)\\nThis is a library for storing and manipulating network structures consisting of\\nnodes and edges.  For visualizing semantic networks, also install the Graphviz library.\\n\\nProver9:(optional)\\nThis is an automated theorem prover for first-order and equational logic, used\\nto support inference in language processing.\\n\\n\\n\\n\\n\\nNatural Language Toolkit (NLTK)\\nNLTK was originally created in 2001 as part of a computational linguistics\\ncourse in the Department of Computer and Information Science at the\\nUniversity of Pennsylvania.  Since then it has been developed\\nand expanded with the help of dozens of contributors.  It has now been\\nadopted in courses in dozens of universities, and serves as the basis\\nof many research projects.  See VIII.1 for a list of the most\\nimportant NLTK modules.\\nTable VIII.1: Language processing tasks and corresponding NLTK modules with examples of functionality\\n\\n\\n\\n\\n\\n\\nLanguage processing task\\nNLTK modules\\nFunctionality\\n\\n\\n\\nAccessing corpora\\ncorpus\\nstandardized interfaces to corpora and lexicons\\n\\nString processing\\ntokenize, stem\\ntokenizers, sentence tokenizers, stemmers\\n\\nCollocation discovery\\ncollocations\\nt-test, chi-squared, point-wise mutual information\\n\\nPart-of-speech tagging\\ntag\\nn-gram, backoff, Brill, HMM, TnT\\n\\nMachine learning\\nclassify, cluster, tbl\\ndecision tree, maximum entropy, naive Bayes, EM, k-means\\n\\nChunking\\nchunk\\nregular expression, n-gram, named-entity\\n\\nParsing\\nparse, ccg\\nchart, feature-based, unification, probabilistic, dependency\\n\\nSemantic interpretation\\nsem, inference\\nlambda calculus, first-order logic, model checking\\n\\nEvaluation metrics\\nmetrics\\nprecision, recall, agreement coefficients\\n\\nProbability and estimation\\nprobability\\nfrequency distributions, smoothed probability distributions\\n\\nApplications\\napp, chat\\ngraphical concordancer, parsers, WordNet browser, chatbots\\n\\nLinguistic fieldwork\\ntoolbox\\nmanipulate data in SIL Toolbox format\\n\\n\\n\\n\\n\\nNLTK was designed with four primary goals in mind:\\n\\n\\n\\n\\nSimplicity:To provide an intuitive framework along with\\nsubstantial building blocks, giving users a practical\\nknowledge of NLP without getting bogged down in the tedious\\nhouse-keeping usually associated with processing annotated\\nlanguage data\\n\\nConsistency:To provide a uniform framework with consistent\\ninterfaces and data structures, and easily-guessable method names\\n\\nExtensibility:To provide a structure into which new software\\nmodules can be easily accommodated, including alternative\\nimplementations and competing approaches to the same task\\n\\nModularity:To provide components that can be used\\nindependently without needing to understand the rest of\\nthe toolkit\\n\\n\\n\\nContrasting with these goals are three non-requirements —\\npotentially useful qualities that we have deliberately avoided.  First,\\nwhile the toolkit provides a wide range of functions, it is not\\nencyclopedic; it is a toolkit, not a system, and it will\\ncontinue to evolve with the field of NLP.\\nSecond, while the toolkit is efficient enough to support\\nmeaningful tasks, it is not highly optimized for runtime performance;\\nsuch optimizations often involve more complex algorithms,\\nor implementations in lower-level programming languages such as C or C++.\\nThis would make the software less readable and more difficult to install.\\nThird, we have tried to avoid clever programming tricks,\\nsince we believe that clear implementations are preferable\\nto ingenious yet indecipherable ones.\\n\\n\\nFor Instructors\\nNatural Language Processing is often taught within the\\nconfines of a single-semester course at advanced undergraduate level\\nor postgraduate level.  Many instructors have found that it is\\ndifficult to cover both the theoretical and practical sides of the\\nsubject in such a short span of time.  Some courses focus on theory to\\nthe exclusion of practical exercises, and deprive students of the\\nchallenge and excitement of writing programs to automatically process\\nlanguage.  Other courses are simply designed to teach programming for\\nlinguists, and do not manage to cover any significant NLP content.\\nNLTK was originally developed to address this problem,\\nmaking it feasible to cover a substantial amount of theory and\\npractice within a single-semester course, even if students have no\\nprior programming experience.\\nA significant fraction of any NLP syllabus deals with\\nalgorithms and data structures.  On their own these can be rather\\ndry, but NLTK brings them to life with the help of\\ninteractive graphical user interfaces that make it possible\\nto view algorithms step-by-step.  Most NLTK components include\\na demonstration that performs an interesting task without\\nrequiring any special input from the user.\\nAn effective way to deliver the materials is through interactive\\npresentation of the examples in this book, entering them in a Python session,\\nobserving what they do, and modifying them to explore some empirical\\nor theoretical issue.\\nThis book contains hundreds of exercises that can be used\\nas the basis for student assignments.  The simplest exercises involve\\nmodifying a supplied program fragment in a specified way in order to\\nanswer a concrete question.  At the other end of the spectrum, NLTK\\nprovides a flexible framework for graduate-level research projects,\\nwith standard implementations of all the basic data structures\\nand algorithms, interfaces to dozens of widely used datasets (corpora),\\nand a flexible and extensible architecture.  Additional support for\\nteaching using NLTK is available on the NLTK website.\\n\\nWe believe this book is unique in providing a comprehensive\\nframework for students to learn about NLP in the context of learning\\nto program.  What sets these\\nmaterials apart is the tight coupling of the chapters\\nand exercises with NLTK, giving students — even those with\\nno prior programming experience — a practical introduction to\\nNLP.  After completing these materials, students will be ready to\\nattempt one of the more advanced textbooks, such as Speech and\\nLanguage Processing, by Jurafsky and Martin (Prentice Hall, 2008).\\nThis book presents programming concepts in an unusual order, beginning\\nwith a non-trivial data type — lists of strings — then introducing\\nnon-trivial control structures such as comprehensions and conditionals.\\nThese idioms permit us to do useful language processing from the start.\\nOnce this motivation is in place, we return to a systematic presentation\\nof fundamental concepts such as strings, loops, files, and so forth.\\nIn this way, we cover the same ground as more conventional approaches,\\nwithout expecting readers to be interested in the programming\\nlanguage for its own sake.\\nTwo possible course plans are illustrated in IX.1.  The first\\none presumes an arts/humanities audience, whereas the second one presumes\\na science/engineering audience.  Other course plans could cover the first\\nfive chapters, then devote the remaining amount of time to a single area,\\nsuch as text classification (Chapters 6-7), syntax (Chapters 8-9),\\nsemantics (Chapter 10), or linguistic data management (Chapter 11).\\nTable IX.1: Suggested course plans; approximate number of lectures per chapter\\n\\n\\n\\n\\n\\n\\nChapter\\nArts and Humanities\\nScience and Engineering\\n\\n\\n\\n1  Language Processing and Python\\n2-4\\n2\\n\\n2  Accessing Text Corpora and Lexical Resources\\n2-4\\n2\\n\\n3  Processing Raw Text\\n2-4\\n2\\n\\n4  Writing Structured Programs\\n2-4\\n1-2\\n\\n5  Categorizing and Tagging Words\\n2-4\\n2-4\\n\\n6  Learning to Classify Text\\n0-2\\n2-4\\n\\n7  Extracting Information from Text\\n2\\n2-4\\n\\n8  Analyzing Sentence Structure\\n2-4\\n2-4\\n\\n9  Building Feature Based Grammars\\n2-4\\n1-4\\n\\n10 Analyzing the Meaning of Sentences\\n1-2\\n1-4\\n\\n11 Managing Linguistic Data\\n1-2\\n1-4\\n\\nTotal\\n18-36\\n18-36\\n\\n\\n\\n\\n\\n\\n\\nConventions Used in This Book\\nThe following typographical conventions are used in this book:\\nBold -- Indicates new terms.\\nItalic -- Used within paragraphs to refer to linguistic examples,\\nthe names of texts, and URLs; also used for filenames and file extensions.\\nConstant width -- Used for program listings,\\nas well as within paragraphs to refer to program elements\\nsuch as variable or function names, statements, and keywords;\\nalso used for program names.\\nConstant width bold -- Shows commands or other text that should\\nbe typed literally by the user.\\nConstant width italic -- Shows text that should be\\nreplaced with user-supplied values or by values\\ndetermined by context; also used for metavariables within program code examples.\\n\\nNote\\nThis icon signifies a tip, suggestion, or general note.\\n\\n\\nCaution!\\nThis icon indicates a warning or caution.\\n\\n\\n\\nUsing Code Examples\\nThis book is here to help you get your job done. In general, you may use the code in\\nthis book in your programs and documentation. You do not need to contact us for\\npermission unless youÕre reproducing a significant portion of the code. For example,\\nwriting a program that uses several chunks of code from this book does not require\\npermission. Selling or distributing a CD-ROM of examples from OÕReilly books does\\nrequire permission. Answering a question by citing this book and quoting example\\ncode does not require permission. Incorporating a significant amount of example code\\nfrom this book into your productÕs documentation does require permission.\\nWe appreciate, but do not require, attribution. An attribution usually includes the title,\\nauthor, publisher, and ISBN. For example: ÒNatural Language Processing with Python,\\nby Steven Bird, Ewan Klein, and Edward Loper.  O\\'Reilly Media, 978-0-596-51649-9.Ó\\nIf you feel your use of code examples falls outside fair use or the permission given above,\\nfeel free to contact us at permissions@oreilly.com.\\n\\n\\nAcknowledgments\\nThe authors are indebted to the following people for feedback on\\nearlier drafts of this book:\\nDoug Arnold,\\nMichaela Atterer,\\nGreg Aumann,\\nKenneth Beesley,\\nSteven Bethard,\\nOndrej Bojar,\\nChris Cieri,\\nRobin Cooper,\\nGrev Corbett,\\nJames Curran,\\nDan Garrette,\\nJean Mark Gawron,\\nDoug Hellmann,\\nNitin Indurkhya,\\nMark Liberman,\\nPeter Ljunglöf,\\nStefan Müller,\\nRobin Munn,\\nJoel Nothman,\\nAdam Przepiorkowski,\\nBrandon Rhodes,\\nStuart Robinson,\\nJussi Salmela,\\nKyle Schlansker,\\nRob Speer,\\nand\\nRichard Sproat.\\nWe are thankful to many students and colleagues for their comments on\\nthe class materials that evolved into these chapters,\\nincluding participants at NLP and linguistics summer schools\\nin Brazil, India, and the USA.\\nThis book would not exist without the members of the nltk-dev\\ndeveloper community, named on the NLTK website,\\nwho have given so freely of their time and expertise in building and extending NLTK.\\nWe are grateful to the U.S. National Science Foundation, the Linguistic Data Consortium,\\nan Edward Clarence Dyason Fellowship,\\nand the Universities of Pennsylvania, Edinburgh, and Melbourne for supporting our work\\non this book.\\nWe thank Julie Steele, Abby Fox, Loranah Dimant, and the rest of the O\\'Reilly team, for\\norganizing comprehensive reviews of our drafts from people across the NLP\\nand Python communities, for cheerfully customizing O\\'Reilly\\'s production tools,\\nand for meticulous copy-editing work.\\nIn preparing the revised edition for Python 3, we are grateful to\\nMichael Korobov for leading the effort to port NLTK to Python 3, and to\\nAntoine Trux for his meticulous feedback on the first edition.\\nFinally, we owe a huge debt of gratitude to Mimo and Jee\\nfor their love, patience, and support over the many years that we worked on this book.\\nWe hope that our children — Andrew, Alison, Kirsten, Leonie, and Maaike —\\ncatch our enthusiasm for language and computation from these pages.\\n\\n\\n\\nAbout the Authors\\nSteven Bird is Associate Professor in the\\nDepartment of Computer Science and Software Engineering\\nat the University of Melbourne, and Senior Research Associate in the\\nLinguistic Data Consortium at the University of Pennsylvania.\\nHe completed a PhD on computational phonology at the University of\\nEdinburgh in 1990, supervised by Ewan Klein.\\nHe later moved to Cameroon to conduct linguistic fieldwork on the\\nGrassfields Bantu languages under the auspices of the Summer Institute\\nof Linguistics.  More recently, he spent\\nseveral years as Associate Director of the Linguistic Data Consortium\\nwhere he led an R&amp;D team to create models and tools for large\\ndatabases of annotated text.  At Melbourne University,\\nhe established a language technology research group and has\\ntaught at all levels of the undergraduate computer science curriculum.\\nIn 2009, Steven is President of the Association for Computational Linguistics.\\nEwan Klein is Professor of Language Technology in the School of\\nInformatics at the University of Edinburgh. He completed a PhD on\\nformal semantics at the University of Cambridge in 1978. After some\\nyears working at the Universities of Sussex and Newcastle upon Tyne,\\nEwan took up a teaching position at Edinburgh. He was involved in the\\nestablishment of Edinburgh\\'s Language Technology Group in 1993, and has\\nbeen closely associated with it ever since.  From 2000–2002,\\nhe took leave from the University to act as Research Manager for the\\nEdinburgh-based Natural Language Research Group of Edify Corporation,\\nSanta Clara, and was responsible for spoken dialogue processing.  Ewan\\nis a past President of the European Chapter of the Association for\\nComputational Linguistics and was a founding member and Coordinator of\\nthe European Network of Excellence in Human Language Technologies\\n(ELSNET).\\nEdward Loper has recently completed a PhD\\non machine learning for natural language processing\\nat the the University of Pennsylvania.\\nEdward was a student in Steven\\'s graduate course on\\ncomputational linguistics in the fall of 2000, and\\nwent on to be a TA and share in the development of\\nNLTK.  In addition to NLTK, he has\\nhelped develop two packages for documenting and\\ntesting Python software, epydoc and doctest.\\n\\n\\nRoyalties\\nRoyalties from the sale of this book are being used to support\\nthe development of the Natural Language Toolkit.\\n\\n\\nFigure XIV.1: Edward Loper, Ewan Klein, and Steven Bird, Stanford, July 2007\\n\\n\\n\\n\\nAbout this document...\\nUPDATED FOR NLTK 3.0.\\nThis is a chapter from Natural Language Processing with Python,\\nby Steven Bird, Ewan Klein and Edward Loper,\\nCopyright © 2019 the authors.\\nIt is distributed with the Natural Language Toolkit [http://nltk.org/],\\nVersion 3.0, under the terms of the\\nCreative Commons Attribution-Noncommercial-No Derivative Works 3.0 United States License\\n[http://creativecommons.org/licenses/by-nc-nd/3.0/us/].\\nThis document was built on\\nWed  4 Sep 2019 11:40:48 ACST\\n\\n\\n\\n\\n\\n',\n",
       " '\\n\\n\\n\\nfunction astext(node)\\n{\\n    return node.innerHTML.replace(/(]+)>)/ig,\"\")\\n                         .replace(/&gt;/ig, \">\")\\n                         .replace(/&lt;/ig, \"<\")\\n                         .replace(/&quot;/ig, \\'\"\\')\\n                         .replace(/&amp;/ig, \"&\");\\n}\\n\\nfunction copy_notify(node, bar_color, data)\\n{\\n    // The outer box: relative + inline positioning.\\n    var box1 = document.createElement(\"div\");\\n    box1.style.position = \"relative\";\\n    box1.style.display = \"inline\";\\n    box1.style.top = \"2em\";\\n    box1.style.left = \"1em\";\\n  \\n    // A shadow for fun\\n    var shadow = document.createElement(\"div\");\\n    shadow.style.position = \"absolute\";\\n    shadow.style.left = \"-1.3em\";\\n    shadow.style.top = \"-1.3em\";\\n    shadow.style.background = \"#404040\";\\n    \\n    // The inner box: absolute positioning.\\n    var box2 = document.createElement(\"div\");\\n    box2.style.position = \"relative\";\\n    box2.style.border = \"1px solid #a0a0a0\";\\n    box2.style.left = \"-.2em\";\\n    box2.style.top = \"-.2em\";\\n    box2.style.background = \"white\";\\n    box2.style.padding = \".3em .4em .3em .4em\";\\n    box2.style.fontStyle = \"normal\";\\n    box2.style.background = \"#f0e0e0\";\\n\\n    node.insertBefore(box1, node.childNodes.item(0));\\n    box1.appendChild(shadow);\\n    shadow.appendChild(box2);\\n    box2.innerHTML=\"Copied&nbsp;to&nbsp;the&nbsp;clipboard: \" +\\n                   \"\"+\\n                   data+\"\";\\n    setTimeout(function() { node.removeChild(box1); }, 1000);\\n\\n    var elt = node.parentNode.firstChild;\\n    elt.style.background = \"#ffc0c0\";\\n    setTimeout(function() { elt.style.background = bar_color; }, 200);\\n}\\n\\nfunction copy_codeblock_to_clipboard(node)\\n{\\n    var data = astext(node)+\"\\\\n\";\\n    if (copy_text_to_clipboard(data)) {\\n        copy_notify(node, \"#40a060\", data);\\n    }\\n}\\n\\nfunction copy_doctest_to_clipboard(node)\\n{\\n    var s = astext(node)+\"\\\\n   \";\\n    var data = \"\";\\n\\n    var start = 0;\\n    var end = s.indexOf(\"\\\\n\");\\n    while (end >= 0) {\\n        if (s.substring(start, start+4) == \">>> \") {\\n            data += s.substring(start+4, end+1);\\n        }\\n        else if (s.substring(start, start+4) == \"... \") {\\n            data += s.substring(start+4, end+1);\\n        }\\n        /*\\n        else if (end-start > 1) {\\n            data += \"# \" + s.substring(start, end+1);\\n        }*/\\n        // Grab the next line.\\n        start = end+1;\\n        end = s.indexOf(\"\\\\n\", start);\\n    }\\n    \\n    if (copy_text_to_clipboard(data)) {\\n        copy_notify(node, \"#4060a0\", data);\\n    }\\n}\\n    \\nfunction copy_text_to_clipboard(data)\\n{\\n    if (window.clipboardData) {\\n        window.clipboardData.setData(\"Text\", data);\\n        return true;\\n     }\\n    else if (window.netscape) {\\n        // w/ default firefox settings, permission will be denied for this:\\n        netscape.security.PrivilegeManager\\n                      .enablePrivilege(\"UniversalXPConnect\");\\n    \\n        var clip = Components.classes[\"@mozilla.org/widget/clipboard;1\"]\\n                      .createInstance(Components.interfaces.nsIClipboard);\\n        if (!clip) return;\\n    \\n        var trans = Components.classes[\"@mozilla.org/widget/transferable;1\"]\\n                       .createInstance(Components.interfaces.nsITransferable);\\n        if (!trans) return;\\n    \\n        trans.addDataFlavor(\"text/unicode\");\\n    \\n        var str = new Object();\\n        var len = new Object();\\n    \\n        var str = Components.classes[\"@mozilla.org/supports-string;1\"]\\n                     .createInstance(Components.interfaces.nsISupportsString);\\n        var datacopy=data;\\n        str.data=datacopy;\\n        trans.setTransferData(\"text/unicode\",str,datacopy.length*2);\\n        var clipid=Components.interfaces.nsIClipboard;\\n    \\n        if (!clip) return false;\\n    \\n        clip.setData(trans,null,clipid.kGlobalClipboard);\\n        return true;\\n    }\\n    return false;\\n}\\n//-->\\n\\n\\n\\n\\n\\n\\n1. Language Processing and Python\\n\\n\\n/*\\n:Author: Edward Loper, James Curran\\n:Copyright: This stylesheet has been placed in the public domain.\\n\\nStylesheet for use with Docutils.\\n\\nThis stylesheet defines new css classes used by NLTK.\\n\\nIt uses a Python syntax highlighting scheme that matches\\nthe colour scheme used by IDLE, which makes it easier for\\nbeginners to check they are typing things in correctly.\\n*/\\n\\n/* Include the standard docutils stylesheet. */\\n@import url(default.css);\\n\\n/* Custom inline roles */\\nspan.placeholder    { font-style: italic; font-family: monospace; }\\nspan.example        { font-style: italic; }\\nspan.emphasis       { font-style: italic; }\\nspan.termdef        { font-weight: bold; }\\n/*span.term           { font-style: italic; }*/\\nspan.category       { font-variant: small-caps; }\\nspan.feature        { font-variant: small-caps; }\\nspan.fval           { font-style: italic; }\\nspan.math           { font-style: italic; }\\nspan.mathit         { font-style: italic; }\\nspan.lex            { font-variant: small-caps; }\\nspan.guide-linecount{ text-align: right; display: block;}\\n\\n/* Python souce code listings */\\nspan.pysrc-prompt   { color: #9b0000; }\\nspan.pysrc-more     { color: #9b00ff; }\\nspan.pysrc-keyword  { color: #e06000; }\\nspan.pysrc-builtin  { color: #940094; }\\nspan.pysrc-string   { color: #00aa00; }\\nspan.pysrc-comment  { color: #ff0000; }\\nspan.pysrc-output   { color: #0000ff; }\\nspan.pysrc-except   { color: #ff0000; }\\nspan.pysrc-defname  { color: #008080; }\\n\\n\\n/* Doctest blocks */\\npre.doctest         { margin: 0; padding: 0; font-weight: bold; }\\ndiv.doctest         { margin: 0 1em 1em 1em; padding: 0; }\\ntable.doctest       { margin: 0; padding: 0;\\n                      border-top: 1px solid gray;\\n                      border-bottom: 1px solid gray; }\\npre.copy-notify     { margin: 0; padding: 0.2em; font-weight: bold;\\n                      background-color: #ffffff; }\\n\\n/* Python source listings */\\ndiv.pylisting       { margin: 0 1em 1em 1em; padding: 0; }\\ntable.pylisting     { margin: 0; padding: 0;\\n                      border-top: 1px solid gray; }\\ntd.caption { border-top: 1px solid black; margin: 0; padding: 0; }\\n.caption-label { font-weight: bold;  }\\ntd.caption p { margin: 0; padding: 0; font-style: normal;}\\n\\ntable tr td.codeblock { \\n  padding: 0.2em ! important; margin: 0;\\n  border-left: 1px solid gray;\\n  border-right: 2px solid gray;\\n  border-top: 0px solid gray;\\n  border-bottom: 1px solid gray;\\n  font-weight: bold; background-color: #eeffee;\\n}\\ntable pre span {\\n  white-space: pre-wrap;\\n}\\ntable tr td.doctest  { \\n  padding: 0.2em; margin: 0;\\n  border-left: 1px solid gray;\\n  border-right: 2px solid gray;\\n  border-top: 0px solid gray;\\n  border-bottom: 1px solid gray;\\n  font-weight: bold; background-color: #eeeeff;\\n}\\n\\ntd.codeblock table tr td.copybar {\\n    background: #40a060; border: 1px solid gray;\\n    font-family: monospace; padding: 0; margin: 0; }\\ntd.doctest table tr td.copybar {\\n    background: #4060a0; border: 1px solid gray;\\n    font-family: monospace; padding: 0; margin: 0; }\\n\\ntd.pysrc { padding-left: 0.5em; }\\n\\nimg.callout { border-width: 0px; }\\n\\ntable.docutils {\\n    border-style: solid;\\n    border-width: 1px;\\n    margin-top: 6px;\\n    border-color: grey;\\n    border-collapse: collapse; }\\n\\ntable.docutils th {\\n    border-style: none;\\n    border-width: 1px;\\n    border-color: grey;\\n    padding: 0 .5em 0 .5em; }\\n\\ntable.docutils td {\\n    border-style: none;\\n    border-width: 1px;\\n    border-color: grey; \\n    padding: 0 .5em 0 .5em; }\\n\\ntable.footnote td { padding: 0; }\\ntable.footnote { border-width: 0; }\\ntable.footnote td { border-width: 0; }\\ntable.footnote th { border-width: 0; }\\n\\ntable.noborder { border-width: 0; }\\n\\ntable.example pre { margin-top: 4px; margin-bottom: 0; }\\n\\n/* For figures & tables */\\np.caption { margin-bottom: 0; }\\ndiv.figure { text-align: center; }\\n\\n/* The index */\\ndiv.index { border: 1px solid black;\\n            background-color: #eeeeee; }\\ndiv.index h1 { padding-left: 0.5em; margin-top: 0.5ex;\\n               border-bottom: 1px solid black; }\\nul.index { margin-left: 0.5em; padding-left: 0; }\\nli.index { list-style-type: none; }\\np.index-heading { font-size: 120%; font-style: italic; margin: 0; }\\nli.index ul { margin-left: 2em; padding-left: 0; }\\n\\n/* \\'Note\\' callouts */\\ndiv.note\\n{\\n  border-right:   #87ceeb 1px solid;\\n  padding-right: 4px;\\n  border-top: #87ceeb 1px solid;\\n  padding-left: 4px;\\n  padding-bottom: 4px;\\n  margin: 2px 5% 10px;\\n  border-left: #87ceeb 1px solid;\\n  padding-top: 4px;\\n  border-bottom: #87ceeb 1px solid;\\n  font-style: normal;\\n  font-family: verdana, arial;\\n  background-color: #b0c4de;\\n}\\n\\ntable.avm { border: 0px solid black; width: 0; }\\ntable.avm tbody tr {border: 0px solid black; }\\ntable.avm tbody tr td { padding: 2px; }\\ntable.avm tbody tr td.avm-key { padding: 5px; font-variant: small-caps; }\\ntable.avm tbody tr td.avm-eq { padding: 5px; }\\ntable.avm tbody tr td.avm-val { padding: 5px; font-style: italic; }\\np.avm-empty { font-style: normal; }\\ntable.avm colgroup col { border: 0px solid black; }\\ntable.avm tbody tr td.avm-topleft \\n    { border-left: 2px solid #000080; border-top: 2px solid #000080; }\\ntable.avm tbody tr td.avm-botleft \\n    { border-left: 2px solid #000080; border-bottom: 2px solid #000080; }\\ntable.avm tbody tr td.avm-topright\\n    { border-right: 2px solid #000080; border-top: 2px solid #000080; }\\ntable.avm tbody tr td.avm-botright\\n    { border-right: 2px solid #000080; border-bottom: 2px solid #000080; }\\ntable.avm tbody tr td.avm-left\\n    { border-left: 2px solid #000080; }\\ntable.avm tbody tr td.avm-right\\n    { border-right: 2px solid #000080; }\\ntable.avm tbody tr td.avm-topbotleft\\n    { border: 2px solid #000080; border-right: 0px solid black; }\\ntable.avm tbody tr td.avm-topbotright\\n    { border: 2px solid #000080; border-left: 0px solid black; }\\ntable.avm tbody tr td.avm-ident\\n    { font-size: 80%; padding: 0; padding-left: 2px; vertical-align: top; }\\n.avm-pointer\\n{ border: 1px solid #008000; padding: 1px; color: #008000; \\n  background: #c0ffc0; font-style: normal; }\\n\\ntable.gloss { border: 0px solid black; width: 0; }\\ntable.gloss tbody tr { border: 0px solid black; }\\ntable.gloss tbody tr td { border: 0px solid black; }\\ntable.gloss colgroup col { border: 0px solid black; }\\ntable.gloss p { margin: 0; padding: 0; }\\n\\ntable.rst-example { border: 1px solid black; }\\ntable.rst-example tbody tr td { background: #eeeeee; }\\ntable.rst-example thead tr th { background: #c0ffff; }\\ntd.rst-raw { width: 0; }\\n\\n/* Used by nltk.org/doc/test: */\\ndiv.doctest-list { text-align: center; }\\ntable.doctest-list { border: 1px solid black;\\n  margin-left: auto; margin-right: auto;\\n}\\ntable.doctest-list tbody tr td { background: #eeeeee;\\n  border: 1px solid #cccccc; text-align: left; }\\ntable.doctest-list thead tr th { background: #304050; color: #ffffff;\\n  border: 1px solid #000000;}\\ntable.doctest-list thead tr a { color: #ffffff; }\\nspan.doctest-passed { color: #008000; }\\nspan.doctest-failed { color: #800000; }\\n\\n\\n\\n\\n\\n\\n1. Language Processing and Python\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n<!-- Grammatical Category - e.g. NP and verb as technical terms\\n.. role:: gc\\n   :class: category -->\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n<!-- TODO: adopt simpler hacker example with only single character transpositions;\\nmove hacker example to later section (later chapter?) -->\\n\\n\\nIt is easy to get our hands on millions of words of text.\\nWhat can we do with it, assuming we can write some simple programs?\\nIn this chapter we\\'ll address the following questions:\\n\\nWhat can we achieve by combining simple programming techniques with large quantities of text?\\nHow can we automatically extract key words and phrases that sum up the style and content of a text?\\nWhat tools and techniques does the Python programming language provide for such work?\\nWhat are some of the interesting challenges of natural language processing?\\n\\nThis chapter is divided into sections that skip between two quite\\ndifferent styles.  In the \"computing with language\" sections we will\\ntake on some linguistically motivated programming tasks without necessarily\\nexplaining how they work.  In the \"closer look at Python\" sections we\\nwill systematically review key programming concepts.  We\\'ll flag the two styles in the section titles,\\nbut later chapters will mix both styles without being so up-front about it.\\nWe hope this style of introduction gives you an\\nauthentic taste of what will come later, while covering a range of\\nelementary concepts in linguistics and computer science.\\nIf you have basic familiarity with both areas, you can skip to\\n5;\\nwe will repeat any important points in later chapters, and if you miss anything\\nyou can easily consult the online reference material at http://nltk.org/.\\nIf the material is completely new to you, this chapter will raise\\nmore questions than it answers, questions that are addressed in\\nthe rest of this book.\\n\\n1&nbsp;&nbsp;&nbsp;Computing with Language: Texts and Words\\nWe\\'re all very familiar with text, since we read and write it every day.\\nHere we will treat text as raw data for the programs we write,\\nprograms that manipulate and analyze it in a variety of interesting ways.\\nBut before we can do this, we have to get started with the Python interpreter.\\n\\n1.1&nbsp;&nbsp;&nbsp;Getting Started with Python\\nOne of the friendly things about Python is that it allows you\\nto type directly into the interactive interpreter —\\nthe program that will be running your Python programs.\\nYou can access the Python interpreter using a simple graphical interface\\ncalled the Interactive DeveLopment Environment (IDLE).\\nOn a Mac you can find this under Applications→MacPython,\\nand on Windows under All Programs→Python.\\nUnder Unix you can run Python from the shell by typing idle\\n(if this is not installed, try typing python).\\nThe interpreter will print a blurb about your Python version;\\nsimply check that you are running Python 3.2 or later\\n(here it is for 3.4.2):\\n\\n\\n\\n\\n&nbsp;\\nPython 3.4.2 (default, Oct 15 2014, 22:01:37)\\n[GCC 4.2.1 Compatible Apple LLVM 5.1 (clang-503.0.40)] on darwin\\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\\n&gt;&gt;&gt;\\n\\n\\n\\n\\nNote\\nIf you are unable to run the Python interpreter, you probably don\\'t\\nhave Python installed correctly.  Please visit http://python.org/ for\\ndetailed instructions. NLTK 3.0 works for Python 2.6 and\\n2.7. If you are using one of these older versions, note that\\nthe / operator rounds\\nfractional results downwards (so 1/3 will give you 0).\\nIn order to get the expected behavior of division\\nyou need to type: from __future__ import division\\n\\nThe &gt;&gt;&gt; prompt indicates that the Python interpreter is now waiting\\nfor input.  When copying examples from this book, don\\'t type\\nthe \"&gt;&gt;&gt;\" yourself.  Now, let\\'s begin by using Python as a calculator:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; 1 + 5 * 2 - 3\\n8\\n&gt;&gt;&gt;\\n\\n\\n\\nOnce the interpreter has finished calculating the answer and displaying it, the\\nprompt reappears. This means the Python interpreter is waiting for another instruction.\\n\\nNote\\nYour Turn:\\nEnter a few more expressions of your own. You can use asterisk (*)\\nfor multiplication and slash (/) for division, and parentheses for\\nbracketing expressions.\\n\\n<!-- XXX The following example currently wraps over a page boundary, which\\nmakes it difficult to read, esp since you can\\'t see where the \"^\" is\\npointing. -->\\nThe preceding examples demonstrate how you can work interactively with the\\nPython interpreter, experimenting with various expressions in the language\\nto see what they do.\\nNow let\\'s try a nonsensical expression to see how the interpreter handles it:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; 1 +\\n  File \"&lt;stdin&gt;\", line 1\\n    1 +\\n      ^\\nSyntaxError: invalid syntax\\n&gt;&gt;&gt;\\n\\n\\n\\nThis produced a syntax error.  In Python, it doesn\\'t make sense\\nto end an instruction with a plus sign. The Python interpreter\\nindicates the line where the problem occurred (line 1 of &lt;stdin&gt;,\\nwhich stands for \"standard input\").\\nNow that we can use the Python interpreter, we\\'re ready to start working\\nwith language data.\\n\\n\\n1.2&nbsp;&nbsp;&nbsp;Getting Started with NLTK\\nBefore going further you should install NLTK 3.0, downloadable for free from http://nltk.org/.\\nFollow the instructions there to download the version required for your platform.\\nOnce you\\'ve installed NLTK, start up the Python interpreter as\\nbefore, and install the data required for the book by\\ntyping the following two commands at the Python prompt, then selecting\\nthe book collection as shown in 1.1.\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; import nltk\\n&gt;&gt;&gt; nltk.download()\\n\\n\\n\\n\\n\\nFigure 1.1: Downloading the NLTK Book Collection: browse the available packages\\nusing nltk.download().  The Collections tab on the downloader\\nshows how the packages are grouped into sets, and you should select the line labeled\\nbook to obtain all\\ndata required for the examples and exercises in this book.  It consists\\nof about 30 compressed files requiring about 100Mb disk space.\\nThe full collection of data (i.e., all in the downloader) is\\nnearly ten times this size (at the time of writing) and continues to expand.\\n\\nOnce the data is downloaded to your machine, you can load some of it\\nusing the Python interpreter.\\nThe first step is to type a special command at the\\nPython prompt which tells the interpreter to load some texts for us to\\nexplore: from nltk.book import *.\\nThis says \"from NLTK\\'s book module, load\\nall items.\"  The book module contains all the data you will need\\nas you read this chapter.  After printing a welcome message, it loads\\nthe text of several books (this will take a few seconds).  Here\\'s the\\ncommand again, together with the output that\\nyou will see.  Take care to get spelling and punctuation right, and\\nremember that you don\\'t type the &gt;&gt;&gt;.\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; from nltk.book import *\\n*** Introductory Examples for the NLTK Book ***\\nLoading text1, ..., text9 and sent1, ..., sent9\\nType the name of the text or sentence to view it.\\nType: \\'texts()\\' or \\'sents()\\' to list the materials.\\ntext1: Moby Dick by Herman Melville 1851\\ntext2: Sense and Sensibility by Jane Austen 1811\\ntext3: The Book of Genesis\\ntext4: Inaugural Address Corpus\\ntext5: Chat Corpus\\ntext6: Monty Python and the Holy Grail\\ntext7: Wall Street Journal\\ntext8: Personals Corpus\\ntext9: The Man Who Was Thursday by G . K . Chesterton 1908\\n&gt;&gt;&gt;\\n\\n\\n\\nAny time we want to find out about these texts, we just have\\nto enter their names at the Python prompt:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; text1\\n&lt;Text: Moby Dick by Herman Melville 1851&gt;\\n&gt;&gt;&gt; text2\\n&lt;Text: Sense and Sensibility by Jane Austen 1811&gt;\\n&gt;&gt;&gt;\\n\\n\\n\\nNow that we can use the Python interpreter, and have some data to work with,\\nwe\\'re ready to get started.\\n\\n\\n1.3&nbsp;&nbsp;&nbsp;Searching Text\\nThere are many ways to examine the context of a text apart from\\nsimply reading it.  A concordance view shows us every occurrence of a given word, together\\nwith some context.  Here we look up the word monstrous in Moby\\nDick by entering text1 followed by a period, then the term\\nconcordance, and then placing \"monstrous\" in parentheses:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; text1.concordance(\"monstrous\")\\nDisplaying 11 of 11 matches:\\nong the former , one was of a most monstrous size . ... This came towards us ,\\nON OF THE PSALMS . \" Touching that monstrous bulk of the whale or ork we have r\\nll over with a heathenish array of monstrous clubs and spears . Some were thick\\nd as you gazed , and wondered what monstrous cannibal and savage could ever hav\\nthat has survived the flood ; most monstrous and most mountainous ! That Himmal\\nthey might scout at Moby Dick as a monstrous fable , or still worse and more de\\nth of Radney .\\'\" CHAPTER 55 Of the monstrous Pictures of Whales . I shall ere l\\ning Scenes . In connexion with the monstrous pictures of whales , I am strongly\\nere to enter upon those still more monstrous stories of them which are to be fo\\nght have been rummaged out of this monstrous cabinet there is no telling . But\\nof Whale - Bones ; for Whales of a monstrous size are oftentimes cast up dead u\\n&gt;&gt;&gt;\\n\\n\\n\\nThe first time you use a concordance on a particular text, it takes a\\nfew extra seconds to build an index so that subsequent searches are fast.\\n\\nNote\\nYour Turn:\\nTry searching for other words; to save re-typing, you might be able to\\nuse up-arrow, Ctrl-up-arrow or Alt-p to access the previous command and modify the word being searched.\\nYou can also try searches on some of the other texts we have included.\\nFor example, search Sense and Sensibility for the word\\naffection, using text2.concordance(\"affection\").  Search the book of Genesis\\nto find out how long some people lived, using\\ntext3.concordance(\"lived\").  You could look at text4, the\\nInaugural Address Corpus, to see examples of English going\\nback to 1789, and search for words like nation, terror, god\\nto see how these words have been used differently over time.\\nWe\\'ve also included text5, the NPS Chat Corpus: search this for\\nunconventional words like im, ur, lol.\\n(Note that this corpus is uncensored!)\\n\\nOnce you\\'ve spent a little while examining these texts, we hope you have a new\\nsense of the richness and diversity of language.  In the next chapter\\nyou will learn how to access a broader range of text, including text in\\nlanguages other than English.\\nA concordance permits us to see words in context.  For example, we saw that\\nmonstrous occurred in contexts such as the ___ pictures\\nand a ___ size .  What other words appear in a similar range\\nof contexts?  We can find out\\nby appending the term similar to the name of the text in\\nquestion, then inserting the relevant word in parentheses:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; text1.similar(\"monstrous\")\\nmean part maddens doleful gamesome subtly uncommon careful untoward\\nexasperate loving passing mouldy christian few true mystifying\\nimperial modifies contemptible\\n&gt;&gt;&gt; text2.similar(\"monstrous\")\\nvery heartily so exceedingly remarkably as vast a great amazingly\\nextremely good sweet\\n&gt;&gt;&gt;\\n\\n\\n\\nObserve that we get different results for different texts.\\nAusten uses this word quite differently from Melville; for her, monstrous has\\npositive connotations, and sometimes functions as an intensifier like the word\\nvery.\\nThe term common_contexts allows us to examine just the\\ncontexts that are shared by two or more words, such as monstrous\\nand very. We have to enclose these words by square brackets as\\nwell as parentheses, and separate them with a comma:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; text2.common_contexts([\"monstrous\", \"very\"])\\na_pretty is_pretty am_glad be_glad a_lucky\\n&gt;&gt;&gt;\\n\\n\\n\\n\\nNote\\nYour Turn:\\nPick another pair of words and compare their usage in two different texts, using\\nthe similar() and common_contexts() functions.\\n\\nIt is one thing to automatically detect that a particular word occurs in a text,\\nand to display some words that appear in the same context.  However, we can also determine\\nthe location of a word in the text: how many words from the beginning it appears.\\nThis positional information can be displayed using a dispersion plot.\\nEach stripe represents an instance\\nof a word, and each row represents the entire text.  In 1.2 we\\nsee some striking patterns of word usage over the last 220 years\\n(in an artificial text constructed by joining\\nthe texts of the Inaugural Address Corpus end-to-end).\\nYou can produce this plot as shown below.\\nYou might like to try more words (e.g., liberty, constitution),\\nand different texts.  Can you predict the\\ndispersion of a word before you view it?  As before, take\\ncare to get the quotes, commas, brackets and parentheses exactly right.\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; text4.dispersion_plot([\"citizens\", \"democracy\", \"freedom\", \"duties\", \"America\"])\\n&gt;&gt;&gt;\\n\\n\\n\\n\\n\\nFigure 1.2: Lexical Dispersion Plot for Words in U.S. Presidential Inaugural Addresses:\\nThis can be used to investigate changes in language use over time.\\n\\n\\nNote\\nImportant:\\nYou need to have Python\\'s NumPy and Matplotlib packages installed\\nin order to produce the graphical plots used in this book.\\nPlease see http://nltk.org/ for installation instructions.\\n\\n\\nNote\\nYou can also plot the frequency of word usage through time using\\nhttps://books.google.com/ngrams\\n\\nNow, just for fun, let\\'s try generating some random text in the various\\nstyles we have just seen.  To do this, we type the name of the text\\nfollowed by the term generate. (We need to include the\\nparentheses, but there\\'s nothing that goes between them.)\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; text3.generate()\\nIn the beginning of his brother is a hairy man , whose top may reach\\nunto heaven ; and ye shall sow the land of Egypt there was no bread in\\nall that he was taken out of the month , upon the earth . So shall thy\\nwages be ? And they made their father ; and Isaac was old , and kissed\\nhim : and Laban with his cattle in the midst of the hands of Esau thy\\nfirst born , and Phichol the chief butler unto his son Isaac , she\\n&gt;&gt;&gt;\\n\\n\\n\\n\\nNote\\nThe generate() method is not available in NLTK 3.0 but will be\\nreinstated in a subsequent version.\\n\\n<!-- Note that the first time you run this command, it is slow because it gathers statistics\\nabout word sequences.  Each time you run it, you will get different output text.\\nNow try generating random text in the style of an inaugural address or an\\nInternet chat room.  Although the text is random, it re-uses common words and\\nphrases from the source text and gives us a sense of its style and content.\\n(What is lacking in this randomly generated text?) -->\\n<!-- note\\nWhen ``generate`` produces its output, punctuation is split off\\nfrom the preceding word.  While this is not correct formatting\\nfor English text, we do it to make clear that words and\\npunctuation are independent of one another. You will learn\\nmore about this in chap-words_. -->\\n\\n\\n1.4&nbsp;&nbsp;&nbsp;Counting Vocabulary\\nThe most obvious fact about texts that emerges from the preceding examples is that\\nthey differ in the vocabulary they use.  In this section we will see how to use the\\ncomputer to count the words in a text in a variety of useful ways.\\nAs before, you will jump right in and experiment with\\nthe Python interpreter, even though you may not have studied Python systematically\\nyet.  Test your understanding by modifying the examples, and trying the\\nexercises at the end of the chapter.\\nLet\\'s begin by finding out the length of a text from start to finish,\\nin terms of the words and punctuation symbols that appear.  We use the\\nterm len to get the length of something, which we\\'ll apply here to the\\nbook of Genesis:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; len(text3)\\n44764\\n&gt;&gt;&gt;\\n\\n\\n\\nSo Genesis has 44,764 words and punctuation symbols, or \"tokens.\"\\nA token is the technical name for a sequence of characters\\n— such as hairy, his, or :) — that we want to treat as a\\ngroup. When we count the number of tokens in a text, say, the phrase\\nto be or not to be, we are counting occurrences of these\\nsequences. Thus, in our example phrase there are two occurrences of to,\\ntwo of be, and one each of or and not. But there are\\nonly four distinct vocabulary items in this phrase.\\nHow many distinct words does the book of Genesis contain?\\nTo work this out in Python, we have to pose the question slightly\\ndifferently.  The vocabulary of a text is just the set of tokens\\nthat it uses, since in a set, all duplicates are collapsed\\ntogether. In Python we can obtain the vocabulary items of text3 with the\\ncommand: set(text3).  When you do this, many screens of words will\\nfly past.  Now try the following:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; sorted(set(text3)) \\n[\\'!\\', \"\\'\", \\'(\\', \\')\\', \\',\\', \\',)\\', \\'.\\', \\'.)\\', \\':\\', \\';\\', \\';)\\', \\'?\\', \\'?)\\',\\n\\'A\\', \\'Abel\\', \\'Abelmizraim\\', \\'Abidah\\', \\'Abide\\', \\'Abimael\\', \\'Abimelech\\',\\n\\'Abr\\', \\'Abrah\\', \\'Abraham\\', \\'Abram\\', \\'Accad\\', \\'Achbor\\', \\'Adah\\', ...]\\n&gt;&gt;&gt; len(set(text3)) \\n2789\\n&gt;&gt;&gt;\\n\\n\\n\\nBy wrapping sorted() around the Python expression set(text3)\\n,  we obtain a sorted list of vocabulary items, beginning\\nwith various punctuation symbols and continuing with words starting with A.  All\\ncapitalized words precede lowercase words.\\nWe discover the size of the vocabulary indirectly, by asking\\nfor the number of items in the set, and again we can use len to\\nobtain this number .  Although it has 44,764 tokens, this book\\nhas only 2,789 distinct words, or \"word types.\"\\nA word type is the form or spelling of the word independently of its\\nspecific occurrences in a text — that is, the\\nword considered as a unique item of vocabulary.  Our count of 2,789 items\\nwill include punctuation symbols, so we will generally call these\\nunique items types instead of word types.\\nNow, let\\'s calculate a measure of the lexical\\nrichness of the text.  The next example shows us that the number of\\ndistinct words is just 6% of the total number of words, or equivalently\\nthat each word is used 16 times on average\\n(remember if you\\'re using Python 2, to start with from __future__ import division).\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; len(set(text3)) / len(text3)\\n0.06230453042623537\\n&gt;&gt;&gt;\\n\\n\\n\\nNext, let\\'s focus on particular words.  We can count how often a word occurs\\nin a text, and compute what percentage of the text is taken up by a specific word:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; text3.count(\"smote\")\\n5\\n&gt;&gt;&gt; 100 * text4.count(\\'a\\') / len(text4)\\n1.4643016433938312\\n&gt;&gt;&gt;\\n\\n\\n\\n\\nNote\\nYour Turn:\\nHow many times does the word lol appear in text5?\\nHow much is this as a percentage of the total number of words\\nin this text?\\n\\nYou may want to repeat such calculations on several texts,\\nbut it is tedious to keep retyping the formula.  Instead,\\nyou can come up with your own name for a task, like\\n\"lexical_diversity\" or \"percentage\", and associate it with a block of code.\\nNow you only have to type a short\\nname instead of one or more complete lines of Python code, and\\nyou can re-use it as often as you like. The block of code that does a\\ntask for us is called a function, and\\nwe define a short name for our function with the keyword def. The\\nnext example shows how to define two new functions,\\nlexical_diversity() and   percentage():\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; def lexical_diversity(text): \\n...     return len(set(text)) / len(text) \\n...\\n&gt;&gt;&gt; def percentage(count, total): \\n...     return 100 * count / total\\n...\\n\\n\\n\\n\\nCaution!\\nThe Python interpreter changes the prompt from\\n&gt;&gt;&gt; to ... after encountering the colon at the\\nend of the first line.  The ... prompt indicates\\nthat Python expects an indented code block to appear next.\\nIt is up to you to do the indentation, by typing four\\nspaces or hitting the tab key.  To finish the indented block just\\nenter a blank line.\\n\\nIn the definition of lexical_diversity() , we\\nspecify a parameter named text . This parameter is\\na \"placeholder\" for the actual text whose lexical diversity we want to\\ncompute, and reoccurs in the block of code that will run when the\\nfunction is used . Similarly, percentage() is defined to\\ntake two parameters, named count and total .\\nOnce Python knows that lexical_diversity() and percentage()\\nare the names for specific blocks\\nof code, we can go ahead and use these functions:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; lexical_diversity(text3)\\n0.06230453042623537\\n&gt;&gt;&gt; lexical_diversity(text5)\\n0.13477005109975562\\n&gt;&gt;&gt; percentage(4, 5)\\n80.0\\n&gt;&gt;&gt; percentage(text4.count(\\'a\\'), len(text4))\\n1.4643016433938312\\n&gt;&gt;&gt;\\n\\n\\n\\nTo recap, we use or call a function such as lexical_diversity() by typing its name, followed\\nby an open parenthesis, the name of the text, and then a close\\nparenthesis. These parentheses will show up often; their role is to separate\\nthe name of a task — such as lexical_diversity() — from the data\\nthat the task is to be performed on — such as text3.\\nThe data value that we place in the parentheses when we call a\\nfunction is an argument to the function.\\nYou have already encountered several functions in this chapter, such\\nas len(), set(), and sorted(). By convention, we will\\nalways add an empty pair of parentheses after a function name, as in\\nlen(), just to make clear that what we are talking about is a\\nfunction rather than some other kind of Python expression.\\nFunctions are an important concept in programming, and we only\\nmention them at the outset to give newcomers a sense of the\\npower and creativity of programming.  Don\\'t worry if you find it a bit\\nconfusing right now.\\nLater we\\'ll see how to use functions when tabulating data, as in 1.1.\\nEach row of the table will involve the same computation but\\nwith different data, and we\\'ll do this repetitive work using a function.\\nTable 1.1: Lexical Diversity of Various Genres in the Brown Corpus\\n\\n\\n\\n\\n\\n\\n\\nGenre\\nTokens\\nTypes\\nLexical diversity\\n\\n\\n\\nskill and hobbies\\n82345\\n11935\\n0.145\\n\\nhumor\\n21695\\n5017\\n0.231\\n\\nfiction: science\\n14470\\n3233\\n0.223\\n\\npress: reportage\\n100554\\n14394\\n0.143\\n\\nfiction: romance\\n70022\\n8452\\n0.121\\n\\nreligion\\n39399\\n6373\\n0.162\\n\\n\\n\\n\\n\\n\\n\\n\\n2&nbsp;&nbsp;&nbsp;A Closer Look at Python: Texts as Lists of Words\\n<!-- reimport\\n\\n>>> from nltk.book import *\\n>>> def lexical_diversity(text):\\n...     return len(text) / len(set(text)) -->\\nYou\\'ve seen some important elements of the Python programming language.\\nLet\\'s take a few moments to review them systematically.\\n\\n2.1&nbsp;&nbsp;&nbsp;Lists\\n<!-- XXX it\\'s a little confusing that we assign a value to sent1 here,\\nwhen it\\'s already received on from the \"from nltk.book import *\"\\nstatement.  Granted it\\'s the same value, but still... -->\\nWhat is a text?  At one level, it is a sequence of symbols on a page such\\nas this one.  At another level, it is a sequence of chapters, made up\\nof a sequence of sections, where each section is a sequence of paragraphs,\\nand so on.  However, for our purposes, we will think of a text as nothing\\nmore than a sequence of words and punctuation.  Here\\'s how we represent\\ntext in Python, in this case the opening sentence of Moby Dick:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; sent1 = [\\'Call\\', \\'me\\', \\'Ishmael\\', \\'.\\']\\n&gt;&gt;&gt;\\n\\n\\n\\nAfter the prompt we\\'ve given a name we made up, sent1, followed\\nby the equals sign, and then some quoted words, separated with\\ncommas, and surrounded with brackets.  This bracketed material\\nis known as a list in Python: it is how we store a text.\\nWe can inspect it by typing the name . We can ask for its length .\\nWe can even apply our own lexical_diversity() function to it .\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; sent1 \\n[\\'Call\\', \\'me\\', \\'Ishmael\\', \\'.\\']\\n&gt;&gt;&gt; len(sent1) \\n4\\n&gt;&gt;&gt; lexical_diversity(sent1) \\n1.0\\n&gt;&gt;&gt;\\n\\n\\n\\nSome more lists have been defined for you,\\none for the opening sentence of each of our texts,\\nsent2 … sent9.  We inspect two of them\\nhere; you can see the rest for yourself using the Python interpreter\\n(if you get an error which says that sent2 is not defined, you\\nneed to first type from nltk.book import *).\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; sent2\\n[\\'The\\', \\'family\\', \\'of\\', \\'Dashwood\\', \\'had\\', \\'long\\',\\n\\'been\\', \\'settled\\', \\'in\\', \\'Sussex\\', \\'.\\']\\n&gt;&gt;&gt; sent3\\n[\\'In\\', \\'the\\', \\'beginning\\', \\'God\\', \\'created\\', \\'the\\',\\n\\'heaven\\', \\'and\\', \\'the\\', \\'earth\\', \\'.\\']\\n&gt;&gt;&gt;\\n\\n\\n\\n\\nNote\\nYour Turn:\\nMake up a few sentences of your own, by typing a name, equals\\nsign, and a list of words, like this:\\nex1 = [\\'Monty\\', \\'Python\\', \\'and\\', \\'the\\', \\'Holy\\', \\'Grail\\'].\\nRepeat some of the other Python operations we saw earlier in\\n1,\\ne.g., sorted(ex1), len(set(ex1)), ex1.count(\\'the\\').\\n\\nA pleasant surprise is that we can use Python\\'s addition operator on lists.\\nAdding two lists  creates a new list\\nwith everything from the first list, followed\\nby everything from the second list:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; [\\'Monty\\', \\'Python\\'] + [\\'and\\', \\'the\\', \\'Holy\\', \\'Grail\\'] \\n[\\'Monty\\', \\'Python\\', \\'and\\', \\'the\\', \\'Holy\\', \\'Grail\\']\\n&gt;&gt;&gt;\\n\\n\\n\\n\\nNote\\nThis special use of the addition operation is called concatenation;\\nit combines the lists together into a single list.  We can concatenate\\nsentences to build up a text.\\n\\nWe don\\'t have to literally type the lists either; we can use short\\nnames that refer to pre-defined lists.\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; sent4 + sent1\\n[\\'Fellow\\', \\'-\\', \\'Citizens\\', \\'of\\', \\'the\\', \\'Senate\\', \\'and\\', \\'of\\', \\'the\\',\\n\\'House\\', \\'of\\', \\'Representatives\\', \\':\\', \\'Call\\', \\'me\\', \\'Ishmael\\', \\'.\\']\\n&gt;&gt;&gt;\\n\\n\\n\\nWhat if we want to add a single item to a list? This is known as appending.\\nWhen we append() to a list, the list itself is updated as a result\\nof the operation.\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; sent1.append(\"Some\")\\n&gt;&gt;&gt; sent1\\n[\\'Call\\', \\'me\\', \\'Ishmael\\', \\'.\\', \\'Some\\']\\n&gt;&gt;&gt;\\n\\n\\n\\n\\n\\n2.2&nbsp;&nbsp;&nbsp;Indexing Lists\\n<!-- XXX I think a picture would be very helpful for this section, namely\\none showing something like:\\n    | Call   | me     | Ishmael | .     |\\n    0        1        2         3       4\\nThis might obviate the need to use a contrived sentence \"word1\\nword2 etc\".  I find this picture especially useful for understanding\\nslicing, but it also gives a reasonable motivation for zero-indexing. -->\\n<!-- XXX If we end up doing this, can we offset the integers slightly so\\nthat they (just) fall inside the corresponding cell?\\n\\n    +- - - - - - - -+- - - - - - - -+- - - - - - - -+- - - - - - - -+\\n    | Call   | me     | Ishmael| .      |\\n    |0       |1       |2       |3       |4\\n    +- - - - - - - -+- - - - - - - -+- - - - - - - -+- - - - - - - -+ -->\\nAs we have seen, a text in Python is a list of words, represented\\nusing a combination of brackets and quotes.  Just as with an ordinary\\npage of text, we can count up the total number of words in text1\\nwith len(text1), and count the occurrences in a text of a\\nparticular word — say, \\'heaven\\' — using text1.count(\\'heaven\\').\\nWith some patience, we can pick out the 1st, 173rd, or even 14,278th\\nword in a printed text. Analogously, we can identify the elements of a\\nPython list by their order of occurrence in the list. The number that\\nrepresents this position is the item\\'s index.  We instruct Python\\nto show us the item that occurs at an index such as 173 in a text\\nby writing the name of the text followed by the index inside square brackets:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; text4[173]\\n\\'awaken\\'\\n&gt;&gt;&gt;\\n\\n\\n\\nWe can do the converse; given a word, find the index of when it first\\noccurs:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; text4.index(\\'awaken\\')\\n173\\n&gt;&gt;&gt;\\n\\n\\n\\nIndexes are a common way to access the words of a text,\\nor, more generally, the elements of any list.\\nPython permits us to access sublists as well, extracting\\nmanageable pieces of language from large texts, a technique\\nknown as slicing.\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; text5[16715:16735]\\n[\\'U86\\', \\'thats\\', \\'why\\', \\'something\\', \\'like\\', \\'gamefly\\', \\'is\\', \\'so\\', \\'good\\',\\n\\'because\\', \\'you\\', \\'can\\', \\'actually\\', \\'play\\', \\'a\\', \\'full\\', \\'game\\', \\'without\\',\\n\\'buying\\', \\'it\\']\\n&gt;&gt;&gt; text6[1600:1625]\\n[\\'We\\', \"\\'\", \\'re\\', \\'an\\', \\'anarcho\\', \\'-\\', \\'syndicalist\\', \\'commune\\', \\'.\\', \\'We\\',\\n\\'take\\', \\'it\\', \\'in\\', \\'turns\\', \\'to\\', \\'act\\', \\'as\\', \\'a\\', \\'sort\\', \\'of\\', \\'executive\\',\\n\\'officer\\', \\'for\\', \\'the\\', \\'week\\']\\n&gt;&gt;&gt;\\n\\n\\n\\nIndexes have some subtleties, and we\\'ll explore these with\\nthe help of an artificial sentence:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; sent = [\\'word1\\', \\'word2\\', \\'word3\\', \\'word4\\', \\'word5\\',\\n...         \\'word6\\', \\'word7\\', \\'word8\\', \\'word9\\', \\'word10\\']\\n&gt;&gt;&gt; sent[0]\\n\\'word1\\'\\n&gt;&gt;&gt; sent[9]\\n\\'word10\\'\\n&gt;&gt;&gt;\\n\\n\\n\\nNotice that our indexes start from zero: sent element zero, written sent[0],\\nis the first word, \\'word1\\', whereas sent element 9 is \\'word10\\'.\\nThe reason is simple: the moment Python accesses the content of a list from\\nthe computer\\'s memory, it is already at the first element;\\nwe have to tell it how many elements forward to go.\\nThus, zero steps forward leaves it at the first element.\\n\\nNote\\nThis practice of counting from zero is initially confusing,\\nbut typical of modern programming languages.\\nYou\\'ll quickly get the hang of it if\\nyou\\'ve mastered the system of counting centuries where 19XY is a year\\nin the 20th century, or if you live in a country where the floors of\\na building are numbered from 1, and so walking up n-1 flights of\\nstairs takes you to level n.\\n\\nNow, if we accidentally use an index that is too large, we get an error:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; sent[10]\\nTraceback (most recent call last):\\n  File \"&lt;stdin&gt;\", line 1, in ?\\nIndexError: list index out of range\\n&gt;&gt;&gt;\\n\\n\\n\\nThis time it is not a syntax error, because the program fragment is syntactically correct.\\nInstead, it is a runtime error, and it produces a Traceback message that\\nshows the context of the error, followed by the name of the error,\\nIndexError, and a brief explanation.\\nLet\\'s take a closer look at slicing, using our artificial sentence again.\\nHere we verify that the slice 5:8 includes sent elements at\\nindexes 5, 6, and 7:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; sent[5:8]\\n[\\'word6\\', \\'word7\\', \\'word8\\']\\n&gt;&gt;&gt; sent[5]\\n\\'word6\\'\\n&gt;&gt;&gt; sent[6]\\n\\'word7\\'\\n&gt;&gt;&gt; sent[7]\\n\\'word8\\'\\n&gt;&gt;&gt;\\n\\n\\n\\nBy convention, m:n means elements m…n-1.\\nAs the next example shows,\\nwe can omit the first number if the slice begins at the start of the\\nlist , and we can omit the second number if the slice goes to the end :\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; sent[:3] \\n[\\'word1\\', \\'word2\\', \\'word3\\']\\n&gt;&gt;&gt; text2[141525:] \\n[\\'among\\', \\'the\\', \\'merits\\', \\'and\\', \\'the\\', \\'happiness\\', \\'of\\', \\'Elinor\\', \\'and\\', \\'Marianne\\',\\n\\',\\', \\'let\\', \\'it\\', \\'not\\', \\'be\\', \\'ranked\\', \\'as\\', \\'the\\', \\'least\\', \\'considerable\\', \\',\\',\\n\\'that\\', \\'though\\', \\'sisters\\', \\',\\', \\'and\\', \\'living\\', \\'almost\\', \\'within\\', \\'sight\\', \\'of\\',\\n\\'each\\', \\'other\\', \\',\\', \\'they\\', \\'could\\', \\'live\\', \\'without\\', \\'disagreement\\', \\'between\\',\\n\\'themselves\\', \\',\\', \\'or\\', \\'producing\\', \\'coolness\\', \\'between\\', \\'their\\', \\'husbands\\', \\'.\\',\\n\\'THE\\', \\'END\\']\\n&gt;&gt;&gt;\\n\\n\\n\\nWe can modify an element of a list by assigning to one of its index values.\\nIn the next example, we put sent[0] on the left of the equals sign .  We can also\\nreplace an entire slice with new material .  A consequence of this\\nlast change is that the list only has four elements, and accessing a later value\\ngenerates an error .\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; sent[0] = \\'First\\' \\n&gt;&gt;&gt; sent[9] = \\'Last\\'\\n&gt;&gt;&gt; len(sent)\\n10\\n&gt;&gt;&gt; sent[1:9] = [\\'Second\\', \\'Third\\'] \\n&gt;&gt;&gt; sent\\n[\\'First\\', \\'Second\\', \\'Third\\', \\'Last\\']\\n&gt;&gt;&gt; sent[9] \\nTraceback (most recent call last):\\n  File \"&lt;stdin&gt;\", line 1, in ?\\nIndexError: list index out of range\\n&gt;&gt;&gt;\\n\\n\\n\\n\\nNote\\nYour Turn:\\nTake a few minutes to define a sentence of your own and modify individual words and\\ngroups of words (slices) using the same methods used earlier.  Check your understanding\\nby trying the exercises on lists at the end of this chapter.\\n\\n\\n\\n2.3&nbsp;&nbsp;&nbsp;Variables\\nFrom the start of 1, you have had\\naccess to texts called text1, text2, and so on.  It saved a lot\\nof typing to be able to refer to a 250,000-word book with a short name\\nlike this!  In general, we can make up names for anything we care\\nto calculate.  We did this ourselves in the previous sections, e.g.,\\ndefining a variable sent1, as follows:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; sent1 = [\\'Call\\', \\'me\\', \\'Ishmael\\', \\'.\\']\\n&gt;&gt;&gt;\\n\\n\\n\\nSuch lines have the form: variable = expression.  Python will evaluate\\nthe expression, and save its result to the variable.  This process is\\ncalled assignment.  It does not generate any output;\\nyou have to type the variable on a line of its\\nown to inspect its contents.  The equals sign is slightly misleading,\\nsince information is moving from the right side to the left.\\nIt might help to think of it as a left-arrow.\\nThe name of the variable can be anything you like, e.g., my_sent, sentence, xyzzy.\\nIt must start with a letter, and can include numbers and underscores.\\nHere are some examples of variables and assignments:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; my_sent = [\\'Bravely\\', \\'bold\\', \\'Sir\\', \\'Robin\\', \\',\\', \\'rode\\',\\n... \\'forth\\', \\'from\\', \\'Camelot\\', \\'.\\']\\n&gt;&gt;&gt; noun_phrase = my_sent[1:4]\\n&gt;&gt;&gt; noun_phrase\\n[\\'bold\\', \\'Sir\\', \\'Robin\\']\\n&gt;&gt;&gt; wOrDs = sorted(noun_phrase)\\n&gt;&gt;&gt; wOrDs\\n[\\'Robin\\', \\'Sir\\', \\'bold\\']\\n&gt;&gt;&gt;\\n\\n\\n\\nRemember that capitalized words appear before lowercase words in sorted lists.\\n\\nNote\\nNotice in the previous example that we split the definition\\nof my_sent over two lines.  Python expressions can be split across\\nmultiple lines, so long as this happens within any kind of brackets.\\nPython uses the \"...\" prompt to indicate that more input is\\nexpected.  It doesn\\'t matter how much indentation is used in these\\ncontinuation lines, but some indentation usually makes them easier to read.\\n\\nIt is good to choose meaningful variable names to remind you — and to help anyone\\nelse who reads your Python code — what your code is meant to do.\\nPython does not try to make sense of the names; it blindly follows your instructions,\\nand does not object if you do something confusing, such as one = \\'two\\' or two = 3.\\nThe only restriction is that\\na variable name cannot be any of Python\\'s reserved words, such as\\ndef, if, not,\\nand import.  If you use a reserved word, Python will produce a syntax error:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; not = \\'Camelot\\'           \\nFile \"&lt;stdin&gt;\", line 1\\n    not = \\'Camelot\\'\\n        ^\\nSyntaxError: invalid syntax\\n&gt;&gt;&gt;\\n\\n\\n\\nWe will often use variables to hold intermediate steps of a computation, especially\\nwhen this makes the code easier to follow.  Thus len(set(text1)) could also be written:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; vocab = set(text1)\\n&gt;&gt;&gt; vocab_size = len(vocab)\\n&gt;&gt;&gt; vocab_size\\n19317\\n&gt;&gt;&gt;\\n\\n\\n\\n\\nCaution!\\nTake care with your choice of names (or identifiers) for Python\\nvariables.  First, you should start the name with a letter, optionally\\nfollowed by digits (0 to 9) or letters. Thus, abc23 is fine, but\\n23abc will cause a syntax error.\\nNames are case-sensitive, which means that myVar and myvar\\nare distinct variables.  Variable names cannot contain whitespace,\\nbut you can separate words using an underscore, e.g.,\\nmy_var. Be careful not to insert a hyphen instead of an\\nunderscore: my-var is wrong, since Python interprets the\\n\"-\" as a minus sign.\\n\\n\\n\\n2.4&nbsp;&nbsp;&nbsp;Strings\\nSome of the methods we used to access the elements of a list also work with individual words,\\nor strings.  For example, we can assign a string to a variable ,\\nindex a string , and slice a string :\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; name = \\'Monty\\' \\n&gt;&gt;&gt; name[0] \\n\\'M\\'\\n&gt;&gt;&gt; name[:4] \\n\\'Mont\\'\\n&gt;&gt;&gt;\\n\\n\\n\\nWe can also perform multiplication and addition with strings:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; name * 2\\n\\'MontyMonty\\'\\n&gt;&gt;&gt; name + \\'!\\'\\n\\'Monty!\\'\\n&gt;&gt;&gt;\\n\\n\\n\\nWe can join the words of a list to make a single string, or split a string into a list, as follows:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; \\' \\'.join([\\'Monty\\', \\'Python\\'])\\n\\'Monty Python\\'\\n&gt;&gt;&gt; \\'Monty Python\\'.split()\\n[\\'Monty\\', \\'Python\\']\\n&gt;&gt;&gt;\\n\\n\\n\\nWe will come back to the topic of strings in 3.\\nFor the time being, we have two important building blocks\\n— lists and strings —\\nand are ready to get back to some language analysis.\\n\\n\\n\\n3&nbsp;&nbsp;&nbsp;Computing with Language: Simple Statistics\\n<!-- reimport\\n\\n>>> from nltk.book import * -->\\nLet\\'s return to our exploration of the ways we can bring our computational\\nresources to bear on large quantities of text.  We began this discussion in\\n1, and saw how to search for words\\nin context, how to compile the vocabulary of a text, how to generate random\\ntext in the same style, and so on.\\nIn this section we pick up the question of what makes a text distinct,\\nand use automatic methods to find characteristic words and expressions\\nof a text.  As in 1, you can try\\nnew features of the Python language by copying them into the interpreter,\\nand you\\'ll learn about these features systematically in the following section.\\nBefore continuing further, you might like to check your understanding of the\\nlast section by predicting the output of the following code.  You can use\\nthe interpreter to check whether you got it right.  If you\\'re not sure how\\nto do this task, it would be a good idea to review the previous section\\nbefore continuing further.\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; saying = [\\'After\\', \\'all\\', \\'is\\', \\'said\\', \\'and\\', \\'done\\',\\n...           \\'more\\', \\'is\\', \\'said\\', \\'than\\', \\'done\\']\\n&gt;&gt;&gt; tokens = set(saying)\\n&gt;&gt;&gt; tokens = sorted(tokens)\\n&gt;&gt;&gt; tokens[-2:]\\nwhat output do you expect here?\\n&gt;&gt;&gt;\\n\\n\\n\\n\\n3.1&nbsp;&nbsp;&nbsp;Frequency Distributions\\nHow can we automatically identify the words of a text that are most\\ninformative about the topic and genre of the text?  Imagine how you might\\ngo about finding the 50 most frequent words of a book.  One method\\nwould be to keep a tally for each vocabulary item, like that shown in 3.1.\\nThe tally would need thousands of rows, and it would be an exceedingly\\nlaborious process — so laborious that we would rather assign the task to a machine.\\n\\n\\nFigure 3.1: Counting Words Appearing in a Text (a frequency distribution)\\n\\nThe table in 3.1 is known as a frequency distribution,\\nand it tells us the frequency of each vocabulary item in the text.\\n(In general, it could count any kind of observable event.)\\nIt is a \"distribution\"\\nbecause it tells us how the total number of word tokens in the text\\nare distributed across the vocabulary items.\\nSince we often need frequency distributions in language processing, NLTK\\nprovides built-in support for them.  Let\\'s use a FreqDist to find the\\n50 most frequent words of Moby Dick:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; fdist1 = FreqDist(text1) \\n&gt;&gt;&gt; print(fdist1) \\n&lt;FreqDist with 19317 samples and 260819 outcomes&gt;\\n&gt;&gt;&gt; fdist1.most_common(50) \\n[(\\',\\', 18713), (\\'the\\', 13721), (\\'.\\', 6862), (\\'of\\', 6536), (\\'and\\', 6024),\\n(\\'a\\', 4569), (\\'to\\', 4542), (\\';\\', 4072), (\\'in\\', 3916), (\\'that\\', 2982),\\n(\"\\'\", 2684), (\\'-\\', 2552), (\\'his\\', 2459), (\\'it\\', 2209), (\\'I\\', 2124),\\n(\\'s\\', 1739), (\\'is\\', 1695), (\\'he\\', 1661), (\\'with\\', 1659), (\\'was\\', 1632),\\n(\\'as\\', 1620), (\\'\"\\', 1478), (\\'all\\', 1462), (\\'for\\', 1414), (\\'this\\', 1280),\\n(\\'!\\', 1269), (\\'at\\', 1231), (\\'by\\', 1137), (\\'but\\', 1113), (\\'not\\', 1103),\\n(\\'--\\', 1070), (\\'him\\', 1058), (\\'from\\', 1052), (\\'be\\', 1030), (\\'on\\', 1005),\\n(\\'so\\', 918), (\\'whale\\', 906), (\\'one\\', 889), (\\'you\\', 841), (\\'had\\', 767),\\n(\\'have\\', 760), (\\'there\\', 715), (\\'But\\', 705), (\\'or\\', 697), (\\'were\\', 680),\\n(\\'now\\', 646), (\\'which\\', 640), (\\'?\\', 637), (\\'me\\', 627), (\\'like\\', 624)]\\n&gt;&gt;&gt; fdist1[\\'whale\\']\\n906\\n&gt;&gt;&gt;\\n\\n\\n\\nWhen we first invoke FreqDist, we pass the name of the text as an\\nargument . We can inspect the total number of words (\"outcomes\")\\nthat have been counted up  — 260,819 in the\\ncase of Moby Dick. The expression most_common(50) gives us a list of\\nthe 50 most frequently occurring types in the text .\\n\\nNote\\nYour Turn:\\nTry the preceding frequency distribution example for yourself, for\\ntext2.  Be careful to use the correct parentheses and uppercase letters.\\nIf you get an error message NameError: name \\'FreqDist\\' is not defined,\\nyou need to start your work with from nltk.book import *\\n\\n\\nDo any words produced in the last example help us grasp the topic or genre of this text?\\nOnly one word, whale, is slightly informative!  It occurs over 900 times.\\nThe rest of the words tell us nothing about the text; they\\'re just English \"plumbing.\"\\nWhat proportion of the text is taken up with such words?\\nWe can generate a cumulative frequency plot for these words,\\nusing fdist1.plot(50, cumulative=True), to produce the graph in 3.2.\\nThese 50 words account for nearly half the book!\\n\\n\\nFigure 3.2: Cumulative Frequency Plot for 50 Most Frequently Words in Moby Dick:\\nthese account for nearly half of the tokens.\\n\\nIf the frequent words don\\'t help us, how about the words that occur once\\nonly, the so-called hapaxes?  View them by typing fdist1.hapaxes().\\nThis list contains lexicographer, cetological,\\ncontraband, expostulations, and about 9,000 others.\\nIt seems that there are too many rare words, and without seeing the\\ncontext we probably can\\'t guess what half of the hapaxes mean in any case!\\nSince neither frequent nor infrequent words help, we need to try\\nsomething else.\\n\\n\\n3.2&nbsp;&nbsp;&nbsp;Fine-grained Selection of Words\\nNext, let\\'s look at the long words of a text; perhaps these will be\\nmore characteristic and informative.  For this we adapt some notation\\nfrom set theory.  We would like to find the words from the vocabulary\\nof the text that are more than 15 characters long.  Let\\'s call\\nthis property P, so that P(w) is true\\nif and only if w is more than 15 characters long.\\nNow we can express the words of interest using mathematical\\nset notation as shown in (1a).\\nThis means \"the set of all w such that w is an\\nelement of V (the vocabulary) and w has property P\".\\n\\n  (1)\\n  a.{w | w ∈ V &amp; P(w)}\\n\\n  b.[w for w in V if p(w)]\\n\\nThe corresponding Python expression is given in (1b).\\n(Note that it produces a list, not a set, which means that duplicates are possible.)\\nObserve how similar the two notations are.  Let\\'s go one more step and\\nwrite executable Python code:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; V = set(text1)\\n&gt;&gt;&gt; long_words = [w for w in V if len(w) &gt; 15]\\n&gt;&gt;&gt; sorted(long_words)\\n[\\'CIRCUMNAVIGATION\\', \\'Physiognomically\\', \\'apprehensiveness\\', \\'cannibalistically\\',\\n\\'characteristically\\', \\'circumnavigating\\', \\'circumnavigation\\', \\'circumnavigations\\',\\n\\'comprehensiveness\\', \\'hermaphroditical\\', \\'indiscriminately\\', \\'indispensableness\\',\\n\\'irresistibleness\\', \\'physiognomically\\', \\'preternaturalness\\', \\'responsibilities\\',\\n\\'simultaneousness\\', \\'subterraneousness\\', \\'supernaturalness\\', \\'superstitiousness\\',\\n\\'uncomfortableness\\', \\'uncompromisedness\\', \\'undiscriminating\\', \\'uninterpenetratingly\\']\\n&gt;&gt;&gt;\\n\\n\\n\\nFor each word w in the vocabulary V, we check whether\\nlen(w) is greater than 15; all other words will\\nbe ignored.  We will discuss this syntax more carefully later.\\n\\nNote\\nYour Turn:\\nTry out the previous statements in the Python interpreter,\\nand experiment with changing the text and changing the length condition.\\nDoes it make a difference to your results if you change the\\nvariable names, e.g., using [word for word in vocab if ...]?\\n\\nLet\\'s return to our task of finding words that characterize a text.\\nNotice that the long words in text4 reflect its national focus\\n— constitutionally, transcontinental —\\nwhereas those in text5 reflect its informal content:\\nboooooooooooglyyyyyy and yuuuuuuuuuuuummmmmmmmmmmm.\\nHave we succeeded in automatically extracting words that typify\\na text?  Well, these very long words are often hapaxes (i.e., unique)\\nand perhaps it would be better to find frequently occurring\\nlong words.  This seems promising since it eliminates\\nfrequent short words (e.g., the) and infrequent long words\\n(e.g. antiphilosophists).\\nHere are all words from the chat corpus\\nthat are longer than seven characters, that occur more than seven times:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; fdist5 = FreqDist(text5)\\n&gt;&gt;&gt; sorted(w for w in set(text5) if len(w) &gt; 7 and fdist5[w] &gt; 7)\\n[\\'#14-19teens\\', \\'#talkcity_adults\\', \\'((((((((((\\', \\'........\\', \\'Question\\',\\n\\'actually\\', \\'anything\\', \\'computer\\', \\'cute.-ass\\', \\'everyone\\', \\'football\\',\\n\\'innocent\\', \\'listening\\', \\'remember\\', \\'seriously\\', \\'something\\', \\'together\\',\\n\\'tomorrow\\', \\'watching\\']\\n&gt;&gt;&gt;\\n\\n\\n\\nNotice how we have used two conditions: len(w) &gt; 7 ensures that the\\nwords are longer than seven letters, and fdist5[w] &gt; 7 ensures that\\nthese words occur more than seven times.  At last we have managed to\\nautomatically identify the frequently-occurring content-bearing\\nwords of the text.  It is a modest but important milestone: a tiny piece of code,\\nprocessing tens of thousands of words, produces some informative output.\\n\\n\\n3.3&nbsp;&nbsp;&nbsp;Collocations and Bigrams\\nA collocation is a sequence of words that occur together\\nunusually often. Thus red wine is a collocation, whereas the\\nwine is not. A characteristic of collocations is that they are\\nresistant to substitution with words that have similar senses;\\nfor example, maroon wine sounds definitely odd.\\nTo get a handle on collocations, we start off by extracting from a text\\na list of word pairs, also known as bigrams. This is easily\\naccomplished with the function bigrams():\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; list(bigrams([\\'more\\', \\'is\\', \\'said\\', \\'than\\', \\'done\\']))\\n[(\\'more\\', \\'is\\'), (\\'is\\', \\'said\\'), (\\'said\\', \\'than\\'), (\\'than\\', \\'done\\')]\\n&gt;&gt;&gt;\\n\\n\\n\\n\\nNote\\nIf you omitted list() above, and just typed bigrams([\\'more\\', ...]),\\nyou would have seen output of the form &lt;generator object bigrams at 0x10fb8b3a8&gt;.\\nThis is Python\\'s way of saying that it is ready to compute\\na sequence of items, in this case, bigrams. For now, you just need\\nto know to tell Python to convert it into a list, using list().\\n\\nHere we see that the pair of words than-done is a bigram, and we write\\nit in Python as (\\'than\\', \\'done\\').  Now, collocations are essentially\\njust frequent bigrams, except that we want to pay more attention to the\\ncases that involve rare words.  In particular, we want to find\\nbigrams that occur more often than we would expect based on\\nthe frequency of the individual words.  The collocations() function\\ndoes this for us. We will see how it works later.\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; text4.collocations()\\nUnited States; fellow citizens; four years; years ago; Federal\\nGovernment; General Government; American people; Vice President; Old\\nWorld; Almighty God; Fellow citizens; Chief Magistrate; Chief Justice;\\nGod bless; every citizen; Indian tribes; public debt; one another;\\nforeign nations; political parties\\n&gt;&gt;&gt; text8.collocations()\\nwould like; medium build; social drinker; quiet nights; non smoker;\\nlong term; age open; Would like; easy going; financially secure; fun\\ntimes; similar interests; Age open; weekends away; poss rship; well\\npresented; never married; single mum; permanent relationship; slim\\nbuild\\n&gt;&gt;&gt;\\n\\n\\n\\nThe collocations that emerge are very specific to the genre of the\\ntexts. In order to find  red wine as a collocation, we would\\nneed to process a much larger body of text.\\n\\n\\n3.4&nbsp;&nbsp;&nbsp;Counting Other Things\\nCounting words is useful, but we can count other things too.  For example, we can\\nlook at the distribution of word lengths in a text, by creating a FreqDist\\nout of a long list of numbers, where each number is the length of the corresponding\\nword in the text:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; [len(w) for w in text1] \\n[1, 4, 4, 2, 6, 8, 4, 1, 9, 1, 1, 8, 2, 1, 4, 11, 5, 2, 1, 7, 6, 1, 3, 4, 5, 2, ...]\\n&gt;&gt;&gt; fdist = FreqDist(len(w) for w in text1)  \\n&gt;&gt;&gt; print(fdist)  \\n&lt;FreqDist with 19 samples and 260819 outcomes&gt;\\n&gt;&gt;&gt; fdist\\nFreqDist({3: 50223, 1: 47933, 4: 42345, 2: 38513, 5: 26597, 6: 17111, 7: 14399,\\n  8: 9966, 9: 6428, 10: 3528, ...})\\n&gt;&gt;&gt;\\n\\n\\n\\nWe start by deriving a list of the lengths of words in text1\\n,\\nand the FreqDist then counts the number of times each of these\\noccurs . The result  is a distribution containing\\na quarter of a million items, each of which is a number corresponding to a\\nword token in the text.  But there are at most only 20 distinct\\nitems being counted, the numbers 1 through 20, because there are only 20\\ndifferent word lengths.  I.e., there are words consisting of just one character,\\ntwo characters, ..., twenty characters, but none with twenty one or more\\ncharacters.  One might wonder how frequent the different lengths of word are\\n(e.g., how many words of length four appear in the text, are there more words of length five\\nthan length four, etc). We can do this as follows:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; fdist.most_common()\\n[(3, 50223), (1, 47933), (4, 42345), (2, 38513), (5, 26597), (6, 17111), (7, 14399),\\n(8, 9966), (9, 6428), (10, 3528), (11, 1873), (12, 1053), (13, 567), (14, 177),\\n(15, 70), (16, 22), (17, 12), (18, 1), (20, 1)]\\n&gt;&gt;&gt; fdist.max()\\n3\\n&gt;&gt;&gt; fdist[3]\\n50223\\n&gt;&gt;&gt; fdist.freq(3)\\n0.19255882431878046\\n&gt;&gt;&gt;\\n\\n\\n\\nFrom this we see that the most frequent word length is 3, and that\\nwords of length 3 account for roughly 50,000 (or 20%) of the words making up the\\nbook.  Although we will not pursue it here, further analysis of word\\nlength might help us understand differences between authors, genres, or\\nlanguages.\\n3.1 summarizes the functions defined in frequency distributions.\\nTable 3.1: Functions Defined for NLTK\\'s Frequency Distributions\\n\\n\\n\\n\\n\\nExample\\nDescription\\n\\n\\n\\nfdist = FreqDist(samples)\\ncreate a frequency distribution containing the given samples\\n\\nfdist[sample] += 1\\nincrement the count for this sample\\n\\nfdist[\\'monstrous\\']\\ncount of the number of times a given sample occurred\\n\\nfdist.freq(\\'monstrous\\')\\nfrequency of a given sample\\n\\nfdist.N()\\ntotal number of samples\\n\\nfdist.most_common(n)\\nthe n most common samples and their frequencies\\n\\nfor sample in fdist:\\niterate over the samples\\n\\nfdist.max()\\nsample with the greatest count\\n\\nfdist.tabulate()\\ntabulate the frequency distribution\\n\\nfdist.plot()\\ngraphical plot of the frequency distribution\\n\\nfdist.plot(cumulative=True)\\ncumulative plot of the frequency distribution\\n\\nfdist1 |= fdist2\\nupdate fdist1 with counts from fdist2\\n\\nfdist1 &lt; fdist2\\ntest if samples in fdist1 occur less frequently than in fdist2\\n\\n\\n\\n\\n\\nOur discussion of frequency distributions has introduced some important Python concepts,\\nand we will look at them systematically in 4.\\n<!-- We\\'ve also touched on the topic of normalization, and we\\'ll explore this in\\ndepth in chap-words_. -->\\n\\n\\n\\n4&nbsp;&nbsp;&nbsp;Back to Python: Making Decisions and Taking Control\\n<!-- reimport\\n\\n>>> from nltk.book import * -->\\nSo far, our little programs have had some interesting qualities:\\nthe ability to work with language, and\\nthe potential to save human effort through automation.\\nA key feature of programming is the ability of machines to\\nmake decisions on our behalf, executing instructions when\\ncertain conditions are met, or repeatedly looping through\\ntext data until some condition is satisfied.  This feature\\nis known as control, and is the focus of this section.\\n\\n4.1&nbsp;&nbsp;&nbsp;Conditionals\\nPython supports a wide range of operators, such as &lt; and &gt;=, for\\ntesting the relationship between values. The full set of these relational\\noperators is shown in 4.1.\\nTable 4.1: Numerical Comparison Operators\\n\\n\\n\\n\\n\\nOperator\\nRelationship\\n\\n\\n\\n&lt;\\nless than\\n\\n&lt;=\\nless than or equal to\\n\\n==\\nequal to (note this is two \"=\" signs, not one)\\n\\n!=\\nnot equal to\\n\\n&gt;\\ngreater than\\n\\n&gt;=\\ngreater than or equal to\\n\\n\\n\\n\\n\\nWe can use these to select different words from a sentence of news text.\\nHere are some examples — only the operator is changed from one\\nline to the next.  They all use sent7, the first sentence from text7\\n(Wall Street Journal).  As before, if you get an error saying that sent7\\nis undefined, you need to first type: from nltk.book import *\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; sent7\\n[\\'Pierre\\', \\'Vinken\\', \\',\\', \\'61\\', \\'years\\', \\'old\\', \\',\\', \\'will\\', \\'join\\', \\'the\\',\\n\\'board\\', \\'as\\', \\'a\\', \\'nonexecutive\\', \\'director\\', \\'Nov.\\', \\'29\\', \\'.\\']\\n&gt;&gt;&gt; [w for w in sent7 if len(w) &lt; 4]\\n[\\',\\', \\'61\\', \\'old\\', \\',\\', \\'the\\', \\'as\\', \\'a\\', \\'29\\', \\'.\\']\\n&gt;&gt;&gt; [w for w in sent7 if len(w) &lt;= 4]\\n[\\',\\', \\'61\\', \\'old\\', \\',\\', \\'will\\', \\'join\\', \\'the\\', \\'as\\', \\'a\\', \\'Nov.\\', \\'29\\', \\'.\\']\\n&gt;&gt;&gt; [w for w in sent7 if len(w) == 4]\\n[\\'will\\', \\'join\\', \\'Nov.\\']\\n&gt;&gt;&gt; [w for w in sent7 if len(w) != 4]\\n[\\'Pierre\\', \\'Vinken\\', \\',\\', \\'61\\', \\'years\\', \\'old\\', \\',\\', \\'the\\', \\'board\\',\\n\\'as\\', \\'a\\', \\'nonexecutive\\', \\'director\\', \\'29\\', \\'.\\']\\n&gt;&gt;&gt;\\n\\n\\n\\nThere is a common pattern to all of these examples:\\n[w for w in text if condition ], where condition is a\\nPython \"test\" that yields either true or false.\\nIn the cases shown in the previous code example, the condition is always a numerical comparison.\\nHowever, we can also test various properties of words,\\nusing the functions listed in 4.2.\\nTable 4.2: Some Word Comparison Operators\\n\\n\\n\\n\\n\\nFunction\\nMeaning\\n\\n\\n\\ns.startswith(t)\\ntest if s starts with t\\n\\ns.endswith(t)\\ntest if s ends with t\\n\\nt in s\\ntest if t is a substring of s\\n\\ns.islower()\\ntest if s contains cased characters and all are lowercase\\n\\ns.isupper()\\ntest if s contains cased characters and all are uppercase\\n\\ns.isalpha()\\ntest if s is non-empty and all characters in s are alphabetic\\n\\ns.isalnum()\\ntest if s is non-empty and all characters in s are alphanumeric\\n\\ns.isdigit()\\ntest if s is non-empty and all characters in s are digits\\n\\ns.istitle()\\ntest if s contains cased characters and is titlecased\\n(i.e. all words in s have initial capitals)\\n\\n\\n\\n\\n\\nHere are some examples of these operators being used to\\nselect words from our texts:\\nwords ending with -ableness;\\nwords containing gnt;\\nwords having an initial capital;\\nand words consisting entirely of digits.\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; sorted(w for w in set(text1) if w.endswith(\\'ableness\\'))\\n[\\'comfortableness\\', \\'honourableness\\', \\'immutableness\\', \\'indispensableness\\', ...]\\n&gt;&gt;&gt; sorted(term for term in set(text4) if \\'gnt\\' in term)\\n[\\'Sovereignty\\', \\'sovereignties\\', \\'sovereignty\\']\\n&gt;&gt;&gt; sorted(item for item in set(text6) if item.istitle())\\n[\\'A\\', \\'Aaaaaaaaah\\', \\'Aaaaaaaah\\', \\'Aaaaaah\\', \\'Aaaah\\', \\'Aaaaugh\\', \\'Aaagh\\', ...]\\n&gt;&gt;&gt; sorted(item for item in set(sent7) if item.isdigit())\\n[\\'29\\', \\'61\\']\\n&gt;&gt;&gt;\\n\\n\\n\\nWe can also create more complex conditions.  If c is a\\ncondition, then not c is also a condition.\\nIf we have two conditions c1 and c2,\\nthen we can combine them to form a new condition using conjunction and disjunction:\\nc1 and c2,\\nc1 or c2.\\n\\nNote\\nYour Turn:\\nRun the following examples and try to explain what is going on in each one.\\nNext, try to make up some conditions of your own.\\n\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; sorted(w for w in set(text7) if \\'-\\' in w and \\'index\\' in w)\\n&gt;&gt;&gt; sorted(wd for wd in set(text3) if wd.istitle() and len(wd) &gt; 10)\\n&gt;&gt;&gt; sorted(w for w in set(sent7) if not w.islower())\\n&gt;&gt;&gt; sorted(t for t in set(text2) if \\'cie\\' in t or \\'cei\\' in t)\\n\\n\\n\\n\\n\\n4.2&nbsp;&nbsp;&nbsp;Operating on Every Element\\nIn 3, we saw some examples of\\ncounting items other than words.  Let\\'s take a closer look at the notation we used:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; [len(w) for w in text1]\\n[1, 4, 4, 2, 6, 8, 4, 1, 9, 1, 1, 8, 2, 1, 4, 11, 5, 2, 1, 7, 6, 1, 3, 4, 5, 2, ...]\\n&gt;&gt;&gt; [w.upper() for w in text1]\\n[\\'[\\', \\'MOBY\\', \\'DICK\\', \\'BY\\', \\'HERMAN\\', \\'MELVILLE\\', \\'1851\\', \\']\\', \\'ETYMOLOGY\\', \\'.\\', ...]\\n&gt;&gt;&gt;\\n\\n\\n\\nThese expressions have the form [f(w) for ...] or [w.f() for ...], where\\nf is a function that operates on a word to compute its length, or to\\nconvert it to uppercase.\\nFor now, you don\\'t need to understand the difference between the notations f(w) and\\nw.f().  Instead, simply learn this Python idiom which performs the\\nsame operation on every element of a list.  In the preceding examples, it goes through\\neach word in text1, assigning each one in turn to the variable w and\\nperforming the specified operation on the variable.\\n\\nNote\\nThe notation just described is called a \"list comprehension.\"  This is our first example\\nof a Python idiom, a fixed notation that we use habitually without bothering to\\nanalyze each time.  Mastering such idioms is an important part of becoming a\\nfluent Python programmer.\\n\\nLet\\'s return to the question of vocabulary size, and apply the same idiom here:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; len(text1)\\n260819\\n&gt;&gt;&gt; len(set(text1))\\n19317\\n&gt;&gt;&gt; len(set(word.lower() for word in text1))\\n17231\\n&gt;&gt;&gt;\\n\\n\\n\\nNow that we are not double-counting words like This and this, which differ only\\nin capitalization, we\\'ve wiped 2,000 off the vocabulary count!  We can go a step further\\nand eliminate numbers and punctuation from the vocabulary count by filtering out any\\nnon-alphabetic items:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; len(set(word.lower() for word in text1 if word.isalpha()))\\n16948\\n&gt;&gt;&gt;\\n\\n\\n\\nThis example is slightly complicated: it lowercases all the purely alphabetic items.\\nPerhaps it would have been simpler just to count the lowercase-only items, but this\\ngives the wrong answer (why?).\\nDon\\'t worry if you don\\'t feel confident with list comprehensions yet,\\nsince you\\'ll see many more examples along with explanations in the following chapters.\\n\\n\\n4.3&nbsp;&nbsp;&nbsp;Nested Code Blocks\\nMost programming languages permit us to execute a block of code when a\\nconditional expression, or if statement, is satisfied.  We\\nalready saw examples of conditional tests in code like [w for w in\\nsent7 if len(w) &lt; 4]. In the following program, we have created a\\nvariable called word containing the string value \\'cat\\'. The\\nif statement checks whether the test len(word) &lt; 5 is true.\\nIt is, so the body of the if statement is invoked and the\\nprint statement is executed, displaying a message to the user.\\nRemember to indent the print statement by typing four spaces.\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; word = \\'cat\\'\\n&gt;&gt;&gt; if len(word) &lt; 5:\\n...     print(\\'word length is less than 5\\')\\n...   \\nword length is less than 5\\n&gt;&gt;&gt;\\n\\n\\n\\nWhen we use the Python interpreter we have to add an extra blank line \\nin order for it to detect that the nested block is complete.\\n\\nNote\\nIf you are using Python 2.6 or 2.7, you need to include the following\\nline in order for the above print function to be recognized:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; from __future__ import print_function\\n\\n\\n\\n\\nIf we change the conditional test to len(word) &gt;= 5,\\nto check that the length of word is greater than or equal to 5,\\nthen the test will no longer be true.\\nThis time, the body of the if statement will not be executed,\\nand no message is shown to the user:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; if len(word) &gt;= 5:\\n...   print(\\'word length is greater than or equal to 5\\')\\n...\\n&gt;&gt;&gt;\\n\\n\\n\\nAn if statement is known as a control structure\\nbecause it controls whether the code in the indented block will be run.\\nAnother control structure is the for loop.\\nTry the following, and remember to include the colon and the four spaces:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; for word in [\\'Call\\', \\'me\\', \\'Ishmael\\', \\'.\\']:\\n...     print(word)\\n...\\nCall\\nme\\nIshmael\\n.\\n&gt;&gt;&gt;\\n\\n\\n\\nThis is called a loop because Python executes the code in\\ncircular fashion.  It starts by performing the\\nassignment word = \\'Call\\',\\neffectively using the word variable to name the first\\nitem of the list.  Then, it displays the value of word\\nto the user.  Next, it goes back to the for statement,\\nand performs the assignment word = \\'me\\', before displaying this new value\\nto the user, and so on.  It continues in this fashion until\\nevery item of the list has been processed.\\n\\n\\n4.4&nbsp;&nbsp;&nbsp;Looping with Conditions\\nNow we can combine the if and for statements.\\nWe will loop over every item of the list, and print\\nthe item only if it ends with the letter l.  We\\'ll pick another\\nname for the variable to demonstrate that Python doesn\\'t\\ntry to make sense of variable names.\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; sent1 = [\\'Call\\', \\'me\\', \\'Ishmael\\', \\'.\\']\\n&gt;&gt;&gt; for xyzzy in sent1:\\n...     if xyzzy.endswith(\\'l\\'):\\n...         print(xyzzy)\\n...\\nCall\\nIshmael\\n&gt;&gt;&gt;\\n\\n\\n\\nYou will notice that if and for statements\\nhave a colon at the end of the line,\\nbefore the indentation begins. In fact, all Python\\ncontrol structures end with a colon.  The colon\\nindicates that the current statement relates to the\\nindented block that follows.\\nWe can also specify an action to be taken if\\nthe condition of the if statement is not met.\\nHere we see the elif (else if) statement, and\\nthe else statement.  Notice that these also have\\ncolons before the indented code.\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; for token in sent1:\\n...     if token.islower():\\n...         print(token, \\'is a lowercase word\\')\\n...     elif token.istitle():\\n...         print(token, \\'is a titlecase word\\')\\n...     else:\\n...         print(token, \\'is punctuation\\')\\n...\\nCall is a titlecase word\\nme is a lowercase word\\nIshmael is a titlecase word\\n. is punctuation\\n&gt;&gt;&gt;\\n\\n\\n\\nAs you can see, even with this small amount of Python knowledge,\\nyou can start to build multiline Python programs.\\nIt\\'s important to develop such programs in pieces,\\ntesting that each piece does what you expect before\\ncombining them into a program.  This is why the Python\\ninteractive interpreter is so invaluable, and why you should get\\ncomfortable using it.\\nFinally, let\\'s combine the idioms we\\'ve been exploring.\\nFirst, we create a list of cie and cei words,\\nthen we loop over each item and print it.  Notice the\\nextra information given in the print statement: end=\\' \\'.\\nThis tells Python to print a space (not the default newline) after each word.\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; tricky = sorted(w for w in set(text2) if \\'cie\\' in w or \\'cei\\' in w)\\n&gt;&gt;&gt; for word in tricky:\\n...     print(word, end=\\' \\')\\nancient ceiling conceit conceited conceive conscience\\nconscientious conscientiously deceitful deceive ...\\n&gt;&gt;&gt;\\n\\n\\n\\n\\n\\n\\n5&nbsp;&nbsp;&nbsp;Automatic Natural Language Understanding\\n>> from nltk.misc import babelize_shell -->\\nWe have been exploring language bottom-up, with the help of texts and\\nthe Python programming\\nlanguage.  However, we\\'re also interested in exploiting our knowledge of language and computation\\nby building useful language technologies. We\\'ll take the opportunity\\nnow to step back from the nitty-gritty of code in order to paint a\\nbigger picture of natural language processing.\\nAt a purely practical level, we all need help to navigate the universe of information\\nlocked up in text on the Web.  Search engines have been crucial to the\\ngrowth and popularity of the Web, but have some shortcomings.\\nIt takes skill, knowledge, and some luck,\\nto extract answers to such questions as: What tourist sites can I\\nvisit between Philadelphia and Pittsburgh on a limited budget?\\nWhat do experts say about digital SLR cameras? What\\npredictions about the steel market were made by credible commentators\\nin the past week? Getting a computer to answer them automatically\\ninvolves a range of language processing tasks, including information extraction,\\ninference, and summarization, and would need to be carried out on a scale\\nand with a level of robustness that is still beyond our current capabilities.\\nOn a more philosophical level, a long-standing challenge within artificial intelligence\\nhas been to build intelligent machines, and a major part of intelligent behaviour is understanding\\nlanguage.  For many years this goal has been seen as too difficult.\\nHowever, as NLP technologies become more mature, and robust methods for\\nanalyzing unrestricted text become more widespread, the prospect of\\nnatural language understanding has re-emerged as a plausible goal.\\nIn this section we describe some language understanding technologies,\\nto give you a sense of the interesting challenges that are waiting for you.\\n\\n5.1&nbsp;&nbsp;&nbsp;Word Sense Disambiguation\\nIn word sense disambiguation we want to work out\\nwhich sense of a word was intended in a given context.  Consider the\\nambiguous words serve and dish:\\n\\n  (2)\\n  a.serve: help with food or drink; hold an office; put ball into play\\n\\n  b.dish: plate; course of a meal; communications device\\n\\nIn a sentence containing the phrase: he served the dish, you\\ncan detect that both serve and dish are being used with\\ntheir food meanings.  It\\'s unlikely that the topic of discussion\\nshifted from sports to crockery in the space of three words.\\nThis would force you to invent bizarre images, like a tennis pro\\ntaking out his or her frustrations on a china tea-set laid out beside the court.\\nIn other words, we automatically disambiguate words using context, exploiting\\nthe simple fact that nearby words have closely related meanings.\\nAs another example of this contextual effect, consider the word\\nby, which has several meanings, e.g.: the book by\\nChesterton (agentive — Chesterton was the author of the book);\\nthe cup by the stove (locative — the stove is where the\\ncup is); and submit by Friday (temporal — Friday is the\\ntime of the submitting).\\nObserve in (3c) that the meaning of the italicized word helps us\\ninterpret the meaning of by.\\n\\n  (3)\\n  a.The lost children were found by the searchers  (agentive)\\n\\n  b.The lost children were found by the mountain   (locative)\\n\\n  c.The lost children were found by the afternoon  (temporal)\\n\\n\\n\\n5.2&nbsp;&nbsp;&nbsp;Pronoun Resolution\\nA deeper kind of language understanding is to work out \"who did what to whom\" —\\ni.e., to detect the subjects and objects of verbs.  You learnt to do this in\\nelementary school, but it\\'s harder than you might think.\\nIn the sentence the thieves stole the paintings\\nit is easy to tell who performed the stealing action.\\nConsider three possible following sentences in (4c), and try to determine\\nwhat was sold, caught, and found (one case is ambiguous).\\n\\n  (4)\\n  a.The thieves stole the paintings.  They were subsequently sold.\\n\\n  b.The thieves stole the paintings.  They were subsequently caught.\\n\\n  c.The thieves stole the paintings.  They were subsequently found.\\n\\nAnswering this question involves finding the antecedent of the pronoun they,\\neither thieves or paintings.  Computational techniques for tackling this problem\\ninclude anaphora resolution — identifying what a pronoun or noun phrase\\nrefers to — and semantic role labeling — identifying how a noun phrase\\nrelates to the verb (as agent, patient, instrument, and so on).\\n\\n\\n5.3&nbsp;&nbsp;&nbsp;Generating Language Output\\nIf we can automatically solve such problems of language understanding, we will\\nbe able to move on to tasks that involve generating language output, such as\\nquestion answering and machine translation.  In the first case,\\na machine should be able to answer a user\\'s questions relating to collection of texts:\\n\\n  (5)\\n  a.Text: ... The thieves stole the paintings.  They were subsequently sold. ...\\n\\n  b.Human: Who or what was sold?\\n\\n  c.Machine: The paintings.\\n\\nThe machine\\'s answer demonstrates that it has correctly worked out that they\\nrefers to paintings and not to thieves.  In the second case, the machine should\\nbe able to translate the text into another language, accurately\\nconveying the meaning of the original text.  In translating the example text into French,\\nwe are forced to choose the gender of the pronoun in the second sentence:\\nils (masculine) if the thieves are found, and elles (feminine) if\\nthe paintings are found.  Correct translation actually depends on correct understanding of\\nthe pronoun.\\n\\n  (6)\\n  a.The thieves stole the paintings.  They were subsequently found.\\n\\n  b.Les voleurs ont volé les peintures. Ils ont été trouvés plus tard.  (the thieves)\\n\\n  c.Les voleurs ont volé les peintures. Elles ont été trouvées plus tard.  (the paintings)\\n\\nIn all of these examples, working out the sense of a word, the subject of a verb, and the\\nantecedent of a pronoun are steps in establishing the meaning of a sentence, things\\nwe would expect a language understanding system to be able to do.\\n\\n\\n5.4&nbsp;&nbsp;&nbsp;Machine Translation\\nFor a long time now, machine translation (MT) has\\nbeen the holy grail of language understanding,\\nultimately seeking to provide high-quality,\\nidiomatic translation between any pair of languages.\\nIts roots go back to the early days of the Cold War, when the promise\\nof automatic translation led to substantial government sponsorship,\\nand with it, the genesis of NLP itself.\\nToday, practical translation systems exist for particular pairs\\nof languages, and some are integrated into web search engines.\\nHowever, these systems have some serious shortcomings, which\\nare starkly revealed by translating a sentence back and forth\\nbetween a pair of languages until equilibrium is reached, e.g.:\\n\\n0&gt; how long before the next flight to Alice Springs?\\n1&gt; wie lang vor dem folgenden Flug zu Alice Springs?\\n2&gt; how long before the following flight to Alice jump?\\n3&gt; wie lang vor dem folgenden Flug zu Alice springen Sie?\\n4&gt; how long before the following flight to Alice do you jump?\\n5&gt; wie lang, bevor der folgende Flug zu Alice tun, Sie springen?\\n6&gt; how long, before the following flight to Alice does, do you jump?\\n7&gt; wie lang bevor der folgende Flug zu Alice tut, tun Sie springen?\\n8&gt; how long before the following flight to Alice does, do you jump?\\n9&gt; wie lang, bevor der folgende Flug zu Alice tut, tun Sie springen?\\n10&gt; how long, before the following flight does to Alice, do do you jump?\\n11&gt; wie lang bevor der folgende Flug zu Alice tut, Sie tun Sprung?\\n12&gt; how long before the following flight does leap to Alice, does you?\\n\\nObserve that the system correctly translates Alice Springs from English\\nto German (in the line starting 1&gt;), but on the way back to English, this ends up as Alice jump\\n(line 2).  The preposition before is initially translated into the corresponding\\nGerman preposition vor, but later into the conjunction bevor (line 5).\\nAfter line 5 the sentences become nonsensical (but notice the various phrasings\\nindicated by the commas, and the change from jump to leap).\\nThe translation system did not recognize when a word was part of a proper name,\\nand it misinterpreted the grammatical structure.\\n\\nNote\\nYour Turn: Try this yourself using http://translationparty.com/\\n\\nMachine translation is difficult because a given word could have several possible\\ntranslations (depending on its meaning), and because word order must be changed\\nin keeping with the grammatical structure of the target language.\\nToday these difficulties are being faced by collecting massive quantities of\\nparallel texts from news and government websites that publish documents\\nin two or more languages.  Given a document in German and English, and possibly\\na bilingual dictionary, we can automatically pair up the sentences,\\na process called text alignment.  Once we have a million or more sentence\\npairs, we can detect corresponding words and phrases, and build a model\\nthat can be used for translating new text.\\n\\n\\n5.5&nbsp;&nbsp;&nbsp;Spoken Dialog Systems\\nIn the history of artificial intelligence, the chief measure of intelligence\\nhas been a linguistic one, namely the Turing Test: can a dialogue system,\\nresponding to a user\\'s text input, perform so naturally that we cannot distinguish\\nit from a human-generated response?  In contrast, today\\'s commercial dialogue systems\\nare very limited, but still perform useful functions in narrowly-defined domains,\\nas we see here:\\n\\nS: How may I help you?\\nU: When is Saving Private Ryan playing?\\nS: For what theater?\\nU: The Paramount theater.\\nS: Saving Private Ryan is not playing at the Paramount theater, but\\nit\\'s playing at the Madison theater at 3:00, 5:30, 8:00, and 10:30.\\n\\nYou could not ask this system to provide driving instructions or\\ndetails of nearby restaurants unless the required information\\nhad already been stored and suitable question-answer pairs\\nhad been incorporated into the language processing system.\\nObserve that this system seems to understand the user\\'s goals:\\nthe user asks when a movie is showing and the system\\ncorrectly determines from this that the user wants to see\\nthe movie. This inference seems so obvious that you probably\\ndidn\\'t notice it was made, yet a natural language system\\nneeds to be endowed with this capability in order to interact\\nnaturally.  Without it, when asked Do you know when Saving Private\\nRyan is playing?, a system might unhelpfully respond with a cold Yes.\\nHowever, the developers of commercial dialogue systems use\\ncontextual assumptions and business logic to ensure that the different ways in which a user might\\nexpress requests or provide information are handled in a way that\\nmakes sense for the particular application.  So, if you type\\nWhen is ..., or I want to know when ..., or Can you tell me\\nwhen ..., simple rules will always yield screening times.  This is\\nenough for the system to provide a useful service.\\n\\n\\nFigure 5.1: Simple Pipeline Architecture for a Spoken Dialogue System:\\nSpoken input (top left) is analyzed, words are recognized, sentences are parsed and\\ninterpreted in context, application-specific actions take place (top right);\\na response is planned, realized as a syntactic structure, then to suitably\\ninflected words, and finally to spoken output; different types of\\nlinguistic knowledge inform each stage of the process.\\n\\nDialogue systems give us an opportunity to mention the\\ncommonly assumed pipeline for NLP.\\n5.1 shows the architecture of a simple dialogue system.\\nAlong the top of the diagram, moving from left to right, is a\\n\"pipeline\" of some language understanding components.\\nThese map from speech input via syntactic parsing\\nto some kind of meaning representation.  Along the middle, moving from\\nright to left, is the reverse pipeline of components for converting\\nconcepts to speech.  These components make up the dynamic aspects of the system.\\nAt the bottom of the diagram are some representative bodies of\\nstatic information: the repositories of language-related data that\\nthe processing components draw on to do their work.\\n\\nNote\\nYour Turn:\\nFor an example of a primitive dialogue system, try having\\na conversation with an NLTK chatbot.  To see the available chatbots,\\nrun nltk.chat.chatbots().\\n(Remember to import nltk first.)\\n\\n\\n\\n5.6&nbsp;&nbsp;&nbsp;Textual Entailment\\nThe challenge of language understanding has been brought into focus in recent years by a public\\n\"shared task\" called Recognizing Textual Entailment (RTE). The basic\\nscenario is simple.  Suppose you want to find evidence to support\\nthe hypothesis: Sandra Goudie was defeated by Max Purnell, and\\nthat you have another short text that seems to be relevant, for example,\\nSandra Goudie was first elected to Parliament in the 2002 elections,\\nnarrowly winning the seat of Coromandel by defeating Labour candidate\\nMax Purnell and pushing incumbent Green MP Jeanette Fitzsimons into\\nthird place.  Does the text provide enough evidence for you to\\naccept the hypothesis?  In this particular case, the answer will be \"No.\"\\nYou can draw this conclusion easily, but it is very hard to come up with\\nautomated methods for making the right decision. The RTE\\nChallenges provide data that allow competitors to develop their\\nsystems, but not enough data for \"brute force\" machine learning techniques (a topic\\nwe will cover in chap-data-intensive).  Consequently, some\\nlinguistic analysis is crucial. In the previous example, it is important\\nfor the system to note that Sandra Goudie names the person being\\ndefeated in the hypothesis, not the person doing the defeating in the\\ntext. As another illustration of the difficulty of the task, consider\\nthe following text-hypothesis pair:\\n\\n  (7)\\n  a.Text: David Golinkin is the editor or author of eighteen books, and over 150 responsa, articles, sermons and books\\n\\n  b.Hypothesis: Golinkin has written eighteen books\\n\\nIn order to determine whether the hypothesis is supported by the\\ntext, the system needs the following background knowledge:\\n(i) if someone is an author of a book, then he/she has written that\\nbook; (ii) if someone is an editor of a book, then he/she has not\\nwritten (all of) that book; (iii) if someone is editor or author of eighteen\\nbooks, then one cannot conclude that he/she is author of eighteen books.\\n\\n\\n5.7&nbsp;&nbsp;&nbsp;Limitations of NLP\\nDespite the research-led advances in tasks like RTE, natural language\\nsystems that have been deployed for real-world applications still cannot perform\\ncommon-sense reasoning or draw on world knowledge in a general and\\nrobust manner.  We can wait for these difficult artificial\\nintelligence problems to be solved, but in the meantime it is\\nnecessary to live with some severe limitations on the reasoning and\\nknowledge capabilities of natural language systems. Accordingly, right\\nfrom the beginning, an important goal of NLP research has been to\\nmake progress on the difficult task of building technologies that\\n\"understand language,\" using superficial yet powerful techniques instead of\\nunrestricted knowledge and reasoning capabilities.\\nIndeed, this is one of the goals of this book, and we hope to equip you with\\nthe knowledge and skills to build useful NLP systems, and to\\ncontribute to the long-term aspiration of building intelligent machines.\\n\\n\\n\\n6&nbsp;&nbsp;&nbsp;Summary\\n\\nTexts are represented in Python using lists:\\n[\\'Monty\\', \\'Python\\'].  We can use indexing, slicing,\\nand the len() function on lists.\\nA word \"token\" is a particular appearance of a given word in a text;\\na word \"type\" is the unique form of the word as a particular sequence\\nof letters.  We count word tokens using len(text) and word types using\\nlen(set(text)).\\nWe obtain the vocabulary of a text t using sorted(set(t)).\\nWe operate on each item of a text using [f(x) for x in text].\\nTo derive the vocabulary, collapsing case distinctions and ignoring punctuation,\\nwe can write set(w.lower() for w in text if w.isalpha()).\\nWe process each word in a text using a for statement, such\\nas for w in t: or for word in text:.  This must be followed by the colon character\\nand an indented block of code, to be executed each time through the loop.\\nWe test a condition using an if statement: if len(word) &lt; 5:.\\nThis must be followed by the colon character and an indented block of\\ncode, to be executed only if the condition is true.\\nA frequency distribution is a collection of items along with their frequency counts\\n(e.g., the words of a text and their frequency of appearance).\\nA function is a block of code that has been assigned a name and can\\nbe reused. Functions are defined using the def keyword, as in\\ndef mult(x, y); x and y are parameters of the function,\\nand act as placeholders for actual data values.\\nA function is called by specifying its name followed by zero or more\\narguments inside parentheses, like this: texts(), mult(3, 4), len(text1).\\n\\n\\n\\n7&nbsp;&nbsp;&nbsp;Further Reading\\nThis chapter has introduced new concepts in programming, natural language processing,\\nand linguistics, all mixed in together.\\nMany of them are consolidated in the following chapters.  However, you may also want to\\nconsult the online materials provided with this chapter (at http://nltk.org/), including links\\nto additional background materials, and links to online NLP systems.\\nYou may also like to read up on\\nsome linguistics and NLP-related concepts in Wikipedia (e.g., collocations,\\nthe Turing Test, the type-token distinction).\\nYou should acquaint yourself with the Python documentation available at http://docs.python.org/,\\nincluding the many tutorials and comprehensive reference materials linked there.\\nA Beginner\\'s Guide to Python is available at http://wiki.python.org/moin/BeginnersGuide.\\nMiscellaneous questions about Python might be answered in the FAQ at\\nhttp://python.org/doc/faq/general/.\\nAs you delve into NLTK, you might want to subscribe to the mailing list where new\\nreleases of the toolkit are announced.  There is also an NLTK-Users mailing list,\\nwhere users help each other as they learn how to use Python and NLTK for\\nlanguage analysis work.  Details of these lists are available at http://nltk.org/.\\nFor more information on the topics covered in 5,\\nand on NLP more generally, you might like to consult one of the following excellent\\nbooks:\\n\\nIndurkhya, Nitin and Fred Damerau (eds, 2010) Handbook of Natural Language Processing\\n(Second Edition) Chapman &amp; Hall/CRC. 2010.  (Indurkhya &amp; Damerau, 2010) (Dale, Moisl, &amp; Somers, 2000)\\nJurafsky, Daniel and James Martin (2008) Speech and Language Processing (Second Edition).  Prentice Hall.\\n(Jurafsky &amp; Martin, 2008)\\nMitkov, Ruslan (ed, 2003) The Oxford Handbook of Computational Linguistics.  Oxford University Press.\\n(second edition expected in 2010).  (Mitkov, 2002)\\n\\nThe Association for Computational Linguistics is the international organization that\\nrepresents the field of NLP.  The ACL website (http://www.aclweb.org/) hosts many useful resources, including:\\ninformation about international and regional conferences and workshops;\\nthe ACL Wiki with links to hundreds of useful resources;\\nand the ACL Anthology, which contains most of the NLP research literature\\nfrom the past 50+ years, fully indexed and freely downloadable.\\nSome excellent introductory Linguistics textbooks are:\\n[Finegan2007]_, (O\\'Grady et al, 2004), (OSU, 2007).  You might like to consult\\nLanguageLog, a popular linguistics blog with occasional posts that\\nuse the techniques described in this book.\\n\\n\\n8&nbsp;&nbsp;&nbsp;Exercises\\n\\n\\n☼ Try using the Python interpreter as a calculator, and\\ntyping expressions like 12 / (4 + 1).\\n\\n☼ Given an alphabet of 26 letters, there are 26 to the power\\n10, or 26 ** 10, ten-letter strings we can form.  That works out\\nto 141167095653376.  How many hundred-letter strings are possible?\\n\\n☼ The Python multiplication operation can be applied to lists.\\nWhat happens when you type [\\'Monty\\', \\'Python\\'] * 20,\\nor 3 * sent1?\\n\\n☼ Review 1 on\\ncomputing with language.  How many words are there in text2?\\nHow many distinct words are there?\\n\\n☼ Compare the lexical diversity scores for humor\\nand romance fiction in 1.1.  Which genre is\\nmore lexically diverse?\\n\\n☼ Produce a dispersion plot of the four main protagonists in\\nSense and Sensibility: Elinor, Marianne, Edward, and Willoughby.\\nWhat can you observe about the different roles played by the males\\nand females in this novel?  Can you identify the couples?\\n\\n☼ Find the collocations in text5.\\n\\n☼ Consider the following Python expression: len(set(text4)).\\nState the purpose of this expression.  Describe the two steps\\ninvolved in performing this computation.\\n\\n☼ Review 2\\non lists and strings.\\n\\nDefine a string and assign it to a variable, e.g.,\\nmy_string = \\'My String\\' (but put something more interesting in the string).\\nPrint the contents of this variable in two ways, first\\nby simply typing the variable name and pressing enter, then\\nby using the print statement.\\nTry adding the string to itself using my_string + my_string, or multiplying\\nit by a number, e.g., my_string * 3.  Notice that the strings\\nare joined together without any spaces.  How could you fix this?\\n\\n\\n☼ Define a variable my_sent to be a list of words, using\\nthe syntax my_sent = [\"My\", \"sent\"] (but with your own words,\\nor a favorite saying).\\n\\nUse \\' \\'.join(my_sent) to convert this into a string.\\nUse split() to split the string back into the list form\\nyou had to start with.\\n\\n\\n☼ Define several variables containing lists of words, e.g., phrase1,\\nphrase2, and so on.  Join them together in various combinations (using the plus operator)\\nto form whole sentences.  What is the relationship between\\nlen(phrase1 + phrase2) and len(phrase1) + len(phrase2)?\\n\\n☼ Consider the following two expressions, which have the same\\nvalue.  Which one will typically be more relevant in NLP?  Why?\\n\\n\"Monty Python\"[6:12]\\n[\"Monty\", \"Python\"][1]\\n\\n\\n☼ We have seen how to represent a sentence as a list of words, where\\neach word is a sequence of characters.  What does sent1[2][2] do?\\nWhy?  Experiment with other index values.\\n\\n☼ The first sentence of text3 is provided to you in the\\nvariable sent3.  The index of the in sent3 is 1, because sent3[1]\\ngives us \\'the\\'.  What are the indexes of the two other occurrences\\nof this word in sent3?\\n\\n☼ Review the discussion of conditionals in 4.\\nFind all words in the Chat Corpus (text5)\\nstarting with the letter b.  Show them in alphabetical order.\\n\\n☼ Type the expression list(range(10)) at the interpreter prompt.\\nNow try list(range(10, 20)), list(range(10, 20, 2)), and list(range(20, 10, -2)).\\nWe will see a variety of uses for this built-in function in later chapters.\\n\\n◑ Use text9.index() to find the index of the word sunset.\\nYou\\'ll need to insert this word as an argument between the parentheses.\\nBy a process of trial and error, find the slice for the complete sentence that\\ncontains this word.\\n\\n◑ Using list addition, and the set and sorted operations, compute the\\nvocabulary of the sentences sent1 ... sent8.\\n\\n◑ What is the difference between the following two lines?\\nWhich one will give a larger value?  Will this be the case for other texts?\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; sorted(set(w.lower() for w in text1))\\n&gt;&gt;&gt; sorted(w.lower() for w in set(text1))\\n\\n\\n\\n\\n◑ What is the difference between the following two tests:\\nw.isupper() and not w.islower()?\\n\\n◑ Write the slice expression that extracts the last two words of text2.\\n\\n◑ Find all the four-letter words in the Chat Corpus (text5).\\nWith the help of a frequency distribution (FreqDist), show these\\nwords in decreasing order of frequency.\\n\\n◑ Review the discussion of looping with conditions in 4.\\nUse a combination of for and if statements to loop over the words of\\nthe movie script for Monty Python and the Holy Grail (text6)\\nand print all the uppercase words, one per line.\\n\\n◑ Write expressions for finding all words in text6 that\\nmeet the conditions listed below.  The result should be in the form of\\na list of words: [\\'word1\\', \\'word2\\', ...].\\n\\nEnding in ise\\nContaining the letter z\\nContaining the sequence of letters pt\\nHaving all lowercase letters except for an initial capital (i.e., titlecase)\\n\\n\\n◑ Define sent to be the list of words\\n[\\'she\\', \\'sells\\', \\'sea\\', \\'shells\\', \\'by\\', \\'the\\', \\'sea\\', \\'shore\\'].\\nNow write code to perform the following tasks:\\n\\nPrint all words beginning with sh\\nPrint all words longer than four characters\\n\\n\\n◑ What does the following Python code do?  sum(len(w) for w in text1)\\nCan you use it to work out the average word length of a text?\\n\\n◑ Define a function called vocab_size(text) that has a single\\nparameter for the text, and which returns the vocabulary size of the text.\\n\\n◑ Define a function percent(word, text) that calculates\\nhow often a given word occurs in a text, and expresses the result\\nas a percentage.\\n\\n◑ We have been using sets to store vocabularies.  Try the following\\nPython expression: set(sent3) &lt; set(text1).  Experiment with this using\\ndifferent arguments to set().  What does it do?\\nCan you think of a practical application for this?\\n\\n\\n\\n\\nAbout this document...\\nUPDATED FOR NLTK 3.0.\\nThis is a chapter from Natural Language Processing with Python,\\nby Steven Bird, Ewan Klein and Edward Loper,\\nCopyright © 2019 the authors.\\nIt is distributed with the Natural Language Toolkit [http://nltk.org/],\\nVersion 3.0, under the terms of the\\nCreative Commons Attribution-Noncommercial-No Derivative Works 3.0 United States License\\n[http://creativecommons.org/licenses/by-nc-nd/3.0/us/].\\nThis document was built on\\nWed  4 Sep 2019 11:40:48 ACST\\n\\n\\n\\nDocutils System Messages\\n\\nSystem Message: ERROR/3 (ch01.rst2, line 1889); backlink\\nUnknown target name: \"finegan2007\".\\n\\n\\n\\n\\n',\n",
       " '\\n\\n\\n\\nfunction astext(node)\\n{\\n    return node.innerHTML.replace(/(]+)>)/ig,\"\")\\n                         .replace(/&gt;/ig, \">\")\\n                         .replace(/&lt;/ig, \"<\")\\n                         .replace(/&quot;/ig, \\'\"\\')\\n                         .replace(/&amp;/ig, \"&\");\\n}\\n\\nfunction copy_notify(node, bar_color, data)\\n{\\n    // The outer box: relative + inline positioning.\\n    var box1 = document.createElement(\"div\");\\n    box1.style.position = \"relative\";\\n    box1.style.display = \"inline\";\\n    box1.style.top = \"2em\";\\n    box1.style.left = \"1em\";\\n  \\n    // A shadow for fun\\n    var shadow = document.createElement(\"div\");\\n    shadow.style.position = \"absolute\";\\n    shadow.style.left = \"-1.3em\";\\n    shadow.style.top = \"-1.3em\";\\n    shadow.style.background = \"#404040\";\\n    \\n    // The inner box: absolute positioning.\\n    var box2 = document.createElement(\"div\");\\n    box2.style.position = \"relative\";\\n    box2.style.border = \"1px solid #a0a0a0\";\\n    box2.style.left = \"-.2em\";\\n    box2.style.top = \"-.2em\";\\n    box2.style.background = \"white\";\\n    box2.style.padding = \".3em .4em .3em .4em\";\\n    box2.style.fontStyle = \"normal\";\\n    box2.style.background = \"#f0e0e0\";\\n\\n    node.insertBefore(box1, node.childNodes.item(0));\\n    box1.appendChild(shadow);\\n    shadow.appendChild(box2);\\n    box2.innerHTML=\"Copied&nbsp;to&nbsp;the&nbsp;clipboard: \" +\\n                   \"\"+\\n                   data+\"\";\\n    setTimeout(function() { node.removeChild(box1); }, 1000);\\n\\n    var elt = node.parentNode.firstChild;\\n    elt.style.background = \"#ffc0c0\";\\n    setTimeout(function() { elt.style.background = bar_color; }, 200);\\n}\\n\\nfunction copy_codeblock_to_clipboard(node)\\n{\\n    var data = astext(node)+\"\\\\n\";\\n    if (copy_text_to_clipboard(data)) {\\n        copy_notify(node, \"#40a060\", data);\\n    }\\n}\\n\\nfunction copy_doctest_to_clipboard(node)\\n{\\n    var s = astext(node)+\"\\\\n   \";\\n    var data = \"\";\\n\\n    var start = 0;\\n    var end = s.indexOf(\"\\\\n\");\\n    while (end >= 0) {\\n        if (s.substring(start, start+4) == \">>> \") {\\n            data += s.substring(start+4, end+1);\\n        }\\n        else if (s.substring(start, start+4) == \"... \") {\\n            data += s.substring(start+4, end+1);\\n        }\\n        /*\\n        else if (end-start > 1) {\\n            data += \"# \" + s.substring(start, end+1);\\n        }*/\\n        // Grab the next line.\\n        start = end+1;\\n        end = s.indexOf(\"\\\\n\", start);\\n    }\\n    \\n    if (copy_text_to_clipboard(data)) {\\n        copy_notify(node, \"#4060a0\", data);\\n    }\\n}\\n    \\nfunction copy_text_to_clipboard(data)\\n{\\n    if (window.clipboardData) {\\n        window.clipboardData.setData(\"Text\", data);\\n        return true;\\n     }\\n    else if (window.netscape) {\\n        // w/ default firefox settings, permission will be denied for this:\\n        netscape.security.PrivilegeManager\\n                      .enablePrivilege(\"UniversalXPConnect\");\\n    \\n        var clip = Components.classes[\"@mozilla.org/widget/clipboard;1\"]\\n                      .createInstance(Components.interfaces.nsIClipboard);\\n        if (!clip) return;\\n    \\n        var trans = Components.classes[\"@mozilla.org/widget/transferable;1\"]\\n                       .createInstance(Components.interfaces.nsITransferable);\\n        if (!trans) return;\\n    \\n        trans.addDataFlavor(\"text/unicode\");\\n    \\n        var str = new Object();\\n        var len = new Object();\\n    \\n        var str = Components.classes[\"@mozilla.org/supports-string;1\"]\\n                     .createInstance(Components.interfaces.nsISupportsString);\\n        var datacopy=data;\\n        str.data=datacopy;\\n        trans.setTransferData(\"text/unicode\",str,datacopy.length*2);\\n        var clipid=Components.interfaces.nsIClipboard;\\n    \\n        if (!clip) return false;\\n    \\n        clip.setData(trans,null,clipid.kGlobalClipboard);\\n        return true;\\n    }\\n    return false;\\n}\\n//-->\\n\\n\\n\\n\\n\\n\\n2. Accessing Text Corpora and Lexical Resources\\n\\n\\n/*\\n:Author: Edward Loper, James Curran\\n:Copyright: This stylesheet has been placed in the public domain.\\n\\nStylesheet for use with Docutils.\\n\\nThis stylesheet defines new css classes used by NLTK.\\n\\nIt uses a Python syntax highlighting scheme that matches\\nthe colour scheme used by IDLE, which makes it easier for\\nbeginners to check they are typing things in correctly.\\n*/\\n\\n/* Include the standard docutils stylesheet. */\\n@import url(default.css);\\n\\n/* Custom inline roles */\\nspan.placeholder    { font-style: italic; font-family: monospace; }\\nspan.example        { font-style: italic; }\\nspan.emphasis       { font-style: italic; }\\nspan.termdef        { font-weight: bold; }\\n/*span.term           { font-style: italic; }*/\\nspan.category       { font-variant: small-caps; }\\nspan.feature        { font-variant: small-caps; }\\nspan.fval           { font-style: italic; }\\nspan.math           { font-style: italic; }\\nspan.mathit         { font-style: italic; }\\nspan.lex            { font-variant: small-caps; }\\nspan.guide-linecount{ text-align: right; display: block;}\\n\\n/* Python souce code listings */\\nspan.pysrc-prompt   { color: #9b0000; }\\nspan.pysrc-more     { color: #9b00ff; }\\nspan.pysrc-keyword  { color: #e06000; }\\nspan.pysrc-builtin  { color: #940094; }\\nspan.pysrc-string   { color: #00aa00; }\\nspan.pysrc-comment  { color: #ff0000; }\\nspan.pysrc-output   { color: #0000ff; }\\nspan.pysrc-except   { color: #ff0000; }\\nspan.pysrc-defname  { color: #008080; }\\n\\n\\n/* Doctest blocks */\\npre.doctest         { margin: 0; padding: 0; font-weight: bold; }\\ndiv.doctest         { margin: 0 1em 1em 1em; padding: 0; }\\ntable.doctest       { margin: 0; padding: 0;\\n                      border-top: 1px solid gray;\\n                      border-bottom: 1px solid gray; }\\npre.copy-notify     { margin: 0; padding: 0.2em; font-weight: bold;\\n                      background-color: #ffffff; }\\n\\n/* Python source listings */\\ndiv.pylisting       { margin: 0 1em 1em 1em; padding: 0; }\\ntable.pylisting     { margin: 0; padding: 0;\\n                      border-top: 1px solid gray; }\\ntd.caption { border-top: 1px solid black; margin: 0; padding: 0; }\\n.caption-label { font-weight: bold;  }\\ntd.caption p { margin: 0; padding: 0; font-style: normal;}\\n\\ntable tr td.codeblock { \\n  padding: 0.2em ! important; margin: 0;\\n  border-left: 1px solid gray;\\n  border-right: 2px solid gray;\\n  border-top: 0px solid gray;\\n  border-bottom: 1px solid gray;\\n  font-weight: bold; background-color: #eeffee;\\n}\\ntable pre span {\\n  white-space: pre-wrap;\\n}\\ntable tr td.doctest  { \\n  padding: 0.2em; margin: 0;\\n  border-left: 1px solid gray;\\n  border-right: 2px solid gray;\\n  border-top: 0px solid gray;\\n  border-bottom: 1px solid gray;\\n  font-weight: bold; background-color: #eeeeff;\\n}\\n\\ntd.codeblock table tr td.copybar {\\n    background: #40a060; border: 1px solid gray;\\n    font-family: monospace; padding: 0; margin: 0; }\\ntd.doctest table tr td.copybar {\\n    background: #4060a0; border: 1px solid gray;\\n    font-family: monospace; padding: 0; margin: 0; }\\n\\ntd.pysrc { padding-left: 0.5em; }\\n\\nimg.callout { border-width: 0px; }\\n\\ntable.docutils {\\n    border-style: solid;\\n    border-width: 1px;\\n    margin-top: 6px;\\n    border-color: grey;\\n    border-collapse: collapse; }\\n\\ntable.docutils th {\\n    border-style: none;\\n    border-width: 1px;\\n    border-color: grey;\\n    padding: 0 .5em 0 .5em; }\\n\\ntable.docutils td {\\n    border-style: none;\\n    border-width: 1px;\\n    border-color: grey; \\n    padding: 0 .5em 0 .5em; }\\n\\ntable.footnote td { padding: 0; }\\ntable.footnote { border-width: 0; }\\ntable.footnote td { border-width: 0; }\\ntable.footnote th { border-width: 0; }\\n\\ntable.noborder { border-width: 0; }\\n\\ntable.example pre { margin-top: 4px; margin-bottom: 0; }\\n\\n/* For figures & tables */\\np.caption { margin-bottom: 0; }\\ndiv.figure { text-align: center; }\\n\\n/* The index */\\ndiv.index { border: 1px solid black;\\n            background-color: #eeeeee; }\\ndiv.index h1 { padding-left: 0.5em; margin-top: 0.5ex;\\n               border-bottom: 1px solid black; }\\nul.index { margin-left: 0.5em; padding-left: 0; }\\nli.index { list-style-type: none; }\\np.index-heading { font-size: 120%; font-style: italic; margin: 0; }\\nli.index ul { margin-left: 2em; padding-left: 0; }\\n\\n/* \\'Note\\' callouts */\\ndiv.note\\n{\\n  border-right:   #87ceeb 1px solid;\\n  padding-right: 4px;\\n  border-top: #87ceeb 1px solid;\\n  padding-left: 4px;\\n  padding-bottom: 4px;\\n  margin: 2px 5% 10px;\\n  border-left: #87ceeb 1px solid;\\n  padding-top: 4px;\\n  border-bottom: #87ceeb 1px solid;\\n  font-style: normal;\\n  font-family: verdana, arial;\\n  background-color: #b0c4de;\\n}\\n\\ntable.avm { border: 0px solid black; width: 0; }\\ntable.avm tbody tr {border: 0px solid black; }\\ntable.avm tbody tr td { padding: 2px; }\\ntable.avm tbody tr td.avm-key { padding: 5px; font-variant: small-caps; }\\ntable.avm tbody tr td.avm-eq { padding: 5px; }\\ntable.avm tbody tr td.avm-val { padding: 5px; font-style: italic; }\\np.avm-empty { font-style: normal; }\\ntable.avm colgroup col { border: 0px solid black; }\\ntable.avm tbody tr td.avm-topleft \\n    { border-left: 2px solid #000080; border-top: 2px solid #000080; }\\ntable.avm tbody tr td.avm-botleft \\n    { border-left: 2px solid #000080; border-bottom: 2px solid #000080; }\\ntable.avm tbody tr td.avm-topright\\n    { border-right: 2px solid #000080; border-top: 2px solid #000080; }\\ntable.avm tbody tr td.avm-botright\\n    { border-right: 2px solid #000080; border-bottom: 2px solid #000080; }\\ntable.avm tbody tr td.avm-left\\n    { border-left: 2px solid #000080; }\\ntable.avm tbody tr td.avm-right\\n    { border-right: 2px solid #000080; }\\ntable.avm tbody tr td.avm-topbotleft\\n    { border: 2px solid #000080; border-right: 0px solid black; }\\ntable.avm tbody tr td.avm-topbotright\\n    { border: 2px solid #000080; border-left: 0px solid black; }\\ntable.avm tbody tr td.avm-ident\\n    { font-size: 80%; padding: 0; padding-left: 2px; vertical-align: top; }\\n.avm-pointer\\n{ border: 1px solid #008000; padding: 1px; color: #008000; \\n  background: #c0ffc0; font-style: normal; }\\n\\ntable.gloss { border: 0px solid black; width: 0; }\\ntable.gloss tbody tr { border: 0px solid black; }\\ntable.gloss tbody tr td { border: 0px solid black; }\\ntable.gloss colgroup col { border: 0px solid black; }\\ntable.gloss p { margin: 0; padding: 0; }\\n\\ntable.rst-example { border: 1px solid black; }\\ntable.rst-example tbody tr td { background: #eeeeee; }\\ntable.rst-example thead tr th { background: #c0ffff; }\\ntd.rst-raw { width: 0; }\\n\\n/* Used by nltk.org/doc/test: */\\ndiv.doctest-list { text-align: center; }\\ntable.doctest-list { border: 1px solid black;\\n  margin-left: auto; margin-right: auto;\\n}\\ntable.doctest-list tbody tr td { background: #eeeeee;\\n  border: 1px solid #cccccc; text-align: left; }\\ntable.doctest-list thead tr th { background: #304050; color: #ffffff;\\n  border: 1px solid #000000;}\\ntable.doctest-list thead tr a { color: #ffffff; }\\nspan.doctest-passed { color: #008000; }\\nspan.doctest-failed { color: #800000; }\\n\\n\\n\\n\\n\\n\\n2. Accessing Text Corpora and Lexical Resources\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n<!-- Grammatical Category - e.g. NP and verb as technical terms\\n.. role:: gc\\n   :class: category -->\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n<!-- TODO: discussion of resource rich/poor languages in section on corpora in other languages\\nnumber of languages in the world, Ethnologue, etc -->\\n\\n\\n\\n<!-- TODO: The Lexicon:\\n* words are more than just the output of tokenization\\n* explore what it means for a document to contain a word\\n* ways this can fail: mis-spelling; different endings; synonyms; homonyms\\n* type vs token distinction; connection of types to lemmas (cf issue 201)\\n* concept of \"word\", many-to-many mapping between forms and meanings\\n* why the lexicon is an open set, lexical productivity and challenge for NLP\\n* morphology -->\\n<!-- Exploratory data analysis, a technique for learning about a specific\\nlinguistic pattern, consists of four steps: search, categorization,\\ncounting, and hypothesis refinement. -->\\n\\n<!-- TODO: explain reload() in connection with redefining the lexical_diversity function\\n(suggested in issue 170) -->\\n\\nPractical work in Natural Language Processing typically uses\\nlarge bodies of linguistic data, or corpora.\\nThe goal of this chapter is to answer the following questions:\\n\\nWhat are some useful text corpora and lexical resources, and how can we access them with Python?\\nWhich Python constructs are most helpful for this work?\\nHow do we avoid repeating ourselves when writing Python code?\\n\\nThis chapter continues to present programming concepts by example, in the\\ncontext of a linguistic processing task.  We will wait until later before\\nexploring each Python construct systematically.  Don\\'t worry if you see\\nan example that contains something unfamiliar; simply try it out and see\\nwhat it does, and — if you\\'re game — modify it by substituting\\nsome part of the code with a different text or word.  This way you will\\nassociate a task with a programming idiom, and learn the hows and whys later.\\n\\n1&nbsp;&nbsp;&nbsp;Accessing Text Corpora\\nAs just mentioned, a text corpus is a large body of text. Many\\ncorpora are designed to contain a careful balance of material\\nin one or more genres.  We examined some small text collections in\\n1., such as the speeches known as the US Presidential\\nInaugural Addresses.  This particular corpus actually contains dozens\\nof individual texts — one per address — but for convenience\\nwe glued them end-to-end and treated them as a single text.\\n1. also used various pre-defined texts that\\nwe accessed by typing from nltk.book import *.  However, since we want\\nto be able to work with other texts, this section examines a\\nvariety of text corpora. We\\'ll see how\\nto select individual texts, and how to work with them.\\n\\n1.1&nbsp;&nbsp;&nbsp;Gutenberg Corpus\\nNLTK includes a small selection of texts from the Project Gutenberg\\nelectronic text archive, which contains\\nsome 25,000 free electronic books, hosted at http://www.gutenberg.org/.  We begin\\nby getting the Python interpreter to load the NLTK package,\\nthen ask to see nltk.corpus.gutenberg.fileids(), the file identifiers in\\nthis corpus:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; import nltk\\n&gt;&gt;&gt; nltk.corpus.gutenberg.fileids()\\n[\\'austen-emma.txt\\', \\'austen-persuasion.txt\\', \\'austen-sense.txt\\', \\'bible-kjv.txt\\',\\n\\'blake-poems.txt\\', \\'bryant-stories.txt\\', \\'burgess-busterbrown.txt\\',\\n\\'carroll-alice.txt\\', \\'chesterton-ball.txt\\', \\'chesterton-brown.txt\\',\\n\\'chesterton-thursday.txt\\', \\'edgeworth-parents.txt\\', \\'melville-moby_dick.txt\\',\\n\\'milton-paradise.txt\\', \\'shakespeare-caesar.txt\\', \\'shakespeare-hamlet.txt\\',\\n\\'shakespeare-macbeth.txt\\', \\'whitman-leaves.txt\\']\\n\\n\\n\\nLet\\'s pick out the first of these texts — Emma by Jane Austen — and\\ngive it a short name, emma, then find out how many words it contains:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; emma = nltk.corpus.gutenberg.words(\\'austen-emma.txt\\')\\n&gt;&gt;&gt; len(emma)\\n192427\\n\\n\\n\\n\\nNote\\nIn 1, we showed how you\\ncould carry out concordancing of a text such as text1 with the\\ncommand text1.concordance(). However, this assumes that you are\\nusing one of the nine texts obtained as a result of doing from\\nnltk.book import *. Now that you have started examining data from\\nnltk.corpus, as in the previous example, you have to employ the\\nfollowing pair of statements to perform concordancing and other\\ntasks from 1:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; emma = nltk.Text(nltk.corpus.gutenberg.words(\\'austen-emma.txt\\'))\\n&gt;&gt;&gt; emma.concordance(\"surprize\")\\n\\n\\n\\n\\nWhen we defined emma, we invoked the words() function of the gutenberg\\nobject in NLTK\\'s corpus package.\\nBut since it is cumbersome to type such long names all the time, Python provides\\nanother version of the import statement, as follows:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; from nltk.corpus import gutenberg\\n&gt;&gt;&gt; gutenberg.fileids()\\n[\\'austen-emma.txt\\', \\'austen-persuasion.txt\\', \\'austen-sense.txt\\', ...]\\n&gt;&gt;&gt; emma = gutenberg.words(\\'austen-emma.txt\\')\\n\\n\\n\\nLet\\'s write a short program to display other information about each\\ntext, by looping over all the values of fileid corresponding to\\nthe gutenberg file identifiers listed earlier and then computing\\nstatistics for each text.  For a compact output display, we will round\\neach number to the nearest integer, using round().\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; for fileid in gutenberg.fileids():\\n...     num_chars = len(gutenberg.raw(fileid)) \\n...     num_words = len(gutenberg.words(fileid))\\n...     num_sents = len(gutenberg.sents(fileid))\\n...     num_vocab = len(set(w.lower() for w in gutenberg.words(fileid)))\\n...     print(round(num_chars/num_words), round(num_words/num_sents), round(num_words/num_vocab), fileid)\\n...\\n5 25 26 austen-emma.txt\\n5 26 17 austen-persuasion.txt\\n5 28 22 austen-sense.txt\\n4 34 79 bible-kjv.txt\\n5 19 5 blake-poems.txt\\n4 19 14 bryant-stories.txt\\n4 18 12 burgess-busterbrown.txt\\n4 20 13 carroll-alice.txt\\n5 20 12 chesterton-ball.txt\\n5 23 11 chesterton-brown.txt\\n5 18 11 chesterton-thursday.txt\\n4 21 25 edgeworth-parents.txt\\n5 26 15 melville-moby_dick.txt\\n5 52 11 milton-paradise.txt\\n4 12 9 shakespeare-caesar.txt\\n4 12 8 shakespeare-hamlet.txt\\n4 12 7 shakespeare-macbeth.txt\\n5 36 12 whitman-leaves.txt\\n\\n\\n\\nThis program displays three statistics for each text:\\naverage word length, average sentence length, and the number of times each vocabulary\\nitem appears in the text on average (our lexical diversity score).\\nObserve that average word length appears to be a general property of English, since\\nit has a recurrent value of 4.  (In fact, the average word length is really\\n3 not 4, since the num_chars variable counts space characters.)\\nBy contrast average sentence length and lexical diversity\\nappear to be characteristics of particular authors.\\nThe previous example also showed how we can access the \"raw\" text of the book ,\\nnot split up into tokens.  The raw() function gives us the contents of the file\\nwithout any linguistic processing.  So, for example, len(gutenberg.raw(\\'blake-poems.txt\\'))\\ntells us how many letters occur in the text, including the spaces between words.\\nThe sents() function divides the text up into its sentences, where each sentence is\\na list of words:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; macbeth_sentences = gutenberg.sents(\\'shakespeare-macbeth.txt\\')\\n&gt;&gt;&gt; macbeth_sentences\\n[[\\'[\\', \\'The\\', \\'Tragedie\\', \\'of\\', \\'Macbeth\\', \\'by\\', \\'William\\', \\'Shakespeare\\',\\n\\'1603\\', \\']\\'], [\\'Actus\\', \\'Primus\\', \\'.\\'], ...]\\n&gt;&gt;&gt; macbeth_sentences[1116]\\n[\\'Double\\', \\',\\', \\'double\\', \\',\\', \\'toile\\', \\'and\\', \\'trouble\\', \\';\\',\\n\\'Fire\\', \\'burne\\', \\',\\', \\'and\\', \\'Cauldron\\', \\'bubble\\']\\n&gt;&gt;&gt; longest_len = max(len(s) for s in macbeth_sentences)\\n&gt;&gt;&gt; [s for s in macbeth_sentences if len(s) == longest_len]\\n[[\\'Doubtfull\\', \\'it\\', \\'stood\\', \\',\\', \\'As\\', \\'two\\', \\'spent\\', \\'Swimmers\\', \\',\\', \\'that\\',\\n\\'doe\\', \\'cling\\', \\'together\\', \\',\\', \\'And\\', \\'choake\\', \\'their\\', \\'Art\\', \\':\\', \\'The\\',\\n\\'mercilesse\\', \\'Macdonwald\\', ...]]\\n\\n\\n\\n\\nNote\\nMost NLTK corpus readers include a variety of access methods\\napart from words(), raw(), and sents().  Richer\\nlinguistic content is available from some corpora, such as part-of-speech\\ntags, dialogue tags, syntactic trees, and so forth; we will see these\\nin later chapters.\\n\\n\\n\\n1.2&nbsp;&nbsp;&nbsp;Web and Chat Text\\nAlthough Project Gutenberg contains thousands of books, it represents established\\nliterature.  It is important to consider less formal language as well.  NLTK\\'s\\nsmall collection of web text includes content from a Firefox discussion forum,\\nconversations overheard in New York, the movie script of Pirates of the Carribean,\\npersonal advertisements, and wine reviews:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; from nltk.corpus import webtext\\n&gt;&gt;&gt; for fileid in webtext.fileids():\\n...     print(fileid, webtext.raw(fileid)[:65], \\'...\\')\\n...\\nfirefox.txt Cookie Manager: \"Don\\'t allow sites that set removed cookies to se...\\ngrail.txt SCENE 1: [wind] [clop clop clop] KING ARTHUR: Whoa there!  [clop...\\noverheard.txt White guy: So, do you have any plans for this evening? Asian girl...\\npirates.txt PIRATES OF THE CARRIBEAN: DEAD MAN\\'S CHEST, by Ted Elliott &amp; Terr...\\nsingles.txt 25 SEXY MALE, seeks attrac older single lady, for discreet encoun...\\nwine.txt Lovely delicate, fragrant Rhone wine. Polished leather and strawb...\\n\\n\\n\\nThere is also a corpus of instant messaging chat sessions, originally collected\\nby the Naval Postgraduate School for research on automatic detection of Internet predators.\\nThe corpus contains over 10,000 posts, anonymized by replacing usernames with generic\\nnames of the form \"UserNNN\", and manually edited to remove any other identifying information.\\nThe corpus is organized into 15 files, where each file contains several hundred posts\\ncollected on a given date, for an age-specific chatroom (teens, 20s, 30s, 40s, plus a\\ngeneric adults chatroom).  The filename contains the date, chatroom,\\nand number of posts; e.g., 10-19-20s_706posts.xml contains 706 posts gathered from\\nthe 20s chat room on 10/19/2006.\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; from nltk.corpus import nps_chat\\n&gt;&gt;&gt; chatroom = nps_chat.posts(\\'10-19-20s_706posts.xml\\')\\n&gt;&gt;&gt; chatroom[123]\\n[\\'i\\', \\'do\\', \"n\\'t\", \\'want\\', \\'hot\\', \\'pics\\', \\'of\\', \\'a\\', \\'female\\', \\',\\',\\n\\'I\\', \\'can\\', \\'look\\', \\'in\\', \\'a\\', \\'mirror\\', \\'.\\']\\n\\n\\n\\n\\n\\n1.3&nbsp;&nbsp;&nbsp;Brown Corpus\\nThe Brown Corpus was the first million-word electronic\\ncorpus of English, created in 1961 at Brown University.\\nThis corpus contains text from 500 sources, and the sources\\nhave been categorized by genre, such as news, editorial, and so on.\\n1.1 gives an example of each genre\\n(for a complete list, see http://icame.uib.no/brown/bcm-los.html).\\nTable 1.1: Example Document for Each Section of the Brown Corpus\\n\\n\\n\\n\\n\\n\\n\\nID\\nFile\\nGenre\\nDescription\\n\\n\\n\\nA16\\nca16\\nnews\\nChicago Tribune: Society Reportage\\n\\nB02\\ncb02\\neditorial\\nChristian Science Monitor: Editorials\\n\\nC17\\ncc17\\nreviews\\nTime Magazine: Reviews\\n\\nD12\\ncd12\\nreligion\\nUnderwood: Probing the Ethics of Realtors\\n\\nE36\\nce36\\nhobbies\\nNorling: Renting a Car in Europe\\n\\nF25\\ncf25\\nlore\\nBoroff: Jewish Teenage Culture\\n\\nG22\\ncg22\\nbelles_lettres\\nReiner: Coping with Runaway Technology\\n\\nH15\\nch15\\ngovernment\\nUS Office of Civil and Defence Mobilization: The Family Fallout Shelter\\n\\nJ17\\ncj19\\nlearned\\nMosteller: Probability with Statistical Applications\\n\\nK04\\nck04\\nfiction\\nW.E.B. Du Bois: Worlds of Color\\n\\nL13\\ncl13\\nmystery\\nHitchens: Footsteps in the Night\\n\\nM01\\ncm01\\nscience_fiction\\nHeinlein: Stranger in a Strange Land\\n\\nN14\\ncn15\\nadventure\\nField: Rattlesnake Ridge\\n\\nP12\\ncp12\\nromance\\nCallaghan: A Passion in Rome\\n\\nR06\\ncr06\\nhumor\\nThurber: The Future, If Any, of Comedy\\n\\n\\n\\n\\n\\nWe can access the corpus as a list of words, or a list of sentences (where each sentence\\nis itself just a list of words).  We can optionally specify particular categories or files to read:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; from nltk.corpus import brown\\n&gt;&gt;&gt; brown.categories()\\n[\\'adventure\\', \\'belles_lettres\\', \\'editorial\\', \\'fiction\\', \\'government\\', \\'hobbies\\',\\n\\'humor\\', \\'learned\\', \\'lore\\', \\'mystery\\', \\'news\\', \\'religion\\', \\'reviews\\', \\'romance\\',\\n\\'science_fiction\\']\\n&gt;&gt;&gt; brown.words(categories=\\'news\\')\\n[\\'The\\', \\'Fulton\\', \\'County\\', \\'Grand\\', \\'Jury\\', \\'said\\', ...]\\n&gt;&gt;&gt; brown.words(fileids=[\\'cg22\\'])\\n[\\'Does\\', \\'our\\', \\'society\\', \\'have\\', \\'a\\', \\'runaway\\', \\',\\', ...]\\n&gt;&gt;&gt; brown.sents(categories=[\\'news\\', \\'editorial\\', \\'reviews\\'])\\n[[\\'The\\', \\'Fulton\\', \\'County\\'...], [\\'The\\', \\'jury\\', \\'further\\'...], ...]\\n\\n\\n\\nThe Brown Corpus is a convenient resource for studying systematic differences between\\ngenres, a kind of linguistic inquiry known as stylistics.\\nLet\\'s compare genres in their usage of modal verbs.  The first step\\nis to produce the counts for a particular genre.  Remember to\\nimport nltk before doing the following:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; from nltk.corpus import brown\\n&gt;&gt;&gt; news_text = brown.words(categories=\\'news\\')\\n&gt;&gt;&gt; fdist = nltk.FreqDist(w.lower() for w in news_text)\\n&gt;&gt;&gt; modals = [\\'can\\', \\'could\\', \\'may\\', \\'might\\', \\'must\\', \\'will\\']\\n&gt;&gt;&gt; for m in modals:\\n...     print(m + \\':\\', fdist[m], end=\\' \\')\\n...\\ncan: 94 could: 87 may: 93 might: 38 must: 53 will: 389\\n\\n\\n\\n\\nNote\\nWe need to include end=\\' \\' in order for the print function to\\nput its output on a single line.\\n\\n\\nNote\\nYour Turn:\\nChoose a different section of the Brown Corpus, and adapt the previous\\nexample to count a selection of wh words, such as what,\\nwhen, where, who, and why.\\n\\nNext, we need to obtain counts for each genre of interest.  We\\'ll use\\nNLTK\\'s support for conditional frequency distributions. These are\\npresented systematically in 2,\\nwhere we also unpick the following code line by line. For the moment,\\nyou can ignore the details and just concentrate on the output.\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; cfd = nltk.ConditionalFreqDist(\\n...           (genre, word)\\n...           for genre in brown.categories()\\n...           for word in brown.words(categories=genre))\\n&gt;&gt;&gt; genres = [\\'news\\', \\'religion\\', \\'hobbies\\', \\'science_fiction\\', \\'romance\\', \\'humor\\']\\n&gt;&gt;&gt; modals = [\\'can\\', \\'could\\', \\'may\\', \\'might\\', \\'must\\', \\'will\\']\\n&gt;&gt;&gt; cfd.tabulate(conditions=genres, samples=modals)\\n                 can could  may might must will\\n           news   93   86   66   38   50  389\\n       religion   82   59   78   12   54   71\\n        hobbies  268   58  131   22   83  264\\nscience_fiction   16   49    4   12    8   16\\n        romance   74  193   11   51   45   43\\n          humor   16   30    8    8    9   13\\n\\n\\n\\nObserve that the most frequent modal in the news genre is will,\\nwhile the most frequent modal in the romance genre is could.\\nWould you have predicted this?  The idea that word counts\\nmight distinguish genres will be taken up again in chap-data-intensive.\\n\\n\\n\\n1.4&nbsp;&nbsp;&nbsp;Reuters Corpus\\nThe Reuters Corpus contains 10,788 news documents totaling 1.3 million words.\\nThe documents have been classified into 90 topics, and grouped\\ninto two sets, called \"training\" and \"test\"; thus, the text with\\nfileid \\'test/14826\\' is a document drawn from the test set. This split is for\\ntraining and testing algorithms that automatically detect the topic of a document,\\nas we will see in chap-data-intensive.\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; from nltk.corpus import reuters\\n&gt;&gt;&gt; reuters.fileids()\\n[\\'test/14826\\', \\'test/14828\\', \\'test/14829\\', \\'test/14832\\', ...]\\n&gt;&gt;&gt; reuters.categories()\\n[\\'acq\\', \\'alum\\', \\'barley\\', \\'bop\\', \\'carcass\\', \\'castor-oil\\', \\'cocoa\\',\\n\\'coconut\\', \\'coconut-oil\\', \\'coffee\\', \\'copper\\', \\'copra-cake\\', \\'corn\\',\\n\\'cotton\\', \\'cotton-oil\\', \\'cpi\\', \\'cpu\\', \\'crude\\', \\'dfl\\', \\'dlr\\', ...]\\n\\n\\n\\nUnlike the Brown Corpus, categories in the Reuters corpus overlap with\\neach other, simply because a news story often covers multiple topics.\\nWe can ask for the topics covered by one or more documents, or for the\\ndocuments included in one or more categories. For convenience, the\\ncorpus methods accept a single fileid or a list of fileids.\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; reuters.categories(\\'training/9865\\')\\n[\\'barley\\', \\'corn\\', \\'grain\\', \\'wheat\\']\\n&gt;&gt;&gt; reuters.categories([\\'training/9865\\', \\'training/9880\\'])\\n[\\'barley\\', \\'corn\\', \\'grain\\', \\'money-fx\\', \\'wheat\\']\\n&gt;&gt;&gt; reuters.fileids(\\'barley\\')\\n[\\'test/15618\\', \\'test/15649\\', \\'test/15676\\', \\'test/15728\\', \\'test/15871\\', ...]\\n&gt;&gt;&gt; reuters.fileids([\\'barley\\', \\'corn\\'])\\n[\\'test/14832\\', \\'test/14858\\', \\'test/15033\\', \\'test/15043\\', \\'test/15106\\',\\n\\'test/15287\\', \\'test/15341\\', \\'test/15618\\', \\'test/15648\\', \\'test/15649\\', ...]\\n\\n\\n\\nSimilarly, we can specify the words or sentences we want in terms of\\nfiles or categories. The first handful of words in each of these texts are the\\ntitles, which by convention are stored as upper case.\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; reuters.words(\\'training/9865\\')[:14]\\n[\\'FRENCH\\', \\'FREE\\', \\'MARKET\\', \\'CEREAL\\', \\'EXPORT\\', \\'BIDS\\',\\n\\'DETAILED\\', \\'French\\', \\'operators\\', \\'have\\', \\'requested\\', \\'licences\\', \\'to\\', \\'export\\']\\n&gt;&gt;&gt; reuters.words([\\'training/9865\\', \\'training/9880\\'])\\n[\\'FRENCH\\', \\'FREE\\', \\'MARKET\\', \\'CEREAL\\', \\'EXPORT\\', ...]\\n&gt;&gt;&gt; reuters.words(categories=\\'barley\\')\\n[\\'FRENCH\\', \\'FREE\\', \\'MARKET\\', \\'CEREAL\\', \\'EXPORT\\', ...]\\n&gt;&gt;&gt; reuters.words(categories=[\\'barley\\', \\'corn\\'])\\n[\\'THAI\\', \\'TRADE\\', \\'DEFICIT\\', \\'WIDENS\\', \\'IN\\', \\'FIRST\\', ...]\\n\\n\\n\\n\\n\\n1.5&nbsp;&nbsp;&nbsp;Inaugural Address Corpus\\n\\nIn 1, we looked at\\nthe Inaugural Address Corpus,\\nbut treated it as a single text.  The graph in fig-inaugural\\nused \"word offset\" as one of the axes; this is the numerical index of the\\nword in the corpus, counting from the first word of the first address.\\nHowever, the corpus is actually a collection of 55 texts, one for each presidential address.\\nAn interesting property of this collection is its time dimension:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; from nltk.corpus import inaugural\\n&gt;&gt;&gt; inaugural.fileids()\\n[\\'1789-Washington.txt\\', \\'1793-Washington.txt\\', \\'1797-Adams.txt\\', ...]\\n&gt;&gt;&gt; [fileid[:4] for fileid in inaugural.fileids()]\\n[\\'1789\\', \\'1793\\', \\'1797\\', \\'1801\\', \\'1805\\', \\'1809\\', \\'1813\\', \\'1817\\', \\'1821\\', ...]\\n\\n\\n\\nNotice that the year of each text appears in its filename.  To get the year\\nout of the filename, we extracted the first four characters, using fileid[:4].\\nLet\\'s look at how the words America and citizen are used over time.\\nThe following code\\nconverts the words in the Inaugural corpus\\nto lowercase using w.lower() ,\\nthen checks if they start with either of the \"targets\"\\namerica or citizen using startswith() .\\nThus it will count words like American\\'s and Citizens.\\nWe\\'ll learn about conditional frequency distributions in\\n2; for now just consider\\nthe output, shown in 1.1.\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; cfd = nltk.ConditionalFreqDist(\\n...           (target, fileid[:4])\\n...           for fileid in inaugural.fileids()\\n...           for w in inaugural.words(fileid)\\n...           for target in [\\'america\\', \\'citizen\\']\\n...           if w.lower().startswith(target)) \\n&gt;&gt;&gt; cfd.plot()\\n\\n\\n\\n\\n\\nFigure 1.1: Plot of a Conditional Frequency Distribution: all words in the Inaugural Address\\nCorpus that begin with america or citizen are counted; separate counts\\nare kept for each address; these are plotted so that trends in usage over time can\\nbe observed; counts are not normalized for document length.\\n\\n\\n\\n1.6&nbsp;&nbsp;&nbsp;Annotated Text Corpora\\nMany text corpora contain linguistic annotations, representing POS tags,\\nnamed entities, syntactic structures, semantic roles, and so forth.  NLTK provides\\nconvenient ways to access several of these corpora, and has data packages containing corpora\\nand corpus samples, freely downloadable for use in teaching and research.\\n1.2 lists some of the corpora.  For information about\\ndownloading them, see http://nltk.org/data.\\nFor more examples of how to access NLTK corpora,\\nplease consult the Corpus HOWTO at http://nltk.org/howto.\\nTable 1.2: Some of the Corpora and Corpus Samples Distributed with NLTK: For information about downloading\\nand using them, please consult the NLTK website.\\n\\n\\n\\n\\n\\n\\nCorpus\\nCompiler\\nContents\\n\\n\\n\\nBrown Corpus\\nFrancis, Kucera\\n15 genres, 1.15M words, tagged, categorized\\n\\nCESS Treebanks\\nCLiC-UB\\n1M words, tagged and parsed (Catalan, Spanish)\\n\\nChat-80 Data Files\\nPereira &amp; Warren\\nWorld Geographic Database\\n\\nCMU Pronouncing Dictionary\\nCMU\\n127k entries\\n\\nCoNLL 2000 Chunking Data\\nCoNLL\\n270k words, tagged and chunked\\n\\nCoNLL 2002 Named Entity\\nCoNLL\\n700k words, pos- and named-entity-tagged (Dutch, Spanish)\\n\\nCoNLL 2007 Dependency Treebanks (sel)\\nCoNLL\\n150k words, dependency parsed (Basque, Catalan)\\n\\nDependency Treebank\\nNarad\\nDependency parsed version of Penn Treebank sample\\n\\nFrameNet\\nFillmore, Baker et al\\n10k word senses, 170k manually annotated sentences\\n\\nFloresta Treebank\\nDiana Santos et al\\n9k sentences, tagged and parsed (Portuguese)\\n\\nGazetteer Lists\\nVarious\\nLists of cities and countries\\n\\nGenesis Corpus\\nMisc web sources\\n6 texts, 200k words, 6 languages\\n\\nGutenberg (selections)\\nHart, Newby, et al\\n18 texts, 2M words\\n\\nInaugural Address Corpus\\nCSpan\\nUS Presidential Inaugural Addresses (1789-present)\\n\\nIndian POS-Tagged Corpus\\nKumaran et al\\n60k words, tagged (Bangla, Hindi, Marathi, Telugu)\\n\\nMacMorpho Corpus\\nNILC, USP, Brazil\\n1M words, tagged (Brazilian Portuguese)\\n\\nMovie Reviews\\nPang, Lee\\n2k movie reviews with sentiment polarity classification\\n\\nNames Corpus\\nKantrowitz, Ross\\n8k male and female names\\n\\nNIST 1999 Info Extr (selections)\\nGarofolo\\n63k words, newswire and named-entity SGML markup\\n\\nNombank\\nMeyers\\n115k propositions, 1400 noun frames\\n\\nNPS Chat Corpus\\nForsyth, Martell\\n10k IM chat posts, POS-tagged and dialogue-act tagged\\n\\nOpen Multilingual WordNet\\nBond et al\\n15 languages, aligned to English WordNet\\n\\nPP Attachment Corpus\\nRatnaparkhi\\n28k prepositional phrases, tagged as noun or verb modifiers\\n\\nProposition Bank\\nPalmer\\n113k propositions, 3300 verb frames\\n\\nQuestion Classification\\nLi, Roth\\n6k questions, categorized\\n\\nReuters Corpus\\nReuters\\n1.3M words, 10k news documents, categorized\\n\\nRoget\\'s Thesaurus\\nProject Gutenberg\\n200k words, formatted text\\n\\nRTE Textual Entailment\\nDagan et al\\n8k sentence pairs, categorized\\n\\nSEMCOR\\nRus, Mihalcea\\n880k words, part-of-speech and sense tagged\\n\\nSenseval 2 Corpus\\nPedersen\\n600k words, part-of-speech and sense tagged\\n\\nSentiWordNet\\nEsuli, Sebastiani\\nsentiment scores for 145k WordNet synonym sets\\n\\nShakespeare texts (selections)\\nBosak\\n8 books in XML format\\n\\nState of the Union Corpus\\nCSPAN\\n485k words, formatted text\\n\\nStopwords Corpus\\nPorter et al\\n2,400 stopwords for 11 languages\\n\\nSwadesh Corpus\\nWiktionary\\ncomparative wordlists in 24 languages\\n\\nSwitchboard Corpus (selections)\\nLDC\\n36 phonecalls, transcribed, parsed\\n\\nUniv Decl of Human Rights\\nUnited Nations\\n480k words, 300+ languages\\n\\nPenn Treebank (selections)\\nLDC\\n40k words, tagged and parsed\\n\\nTIMIT Corpus (selections)\\nNIST/LDC\\naudio files and transcripts for 16 speakers\\n\\nVerbNet 2.1\\nPalmer et al\\n5k verbs, hierarchically organized, linked to WordNet\\n\\nWordlist Corpus\\nOpenOffice.org et al\\n960k words and 20k affixes for 8 languages\\n\\nWordNet 3.0 (English)\\nMiller, Fellbaum\\n145k synonym sets\\n\\n\\n\\n\\n\\n\\n\\n1.7&nbsp;&nbsp;&nbsp;Corpora in Other Languages\\nNLTK comes with corpora for many languages, though in some cases\\nyou will need to learn how to manipulate character encodings in Python\\nbefore using these corpora (see 3.3).\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; nltk.corpus.cess_esp.words()\\n[\\'El\\', \\'grupo\\', \\'estatal\\', \\'Electricit\\\\xe9_de_France\\', ...]\\n&gt;&gt;&gt; nltk.corpus.floresta.words()\\n[\\'Um\\', \\'revivalismo\\', \\'refrescante\\', \\'O\\', \\'7_e_Meio\\', ...]\\n&gt;&gt;&gt; nltk.corpus.indian.words(\\'hindi.pos\\')\\n[\\'पूर्ण\\', \\'प्रतिबंध\\', \\'हटाओ\\', \\':\\', \\'इराक\\', \\'संयुक्त\\', ...]\\n&gt;&gt;&gt; nltk.corpus.udhr.fileids()\\n[\\'Abkhaz-Cyrillic+Abkh\\', \\'Abkhaz-UTF8\\', \\'Achehnese-Latin1\\', \\'Achuar-Shiwiar-Latin1\\',\\n\\'Adja-UTF8\\', \\'Afaan_Oromo_Oromiffa-Latin1\\', \\'Afrikaans-Latin1\\', \\'Aguaruna-Latin1\\',\\n\\'Akuapem_Twi-UTF8\\', \\'Albanian_Shqip-Latin1\\', \\'Amahuaca\\', \\'Amahuaca-Latin1\\', ...]\\n&gt;&gt;&gt; nltk.corpus.udhr.words(\\'Javanese-Latin1\\')[11:]\\n[\\'Saben\\', \\'umat\\', \\'manungsa\\', \\'lair\\', \\'kanthi\\', \\'hak\\', ...]\\n\\n\\n\\nThe last of these corpora, udhr, contains the Universal Declaration of Human Rights\\nin over 300 languages.  The fileids for this corpus include\\ninformation about the character encoding used in the file,\\nsuch as UTF8 or Latin1.\\nLet\\'s use a conditional frequency distribution to examine the differences in word lengths\\nfor a selection of languages included in the udhr corpus.\\nThe output is shown in 1.2 (run the program yourself to see a color plot).\\nNote that True and False are Python\\'s built-in boolean values.\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; from nltk.corpus import udhr\\n&gt;&gt;&gt; languages = [\\'Chickasaw\\', \\'English\\', \\'German_Deutsch\\',\\n...     \\'Greenlandic_Inuktikut\\', \\'Hungarian_Magyar\\', \\'Ibibio_Efik\\']\\n&gt;&gt;&gt; cfd = nltk.ConditionalFreqDist(\\n...           (lang, len(word))\\n...           for lang in languages\\n...           for word in udhr.words(lang + \\'-Latin1\\'))\\n&gt;&gt;&gt; cfd.plot(cumulative=True)\\n\\n\\n\\n\\n\\nFigure 1.2: Cumulative Word Length Distributions:\\nSix translations of the Universal Declaration of Human Rights are processed;\\nthis graph shows that words having 5 or fewer letters account for about\\n80% of Ibibio text, 60% of German text, and 25% of Inuktitut text.\\n\\n\\n\\nNote\\nYour Turn:\\nPick a language of interest in udhr.fileids(), and define a variable\\nraw_text = udhr.raw(Language-Latin1).  Now plot a frequency\\ndistribution of the letters of the text using nltk.FreqDist(raw_text).plot().\\n\\nUnfortunately, for many languages, substantial corpora are not yet available.  Often there is\\ninsufficient government or industrial support for developing language resources, and individual\\nefforts are piecemeal and hard to discover or re-use.  Some languages have no\\nestablished writing system, or are endangered.  (See 7\\nfor suggestions on how to locate language resources.)\\n\\n\\n1.8&nbsp;&nbsp;&nbsp;Text Corpus Structure\\nWe have seen a variety of corpus structures so far; these are\\nsummarized in 1.3.\\nThe simplest kind lacks any structure: it is just a collection of texts.\\nOften, texts are grouped into categories that might correspond to genre, source, author, language, etc.\\nSometimes these categories overlap, notably in the case of topical categories as a text can be\\nrelevant to more than one topic.  Occasionally, text collections have temporal structure,\\nnews collections being the most common example.\\n\\n\\nFigure 1.3: Common Structures for Text Corpora: The simplest kind of corpus is a collection\\nof isolated texts with no particular organization; some corpora are structured\\ninto categories like genre (Brown Corpus); some categorizations overlap, such as\\ntopic categories (Reuters Corpus); other corpora represent language use over time\\n(Inaugural Address Corpus).\\n\\nTable 1.3: Basic Corpus Functionality defined in NLTK: more documentation can be found using\\nhelp(nltk.corpus.reader) and by reading the online Corpus HOWTO at http://nltk.org/howto.\\n\\n\\n\\n\\n\\nExample\\nDescription\\n\\n\\n\\nfileids()\\nthe files of the corpus\\n\\nfileids([categories])\\nthe files of the corpus corresponding to these categories\\n\\ncategories()\\nthe categories of the corpus\\n\\ncategories([fileids])\\nthe categories of the corpus corresponding to these files\\n\\nraw()\\nthe raw content of the corpus\\n\\nraw(fileids=[f1,f2,f3])\\nthe raw content of the specified files\\n\\nraw(categories=[c1,c2])\\nthe raw content of the specified categories\\n\\nwords()\\nthe words of the whole corpus\\n\\nwords(fileids=[f1,f2,f3])\\nthe words of the specified fileids\\n\\nwords(categories=[c1,c2])\\nthe words of the specified categories\\n\\nsents()\\nthe sentences of the whole corpus\\n\\nsents(fileids=[f1,f2,f3])\\nthe sentences of the specified fileids\\n\\nsents(categories=[c1,c2])\\nthe sentences of the specified categories\\n\\nabspath(fileid)\\nthe location of the given file on disk\\n\\nencoding(fileid)\\nthe encoding of the file (if known)\\n\\nopen(fileid)\\nopen a stream for reading the given corpus file\\n\\nroot\\nif the path to the root of locally installed corpus\\n\\nreadme()\\nthe contents of the README file of the corpus\\n\\n\\n\\n\\n\\nNLTK\\'s corpus readers support efficient access to a variety of corpora, and can\\nbe used to work with new corpora.  1.3 lists functionality\\nprovided by the corpus readers.  We illustrate the difference between some\\nof the corpus access methods below:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; raw = gutenberg.raw(\"burgess-busterbrown.txt\")\\n&gt;&gt;&gt; raw[1:20]\\n\\'The Adventures of B\\'\\n&gt;&gt;&gt; words = gutenberg.words(\"burgess-busterbrown.txt\")\\n&gt;&gt;&gt; words[1:20]\\n[\\'The\\', \\'Adventures\\', \\'of\\', \\'Buster\\', \\'Bear\\', \\'by\\', \\'Thornton\\', \\'W\\', \\'.\\',\\n\\'Burgess\\', \\'1920\\', \\']\\', \\'I\\', \\'BUSTER\\', \\'BEAR\\', \\'GOES\\', \\'FISHING\\', \\'Buster\\',\\n\\'Bear\\']\\n&gt;&gt;&gt; sents = gutenberg.sents(\"burgess-busterbrown.txt\")\\n&gt;&gt;&gt; sents[1:20]\\n[[\\'I\\'], [\\'BUSTER\\', \\'BEAR\\', \\'GOES\\', \\'FISHING\\'], [\\'Buster\\', \\'Bear\\', \\'yawned\\', \\'as\\',\\n\\'he\\', \\'lay\\', \\'on\\', \\'his\\', \\'comfortable\\', \\'bed\\', \\'of\\', \\'leaves\\', \\'and\\', \\'watched\\',\\n\\'the\\', \\'first\\', \\'early\\', \\'morning\\', \\'sunbeams\\', \\'creeping\\', \\'through\\', ...], ...]\\n\\n\\n\\n\\n\\n1.9&nbsp;&nbsp;&nbsp;Loading your own Corpus\\nIf you have your own collection of text files that you would like to access using\\nthe above methods, you can easily load them with the help of NLTK\\'s\\nPlaintextCorpusReader. Check the location of your files on your file system; in\\nthe following example, we have taken this to be the directory\\n/usr/share/dict. Whatever the location, set this to be the value of\\ncorpus_root .\\nThe second parameter of the PlaintextCorpusReader initializer \\ncan be a list of fileids, like [\\'a.txt\\', \\'test/b.txt\\'],\\nor a pattern that matches all fileids, like \\'[abc]/.*\\\\.txt\\'\\n(see 3.4 for information\\nabout regular expressions).\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; from nltk.corpus import PlaintextCorpusReader\\n&gt;&gt;&gt; corpus_root = \\'/usr/share/dict\\' \\n&gt;&gt;&gt; wordlists = PlaintextCorpusReader(corpus_root, \\'.*\\') \\n&gt;&gt;&gt; wordlists.fileids()\\n[\\'README\\', \\'connectives\\', \\'propernames\\', \\'web2\\', \\'web2a\\', \\'words\\']\\n&gt;&gt;&gt; wordlists.words(\\'connectives\\')\\n[\\'the\\', \\'of\\', \\'and\\', \\'to\\', \\'a\\', \\'in\\', \\'that\\', \\'is\\', ...]\\n\\n\\n\\nAs another example, suppose you have your own local copy of Penn Treebank (release 3),\\nin C:\\\\corpora.  We can use the BracketParseCorpusReader to access this\\ncorpus.  We specify the corpus_root to be the location of the parsed Wall Street\\nJournal component of the corpus , and give a file_pattern\\nthat matches the files contained within its subfolders  (using forward slashes).\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; from nltk.corpus import BracketParseCorpusReader\\n&gt;&gt;&gt; corpus_root = r\"C:\\\\corpora\\\\penntreebank\\\\parsed\\\\mrg\\\\wsj\" \\n&gt;&gt;&gt; file_pattern = r\".*/wsj_.*\\\\.mrg\" \\n&gt;&gt;&gt; ptb = BracketParseCorpusReader(corpus_root, file_pattern)\\n&gt;&gt;&gt; ptb.fileids()\\n[\\'00/wsj_0001.mrg\\', \\'00/wsj_0002.mrg\\', \\'00/wsj_0003.mrg\\', \\'00/wsj_0004.mrg\\', ...]\\n&gt;&gt;&gt; len(ptb.sents())\\n49208\\n&gt;&gt;&gt; ptb.sents(fileids=\\'20/wsj_2013.mrg\\')[19]\\n[\\'The\\', \\'55-year-old\\', \\'Mr.\\', \\'Noriega\\', \\'is\\', \"n\\'t\", \\'as\\', \\'smooth\\', \\'as\\', \\'the\\',\\n\\'shah\\', \\'of\\', \\'Iran\\', \\',\\', \\'as\\', \\'well-born\\', \\'as\\', \\'Nicaragua\\', \"\\'s\", \\'Anastasio\\',\\n\\'Somoza\\', \\',\\', \\'as\\', \\'imperial\\', \\'as\\', \\'Ferdinand\\', \\'Marcos\\', \\'of\\', \\'the\\', \\'Philippines\\',\\n\\'or\\', \\'as\\', \\'bloody\\', \\'as\\', \\'Haiti\\', \"\\'s\", \\'Baby\\', Doc\\', \\'Duvalier\\', \\'.\\']\\n\\n\\n\\n\\n\\n\\n2&nbsp;&nbsp;&nbsp;Conditional Frequency Distributions\\nWe introduced frequency distributions in 3.\\nWe saw that given some list mylist of words or other items,\\nFreqDist(mylist) would compute the number of occurrences of each\\nitem in the list.  Here we will generalize this idea.\\nWhen the texts of a corpus are divided into several\\ncategories, by genre, topic, author, etc, we can maintain separate\\nfrequency distributions for each category.  This will allow us to\\nstudy systematic differences between the categories.  In the previous\\nsection we achieved this using NLTK\\'s ConditionalFreqDist data\\ntype.  A conditional frequency distribution is a collection of\\nfrequency distributions, each one for a different \"condition\".  The\\ncondition will often be the category of the text.  2.1\\ndepicts a fragment of a conditional frequency distribution having just\\ntwo conditions, one for news text and one for romance text.\\n\\n\\nFigure 2.1: Counting Words Appearing in a Text Collection (a conditional frequency distribution)\\n\\n\\n2.1&nbsp;&nbsp;&nbsp;Conditions and Events\\nA frequency distribution counts observable events,\\nsuch as the appearance of words in a text.  A conditional\\nfrequency distribution needs to pair each event with a condition.\\nSo instead of processing a sequence of words ,\\nwe have to process a sequence of pairs :\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; text = [\\'The\\', \\'Fulton\\', \\'County\\', \\'Grand\\', \\'Jury\\', \\'said\\', ...] \\n&gt;&gt;&gt; pairs = [(\\'news\\', \\'The\\'), (\\'news\\', \\'Fulton\\'), (\\'news\\', \\'County\\'), ...] \\n\\n\\n\\nEach pair has the form (condition, event).  If we were processing the\\nentire Brown Corpus by genre there would be 15 conditions (one per genre),\\nand 1,161,192 events (one per word).\\n\\n\\n2.2&nbsp;&nbsp;&nbsp;Counting Words by Genre\\nIn 1 we saw a conditional\\nfrequency distribution where the condition was the section of the\\nBrown Corpus, and for each condition we counted words. Whereas\\nFreqDist() takes a simple list as input, ConditionalFreqDist()\\ntakes a list of pairs.\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; from nltk.corpus import brown\\n&gt;&gt;&gt; cfd = nltk.ConditionalFreqDist(\\n...           (genre, word)\\n...           for genre in brown.categories()\\n...           for word in brown.words(categories=genre))\\n\\n\\n\\nLet\\'s break this down, and look at just two genres, news and romance.\\nFor each genre , we loop over every word in the genre ,\\nproducing pairs consisting of the genre and the word :\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; genre_word = [(genre, word) \\n...               for genre in [\\'news\\', \\'romance\\'] \\n...               for word in brown.words(categories=genre)] \\n&gt;&gt;&gt; len(genre_word)\\n170576\\n\\n\\n\\nSo, as we can see below,\\npairs at the beginning of the list genre_word will be of the form\\n(\\'news\\', word) , while those at the end will be of the form\\n(\\'romance\\', word) .\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; genre_word[:4]\\n[(\\'news\\', \\'The\\'), (\\'news\\', \\'Fulton\\'), (\\'news\\', \\'County\\'), (\\'news\\', \\'Grand\\')] # [_start-genre]\\n&gt;&gt;&gt; genre_word[-4:]\\n[(\\'romance\\', \\'afraid\\'), (\\'romance\\', \\'not\\'), (\\'romance\\', \"\\'\\'\"), (\\'romance\\', \\'.\\')] # [_end-genre]\\n\\n\\n\\nWe can now use this list of pairs to create a ConditionalFreqDist, and\\nsave it in a variable cfd.  As usual, we can type the name of the\\nvariable to inspect it , and verify it has two conditions :\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; cfd = nltk.ConditionalFreqDist(genre_word)\\n&gt;&gt;&gt; cfd \\n&lt;ConditionalFreqDist with 2 conditions&gt;\\n&gt;&gt;&gt; cfd.conditions()\\n[\\'news\\', \\'romance\\'] # [_conditions-cfd]\\n\\n\\n\\nLet\\'s access the two conditions, and satisfy ourselves that each is just\\na frequency distribution:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; print(cfd[\\'news\\'])\\n&lt;FreqDist with 14394 samples and 100554 outcomes&gt;\\n&gt;&gt;&gt; print(cfd[\\'romance\\'])\\n&lt;FreqDist with 8452 samples and 70022 outcomes&gt;\\n&gt;&gt;&gt; cfd[\\'romance\\'].most_common(20)\\n[(\\',\\', 3899), (\\'.\\', 3736), (\\'the\\', 2758), (\\'and\\', 1776), (\\'to\\', 1502),\\n(\\'a\\', 1335), (\\'of\\', 1186), (\\'``\\', 1045), (\"\\'\\'\", 1044), (\\'was\\', 993),\\n(\\'I\\', 951), (\\'in\\', 875), (\\'he\\', 702), (\\'had\\', 692), (\\'?\\', 690),\\n(\\'her\\', 651), (\\'that\\', 583), (\\'it\\', 573), (\\'his\\', 559), (\\'she\\', 496)]\\n&gt;&gt;&gt; cfd[\\'romance\\'][\\'could\\']\\n193\\n\\n\\n\\n\\n\\n2.3&nbsp;&nbsp;&nbsp;Plotting and Tabulating Distributions\\nApart from combining two or more frequency distributions, and being easy to initialize,\\na ConditionalFreqDist provides some useful methods for tabulation and plotting.\\nThe plot in 1.1 was based on a conditional frequency distribution\\nreproduced in the code below.\\nThe condition is either of the words america or citizen ,\\nand the counts being plotted are the number of times the word occured in a particular speech.\\nIt exploits the fact that the filename for each speech, e.g., 1865-Lincoln.txt\\ncontains the year as the first four characters .\\nThis code generates the pair (\\'america\\', \\'1865\\') for\\nevery instance of a word whose lowercased form starts with america\\n— such as Americans — in the file 1865-Lincoln.txt.\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; from nltk.corpus import inaugural\\n&gt;&gt;&gt; cfd = nltk.ConditionalFreqDist(\\n...           (target, fileid[:4]) \\n...           for fileid in inaugural.fileids()\\n...           for w in inaugural.words(fileid)\\n...           for target in [\\'america\\', \\'citizen\\'] \\n...           if w.lower().startswith(target))\\n\\n\\n\\nThe plot in 1.2 was also based on a conditional frequency distribution,\\nreproduced below.  This time, the condition is the name of the language\\nand the counts being plotted are derived from word lengths .\\nIt exploits the fact that the filename for each language is the language name followed\\nby \\'-Latin1\\' (the character encoding).\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; from nltk.corpus import udhr\\n&gt;&gt;&gt; languages = [\\'Chickasaw\\', \\'English\\', \\'German_Deutsch\\',\\n...     \\'Greenlandic_Inuktikut\\', \\'Hungarian_Magyar\\', \\'Ibibio_Efik\\']\\n&gt;&gt;&gt; cfd = nltk.ConditionalFreqDist(\\n...           (lang, len(word)) \\n...           for lang in languages\\n...           for word in udhr.words(lang + \\'-Latin1\\'))\\n\\n\\n\\nIn the plot() and tabulate() methods, we can\\noptionally specify which conditions to display with a conditions= parameter.\\nWhen we omit it, we get all the conditions.  Similarly, we can limit the\\nsamples to display with a samples= parameter.  This makes it possible to\\nload a large quantity of data into a conditional frequency distribution, and then\\nto explore it by plotting or tabulating selected conditions and samples.  It also\\ngives us full control over the order of conditions and samples in any displays.\\nFor example, we can tabulate the cumulative frequency data just for two\\nlanguages, and for words less than 10 characters long, as shown below.\\nWe interpret the last cell on the top row to mean that 1,638 words of the\\nEnglish text have 9 or fewer letters.\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; cfd.tabulate(conditions=[\\'English\\', \\'German_Deutsch\\'],\\n...              samples=range(10), cumulative=True)\\n                  0    1    2    3    4    5    6    7    8    9\\n       English    0  185  525  883  997 1166 1283 1440 1558 1638\\nGerman_Deutsch    0  171  263  614  717  894 1013 1110 1213 1275\\n\\n\\n\\n\\nNote\\nYour Turn:\\nWorking with the news and romance genres from the Brown Corpus,\\nfind out which days of the week are most newsworthy, and which are most romantic.\\nDefine a variable called days containing a list of days of the week, i.e.\\n[\\'Monday\\', ...].  Now tabulate the counts for these words using\\ncfd.tabulate(samples=days).  Now try the same thing using plot in place of tabulate.\\nYou may control the output order of days with the help of an extra parameter:\\nsamples=[\\'Monday\\', ...].\\n\\nYou may have noticed that the multi-line expressions we have been\\nusing with conditional frequency distributions look like list\\ncomprehensions, but without the brackets.  In general,\\nwhen we use a list comprehension as a parameter to a function,\\nlike set([w.lower() for w in t]), we are permitted to omit\\nthe square brackets and just write: set(w.lower() for w in t).\\n(See the discussion of \"generator expressions\" in 4.2\\nfor more about this.)\\n\\n\\n2.4&nbsp;&nbsp;&nbsp;Generating Random Text with Bigrams\\nWe can use a conditional frequency distribution to create a table of\\nbigrams (word pairs). (We introducted bigrams in\\n3.)\\nThe bigrams() function takes a list of\\nwords and builds a list of consecutive word pairs.\\nRemember that, in order to see the result and not a cryptic\\n\"generator object\", we need to use the list() function:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; sent = [\\'In\\', \\'the\\', \\'beginning\\', \\'God\\', \\'created\\', \\'the\\', \\'heaven\\',\\n...   \\'and\\', \\'the\\', \\'earth\\', \\'.\\']\\n&gt;&gt;&gt; list(nltk.bigrams(sent))\\n[(\\'In\\', \\'the\\'), (\\'the\\', \\'beginning\\'), (\\'beginning\\', \\'God\\'), (\\'God\\', \\'created\\'),\\n(\\'created\\', \\'the\\'), (\\'the\\', \\'heaven\\'), (\\'heaven\\', \\'and\\'), (\\'and\\', \\'the\\'),\\n(\\'the\\', \\'earth\\'), (\\'earth\\', \\'.\\')]\\n\\n\\n\\nIn 2.2, we treat each word as a condition, and for each one\\nwe effectively create a frequency distribution over the following\\nwords.  The function generate_model() contains a simple loop to\\ngenerate text. When we call the function, we choose a word (such as\\n\\'living\\') as our initial context, then once inside the loop, we\\nprint the current value of the variable word, and reset word\\nto be the most likely token in that context (using max()); next\\ntime through the loop, we use that word as our new context.  As you\\ncan see by inspecting the output, this simple approach to text\\ngeneration tends to get stuck in loops; another method would be to\\nrandomly choose the next word from among the available words.\\n\\n\\n\\n\\n&nbsp;\\ndef generate_model(cfdist, word, num=15):\\n    for i in range(num):\\n        print(word, end=\\' \\')\\n        word = cfdist[word].max()\\n\\ntext = nltk.corpus.genesis.words(\\'english-kjv.txt\\')\\nbigrams = nltk.bigrams(text)\\ncfd = nltk.ConditionalFreqDist(bigrams) \\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; cfd[\\'living\\']\\nFreqDist({\\'creature\\': 7, \\'thing\\': 4, \\'substance\\': 2, \\',\\': 1, \\'.\\': 1, \\'soul\\': 1})\\n&gt;&gt;&gt; generate_model(cfd, \\'living\\')\\nliving creature that he said , and the land of the land of the land\\n\\n\\nExample 2.2 (code_random_text.py): Figure 2.2: Generating Random Text: this program obtains all bigrams\\nfrom the text of the book of Genesis, then constructs a\\nconditional frequency distribution to record which\\nwords are most likely to follow a given word; e.g., after\\nthe word living, the most likely word is\\ncreature; the generate_model() function uses this\\ndata, and a seed word, to generate random text.\\n\\nConditional frequency distributions are a useful data structure for many NLP tasks.\\nTheir commonly-used methods are summarized in 2.1.\\nTable 2.1: NLTK\\'s Conditional Frequency Distributions: commonly-used methods and idioms for defining,\\naccessing, and visualizing a conditional frequency distribution of counters.\\n\\n\\n\\n\\n\\nExample\\nDescription\\n\\n\\n\\ncfdist = ConditionalFreqDist(pairs)\\ncreate a conditional frequency distribution from a list of pairs\\n\\ncfdist.conditions()\\nthe conditions\\n\\ncfdist[condition]\\nthe frequency distribution for this condition\\n\\ncfdist[condition][sample]\\nfrequency for the given sample for this condition\\n\\ncfdist.tabulate()\\ntabulate the conditional frequency distribution\\n\\ncfdist.tabulate(samples, conditions)\\ntabulation limited to the specified samples and conditions\\n\\ncfdist.plot()\\ngraphical plot of the conditional frequency distribution\\n\\ncfdist.plot(samples, conditions)\\ngraphical plot limited to the specified samples and conditions\\n\\ncfdist1 &lt; cfdist2\\ntest if samples in cfdist1 occur less frequently than in cfdist2\\n\\n\\n\\n\\n\\n\\n\\n\\n3&nbsp;&nbsp;&nbsp;More Python: Reusing Code\\nBy this time you\\'ve probably typed and retyped a lot of code in the Python\\ninteractive interpreter.  If you mess up when retyping a complex example you have\\nto enter it again.  Using the arrow keys to access and modify previous commands is helpful but only goes so\\nfar.  In this section we see two important ways to reuse code: text editors and Python functions.\\n\\n3.1&nbsp;&nbsp;&nbsp;Creating Programs with a Text Editor\\nThe Python interactive interpreter performs your instructions as soon as you type\\nthem.  Often, it is better to compose a multi-line program using a text editor,\\nthen ask Python to run the whole program at once.  Using IDLE, you can do\\nthis by going to the File menu and opening a new window.  Try this now, and\\nenter the following one-line program:\\nprint(\\'Monty Python\\')\\n\\nSave this program in a file called monty.py, then\\ngo to the Run menu, and select the command Run Module.\\n(We\\'ll learn what modules are shortly.)\\nThe result in the main IDLE window should look like this:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; ================================ RESTART ================================\\n&gt;&gt;&gt;\\nMonty Python\\n&gt;&gt;&gt;\\n\\n\\n\\nYou can also type from monty import * and it will do the same thing.\\nFrom now on, you have a choice of using the interactive interpreter or a\\ntext editor to create your programs.  It is often convenient to test your ideas\\nusing the interpreter, revising a line of code until it does what you expect.\\nOnce you\\'re ready, you can paste the code\\n(minus any &gt;&gt;&gt; or ... prompts) into the text editor,\\ncontinue to expand it, and finally save the program\\nin a file so that you don\\'t have to type it in again later.\\nGive the file a short but descriptive name, using all lowercase letters and separating\\nwords with underscore, and using the .py filename extension, e.g., monty_python.py.\\n\\nNote\\nImportant:\\nOur inline code examples include the &gt;&gt;&gt; and ... prompts\\nas if we are interacting directly with the interpreter.  As they get more complicated,\\nyou should instead type them into the editor, without the prompts, and run them\\nfrom the editor as shown above.  When we provide longer programs in this book,\\nwe will leave out the prompts to remind you to type them into a file rather\\nthan using the interpreter.  You can see this already in 2.2 above.\\nNote that it still includes a couple of lines with the Python prompt;\\nthis is the interactive part of the task where you inspect some data and invoke a function.\\nRemember that all code samples like 2.2 are downloadable\\nfrom http://nltk.org/.\\n\\n\\n\\n3.2&nbsp;&nbsp;&nbsp;Functions\\nSuppose that you work on analyzing text that involves different forms\\nof the same word, and that part of your program needs to work out\\nthe plural form of a given singular noun.  Suppose it needs to do this\\nwork in two places, once when it is processing some texts, and again\\nwhen it is processing user input.\\nRather than repeating the same code several times over, it is more\\nefficient and reliable to localize this work inside a function.\\nA function is just a named block of code that performs some well-defined\\ntask, as we saw in 1.\\nA function is usually defined to take some inputs, using special variables known as parameters,\\nand it may produce a result, also known as a return value.\\nWe define a function using the keyword def followed by the\\nfunction name and any input parameters, followed by the body of the\\nfunction.  Here\\'s the function we saw in 1\\n(including the import statement that is needed for Python 2, in order to make division behave as expected):\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; from __future__ import division\\n&gt;&gt;&gt; def lexical_diversity(text):\\n...     return len(text) / len(set(text))\\n\\n\\n\\nWe use the keyword return to indicate the value that is\\nproduced as output by the function.  In the above example,\\nall the work of the function is done in the return statement.\\nHere\\'s an equivalent definition which does the same work\\nusing multiple lines of code.  We\\'ll change the parameter name\\nfrom text to my_text_data to remind you that this is an arbitrary choice:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; def lexical_diversity(my_text_data):\\n...     word_count = len(my_text_data)\\n...     vocab_size = len(set(my_text_data))\\n...     diversity_score = vocab_size / word_count\\n...     return diversity_score\\n\\n\\n\\nNotice that we\\'ve created some new variables inside the body of the function.\\nThese are local variables and are not accessible outside the function.\\nSo now we have defined a function with the name lexical_diversity. But just\\ndefining it won\\'t produce any output!\\nFunctions do nothing until they are \"called\" (or \"invoked\"):\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; from nltk.corpus import genesis\\n&gt;&gt;&gt; kjv = genesis.words(\\'english-kjv.txt\\')\\n&gt;&gt;&gt; lexical_diversity(kjv)\\n0.06230453042623537\\n\\n\\n\\nLet\\'s return to our earlier scenario, and actually define a simple\\nfunction to work out English plurals.  The function plural() in 3.1\\ntakes a singular noun and generates a plural form, though it is not always\\ncorrect.  (We\\'ll discuss functions at greater length in 4.4.)\\n\\n\\n\\n\\n&nbsp;\\ndef plural(word):\\n    if word.endswith(\\'y\\'):\\n        return word[:-1] + \\'ies\\'\\n    elif word[-1] in \\'sx\\' or word[-2:] in [\\'sh\\', \\'ch\\']:\\n        return word + \\'es\\'\\n    elif word.endswith(\\'an\\'):\\n        return word[:-2] + \\'en\\'\\n    else:\\n        return word + \\'s\\'\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; plural(\\'fairy\\')\\n\\'fairies\\'\\n&gt;&gt;&gt; plural(\\'woman\\')\\n\\'women\\'\\n\\n\\nExample 3.1 (code_plural.py): Figure 3.1: A Python Function: this function tries to work out the\\nplural form of any English noun; the keyword def (define)\\nis followed by the function name, then a parameter inside\\nparentheses, and a colon; the body of the function is the\\nindented block of code; it tries to recognize patterns\\nwithin the word and process the word accordingly; e.g., if the\\nword ends with y, delete the y and add ies.\\n\\nThe endswith() function is always associated with a string object\\n(e.g., word in 3.1).  To call such functions, we give\\nthe name of the object, a period, and then the name of the function.\\nThese functions are usually known as methods.\\n\\n\\n3.3&nbsp;&nbsp;&nbsp;Modules\\nOver time you will find that you create a variety of useful little text processing functions,\\nand you end up copying them from old programs to new ones.  Which file contains the\\nlatest version of the function you want to use?\\nIt makes life a lot easier if you can collect your work into a single place, and\\naccess previously defined functions without making copies.\\nTo do this, save your function(s) in a file called (say) text_proc.py.\\nNow, you can access your work simply by importing it from the file:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; from text_proc import plural\\n&gt;&gt;&gt; plural(\\'wish\\')\\nwishes\\n&gt;&gt;&gt; plural(\\'fan\\')\\nfen\\n\\n\\n\\nOur plural function obviously has an error, since the plural of\\nfan is fans.\\nInstead of typing in a new version of the function, we can\\nsimply edit the existing one.  Thus, at every\\nstage, there is only one version of our plural function, and no confusion about\\nwhich one is being used.\\nA collection of variable and function definitions in a file is called a Python\\nmodule.  A collection of related modules is called a package.\\nNLTK\\'s code for processing the Brown Corpus is an example of a module,\\nand its collection of code for processing all the different corpora is\\nan example of a package.  NLTK itself is a set of packages, sometimes\\ncalled a library.\\n\\nCaution!\\nIf you are creating a file to contain some of your Python\\ncode, do not name your file nltk.py: it may get imported in\\nplace of the \"real\" NLTK package.  When it imports modules, Python\\nfirst looks in the current directory (folder).\\n\\n\\n\\n\\n4&nbsp;&nbsp;&nbsp;Lexical Resources\\nA lexicon, or lexical resource, is a collection of words and/or phrases along\\nwith associated information such as part of speech and sense definitions.\\nLexical resources are secondary to texts, and are usually created and enriched with the help\\nof texts.  For example, if we have defined a text my_text, then\\nvocab = sorted(set(my_text)) builds the vocabulary of my_text,\\nwhile word_freq = FreqDist(my_text) counts the frequency of each word in the text.  Both\\nof vocab and word_freq are simple lexical resources.  Similarly, a concordance\\nlike the one we saw in 1\\ngives us information about word usage that might help in the preparation of\\na dictionary.  Standard terminology for lexicons is illustrated in 4.1.\\nA lexical entry consists of a headword (also known as a lemma)\\nalong with additional information such as the part of speech and the sense\\ndefinition.  Two distinct words having the same spelling are called homonyms.\\n\\n\\nFigure 4.1: Lexicon Terminology: lexical entries for two lemmas\\nhaving the same spelling (homonyms), providing part of speech\\nand gloss information.\\n\\nThe simplest kind of lexicon is nothing more than a sorted list of words.\\nSophisticated lexicons include complex structure within and across\\nthe individual entries.  In this section we\\'ll look at some lexical resources\\nincluded with NLTK.\\n\\n4.1&nbsp;&nbsp;&nbsp;Wordlist Corpora\\n<!-- XXX There\\'s a useful opportunity here to link back to the discussion of what words\\ncharacterize a text found in ch01. -->\\nNLTK includes some corpora that are nothing more than wordlists.\\nThe Words Corpus is the /usr/share/dict/words file from Unix, used by\\nsome spell checkers.  We can use it to find unusual or mis-spelt\\nwords in a text corpus, as shown in 4.2.\\n\\n\\n\\n\\n&nbsp;\\ndef unusual_words(text):\\n    text_vocab = set(w.lower() for w in text if w.isalpha())\\n    english_vocab = set(w.lower() for w in nltk.corpus.words.words())\\n    unusual = text_vocab - english_vocab\\n    return sorted(unusual)\\n\\n&gt;&gt;&gt; unusual_words(nltk.corpus.gutenberg.words(\\'austen-sense.txt\\'))\\n[\\'abbeyland\\', \\'abhorred\\', \\'abilities\\', \\'abounded\\', \\'abridgement\\', \\'abused\\', \\'abuses\\',\\n\\'accents\\', \\'accepting\\', \\'accommodations\\', \\'accompanied\\', \\'accounted\\', \\'accounts\\',\\n\\'accustomary\\', \\'aches\\', \\'acknowledging\\', \\'acknowledgment\\', \\'acknowledgments\\', ...]\\n&gt;&gt;&gt; unusual_words(nltk.corpus.nps_chat.words())\\n[\\'aaaaaaaaaaaaaaaaa\\', \\'aaahhhh\\', \\'abortions\\', \\'abou\\', \\'abourted\\', \\'abs\\', \\'ack\\',\\n\\'acros\\', \\'actualy\\', \\'adams\\', \\'adds\\', \\'adduser\\', \\'adjusts\\', \\'adoted\\', \\'adreniline\\',\\n\\'ads\\', \\'adults\\', \\'afe\\', \\'affairs\\', \\'affari\\', \\'affects\\', \\'afk\\', \\'agaibn\\', \\'ages\\', ...]\\n\\n\\nExample 4.2 (code_unusual.py): Figure 4.2: Filtering a Text: this program computes the vocabulary of a text,\\nthen removes all items that occur in an existing wordlist,\\nleaving just the uncommon or mis-spelt words.\\n\\nThere is also a corpus of stopwords, that is, high-frequency\\nwords like the, to and also that we sometimes\\nwant to filter out of a document before further processing. Stopwords\\nusually have little lexical content, and their presence in a text fails\\nto distinguish it from other texts.\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; from nltk.corpus import stopwords\\n&gt;&gt;&gt; stopwords.words(\\'english\\')\\n[\\'i\\', \\'me\\', \\'my\\', \\'myself\\', \\'we\\', \\'our\\', \\'ours\\', \\'ourselves\\', \\'you\\', \\'your\\', \\'yours\\',\\n\\'yourself\\', \\'yourselves\\', \\'he\\', \\'him\\', \\'his\\', \\'himself\\', \\'she\\', \\'her\\', \\'hers\\',\\n\\'herself\\', \\'it\\', \\'its\\', \\'itself\\', \\'they\\', \\'them\\', \\'their\\', \\'theirs\\', \\'themselves\\',\\n\\'what\\', \\'which\\', \\'who\\', \\'whom\\', \\'this\\', \\'that\\', \\'these\\', \\'those\\', \\'am\\', \\'is\\', \\'are\\',\\n\\'was\\', \\'were\\', \\'be\\', \\'been\\', \\'being\\', \\'have\\', \\'has\\', \\'had\\', \\'having\\', \\'do\\', \\'does\\',\\n\\'did\\', \\'doing\\', \\'a\\', \\'an\\', \\'the\\', \\'and\\', \\'but\\', \\'if\\', \\'or\\', \\'because\\', \\'as\\', \\'until\\',\\n\\'while\\', \\'of\\', \\'at\\', \\'by\\', \\'for\\', \\'with\\', \\'about\\', \\'against\\', \\'between\\', \\'into\\',\\n\\'through\\', \\'during\\', \\'before\\', \\'after\\', \\'above\\', \\'below\\', \\'to\\', \\'from\\', \\'up\\', \\'down\\',\\n\\'in\\', \\'out\\', \\'on\\', \\'off\\', \\'over\\', \\'under\\', \\'again\\', \\'further\\', \\'then\\', \\'once\\', \\'here\\',\\n\\'there\\', \\'when\\', \\'where\\', \\'why\\', \\'how\\', \\'all\\', \\'any\\', \\'both\\', \\'each\\', \\'few\\', \\'more\\',\\n\\'most\\', \\'other\\', \\'some\\', \\'such\\', \\'no\\', \\'nor\\', \\'not\\', \\'only\\', \\'own\\', \\'same\\', \\'so\\',\\n\\'than\\', \\'too\\', \\'very\\', \\'s\\', \\'t\\', \\'can\\', \\'will\\', \\'just\\', \\'don\\', \\'should\\', \\'now\\']\\n\\n\\n\\nLet\\'s define a function to compute what fraction of words in a text are not in the\\nstopwords list:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; def content_fraction(text):\\n...     stopwords = nltk.corpus.stopwords.words(\\'english\\')\\n...     content = [w for w in text if w.lower() not in stopwords]\\n...     return len(content) / len(text)\\n...\\n&gt;&gt;&gt; content_fraction(nltk.corpus.reuters.words())\\n0.7364374824583169\\n\\n\\n\\nThus, with the help of stopwords we filter out over a quarter of the words of the text.\\nNotice that we\\'ve combined two different kinds of corpus here, using a lexical\\nresource to filter the content of a text corpus.\\n\\n\\nFigure 4.3: A Word Puzzle: a grid of randomly chosen letters with rules for\\ncreating words out of the letters; this puzzle is known as \"Target.\"\\n\\nA wordlist is useful for solving word puzzles, such as the one in 4.3.\\nOur program iterates through every word and, for each one, checks whether it meets\\nthe conditions.  It is easy to check obligatory letter \\nand length constraints  (and we\\'ll\\nonly look for words with six or more letters here).\\nIt is trickier to check that candidate solutions only use combinations of the\\nsupplied letters, especially since some of the supplied letters\\nappear twice (here, the letter v).\\nThe FreqDist comparison method  permits us to check that\\nthe frequency of each letter in the candidate word is less than or equal\\nto the frequency of the corresponding letter in the puzzle.\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; puzzle_letters = nltk.FreqDist(\\'egivrvonl\\')\\n&gt;&gt;&gt; obligatory = \\'r\\'\\n&gt;&gt;&gt; wordlist = nltk.corpus.words.words()\\n&gt;&gt;&gt; [w for w in wordlist if len(w) &gt;= 6 \\n...                      and obligatory in w \\n...                      and nltk.FreqDist(w) &lt;= puzzle_letters] \\n[\\'glover\\', \\'gorlin\\', \\'govern\\', \\'grovel\\', \\'ignore\\', \\'involver\\', \\'lienor\\',\\n\\'linger\\', \\'longer\\', \\'lovering\\', \\'noiler\\', \\'overling\\', \\'region\\', \\'renvoi\\',\\n\\'revolving\\', \\'ringle\\', \\'roving\\', \\'violer\\', \\'virole\\']\\n\\n\\n\\nOne more wordlist corpus is the Names corpus, containing 8,000 first names categorized by gender.\\nThe male and female names are stored in separate files.  Let\\'s find names which appear\\nin both files, i.e. names that are ambiguous for gender:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; names = nltk.corpus.names\\n&gt;&gt;&gt; names.fileids()\\n[\\'female.txt\\', \\'male.txt\\']\\n&gt;&gt;&gt; male_names = names.words(\\'male.txt\\')\\n&gt;&gt;&gt; female_names = names.words(\\'female.txt\\')\\n&gt;&gt;&gt; [w for w in male_names if w in female_names]\\n[\\'Abbey\\', \\'Abbie\\', \\'Abby\\', \\'Addie\\', \\'Adrian\\', \\'Adrien\\', \\'Ajay\\', \\'Alex\\', \\'Alexis\\',\\n\\'Alfie\\', \\'Ali\\', \\'Alix\\', \\'Allie\\', \\'Allyn\\', \\'Andie\\', \\'Andrea\\', \\'Andy\\', \\'Angel\\',\\n\\'Angie\\', \\'Ariel\\', \\'Ashley\\', \\'Aubrey\\', \\'Augustine\\', \\'Austin\\', \\'Averil\\', ...]\\n\\n\\n\\nIt is well known that names ending in the letter a are almost always female.\\nWe can see this and some other patterns in the graph in 4.4,\\nproduced by the following code.  Remember that name[-1] is the last letter\\nof name.\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; cfd = nltk.ConditionalFreqDist(\\n...           (fileid, name[-1])\\n...           for fileid in names.fileids()\\n...           for name in names.words(fileid))\\n&gt;&gt;&gt; cfd.plot()\\n\\n\\n\\n\\n\\nFigure 4.4: Conditional Frequency Distribution: this plot shows the number of female and male names\\nending with each letter of the alphabet; most names ending with a, e or i\\nare female; names ending in h and l are equally likely to be male or female;\\nnames ending in k, o, r, s, and t are likely to be male.\\n\\n\\n\\n4.2&nbsp;&nbsp;&nbsp;A Pronouncing Dictionary\\nA slightly richer kind of lexical resource is a table (or spreadsheet), containing a word\\nplus some properties in each row.  NLTK includes the CMU Pronouncing\\nDictionary for US English, which was designed for\\nuse by speech synthesizers.\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; entries = nltk.corpus.cmudict.entries()\\n&gt;&gt;&gt; len(entries)\\n133737\\n&gt;&gt;&gt; for entry in entries[42371:42379]:\\n...     print(entry)\\n...\\n(\\'fir\\', [\\'F\\', \\'ER1\\'])\\n(\\'fire\\', [\\'F\\', \\'AY1\\', \\'ER0\\'])\\n(\\'fire\\', [\\'F\\', \\'AY1\\', \\'R\\'])\\n(\\'firearm\\', [\\'F\\', \\'AY1\\', \\'ER0\\', \\'AA2\\', \\'R\\', \\'M\\'])\\n(\\'firearm\\', [\\'F\\', \\'AY1\\', \\'R\\', \\'AA2\\', \\'R\\', \\'M\\'])\\n(\\'firearms\\', [\\'F\\', \\'AY1\\', \\'ER0\\', \\'AA2\\', \\'R\\', \\'M\\', \\'Z\\'])\\n(\\'firearms\\', [\\'F\\', \\'AY1\\', \\'R\\', \\'AA2\\', \\'R\\', \\'M\\', \\'Z\\'])\\n(\\'fireball\\', [\\'F\\', \\'AY1\\', \\'ER0\\', \\'B\\', \\'AO2\\', \\'L\\'])\\n\\n\\n\\nFor each word, this lexicon provides a list of phonetic\\ncodes — distinct labels for each contrastive sound —\\nknown as phones.  Observe that fire has two pronunciations\\n(in US English):\\nthe one-syllable F AY1 R, and the two-syllable F AY1 ER0.\\nThe symbols in the CMU Pronouncing Dictionary are from the Arpabet,\\ndescribed in more detail at http://en.wikipedia.org/wiki/Arpabet\\n<!-- XXX Hmm - - would it be better to first explain tuple assignment outside the\\ncontext of a for loop?  (not sure how to fix this; can\\'t use the word \"tuple\" yet) -->\\nEach entry consists of two parts, and we can\\nprocess these individually using a more complex version of the for statement.\\nInstead of writing for entry in entries:, we replace\\nentry with two variable names, word, pron .\\nNow, each time through the loop, word is assigned the first part of the\\nentry, and pron is assigned the second part of the entry:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; for word, pron in entries: \\n...     if len(pron) == 3: \\n...         ph1, ph2, ph3 = pron \\n...         if ph1 == \\'P\\' and ph3 == \\'T\\':\\n...             print(word, ph2, end=\\' \\')\\n...\\npait EY1 pat AE1 pate EY1 patt AE1 peart ER1 peat IY1 peet IY1 peete IY1 pert ER1\\npet EH1 pete IY1 pett EH1 piet IY1 piette IY1 pit IH1 pitt IH1 pot AA1 pote OW1\\npott AA1 pout AW1 puett UW1 purt ER1 put UH1 putt AH1\\n\\n\\n\\nThe above program scans the lexicon looking for entries whose pronunciation consists of\\nthree phones .  If the condition is true, it assigns the contents\\nof pron to three new variables ph1, ph2 and ph3.  Notice the unusual\\nform of the statement which does that work .\\nHere\\'s another example of the same for statement, this time used inside a list\\ncomprehension.  This program finds all words whose pronunciation ends with a syllable\\nsounding like nicks.  You could use this method to find rhyming words.\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; syllable = [\\'N\\', \\'IH0\\', \\'K\\', \\'S\\']\\n&gt;&gt;&gt; [word for word, pron in entries if pron[-4:] == syllable]\\n[\"atlantic\\'s\", \\'audiotronics\\', \\'avionics\\', \\'beatniks\\', \\'calisthenics\\', \\'centronics\\',\\n\\'chamonix\\', \\'chetniks\\', \"clinic\\'s\", \\'clinics\\', \\'conics\\', \\'conics\\', \\'cryogenics\\',\\n\\'cynics\\', \\'diasonics\\', \"dominic\\'s\", \\'ebonics\\', \\'electronics\\', \"electronics\\'\", ...]\\n\\n\\n\\nNotice that the one pronunciation is spelt in several ways: nics, niks, nix,\\neven ntic\\'s with a silent t, for the word atlantic\\'s.  Let\\'s look for some other\\nmismatches between pronunciation and writing.  Can you summarize the purpose of\\nthe following examples and explain how they work?\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; [w for w, pron in entries if pron[-1] == \\'M\\' and w[-1] == \\'n\\']\\n[\\'autumn\\', \\'column\\', \\'condemn\\', \\'damn\\', \\'goddamn\\', \\'hymn\\', \\'solemn\\']\\n&gt;&gt;&gt; sorted(set(w[:2] for w, pron in entries if pron[0] == \\'N\\' and w[0] != \\'n\\'))\\n[\\'gn\\', \\'kn\\', \\'mn\\', \\'pn\\']\\n\\n\\n\\nThe phones contain digits to represent\\nprimary stress (1), secondary stress (2) and no stress (0).\\nAs our final example, we define a function to extract the stress digits\\nand then scan our lexicon to find words having a particular stress pattern.\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; def stress(pron):\\n...     return [char for phone in pron for char in phone if char.isdigit()]\\n&gt;&gt;&gt; [w for w, pron in entries if stress(pron) == [\\'0\\', \\'1\\', \\'0\\', \\'2\\', \\'0\\']]\\n[\\'abbreviated\\', \\'abbreviated\\', \\'abbreviating\\', \\'accelerated\\', \\'accelerating\\',\\n\\'accelerator\\', \\'accelerators\\', \\'accentuated\\', \\'accentuating\\', \\'accommodated\\',\\n\\'accommodating\\', \\'accommodative\\', \\'accumulated\\', \\'accumulating\\', \\'accumulative\\', ...]\\n&gt;&gt;&gt; [w for w, pron in entries if stress(pron) == [\\'0\\', \\'2\\', \\'0\\', \\'1\\', \\'0\\']]\\n[\\'abbreviation\\', \\'abbreviations\\', \\'abomination\\', \\'abortifacient\\', \\'abortifacients\\',\\n\\'academicians\\', \\'accommodation\\', \\'accommodations\\', \\'accreditation\\', \\'accreditations\\',\\n\\'accumulation\\', \\'accumulations\\', \\'acetylcholine\\', \\'acetylcholine\\', \\'adjudication\\', ...]\\n\\n\\n\\n\\nNote\\nA subtlety of the above program is that our\\nuser-defined function stress() is invoked inside the condition of\\na list comprehension.  There is also a doubly-nested for loop.\\nThere\\'s a lot going on here and you might want\\nto return to this once you\\'ve had more experience using list comprehensions.\\n\\nWe can use a conditional frequency distribution to help us find minimally-contrasting\\nsets of words.  Here we find all the p-words consisting of three sounds ,\\nand group them according to their first and last sounds .\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; p3 = [(pron[0]+\\'-\\'+pron[2], word) \\n...       for (word, pron) in entries\\n...       if pron[0] == \\'P\\' and len(pron) == 3] \\n&gt;&gt;&gt; cfd = nltk.ConditionalFreqDist(p3)\\n&gt;&gt;&gt; for template in sorted(cfd.conditions()):\\n...     if len(cfd[template]) &gt; 10:\\n...         words = sorted(cfd[template])\\n...         wordstring = \\' \\'.join(words)\\n...         print(template, wordstring[:70] + \"...\")\\n...\\nP-CH patch pautsch peach perch petsch petsche piche piech pietsch pitch pit...\\nP-K pac pack paek paik pak pake paque peak peake pech peck peek perc perk ...\\nP-L pahl pail paille pal pale pall paul paule paull peal peale pearl pearl...\\nP-N paign pain paine pan pane pawn payne peine pen penh penn pin pine pinn...\\nP-P paap paape pap pape papp paup peep pep pip pipe pipp poop pop pope pop...\\nP-R paar pair par pare parr pear peer pier poor poore por pore porr pour...\\nP-S pace pass pasts peace pearse pease perce pers perse pesce piece piss p...\\nP-T pait pat pate patt peart peat peet peete pert pet pete pett piet piett...\\nP-UW1 peru peugh pew plew plue prew pru prue prugh pshew pugh...\\n\\n\\n\\nRather than iterating over the whole dictionary, we can also access it\\nby looking up particular words.  We will use Python\\'s dictionary data\\nstructure, which we will study systematically in 3.\\nWe look up a dictionary by giving its name followed by a key\\n(such as the word \\'fire\\') inside square brackets .\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; prondict = nltk.corpus.cmudict.dict()\\n&gt;&gt;&gt; prondict[\\'fire\\'] \\n[[\\'F\\', \\'AY1\\', \\'ER0\\'], [\\'F\\', \\'AY1\\', \\'R\\']]\\n&gt;&gt;&gt; prondict[\\'blog\\'] \\nTraceback (most recent call last):\\n  File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\\nKeyError: \\'blog\\'\\n&gt;&gt;&gt; prondict[\\'blog\\'] = [[\\'B\\', \\'L\\', \\'AA1\\', \\'G\\']] \\n&gt;&gt;&gt; prondict[\\'blog\\']\\n[[\\'B\\', \\'L\\', \\'AA1\\', \\'G\\']]\\n\\n\\n\\nIf we try to look up a non-existent key , we get a KeyError.\\nThis is similar to what happens when we index a list with an\\ninteger that is too large, producing an IndexError.\\nThe word blog is missing from the pronouncing dictionary,\\nso we tweak our version by assigning a value for this key \\n(this has no effect on the NLTK corpus; next time we access it,\\nblog will still be absent).\\nWe can use any lexical resource to process a text, e.g., to filter out words having\\nsome lexical property (like nouns), or mapping every word of the text.\\nFor example, the following text-to-speech function looks up each word\\nof the text in the pronunciation dictionary.\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; text = [\\'natural\\', \\'language\\', \\'processing\\']\\n&gt;&gt;&gt; [ph for w in text for ph in prondict[w][0]]\\n[\\'N\\', \\'AE1\\', \\'CH\\', \\'ER0\\', \\'AH0\\', \\'L\\', \\'L\\', \\'AE1\\', \\'NG\\', \\'G\\', \\'W\\', \\'AH0\\', \\'JH\\',\\n\\'P\\', \\'R\\', \\'AA1\\', \\'S\\', \\'EH0\\', \\'S\\', \\'IH0\\', \\'NG\\']\\n\\n\\n\\n\\n\\n\\n4.3&nbsp;&nbsp;&nbsp;Comparative Wordlists\\nAnother example of a tabular lexicon is the comparative wordlist.\\nNLTK includes so-called Swadesh wordlists, lists of about 200 common words\\nin several languages.  The languages are identified using an ISO 639 two-letter code.\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; from nltk.corpus import swadesh\\n&gt;&gt;&gt; swadesh.fileids()\\n[\\'be\\', \\'bg\\', \\'bs\\', \\'ca\\', \\'cs\\', \\'cu\\', \\'de\\', \\'en\\', \\'es\\', \\'fr\\', \\'hr\\', \\'it\\', \\'la\\', \\'mk\\',\\n\\'nl\\', \\'pl\\', \\'pt\\', \\'ro\\', \\'ru\\', \\'sk\\', \\'sl\\', \\'sr\\', \\'sw\\', \\'uk\\']\\n&gt;&gt;&gt; swadesh.words(\\'en\\')\\n[\\'I\\', \\'you (singular), thou\\', \\'he\\', \\'we\\', \\'you (plural)\\', \\'they\\', \\'this\\', \\'that\\',\\n\\'here\\', \\'there\\', \\'who\\', \\'what\\', \\'where\\', \\'when\\', \\'how\\', \\'not\\', \\'all\\', \\'many\\', \\'some\\',\\n\\'few\\', \\'other\\', \\'one\\', \\'two\\', \\'three\\', \\'four\\', \\'five\\', \\'big\\', \\'long\\', \\'wide\\', ...]\\n\\n\\n\\nWe can access cognate words from multiple languages using the entries() method,\\nspecifying a list of languages.  With one further step we can convert this into\\na simple dictionary (we\\'ll learn about dict() in 3).\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; fr2en = swadesh.entries([\\'fr\\', \\'en\\'])\\n&gt;&gt;&gt; fr2en\\n[(\\'je\\', \\'I\\'), (\\'tu, vous\\', \\'you (singular), thou\\'), (\\'il\\', \\'he\\'), ...]\\n&gt;&gt;&gt; translate = dict(fr2en)\\n&gt;&gt;&gt; translate[\\'chien\\']\\n\\'dog\\'\\n&gt;&gt;&gt; translate[\\'jeter\\']\\n\\'throw\\'\\n\\n\\n\\nWe can make our simple translator more useful by adding other source languages.\\nLet\\'s get the German-English and Spanish-English pairs, convert each to a\\ndictionary using dict(), then update our original translate dictionary\\nwith these additional mappings:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; de2en = swadesh.entries([\\'de\\', \\'en\\'])    # German-English\\n&gt;&gt;&gt; es2en = swadesh.entries([\\'es\\', \\'en\\'])    # Spanish-English\\n&gt;&gt;&gt; translate.update(dict(de2en))\\n&gt;&gt;&gt; translate.update(dict(es2en))\\n&gt;&gt;&gt; translate[\\'Hund\\']\\n\\'dog\\'\\n&gt;&gt;&gt; translate[\\'perro\\']\\n\\'dog\\'\\n\\n\\n\\nWe can compare words in various Germanic and Romance languages:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; languages = [\\'en\\', \\'de\\', \\'nl\\', \\'es\\', \\'fr\\', \\'pt\\', \\'la\\']\\n&gt;&gt;&gt; for i in [139, 140, 141, 142]:\\n...     print(swadesh.entries(languages)[i])\\n...\\n(\\'say\\', \\'sagen\\', \\'zeggen\\', \\'decir\\', \\'dire\\', \\'dizer\\', \\'dicere\\')\\n(\\'sing\\', \\'singen\\', \\'zingen\\', \\'cantar\\', \\'chanter\\', \\'cantar\\', \\'canere\\')\\n(\\'play\\', \\'spielen\\', \\'spelen\\', \\'jugar\\', \\'jouer\\', \\'jogar, brincar\\', \\'ludere\\')\\n(\\'float\\', \\'schweben\\', \\'zweven\\', \\'flotar\\', \\'flotter\\', \\'flutuar, boiar\\', \\'fluctuare\\')\\n\\n\\n\\n\\n\\n4.4&nbsp;&nbsp;&nbsp;Shoebox and Toolbox Lexicons\\nPerhaps the single most popular tool used by linguists for managing data\\nis Toolbox, previously known as Shoebox since it replaces\\nthe field linguist\\'s traditional shoebox full of file cards.\\nToolbox is freely downloadable from http://www.sil.org/computing/toolbox/.\\nA Toolbox file consists of a collection of entries,\\nwhere each entry is made up of one or more fields.\\nMost fields are optional or repeatable, which means that this kind of\\nlexical resource cannot be treated as a table or spreadsheet.\\nHere is a dictionary for the Rotokas language.  We see just the first entry,\\nfor the word kaa meaning \"to gag\":\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; from nltk.corpus import toolbox\\n&gt;&gt;&gt; toolbox.entries(\\'rotokas.dic\\')\\n[(\\'kaa\\', [(\\'ps\\', \\'V\\'), (\\'pt\\', \\'A\\'), (\\'ge\\', \\'gag\\'), (\\'tkp\\', \\'nek i pas\\'),\\n(\\'dcsv\\', \\'true\\'), (\\'vx\\', \\'1\\'), (\\'sc\\', \\'???\\'), (\\'dt\\', \\'29/Oct/2005\\'),\\n(\\'ex\\', \\'Apoka ira kaaroi aioa-ia reoreopaoro.\\'),\\n(\\'xp\\', \\'Kaikai i pas long nek bilong Apoka bikos em i kaikai na toktok.\\'),\\n(\\'xe\\', \\'Apoka is gagging from food while talking.\\')]), ...]\\n\\n\\n\\nEntries consist of a series of attribute-value pairs, like (\\'ps\\', \\'V\\')\\nto indicate that the part-of-speech is \\'V\\' (verb), and (\\'ge\\', \\'gag\\')\\nto indicate that the gloss-into-English is \\'gag\\'.\\nThe last three pairs contain\\nan example sentence in Rotokas and its translations into Tok Pisin and English.\\nThe loose structure of Toolbox files makes it hard for us to do much more with them\\nat this stage.  XML provides a powerful way to process this kind of corpus and\\nwe will return to this topic in 11..\\n\\nNote\\nThe Rotokas language is spoken on the island of Bougainville, Papua New Guinea.\\nThis lexicon was contributed to NLTK by Stuart Robinson.\\nRotokas is notable for having an inventory of just 12 phonemes (contrastive sounds),\\nhttp://en.wikipedia.org/wiki/Rotokas_language\\n\\n\\n\\n\\n5&nbsp;&nbsp;&nbsp;WordNet\\n<!-- XXX Can we do a better job of explaining what\\'s going on in WordNet? Trying to\\nprocess the combination of WN\\'s unfamiliar organization and datastructures, plus\\nthe notation, plus the NLTK interface, imposes a very heavy cognitive load.\\nE.g. this whole WN  notion is pretty bizarre in some ways: \"entity ``car.n.01`` is called a `synset`:dt:,\" -->\\nWordNet is a semantically-oriented dictionary of English,\\nsimilar to a traditional thesaurus but with a richer structure.\\nNLTK includes the English WordNet, with 155,287 words\\nand 117,659 synonym sets.  We\\'ll begin by\\nlooking at synonyms and how they are accessed in WordNet.\\n\\n5.1&nbsp;&nbsp;&nbsp;Senses and Synonyms\\n\\n\\nConsider the sentence in (1a).\\nIf we replace the word motorcar in (1a) by automobile,\\nto get (1b), the meaning of the sentence stays pretty much the same:\\n\\n  (1)\\n  a.Benz is credited with the invention of the motorcar.\\n\\n  b.Benz is credited with the invention of the automobile.\\n\\nSince everything else in the sentence has remained unchanged, we can\\nconclude that the words motorcar and automobile have the\\nsame meaning, i.e. they are synonyms.  We can explore these\\nwords with the help of WordNet:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; from nltk.corpus import wordnet as wn\\n&gt;&gt;&gt; wn.synsets(\\'motorcar\\')\\n[Synset(\\'car.n.01\\')]\\n\\n\\n\\nThus, motorcar has just one possible meaning and it is identified as car.n.01,\\nthe first noun sense of car.  The entity car.n.01 is called a synset,\\nor \"synonym set\", a collection of synonymous words (or \"lemmas\"):\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; wn.synset(\\'car.n.01\\').lemma_names()\\n[\\'car\\', \\'auto\\', \\'automobile\\', \\'machine\\', \\'motorcar\\']\\n\\n\\n\\nEach word of a synset can have several meanings, e.g., car can also signify\\na train carriage, a gondola, or an elevator car.  However, we are only interested\\nin the single meaning that is common to all words of the above synset.  Synsets\\nalso come with a prose definition and some example sentences:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; wn.synset(\\'car.n.01\\').definition()\\n\\'a motor vehicle with four wheels; usually propelled by an internal combustion engine\\'\\n&gt;&gt;&gt; wn.synset(\\'car.n.01\\').examples()\\n[\\'he needs a car to get to work\\']\\n\\n\\n\\nAlthough definitions help humans to understand the intended meaning of a synset,\\nthe words of the synset are often more useful for our programs.\\nTo eliminate ambiguity, we will identify these words as\\ncar.n.01.automobile, car.n.01.motorcar, and so on.\\nThis pairing of a synset with a word is called a lemma.\\nWe can get all the lemmas for a given synset ,\\nlook up a particular lemma ,\\nget the synset corresponding to a lemma ,\\nand get the \"name\" of a lemma :\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; wn.synset(\\'car.n.01\\').lemmas() \\n[Lemma(\\'car.n.01.car\\'), Lemma(\\'car.n.01.auto\\'), Lemma(\\'car.n.01.automobile\\'),\\nLemma(\\'car.n.01.machine\\'), Lemma(\\'car.n.01.motorcar\\')]\\n&gt;&gt;&gt; wn.lemma(\\'car.n.01.automobile\\') \\nLemma(\\'car.n.01.automobile\\')\\n&gt;&gt;&gt; wn.lemma(\\'car.n.01.automobile\\').synset() \\nSynset(\\'car.n.01\\')\\n&gt;&gt;&gt; wn.lemma(\\'car.n.01.automobile\\').name() \\n\\'automobile\\'\\n\\n\\n\\nUnlike the word motorcar, which is unambiguous and has one\\nsynset, the word car is ambiguous, having five synsets:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; wn.synsets(\\'car\\')\\n[Synset(\\'car.n.01\\'), Synset(\\'car.n.02\\'), Synset(\\'car.n.03\\'), Synset(\\'car.n.04\\'),\\nSynset(\\'cable_car.n.01\\')]\\n&gt;&gt;&gt; for synset in wn.synsets(\\'car\\'):\\n...     print(synset.lemma_names())\\n...\\n[\\'car\\', \\'auto\\', \\'automobile\\', \\'machine\\', \\'motorcar\\']\\n[\\'car\\', \\'railcar\\', \\'railway_car\\', \\'railroad_car\\']\\n[\\'car\\', \\'gondola\\']\\n[\\'car\\', \\'elevator_car\\']\\n[\\'cable_car\\', \\'car\\']\\n\\n\\n\\nFor convenience, we can access all the lemmas involving the word car\\nas follows.\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; wn.lemmas(\\'car\\')\\n[Lemma(\\'car.n.01.car\\'), Lemma(\\'car.n.02.car\\'), Lemma(\\'car.n.03.car\\'),\\nLemma(\\'car.n.04.car\\'), Lemma(\\'cable_car.n.01.car\\')]\\n\\n\\n\\n\\nNote\\nYour Turn:\\nWrite down all the senses of the word dish that you can think of.  Now, explore this\\nword with the help of WordNet, using the same operations we used above.\\n\\n\\n\\n5.2&nbsp;&nbsp;&nbsp;The WordNet Hierarchy\\nWordNet synsets correspond to abstract concepts, and they don\\'t always\\nhave corresponding words in English.  These concepts are linked together in a hierarchy.\\nSome concepts are very general, such as Entity, State, Event — these are called\\nunique beginners or root synsets.  Others, such as gas guzzler and\\nhatchback, are much more specific. A small portion of a concept\\nhierarchy is illustrated in 5.1.\\n\\n\\nFigure 5.1: Fragment of WordNet Concept Hierarchy: nodes correspond to synsets;\\nedges indicate the hypernym/hyponym relation, i.e. the relation between\\nsuperordinate and subordinate concepts.\\n\\nWordNet makes it easy to navigate between concepts.\\nFor example, given a concept like motorcar,\\nwe can look at the concepts that are more specific;\\nthe (immediate) hyponyms.\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; motorcar = wn.synset(\\'car.n.01\\')\\n&gt;&gt;&gt; types_of_motorcar = motorcar.hyponyms()\\n&gt;&gt;&gt; types_of_motorcar[0]\\nSynset(\\'ambulance.n.01\\')\\n&gt;&gt;&gt; sorted(lemma.name() for synset in types_of_motorcar for lemma in synset.lemmas())\\n[\\'Model_T\\', \\'S.U.V.\\', \\'SUV\\', \\'Stanley_Steamer\\', \\'ambulance\\', \\'beach_waggon\\',\\n\\'beach_wagon\\', \\'bus\\', \\'cab\\', \\'compact\\', \\'compact_car\\', \\'convertible\\',\\n\\'coupe\\', \\'cruiser\\', \\'electric\\', \\'electric_automobile\\', \\'electric_car\\',\\n\\'estate_car\\', \\'gas_guzzler\\', \\'hack\\', \\'hardtop\\', \\'hatchback\\', \\'heap\\',\\n\\'horseless_carriage\\', \\'hot-rod\\', \\'hot_rod\\', \\'jalopy\\', \\'jeep\\', \\'landrover\\',\\n\\'limo\\', \\'limousine\\', \\'loaner\\', \\'minicar\\', \\'minivan\\', \\'pace_car\\', \\'patrol_car\\',\\n\\'phaeton\\', \\'police_car\\', \\'police_cruiser\\', \\'prowl_car\\', \\'race_car\\', \\'racer\\',\\n\\'racing_car\\', \\'roadster\\', \\'runabout\\', \\'saloon\\', \\'secondhand_car\\', \\'sedan\\',\\n\\'sport_car\\', \\'sport_utility\\', \\'sport_utility_vehicle\\', \\'sports_car\\', \\'squad_car\\',\\n\\'station_waggon\\', \\'station_wagon\\', \\'stock_car\\', \\'subcompact\\', \\'subcompact_car\\',\\n\\'taxi\\', \\'taxicab\\', \\'tourer\\', \\'touring_car\\', \\'two-seater\\', \\'used-car\\', \\'waggon\\',\\n\\'wagon\\']\\n\\n\\n\\nWe can also navigate up the hierarchy by visiting hypernyms.  Some words\\nhave multiple paths, because they can be classified in more than one way.\\nThere are two paths between car.n.01 and entity.n.01 because\\nwheeled_vehicle.n.01 can be classified as both a vehicle and a container.\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; motorcar.hypernyms()\\n[Synset(\\'motor_vehicle.n.01\\')]\\n&gt;&gt;&gt; paths = motorcar.hypernym_paths()\\n&gt;&gt;&gt; len(paths)\\n2\\n&gt;&gt;&gt; [synset.name() for synset in paths[0]]\\n[\\'entity.n.01\\', \\'physical_entity.n.01\\', \\'object.n.01\\', \\'whole.n.02\\', \\'artifact.n.01\\',\\n\\'instrumentality.n.03\\', \\'container.n.01\\', \\'wheeled_vehicle.n.01\\',\\n\\'self-propelled_vehicle.n.01\\', \\'motor_vehicle.n.01\\', \\'car.n.01\\']\\n&gt;&gt;&gt; [synset.name() for synset in paths[1]]\\n[\\'entity.n.01\\', \\'physical_entity.n.01\\', \\'object.n.01\\', \\'whole.n.02\\', \\'artifact.n.01\\',\\n\\'instrumentality.n.03\\', \\'conveyance.n.03\\', \\'vehicle.n.01\\', \\'wheeled_vehicle.n.01\\',\\n\\'self-propelled_vehicle.n.01\\', \\'motor_vehicle.n.01\\', \\'car.n.01\\']\\n\\n\\n\\nWe can get the most general hypernyms (or root hypernyms) of\\na synset as follows:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; motorcar.root_hypernyms()\\n[Synset(\\'entity.n.01\\')]\\n\\n\\n\\n\\nNote\\nYour Turn:\\nTry out NLTK\\'s convenient graphical WordNet browser: nltk.app.wordnet().\\nExplore the WordNet hierarchy by following the hypernym and hyponym links.\\n\\n\\n\\n5.3&nbsp;&nbsp;&nbsp;More Lexical Relations\\nHypernyms and hyponyms are called lexical relations because they relate one\\nsynset to another.  These two relations navigate up and down the \"is-a\" hierarchy.\\nAnother important way to navigate the WordNet network is from items to their\\ncomponents (meronyms) or to the things they are contained in (holonyms).\\nFor example, the parts of a tree are its trunk, crown, and so on;\\nthe part_meronyms().\\nThe substance a tree is made of includes heartwood and sapwood;\\nthe substance_meronyms().\\nA collection of trees forms a forest; the member_holonyms():\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; wn.synset(\\'tree.n.01\\').part_meronyms()\\n[Synset(\\'burl.n.02\\'), Synset(\\'crown.n.07\\'), Synset(\\'limb.n.02\\'),\\nSynset(\\'stump.n.01\\'), Synset(\\'trunk.n.01\\')]\\n&gt;&gt;&gt; wn.synset(\\'tree.n.01\\').substance_meronyms()\\n[Synset(\\'heartwood.n.01\\'), Synset(\\'sapwood.n.01\\')]\\n&gt;&gt;&gt; wn.synset(\\'tree.n.01\\').member_holonyms()\\n[Synset(\\'forest.n.01\\')]\\n\\n\\n\\nTo see just how intricate things can get, consider the word mint, which\\nhas several closely-related senses.  We can see that mint.n.04 is part of\\nmint.n.02 and the substance from which mint.n.05 is made.\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; for synset in wn.synsets(\\'mint\\', wn.NOUN):\\n...     print(synset.name() + \\':\\', synset.definition())\\n...\\nbatch.n.02: (often followed by `of\\') a large number or amount or extent\\nmint.n.02: any north temperate plant of the genus Mentha with aromatic leaves and\\n           small mauve flowers\\nmint.n.03: any member of the mint family of plants\\nmint.n.04: the leaves of a mint plant used fresh or candied\\nmint.n.05: a candy that is flavored with a mint oil\\nmint.n.06: a plant where money is coined by authority of the government\\n&gt;&gt;&gt; wn.synset(\\'mint.n.04\\').part_holonyms()\\n[Synset(\\'mint.n.02\\')]\\n&gt;&gt;&gt; wn.synset(\\'mint.n.04\\').substance_holonyms()\\n[Synset(\\'mint.n.05\\')]\\n\\n\\n\\nThere are also relationships between verbs.  For example, the act of walking involves the act of stepping,\\nso walking entails stepping.  Some verbs have multiple entailments:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; wn.synset(\\'walk.v.01\\').entailments()\\n[Synset(\\'step.v.01\\')]\\n&gt;&gt;&gt; wn.synset(\\'eat.v.01\\').entailments()\\n[Synset(\\'chew.v.01\\'), Synset(\\'swallow.v.01\\')]\\n&gt;&gt;&gt; wn.synset(\\'tease.v.03\\').entailments()\\n[Synset(\\'arouse.v.07\\'), Synset(\\'disappoint.v.01\\')]\\n\\n\\n\\nSome lexical relationships hold between lemmas, e.g., antonymy:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; wn.lemma(\\'supply.n.02.supply\\').antonyms()\\n[Lemma(\\'demand.n.02.demand\\')]\\n&gt;&gt;&gt; wn.lemma(\\'rush.v.01.rush\\').antonyms()\\n[Lemma(\\'linger.v.04.linger\\')]\\n&gt;&gt;&gt; wn.lemma(\\'horizontal.a.01.horizontal\\').antonyms()\\n[Lemma(\\'inclined.a.02.inclined\\'), Lemma(\\'vertical.a.01.vertical\\')]\\n&gt;&gt;&gt; wn.lemma(\\'staccato.r.01.staccato\\').antonyms()\\n[Lemma(\\'legato.r.01.legato\\')]\\n\\n\\n\\nYou can see the lexical relations, and the other methods defined\\non a synset, using dir(), for example: dir(wn.synset(\\'harmony.n.02\\')).\\n\\n\\n5.4&nbsp;&nbsp;&nbsp;Semantic Similarity\\n\\nWe have seen that synsets are linked by a complex network of\\nlexical relations.  Given a particular synset, we can traverse\\nthe WordNet network to find synsets with related meanings.\\nKnowing which words are semantically related\\nis useful for indexing a collection of texts, so\\nthat a search for a general term like vehicle will match documents\\ncontaining specific terms like limousine.\\nRecall that each synset has one or more hypernym paths that link it\\nto a root hypernym such as entity.n.01.\\nTwo synsets linked to the same root may have several hypernyms in common\\n(cf 5.1).\\nIf two synsets share a very specific hypernym — one that is low\\ndown in the hypernym hierarchy — they must be closely related.\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; right = wn.synset(\\'right_whale.n.01\\')\\n&gt;&gt;&gt; orca = wn.synset(\\'orca.n.01\\')\\n&gt;&gt;&gt; minke = wn.synset(\\'minke_whale.n.01\\')\\n&gt;&gt;&gt; tortoise = wn.synset(\\'tortoise.n.01\\')\\n&gt;&gt;&gt; novel = wn.synset(\\'novel.n.01\\')\\n&gt;&gt;&gt; right.lowest_common_hypernyms(minke)\\n[Synset(\\'baleen_whale.n.01\\')]\\n&gt;&gt;&gt; right.lowest_common_hypernyms(orca)\\n[Synset(\\'whale.n.02\\')]\\n&gt;&gt;&gt; right.lowest_common_hypernyms(tortoise)\\n[Synset(\\'vertebrate.n.01\\')]\\n&gt;&gt;&gt; right.lowest_common_hypernyms(novel)\\n[Synset(\\'entity.n.01\\')]\\n\\n\\n\\nOf course we know that whale is very specific (and baleen whale even more so),\\nwhile vertebrate is more general and entity is completely general.\\nWe can quantify this concept of generality by looking up the depth of each synset:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; wn.synset(\\'baleen_whale.n.01\\').min_depth()\\n14\\n&gt;&gt;&gt; wn.synset(\\'whale.n.02\\').min_depth()\\n13\\n&gt;&gt;&gt; wn.synset(\\'vertebrate.n.01\\').min_depth()\\n8\\n&gt;&gt;&gt; wn.synset(\\'entity.n.01\\').min_depth()\\n0\\n\\n\\n\\nSimilarity measures have been defined over the collection of WordNet synsets\\nwhich incorporate the above insight.  For example,\\npath_similarity assigns a score in the range 0–1 based on the shortest path that connects the concepts in the hypernym\\nhierarchy (-1 is returned in those cases where a path cannot be\\nfound).  Comparing a synset with itself will return 1.\\nConsider the following similarity scores, relating right whale\\nto minke whale, orca, tortoise, and novel.\\nAlthough the numbers won\\'t mean much, they decrease as\\nwe move away from the semantic space of sea creatures to inanimate objects.\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; right.path_similarity(minke)\\n0.25\\n&gt;&gt;&gt; right.path_similarity(orca)\\n0.16666666666666666\\n&gt;&gt;&gt; right.path_similarity(tortoise)\\n0.07692307692307693\\n&gt;&gt;&gt; right.path_similarity(novel)\\n0.043478260869565216\\n\\n\\n\\n\\nNote\\nSeveral other similarity measures are available; you can type help(wn)\\nfor more information.  NLTK also includes VerbNet, a hierarhical verb lexicon linked to WordNet.\\nIt can be accessed with nltk.corpus.verbnet.\\n\\n\\n\\n\\n6&nbsp;&nbsp;&nbsp;Summary\\n\\nA text corpus is a large, structured collection of texts.  NLTK comes with many corpora,\\ne.g., the Brown Corpus, nltk.corpus.brown.\\nSome text corpora are categorized, e.g., by genre or topic; sometimes the\\ncategories of a corpus overlap each other.\\nA conditional frequency distribution is a collection of frequency distributions,\\neach one for a different condition.  They can be used for counting word frequencies,\\ngiven a context or a genre.\\nPython programs more than a few lines long should be entered using a text editor,\\nsaved to a file with a .py extension, and accessed using an import statement.\\nPython functions permit you to associate a name with a particular block of code,\\nand re-use that code as often as necessary.\\nSome functions, known as \"methods\", are associated with an object and we give the object\\nname followed by a period followed by the function, like this: x.funct(y),\\ne.g., word.isalpha().\\nTo find out about some variable v,\\ntype help(v) in the Python interactive interpreter to read the help entry for this kind of object.\\nWordNet is a semantically-oriented dictionary of English, consisting of synonym sets — or synsets —\\nand organized into a network.\\nSome functions are not available by default, but must be accessed using\\nPython\\'s import statement.\\n\\n\\n\\n7&nbsp;&nbsp;&nbsp;Further Reading\\nExtra materials for this chapter are posted at http://nltk.org/, including links to freely\\navailable resources on the web.  The corpus methods are summarized in the\\nCorpus HOWTO, at http://nltk.org/howto, and documented extensively in the online API documentation.\\nSignificant sources of published corpora are the Linguistic Data Consortium (LDC) and\\nthe European Language Resources Agency (ELRA).  Hundreds of annotated text and speech\\ncorpora are available in dozens of languages.  Non-commercial licences permit the data to\\nbe used in teaching and research.  For some corpora, commercial licenses are also available\\n(but for a higher fee).\\nA good tool for creating annotated text corpora is called Brat,\\nand available from http://brat.nlplab.org/.\\nThese and many other language resources have been documented using OLAC Metadata, and can\\nbe searched via the OLAC homepage at http://www.language-archives.org/.  Corpora List is a mailing list\\nfor discussions about corpora, and you can find resources by searching the list archives\\nor posting to the list.\\nThe most complete inventory of the world\\'s languages is Ethnologue, http://www.ethnologue.com/.\\nOf 7,000 languages, only a few dozen have substantial digital resources suitable for\\nuse in NLP.\\nThis chapter has touched on the field of Corpus Linguistics.  Other useful books in this\\narea include (Biber, Conrad, &amp; Reppen, 1998), (McEnery, 2006), (Meyer, 2002), (Sampson &amp; McCarthy, 2005), (Scott &amp; Tribble, 2006).\\nFurther readings in quantitative data analysis in linguistics are:\\n(Baayen, 2008), (Gries, 2009), (Woods, Fletcher, &amp; Hughes, 1986).\\nThe original description of WordNet is (Fellbaum, 1998).\\nAlthough WordNet was originally developed for research\\nin psycholinguistics, it is now widely used in NLP and Information Retrieval.\\nWordNets are being developed for many other languages, as documented\\nat http://www.globalwordnet.org/.\\nFor a study of WordNet similarity measures, see (Budanitsky &amp; Hirst, 2006).\\nOther topics touched on in this chapter were phonetics and lexical semantics,\\nand we refer readers to chapters 7 and 20 of (Jurafsky &amp; Martin, 2008).\\n\\n\\n8&nbsp;&nbsp;&nbsp;Exercises\\n\\n☼ Create a variable phrase containing a list of words.\\nReview the operations described in the previous chapter, including addition,\\nmultiplication, indexing, slicing, and sorting.\\n☼ Use the corpus module to explore austen-persuasion.txt.\\nHow many word tokens does this book have?  How many word types?\\n☼ Use the Brown corpus reader nltk.corpus.brown.words() or the Web text corpus\\nreader nltk.corpus.webtext.words() to access some sample text in two different genres.\\n☼ Read in the texts of the State of the Union addresses, using the\\nstate_union corpus reader.  Count occurrences of men, women,\\nand people in each document.  What has happened to the usage of these\\nwords over time?\\n☼ Investigate the holonym-meronym relations for some nouns.\\nRemember that there are three kinds of holonym-meronym relation,\\nso you need to use:\\nmember_meronyms(), part_meronyms(), substance_meronyms(),\\nmember_holonyms(), part_holonyms(), and substance_holonyms().\\n☼ In the discussion of comparative wordlists, we created an object\\ncalled translate which you could look up using words in both German and Spanish\\nin order to get corresponding words in English.\\nWhat problem might arise with this approach?\\nCan you suggest a way to avoid this problem?\\n☼ According to Strunk and White\\'s Elements of Style,\\nthe word however, used at the start of a sentence,\\nmeans \"in whatever way\" or \"to whatever extent\", and not\\n\"nevertheless\".  They give this example of correct usage:\\nHowever you advise him, he will probably do as he thinks best.\\n(http://www.bartleby.com/141/strunk3.html)\\nUse the concordance tool to study actual usage of this word\\nin the various texts we have been considering.\\nSee also the LanguageLog posting \"Fossilized prejudices about \\'however\\'\"\\nat http://itre.cis.upenn.edu/~myl/languagelog/archives/001913.html\\n◑ Define a conditional frequency distribution over the Names corpus\\nthat allows you to see which initial letters are more frequent for males\\nvs. females (cf. 4.4).\\n◑ Pick a pair of texts and study the differences between them,\\nin terms of vocabulary, vocabulary richness, genre, etc.  Can you\\nfind pairs of words which have quite different meanings across the\\ntwo texts, such as monstrous in Moby Dick and in Sense and Sensibility?\\n◑ Read the BBC News article: UK\\'s Vicky Pollards \\'left behind\\' http://news.bbc.co.uk/1/hi/education/6173441.stm.\\nThe article gives the following statistic about teen language:\\n\"the top 20 words used, including yeah, no, but and like, account for around a third of all words.\"\\nHow many word types account for a third\\nof all word tokens, for a variety of text sources?  What do you conclude about this statistic?\\nRead more about this on LanguageLog, at http://itre.cis.upenn.edu/~myl/languagelog/archives/003993.html.\\n◑ Investigate the table of modal distributions and look for other patterns.\\nTry to explain them in terms of your own impressionistic understanding\\nof the different genres.  Can you find other closed classes of words that\\nexhibit significant differences across different genres?\\n◑ The CMU Pronouncing Dictionary contains multiple pronunciations\\nfor certain words.  How many distinct words does it contain?  What fraction\\nof words in this dictionary have more than one possible pronunciation?\\n◑ What percentage of noun synsets have no hyponyms?\\nYou can get all noun synsets using wn.all_synsets(\\'n\\').\\n◑ Define a function supergloss(s) that takes a synset s as its argument\\nand returns a string consisting of the concatenation of the definition of s, and\\nthe definitions of all the hypernyms and hyponyms of s.\\n◑ Write a program to find all words that occur at least three times in the Brown Corpus.\\n◑ Write a program to generate a table of lexical diversity scores (i.e. token/type ratios), as we saw in\\n1.1.  Include the full set of Brown Corpus genres (nltk.corpus.brown.categories()).\\nWhich genre has the lowest diversity (greatest number of tokens per type)?\\nIs this what you would have expected?\\n◑ Write a function that finds the 50 most frequently occurring words\\nof a text that are not stopwords.\\n◑ Write a program to print the 50 most frequent bigrams\\n(pairs of adjacent words) of a text, omitting bigrams that contain stopwords.\\n◑ Write a program to create a table of word frequencies by genre,\\nlike the one given in 1 for modals.\\nChoose your own words and try to find words whose presence\\n(or absence) is typical of a genre.  Discuss your findings.\\n◑ Write a function word_freq() that takes a word and the name of a section\\nof the Brown Corpus as arguments, and computes the frequency of the word\\nin that section of the corpus.\\n◑ Write a program to guess the number of syllables contained in a text,\\nmaking use of the CMU Pronouncing Dictionary.\\n◑ Define a function hedge(text) which processes a\\ntext and produces a new version with the word\\n\\'like\\' between every third word.\\n★ Zipf\\'s Law:\\nLet f(w) be the frequency of a word w in free text. Suppose that\\nall the words of a text are ranked according to their frequency,\\nwith the most frequent word first. Zipf\\'s law states that the\\nfrequency of a word type is inversely proportional to its rank\\n(i.e. f × r = k, for some constant k). For example, the 50th most\\ncommon word type should occur three times as frequently as the\\n150th most common word type.\\nWrite a function to process a large text and plot word\\nfrequency against word rank using pylab.plot. Do\\nyou confirm Zipf\\'s law? (Hint: it helps to use a logarithmic scale).\\nWhat is going on at the extreme ends of the plotted line?\\nGenerate random text, e.g., using random.choice(\"abcdefg \"),\\ntaking care to include the space character.  You will need to\\nimport random first.  Use the string\\nconcatenation operator to accumulate characters into a (very)\\nlong string.  Then tokenize this string, and generate the Zipf\\nplot as before, and compare the two plots.  What do you make of\\nZipf\\'s Law in the light of this?\\n\\n\\n★ Modify the text generation program in 2.2 further, to\\ndo the following tasks:\\nStore the n most likely words in a list words then randomly\\nchoose a word from the list using random.choice().  (You will need\\nto import random first.)\\nSelect a particular genre, such as a section of the Brown Corpus,\\nor a genesis translation, one of the Gutenberg texts, or one of the Web texts.  Train\\nthe model on this corpus and get it to generate random text.  You\\nmay have to experiment with different start words. How intelligible\\nis the text?  Discuss the strengths and weaknesses of this method of\\ngenerating random text.\\nNow train your system using two distinct genres and experiment\\nwith generating text in the hybrid genre.  Discuss your observations.\\n\\n\\n★ Define a function find_language() that takes a string\\nas its argument, and returns a list of languages that have that\\nstring as a word.  Use the udhr corpus and limit your searches\\nto files in the Latin-1 encoding.\\n★ What is the branching factor of the noun hypernym hierarchy?\\nI.e. for every noun synset that has hyponyms — or children in the\\nhypernym hierarchy — how many do they have on average?\\nYou can get all noun synsets using wn.all_synsets(\\'n\\').\\n★ The polysemy of a word is the number of senses it has.\\nUsing WordNet, we can determine that the noun dog has 7 senses\\nwith: len(wn.synsets(\\'dog\\', \\'n\\')).\\nCompute the average polysemy of nouns, verbs, adjectives and\\nadverbs according to WordNet.\\n★ Use one of the predefined similarity measures to score\\nthe similarity of each of the following pairs of words.\\nRank the pairs in order of decreasing similarity.\\nHow close is your ranking to the order given here,\\nan order that was established experimentally\\nby (Miller &amp; Charles, 1998):\\ncar-automobile, gem-jewel, journey-voyage, boy-lad,\\ncoast-shore, asylum-madhouse, magician-wizard, midday-noon,\\nfurnace-stove, food-fruit, bird-cock, bird-crane, tool-implement,\\nbrother-monk, lad-brother, crane-implement, journey-car,\\nmonk-oracle, cemetery-woodland, food-rooster, coast-hill,\\nforest-graveyard, shore-woodland, monk-slave, coast-forest,\\nlad-wizard, chord-smile, glass-magician, rooster-voyage, noon-string.\\n\\n\\n\\nAbout this document...\\nUPDATED FOR NLTK 3.0.\\nThis is a chapter from Natural Language Processing with Python,\\nby Steven Bird, Ewan Klein and Edward Loper,\\nCopyright © 2019 the authors.\\nIt is distributed with the Natural Language Toolkit [http://nltk.org/],\\nVersion 3.0, under the terms of the\\nCreative Commons Attribution-Noncommercial-No Derivative Works 3.0 United States License\\n[http://creativecommons.org/licenses/by-nc-nd/3.0/us/].\\nThis document was built on\\nWed  4 Sep 2019 11:40:48 ACST\\n\\n\\n\\n\\n\\n',\n",
       " '\\n\\n\\n\\nfunction astext(node)\\n{\\n    return node.innerHTML.replace(/(]+)>)/ig,\"\")\\n                         .replace(/&gt;/ig, \">\")\\n                         .replace(/&lt;/ig, \"<\")\\n                         .replace(/&quot;/ig, \\'\"\\')\\n                         .replace(/&amp;/ig, \"&\");\\n}\\n\\nfunction copy_notify(node, bar_color, data)\\n{\\n    // The outer box: relative + inline positioning.\\n    var box1 = document.createElement(\"div\");\\n    box1.style.position = \"relative\";\\n    box1.style.display = \"inline\";\\n    box1.style.top = \"2em\";\\n    box1.style.left = \"1em\";\\n  \\n    // A shadow for fun\\n    var shadow = document.createElement(\"div\");\\n    shadow.style.position = \"absolute\";\\n    shadow.style.left = \"-1.3em\";\\n    shadow.style.top = \"-1.3em\";\\n    shadow.style.background = \"#404040\";\\n    \\n    // The inner box: absolute positioning.\\n    var box2 = document.createElement(\"div\");\\n    box2.style.position = \"relative\";\\n    box2.style.border = \"1px solid #a0a0a0\";\\n    box2.style.left = \"-.2em\";\\n    box2.style.top = \"-.2em\";\\n    box2.style.background = \"white\";\\n    box2.style.padding = \".3em .4em .3em .4em\";\\n    box2.style.fontStyle = \"normal\";\\n    box2.style.background = \"#f0e0e0\";\\n\\n    node.insertBefore(box1, node.childNodes.item(0));\\n    box1.appendChild(shadow);\\n    shadow.appendChild(box2);\\n    box2.innerHTML=\"Copied&nbsp;to&nbsp;the&nbsp;clipboard: \" +\\n                   \"\"+\\n                   data+\"\";\\n    setTimeout(function() { node.removeChild(box1); }, 1000);\\n\\n    var elt = node.parentNode.firstChild;\\n    elt.style.background = \"#ffc0c0\";\\n    setTimeout(function() { elt.style.background = bar_color; }, 200);\\n}\\n\\nfunction copy_codeblock_to_clipboard(node)\\n{\\n    var data = astext(node)+\"\\\\n\";\\n    if (copy_text_to_clipboard(data)) {\\n        copy_notify(node, \"#40a060\", data);\\n    }\\n}\\n\\nfunction copy_doctest_to_clipboard(node)\\n{\\n    var s = astext(node)+\"\\\\n   \";\\n    var data = \"\";\\n\\n    var start = 0;\\n    var end = s.indexOf(\"\\\\n\");\\n    while (end >= 0) {\\n        if (s.substring(start, start+4) == \">>> \") {\\n            data += s.substring(start+4, end+1);\\n        }\\n        else if (s.substring(start, start+4) == \"... \") {\\n            data += s.substring(start+4, end+1);\\n        }\\n        /*\\n        else if (end-start > 1) {\\n            data += \"# \" + s.substring(start, end+1);\\n        }*/\\n        // Grab the next line.\\n        start = end+1;\\n        end = s.indexOf(\"\\\\n\", start);\\n    }\\n    \\n    if (copy_text_to_clipboard(data)) {\\n        copy_notify(node, \"#4060a0\", data);\\n    }\\n}\\n    \\nfunction copy_text_to_clipboard(data)\\n{\\n    if (window.clipboardData) {\\n        window.clipboardData.setData(\"Text\", data);\\n        return true;\\n     }\\n    else if (window.netscape) {\\n        // w/ default firefox settings, permission will be denied for this:\\n        netscape.security.PrivilegeManager\\n                      .enablePrivilege(\"UniversalXPConnect\");\\n    \\n        var clip = Components.classes[\"@mozilla.org/widget/clipboard;1\"]\\n                      .createInstance(Components.interfaces.nsIClipboard);\\n        if (!clip) return;\\n    \\n        var trans = Components.classes[\"@mozilla.org/widget/transferable;1\"]\\n                       .createInstance(Components.interfaces.nsITransferable);\\n        if (!trans) return;\\n    \\n        trans.addDataFlavor(\"text/unicode\");\\n    \\n        var str = new Object();\\n        var len = new Object();\\n    \\n        var str = Components.classes[\"@mozilla.org/supports-string;1\"]\\n                     .createInstance(Components.interfaces.nsISupportsString);\\n        var datacopy=data;\\n        str.data=datacopy;\\n        trans.setTransferData(\"text/unicode\",str,datacopy.length*2);\\n        var clipid=Components.interfaces.nsIClipboard;\\n    \\n        if (!clip) return false;\\n    \\n        clip.setData(trans,null,clipid.kGlobalClipboard);\\n        return true;\\n    }\\n    return false;\\n}\\n//-->\\n\\n\\n\\n\\n\\n\\nch03.rst2\\n\\n\\n/*\\n:Author: Edward Loper, James Curran\\n:Copyright: This stylesheet has been placed in the public domain.\\n\\nStylesheet for use with Docutils.\\n\\nThis stylesheet defines new css classes used by NLTK.\\n\\nIt uses a Python syntax highlighting scheme that matches\\nthe colour scheme used by IDLE, which makes it easier for\\nbeginners to check they are typing things in correctly.\\n*/\\n\\n/* Include the standard docutils stylesheet. */\\n@import url(default.css);\\n\\n/* Custom inline roles */\\nspan.placeholder    { font-style: italic; font-family: monospace; }\\nspan.example        { font-style: italic; }\\nspan.emphasis       { font-style: italic; }\\nspan.termdef        { font-weight: bold; }\\n/*span.term           { font-style: italic; }*/\\nspan.category       { font-variant: small-caps; }\\nspan.feature        { font-variant: small-caps; }\\nspan.fval           { font-style: italic; }\\nspan.math           { font-style: italic; }\\nspan.mathit         { font-style: italic; }\\nspan.lex            { font-variant: small-caps; }\\nspan.guide-linecount{ text-align: right; display: block;}\\n\\n/* Python souce code listings */\\nspan.pysrc-prompt   { color: #9b0000; }\\nspan.pysrc-more     { color: #9b00ff; }\\nspan.pysrc-keyword  { color: #e06000; }\\nspan.pysrc-builtin  { color: #940094; }\\nspan.pysrc-string   { color: #00aa00; }\\nspan.pysrc-comment  { color: #ff0000; }\\nspan.pysrc-output   { color: #0000ff; }\\nspan.pysrc-except   { color: #ff0000; }\\nspan.pysrc-defname  { color: #008080; }\\n\\n\\n/* Doctest blocks */\\npre.doctest         { margin: 0; padding: 0; font-weight: bold; }\\ndiv.doctest         { margin: 0 1em 1em 1em; padding: 0; }\\ntable.doctest       { margin: 0; padding: 0;\\n                      border-top: 1px solid gray;\\n                      border-bottom: 1px solid gray; }\\npre.copy-notify     { margin: 0; padding: 0.2em; font-weight: bold;\\n                      background-color: #ffffff; }\\n\\n/* Python source listings */\\ndiv.pylisting       { margin: 0 1em 1em 1em; padding: 0; }\\ntable.pylisting     { margin: 0; padding: 0;\\n                      border-top: 1px solid gray; }\\ntd.caption { border-top: 1px solid black; margin: 0; padding: 0; }\\n.caption-label { font-weight: bold;  }\\ntd.caption p { margin: 0; padding: 0; font-style: normal;}\\n\\ntable tr td.codeblock { \\n  padding: 0.2em ! important; margin: 0;\\n  border-left: 1px solid gray;\\n  border-right: 2px solid gray;\\n  border-top: 0px solid gray;\\n  border-bottom: 1px solid gray;\\n  font-weight: bold; background-color: #eeffee;\\n}\\ntable pre span {\\n  white-space: pre-wrap;\\n}\\ntable tr td.doctest  { \\n  padding: 0.2em; margin: 0;\\n  border-left: 1px solid gray;\\n  border-right: 2px solid gray;\\n  border-top: 0px solid gray;\\n  border-bottom: 1px solid gray;\\n  font-weight: bold; background-color: #eeeeff;\\n}\\n\\ntd.codeblock table tr td.copybar {\\n    background: #40a060; border: 1px solid gray;\\n    font-family: monospace; padding: 0; margin: 0; }\\ntd.doctest table tr td.copybar {\\n    background: #4060a0; border: 1px solid gray;\\n    font-family: monospace; padding: 0; margin: 0; }\\n\\ntd.pysrc { padding-left: 0.5em; }\\n\\nimg.callout { border-width: 0px; }\\n\\ntable.docutils {\\n    border-style: solid;\\n    border-width: 1px;\\n    margin-top: 6px;\\n    border-color: grey;\\n    border-collapse: collapse; }\\n\\ntable.docutils th {\\n    border-style: none;\\n    border-width: 1px;\\n    border-color: grey;\\n    padding: 0 .5em 0 .5em; }\\n\\ntable.docutils td {\\n    border-style: none;\\n    border-width: 1px;\\n    border-color: grey; \\n    padding: 0 .5em 0 .5em; }\\n\\ntable.footnote td { padding: 0; }\\ntable.footnote { border-width: 0; }\\ntable.footnote td { border-width: 0; }\\ntable.footnote th { border-width: 0; }\\n\\ntable.noborder { border-width: 0; }\\n\\ntable.example pre { margin-top: 4px; margin-bottom: 0; }\\n\\n/* For figures & tables */\\np.caption { margin-bottom: 0; }\\ndiv.figure { text-align: center; }\\n\\n/* The index */\\ndiv.index { border: 1px solid black;\\n            background-color: #eeeeee; }\\ndiv.index h1 { padding-left: 0.5em; margin-top: 0.5ex;\\n               border-bottom: 1px solid black; }\\nul.index { margin-left: 0.5em; padding-left: 0; }\\nli.index { list-style-type: none; }\\np.index-heading { font-size: 120%; font-style: italic; margin: 0; }\\nli.index ul { margin-left: 2em; padding-left: 0; }\\n\\n/* \\'Note\\' callouts */\\ndiv.note\\n{\\n  border-right:   #87ceeb 1px solid;\\n  padding-right: 4px;\\n  border-top: #87ceeb 1px solid;\\n  padding-left: 4px;\\n  padding-bottom: 4px;\\n  margin: 2px 5% 10px;\\n  border-left: #87ceeb 1px solid;\\n  padding-top: 4px;\\n  border-bottom: #87ceeb 1px solid;\\n  font-style: normal;\\n  font-family: verdana, arial;\\n  background-color: #b0c4de;\\n}\\n\\ntable.avm { border: 0px solid black; width: 0; }\\ntable.avm tbody tr {border: 0px solid black; }\\ntable.avm tbody tr td { padding: 2px; }\\ntable.avm tbody tr td.avm-key { padding: 5px; font-variant: small-caps; }\\ntable.avm tbody tr td.avm-eq { padding: 5px; }\\ntable.avm tbody tr td.avm-val { padding: 5px; font-style: italic; }\\np.avm-empty { font-style: normal; }\\ntable.avm colgroup col { border: 0px solid black; }\\ntable.avm tbody tr td.avm-topleft \\n    { border-left: 2px solid #000080; border-top: 2px solid #000080; }\\ntable.avm tbody tr td.avm-botleft \\n    { border-left: 2px solid #000080; border-bottom: 2px solid #000080; }\\ntable.avm tbody tr td.avm-topright\\n    { border-right: 2px solid #000080; border-top: 2px solid #000080; }\\ntable.avm tbody tr td.avm-botright\\n    { border-right: 2px solid #000080; border-bottom: 2px solid #000080; }\\ntable.avm tbody tr td.avm-left\\n    { border-left: 2px solid #000080; }\\ntable.avm tbody tr td.avm-right\\n    { border-right: 2px solid #000080; }\\ntable.avm tbody tr td.avm-topbotleft\\n    { border: 2px solid #000080; border-right: 0px solid black; }\\ntable.avm tbody tr td.avm-topbotright\\n    { border: 2px solid #000080; border-left: 0px solid black; }\\ntable.avm tbody tr td.avm-ident\\n    { font-size: 80%; padding: 0; padding-left: 2px; vertical-align: top; }\\n.avm-pointer\\n{ border: 1px solid #008000; padding: 1px; color: #008000; \\n  background: #c0ffc0; font-style: normal; }\\n\\ntable.gloss { border: 0px solid black; width: 0; }\\ntable.gloss tbody tr { border: 0px solid black; }\\ntable.gloss tbody tr td { border: 0px solid black; }\\ntable.gloss colgroup col { border: 0px solid black; }\\ntable.gloss p { margin: 0; padding: 0; }\\n\\ntable.rst-example { border: 1px solid black; }\\ntable.rst-example tbody tr td { background: #eeeeee; }\\ntable.rst-example thead tr th { background: #c0ffff; }\\ntd.rst-raw { width: 0; }\\n\\n/* Used by nltk.org/doc/test: */\\ndiv.doctest-list { text-align: center; }\\ntable.doctest-list { border: 1px solid black;\\n  margin-left: auto; margin-right: auto;\\n}\\ntable.doctest-list tbody tr td { background: #eeeeee;\\n  border: 1px solid #cccccc; text-align: left; }\\ntable.doctest-list thead tr th { background: #304050; color: #ffffff;\\n  border: 1px solid #000000;}\\ntable.doctest-list thead tr a { color: #ffffff; }\\nspan.doctest-passed { color: #008000; }\\nspan.doctest-failed { color: #800000; }\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n<!-- Grammatical Category - e.g. NP and verb as technical terms\\n.. role:: gc\\n   :class: category -->\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n<!-- standard global imports\\n\\n>>> import nltk, re, pprint\\n>>> from nltk import word_tokenize -->\\n\\n\\n\\n<!-- TODO: other issues\\n- nltk.corpus.brown.items returns a tuple, not a list (cf discussion in ch 6)\\n- invocation of pprint.pprint is a little clunky\\n- regexp_tokenize() doesn\\'t work when it is given a compiled pattern -->\\n\\n\\n\\n\\n\\n\\n\\n<!-- TODO: vowel harmony example: extract vowel sequence using re.findall; extract bigrams from the\\nvowel sequences; then build a conditional frequency distribution -->\\n\\n3&nbsp;&nbsp;&nbsp;Processing Raw Text\\nThe most important source of texts is undoubtedly the Web.  It\\'s convenient\\nto have existing text collections to explore, such as the corpora we saw\\nin the previous chapters.  However, you probably have your own text sources\\nin mind, and need to learn how to access them.\\nThe goal of this chapter is to answer the following questions:\\n\\nHow can we write programs to access text from local files and\\nfrom the web, in order to get hold of an unlimited range of\\nlanguage material?\\nHow can we split documents up into individual words and\\npunctuation symbols, so we can carry out the same kinds of\\nanalysis we did with text corpora in earlier chapters?\\nHow can we write programs to produce formatted output\\nand save it in a file?\\n\\nIn order to address these questions, we will be covering\\nkey concepts in NLP, including tokenization and stemming.\\nAlong the way you will consolidate your Python knowledge and\\nlearn about strings, files, and regular expressions.  Since\\nso much text on the web is in HTML format, we will also\\nsee how to dispense with markup.\\n\\nNote\\nImportant:\\nFrom this chapter onwards, our program samples will assume you\\nbegin your interactive session or your program with the following import\\nstatements:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; from __future__ import division  # Python 2 users only\\n&gt;&gt;&gt; import nltk, re, pprint\\n&gt;&gt;&gt; from nltk import word_tokenize\\n\\n\\n\\n\\n\\n3.1&nbsp;&nbsp;&nbsp;Accessing Text from the Web and from Disk\\n\\nElectronic Books\\nA small sample of texts from Project Gutenberg appears in the NLTK corpus collection.\\nHowever, you may be interested in analyzing other texts from Project Gutenberg.\\nYou can browse the catalog of 25,000 free online books at\\nhttp://www.gutenberg.org/catalog/, and obtain a URL to an ASCII text file.\\nAlthough 90% of the texts in Project Gutenberg are in English, it\\nincludes material in over 50 other languages, including Catalan, Chinese, Dutch,\\nFinnish, French, German, Italian, Portuguese and Spanish (with more than\\n100 texts each).\\nText number 2554 is an English translation of Crime and Punishment,\\nand we can access it as follows.\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; from urllib import request\\n&gt;&gt;&gt; url = \"http://www.gutenberg.org/files/2554/2554-0.txt\"\\n&gt;&gt;&gt; response = request.urlopen(url)\\n&gt;&gt;&gt; raw = response.read().decode(\\'utf8\\')\\n&gt;&gt;&gt; type(raw)\\n&lt;class \\'str\\'&gt;\\n&gt;&gt;&gt; len(raw)\\n1176893\\n&gt;&gt;&gt; raw[:75]\\n\\'The Project Gutenberg EBook of Crime and Punishment, by Fyodor Dostoevsky\\\\r\\\\n\\'\\n\\n\\n\\n\\nNote\\nThe read() process will take a few seconds as it downloads this large book.\\nIf you\\'re using an internet proxy which is not correctly detected by Python,\\nyou may need to specify the proxy manually, before using urlopen, as follows:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; proxies = {\\'http\\': \\'http://www.someproxy.com:3128\\'}\\n&gt;&gt;&gt; request.ProxyHandler(proxies)\\n\\n\\n\\n\\nThe variable raw contains a string with 1,176,893 characters.\\n(We can see that it is a string, using type(raw).)\\nThis is the raw content of the book,\\nincluding many details we are not interested in such as\\nwhitespace, line breaks and blank lines.  Notice the \\\\r and \\\\n\\nin the opening line of the file, which is how Python displays the\\nspecial carriage return and line feed characters (the file must\\nhave been created on a Windows machine).  For our language\\nprocessing, we want to break up the string into\\nwords and punctuation, as we saw in 1..  This step is\\ncalled tokenization, and it produces our familiar structure, a list of words\\nand punctuation.\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; tokens = word_tokenize(raw)\\n&gt;&gt;&gt; type(tokens)\\n&lt;class \\'list\\'&gt;\\n&gt;&gt;&gt; len(tokens)\\n254354\\n&gt;&gt;&gt; tokens[:10]\\n[\\'The\\', \\'Project\\', \\'Gutenberg\\', \\'EBook\\', \\'of\\', \\'Crime\\', \\'and\\', \\'Punishment\\', \\',\\', \\'by\\']\\n\\n\\n\\nNotice that NLTK was needed for tokenization, but not for any of the\\nearlier tasks of opening a URL and reading it into a string.\\nIf we now take the further step of creating an NLTK text from this\\nlist, we can carry out all of the other linguistic processing we saw\\nin 1., along with the regular list operations\\nlike slicing:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; text = nltk.Text(tokens)\\n&gt;&gt;&gt; type(text)\\n&lt;class \\'nltk.text.Text\\'&gt;\\n&gt;&gt;&gt; text[1024:1062]\\n[\\'CHAPTER\\', \\'I\\', \\'On\\', \\'an\\', \\'exceptionally\\', \\'hot\\', \\'evening\\', \\'early\\', \\'in\\',\\n \\'July\\', \\'a\\', \\'young\\', \\'man\\', \\'came\\', \\'out\\', \\'of\\', \\'the\\', \\'garret\\', \\'in\\',\\n \\'which\\', \\'he\\', \\'lodged\\', \\'in\\', \\'S.\\', \\'Place\\', \\'and\\', \\'walked\\', \\'slowly\\',\\n \\',\\', \\'as\\', \\'though\\', \\'in\\', \\'hesitation\\', \\',\\', \\'towards\\', \\'K.\\', \\'bridge\\', \\'.\\']\\n&gt;&gt;&gt; text.collocations()\\nKaterina Ivanovna; Pyotr Petrovitch; Pulcheria Alexandrovna; Avdotya\\nRomanovna; Rodion Romanovitch; Marfa Petrovna; Sofya Semyonovna; old\\nwoman; Project Gutenberg-tm; Porfiry Petrovitch; Amalia Ivanovna;\\ngreat deal; Nikodim Fomitch; young man; Ilya Petrovitch; n\\'t know;\\nProject Gutenberg; Dmitri Prokofitch; Andrey Semyonovitch; Hay Market\\n\\n\\n\\nNotice that Project Gutenberg appears as a collocation.\\nThis is because each text downloaded from Project Gutenberg contains a header with the\\nname of the text, the author, the names of people who scanned and\\ncorrected the text, a license, and so on.  Sometimes this information\\nappears in a footer at the end of the file.  We cannot reliably detect\\nwhere the content begins and ends, and so have to resort to manual\\ninspection of the file, to discover unique strings that mark the beginning\\nand the end, before trimming raw to be just the content and nothing else:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; raw.find(\"PART I\")\\n5338\\n&gt;&gt;&gt; raw.rfind(\"End of Project Gutenberg\\'s Crime\")\\n1157743\\n&gt;&gt;&gt; raw = raw[5338:1157743] \\n&gt;&gt;&gt; raw.find(\"PART I\")\\n0\\n\\n\\n\\nThe find() and rfind() (\"reverse find\") methods help us get\\nthe right index values to use for slicing the string .\\nWe overwrite raw with this slice, so now it begins\\nwith \"PART I\" and goes up to (but not including)\\nthe phrase that marks the end of the content.\\nThis was our first brush with the reality of the web:\\ntexts found on the web may contain unwanted material,\\nand there may not be an automatic way to remove it.\\nBut with a small amount of extra work we can extract the material we need.\\n\\n\\nDealing with HTML\\nMuch of the text on the web is in the form of HTML documents.\\nYou can use a web browser to save a page as text to a local\\nfile, then access this as described in the section on files below.\\nHowever, if you\\'re going to do this often, it\\'s easiest to get Python\\nto do the work directly.  The first step is the same as before,\\nusing urlopen.  For fun we\\'ll pick a BBC News story\\ncalled Blondes to die out in 200 years, an urban legend\\npassed along by the BBC as established scientific fact:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; url = \"http://news.bbc.co.uk/2/hi/health/2284783.stm\"\\n&gt;&gt;&gt; html = request.urlopen(url).read().decode(\\'utf8\\')\\n&gt;&gt;&gt; html[:60]\\n\\'&lt;!doctype html public \"-//W3C//DTD HTML 4.0 Transitional//EN\\'\\n\\n\\n\\nYou can type print(html) to see the HTML content in all its glory,\\nincluding meta tags, an image map, JavaScript, forms, and tables.\\nTo get text out of HTML we will use a Python library called BeautifulSoup,\\navailable from http://www.crummy.com/software/BeautifulSoup/:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; from bs4 import BeautifulSoup\\n&gt;&gt;&gt; raw = BeautifulSoup(html, \\'html.parser\\').get_text()\\n&gt;&gt;&gt; tokens = word_tokenize(raw)\\n&gt;&gt;&gt; tokens\\n[\\'BBC\\', \\'NEWS\\', \\'|\\', \\'Health\\', \\'|\\', \\'Blondes\\', \"\\'to\", \\'die\\', \\'out\\', ...]\\n\\n\\n\\nThis still contains unwanted material concerning site navigation and related\\nstories.  With some trial and error you can find the start and end indexes of the\\ncontent and select the tokens of interest, and initialize a text as before.\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; tokens = tokens[110:390]\\n&gt;&gt;&gt; text = nltk.Text(tokens)\\n&gt;&gt;&gt; text.concordance(\\'gene\\')\\nDisplaying 5 of 5 matches:\\nhey say too few people now carry the gene for blondes to last beyond the next\\nblonde hair is caused by a recessive gene . In order for a child to have blond\\nhave blonde hair , it must have the gene on both sides of the family in the g\\nere is a disadvantage of having that gene or by chance . They do n\\'t disappear\\ndes would disappear is if having the gene was a disadvantage and I do not thin\\n\\n\\n\\n\\n\\nProcessing Search Engine Results\\nThe web can be thought of as a huge corpus of unannotated text.  Web\\nsearch engines provide an efficient means of searching this large\\nquantity of text for relevant linguistic examples.  The main advantage\\nof search engines is size: since you are searching such a large set of\\ndocuments, you are more likely to find any linguistic pattern you\\nare interested in.  Furthermore, you can make use of very specific\\npatterns, which would only match one or two examples on a smaller\\nexample, but which might match tens of thousands of examples when run\\non the web.  A second advantage of web search engines is that they are\\nvery easy to use.  Thus, they provide a very convenient tool for\\nquickly checking a theory, to see if it is reasonable.\\n<!-- XXX Accessing a search engine programmatically: search results; counts;\\nPython code to produce the contents of tab-absolutely_; mention\\nYahoo Python API and xref to discussion of this in chap-data_.] -->\\nTable 3.1: Google Hits for Collocations: The number of hits for collocations\\ninvolving the words absolutely or definitely, followed\\nby one of adore, love, like, or prefer.\\n(Liberman, in LanguageLog, 2005).\\n\\n\\n\\n\\n\\n\\n\\n\\nGoogle hits\\nadore\\nlove\\nlike\\nprefer\\n\\n\\n\\nabsolutely\\n289,000\\n905,000\\n16,200\\n644\\n\\ndefinitely\\n1,460\\n51,000\\n158,000\\n62,600\\n\\nratio\\n198:1\\n18:1\\n1:10\\n1:97\\n\\n\\n\\n\\n\\nUnfortunately, search engines have some significant shortcomings.\\nFirst, the allowable range of search patterns is severely restricted.\\nUnlike local corpora, where you write programs to search for\\narbitrarily complex patterns, search engines generally\\nonly allow you to search for individual words or strings of\\nwords, sometimes with wildcards.  Second, search engines give\\ninconsistent results, and can give widely different figures when used\\nat different times or in different geographical regions.  When content has been\\nduplicated across multiple sites, search results may be boosted.\\nFinally, the markup in the result returned by a search engine may change unpredictably,\\nbreaking any pattern-based method of locating particular content (a problem\\nwhich is ameliorated by the use of search engine APIs).\\n\\nNote\\nYour Turn:\\nSearch the web for \"the of\" (inside quotes).  Based on the large\\ncount, can we conclude that the of is a frequent collocation\\nin English?\\n\\n\\n\\nProcessing RSS Feeds\\n\\n\\nThe blogosphere is an important source of text, in both formal and informal registers.\\nWith the help of a Python library called the Universal Feed Parser,\\navailable from https://pypi.python.org/pypi/feedparser, we can access the content\\nof a blog, as shown below:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; import feedparser\\n&gt;&gt;&gt; llog = feedparser.parse(\"http://languagelog.ldc.upenn.edu/nll/?feed=atom\")\\n&gt;&gt;&gt; llog[\\'feed\\'][\\'title\\']\\n\\'Language Log\\'\\n&gt;&gt;&gt; len(llog.entries)\\n15\\n&gt;&gt;&gt; post = llog.entries[2]\\n&gt;&gt;&gt; post.title\\n\"He\\'s My BF\"\\n&gt;&gt;&gt; content = post.content[0].value\\n&gt;&gt;&gt; content[:70]\\n\\'&lt;p&gt;Today I was chatting with three of our visiting graduate students f\\'\\n&gt;&gt;&gt; raw = BeautifulSoup(content, \\'html.parser\\').get_text()\\n&gt;&gt;&gt; word_tokenize(raw)\\n[\\'Today\\', \\'I\\', \\'was\\', \\'chatting\\', \\'with\\', \\'three\\', \\'of\\', \\'our\\', \\'visiting\\',\\n\\'graduate\\', \\'students\\', \\'from\\', \\'the\\', \\'PRC\\', \\'.\\', \\'Thinking\\', \\'that\\', \\'I\\',\\n\\'was\\', \\'being\\', \\'au\\', \\'courant\\', \\',\\', \\'I\\', \\'mentioned\\', \\'the\\', \\'expression\\',\\n\\'DUI4XIANG4\\', \\'\\\\u5c0d\\\\u8c61\\', \\'(\"\\', \\'boy\\', \\'/\\', \\'girl\\', \\'friend\\', \\'\"\\', ...]\\n\\n\\n\\nWith some further work, we can write programs to create a small corpus of blog posts,\\nand use this as the basis for our NLP work.\\n<!-- XXX I\\'ve played around with feeds myself, and found it kind of\\nfrustrating, in the sense that it\\'s very hard to know what kind of\\ndata structure you\\'re getting back, and therefore hard to know what\\nkind of operations you can perform. This snippet illustrates the\\nproblem rather poignantly. How can the reader get a handle on what\\nsomething like this means?\\n   >>> content = post.content[0].value\\nNB this also prints out unicode strings, which haven\\'t been\\nexplained yet. There\\'s also a \"so what\" feeling about this - -\\nthere\\'s a chunk of code, but no discussion about what it amounts to. -->\\n\\n\\nReading Local Files\\n<!-- Monkey-patching to fake the file/web examples in this section:\\n\\n>>> from io import StringIO\\n>>> def fake_open(filename, mode=None):\\n...     return StringIO(\\'Time flies like an arrow.\\\\nFruit flies like a banana.\\\\n\\')\\n>>> def fake_urlopen(url):\\n...     return StringIO(\\'<!doctype html public \"-//W3C//DTD HTML 4.0 Transitional//EN\"\\')\\n>>> open = fake_open\\n>>> from urllib import request\\n>>> request.urlopen.read = lambda: fake_urlopen -->\\nIn order to read a local file, we need to use Python\\'s built-in open() function,\\nfollowed by the read() method.  Suppose you have a file document.txt, you\\ncan load its contents like this:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; f = open(\\'document.txt\\')\\n&gt;&gt;&gt; raw = f.read()\\n\\n\\n\\n\\nNote\\nYour Turn:\\nCreate a file called document.txt using a text editor, and type in a few lines of text,\\nand save it as plain text.\\nIf you are using IDLE, select the New Window command in the File menu, typing\\nthe required text into this window, and then saving the file as\\ndocument.txt inside the directory that IDLE offers in the pop-up dialogue box.\\nNext, in the Python interpreter, open the file using f = open(\\'document.txt\\'), then\\ninspect its contents using print(f.read()).\\n\\nVarious things might have gone wrong when you tried this.\\nIf the interpreter couldn\\'t find your file, you would have seen an\\nerror like this:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; f = open(\\'document.txt\\')\\nTraceback (most recent call last):\\nFile \"&lt;pyshell#7&gt;\", line 1, in -toplevel-\\nf = open(\\'document.txt\\')\\nIOError: [Errno 2] No such file or directory: \\'document.txt\\'\\n\\n\\n\\nTo check that the file that you are trying to open is really in the\\nright directory, use IDLE\\'s Open command in the File menu;\\nthis will display a list of all the files in the directory where\\nIDLE is running. An alternative is to examine the current\\ndirectory from within Python:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; import os\\n&gt;&gt;&gt; os.listdir(\\'.\\')\\n\\n\\n\\nAnother possible problem you might have encountered when accessing a text file\\nis the newline conventions, which are different for different operating systems.\\nThe built-in open() function has a second parameter for controlling how\\nthe file is opened: open(\\'document.txt\\', \\'rU\\') —\\n\\'r\\' means to open the file for reading (the default), and\\n\\'U\\' stands for \"Universal\", which lets us ignore the different\\nconventions used for marking newlines.\\nAssuming that you can open the file, there are several methods for reading it.\\nThe read() method creates a string with the contents of the entire file:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; f.read()\\n\\'Time flies like an arrow.\\\\nFruit flies like a banana.\\\\n\\'\\n\\n\\n\\nRecall that the \\'\\\\n\\' characters are newlines; this\\nis equivalent to pressing Enter on a keyboard and starting a new line.\\n<!-- XXX I think we also mentioned print, for suppressing a newline - -\\ndo they need to know about both of these? -->\\nWe can also read a file one line at a time using a for loop:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; f = open(\\'document.txt\\', \\'rU\\')\\n&gt;&gt;&gt; for line in f:\\n...     print(line.strip())\\nTime flies like an arrow.\\nFruit flies like a banana.\\n\\n\\n\\nHere we use the strip() method to remove the newline character at the end of\\nthe input line.\\nNLTK\\'s corpus files can also be accessed using these methods.  We simply\\nhave to use nltk.data.find() to get the filename for any corpus item.\\nThen we can open and read it in the way we just demonstrated above:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; path = nltk.data.find(\\'corpora/gutenberg/melville-moby_dick.txt\\')\\n&gt;&gt;&gt; raw = open(path, \\'rU\\').read()\\n\\n\\n\\n\\n\\nExtracting Text from PDF, MSWord and other Binary Formats\\nASCII text and HTML text are human readable formats.  Text often comes in binary\\nformats — like PDF and MSWord — that can only be opened using specialized\\nsoftware.  Third-party libraries such as pypdf and pywin32\\nprovide access to\\nthese formats.  Extracting text from multi-column documents is particularly\\nchallenging.  For once-off conversion of a few documents,\\nit is simpler to open the document with a suitable application, then save it as text\\nto your local drive, and access it as described below.\\nIf the document is already on the web, you can enter its URL in Google\\'s search box.\\nThe search result often includes a link to an HTML version of the document,\\nwhich you can save as text.\\n\\n\\nCapturing User Input\\nSometimes we want to capture the text that a user inputs when she is\\ninteracting with our program. To prompt the user\\nto type a line of input, call the Python function input().\\nAfter saving the input to a variable, we can\\nmanipulate it just as we have done for other strings.\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; s = input(\"Enter some text: \")\\nEnter some text: On an exceptionally hot evening early in July\\n&gt;&gt;&gt; print(\"You typed\", len(word_tokenize(s)), \"words.\")\\nYou typed 8 words.\\n\\n\\n\\n\\n\\nThe NLP Pipeline\\n3.1 summarizes what we have covered in this section, including the process\\nof building a vocabulary that we saw in 1..  (One step, normalization,\\nwill be discussed in 3.6.)\\n\\n\\nFigure 3.1: The Processing Pipeline: We open a URL and read its HTML content,\\nremove the markup and select a slice of characters;\\nthis is then tokenized and optionally converted into an nltk.Text object;\\nwe can also lowercase all the words and extract the vocabulary.\\n\\nThere\\'s a lot going on in this pipeline.  To understand it properly, it helps to be\\nclear about the type of each variable that it mentions.  We find out the type\\nof any Python object x using type(x), e.g. type(1) is &lt;int&gt;\\nsince 1 is an integer.\\nWhen we load the contents of a URL or file, and when we strip out HTML markup,\\nwe are dealing with strings, Python\\'s &lt;str&gt; data type.\\n(We will learn more about strings in 3.2):\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; raw = open(\\'document.txt\\').read()\\n&gt;&gt;&gt; type(raw)\\n&lt;class \\'str\\'&gt;\\n\\n\\n\\nWhen we tokenize a string we produce a list (of words), and this is Python\\'s &lt;list&gt;\\ntype.  Normalizing and sorting lists produces other lists:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; tokens = word_tokenize(raw)\\n&gt;&gt;&gt; type(tokens)\\n&lt;class \\'list\\'&gt;\\n&gt;&gt;&gt; words = [w.lower() for w in tokens]\\n&gt;&gt;&gt; type(words)\\n&lt;class \\'list\\'&gt;\\n&gt;&gt;&gt; vocab = sorted(set(words))\\n&gt;&gt;&gt; type(vocab)\\n&lt;class \\'list\\'&gt;\\n\\n\\n\\nThe type of an object determines what operations you can perform on it.\\nSo, for example, we can append to a list but not to a string:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; vocab.append(\\'blog\\')\\n&gt;&gt;&gt; raw.append(\\'blog\\')\\nTraceback (most recent call last):\\n  File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\\nAttributeError: \\'str\\' object has no attribute \\'append\\'\\n\\n\\n\\n\\nSimilarly, we can concatenate strings with strings, and lists with\\nlists, but we cannot concatenate strings with lists:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; query = \\'Who knows?\\'\\n&gt;&gt;&gt; beatles = [\\'john\\', \\'paul\\', \\'george\\', \\'ringo\\']\\n&gt;&gt;&gt; query + beatles\\nTraceback (most recent call last):\\n  File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\\nTypeError: cannot concatenate \\'str\\' and \\'list\\' objects\\n\\n\\n\\n\\n\\n\\n3.2&nbsp;&nbsp;&nbsp;Strings: Text Processing at the Lowest Level\\nIt\\'s time to examine a fundamental data type that we\\'ve been studiously avoiding\\nso far.  In earlier chapters we focused on a text as a list of words.  We didn\\'t\\nlook too closely at words and how they are handled in the programming\\nlanguage.  By using NLTK\\'s corpus interface we were able to ignore\\nthe files that these texts had come from.  The contents of a word, and\\nof a file, are represented by programming languages as a fundamental\\ndata type known as a string.  In this section we explore strings\\nin detail, and show the connection between strings, words, texts and files.\\n\\nBasic Operations with Strings\\nStrings are specified using single quotes \\nor double quotes , as shown below.\\nIf a string contains a single quote, we must backslash-escape\\nthe quote  so Python knows a literal quote character is intended,\\nor else put the string in double quotes .\\nOtherwise, the quote inside the string \\nwill be interpreted as a close quote, and the Python interpreter\\nwill report a syntax error:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; monty = \\'Monty Python\\' \\n&gt;&gt;&gt; monty\\n\\'Monty Python\\'\\n&gt;&gt;&gt; circus = \"Monty Python\\'s Flying Circus\" \\n&gt;&gt;&gt; circus\\n\"Monty Python\\'s Flying Circus\"\\n&gt;&gt;&gt; circus = \\'Monty Python\\\\\\'s Flying Circus\\' \\n&gt;&gt;&gt; circus\\n\"Monty Python\\'s Flying Circus\"\\n&gt;&gt;&gt; circus = \\'Monty Python\\'s Flying Circus\\' \\n  File \"&lt;stdin&gt;\", line 1\\n    circus = \\'Monty Python\\'s Flying Circus\\'\\n                           ^\\nSyntaxError: invalid syntax\\n\\n\\n\\nSometimes strings go over several lines.  Python provides us with various\\nways of entering them.  In the next example, a sequence of two strings is\\njoined into a single string.\\nWe need to use backslash  or parentheses  so that\\nthe interpreter knows that the statement is not complete after the first line.\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; couplet = \"Shall I compare thee to a Summer\\'s day?\"\\\\\\n...           \"Thou are more lovely and more temperate:\" \\n&gt;&gt;&gt; print(couplet)\\nShall I compare thee to a Summer\\'s day?Thou are more lovely and more temperate:\\n&gt;&gt;&gt; couplet = (\"Rough winds do shake the darling buds of May,\"\\n...           \"And Summer\\'s lease hath all too short a date:\") \\n&gt;&gt;&gt; print(couplet)\\nRough winds do shake the darling buds of May,And Summer\\'s lease hath all too short a date:\\n\\n\\n\\nUnfortunately the above methods do not give us a newline between\\nthe two lines of the sonnet.  Instead, we can use a triple-quoted\\nstring as follows:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; couplet = \"\"\"Shall I compare thee to a Summer\\'s day?\\n... Thou are more lovely and more temperate:\"\"\"\\n&gt;&gt;&gt; print(couplet)\\nShall I compare thee to a Summer\\'s day?\\nThou are more lovely and more temperate:\\n&gt;&gt;&gt; couplet = \\'\\'\\'Rough winds do shake the darling buds of May,\\n... And Summer\\'s lease hath all too short a date:\\'\\'\\'\\n&gt;&gt;&gt; print(couplet)\\nRough winds do shake the darling buds of May,\\nAnd Summer\\'s lease hath all too short a date:\\n\\n\\n\\nNow that we can define strings, we can try some simple operations on them.\\nFirst let\\'s look at the + operation, known as concatenation .\\nIt produces a new string that is a copy of the\\ntwo original strings pasted together end-to-end.  Notice that\\nconcatenation doesn\\'t do anything clever like insert a space between\\nthe words.  We can even multiply strings :\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; \\'very\\' + \\'very\\' + \\'very\\' \\n\\'veryveryvery\\'\\n&gt;&gt;&gt; \\'very\\' * 3 \\n\\'veryveryvery\\'\\n\\n\\n\\n\\nNote\\nYour Turn:\\nTry running the following code, then try to use your understanding\\nof the string + and * operations to figure out how it works.\\nBe careful to distinguish between the string \\' \\', which\\nis a single whitespace character, and \\'\\', which is the empty string.\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; a = [1, 2, 3, 4, 5, 6, 7, 6, 5, 4, 3, 2, 1]\\n&gt;&gt;&gt; b = [\\' \\' * 2 * (7 - i) + \\'very\\' * i for i in a]\\n&gt;&gt;&gt; for line in b:\\n...     print(line)\\n\\n\\n\\n\\n\\nWe\\'ve seen that the addition and multiplication operations apply to\\nstrings, not just numbers.  However, note that we cannot use\\nsubtraction or division with strings:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; \\'very\\' - \\'y\\'\\nTraceback (most recent call last):\\n  File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\\nTypeError: unsupported operand type(s) for -: \\'str\\' and \\'str\\'\\n&gt;&gt;&gt; \\'very\\' / 2\\nTraceback (most recent call last):\\n  File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\\nTypeError: unsupported operand type(s) for /: \\'str\\' and \\'int\\'\\n\\n\\n\\nThese error messages are another example of Python telling us that we\\nhave got our data types in a muddle. In the first case, we are told\\nthat the operation of subtraction (i.e., -) cannot apply to\\nobjects of type str (strings), while in the second, we are told that\\ndivision cannot take str and int as its two operands.\\n\\n\\nPrinting Strings\\nSo far, when we have wanted to look at the contents of a variable or\\nsee the result of a calculation, we have just typed the variable name\\ninto the interpreter.  We can also see the contents of a variable\\nusing the print statement:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; print(monty)\\nMonty Python\\n\\n\\n\\nNotice that there are no quotation marks this time.  When we inspect\\na variable by typing its name in the interpreter, the interpreter prints\\nthe Python representation of its value.  Since it\\'s a string,\\nthe result is quoted.  However, when we tell the\\ninterpreter to print the contents of the variable, we don\\'t see\\nquotation characters since there are none inside the string.\\nThe print statement allows us to display more than one item on a line\\nin various ways, as shown below:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; grail = \\'Holy Grail\\'\\n&gt;&gt;&gt; print(monty + grail)\\nMonty PythonHoly Grail\\n&gt;&gt;&gt; print(monty, grail)\\nMonty Python Holy Grail\\n&gt;&gt;&gt; print(monty, \"and the\", grail)\\nMonty Python and the Holy Grail\\n\\n\\n\\n\\n\\nAccessing Individual Characters\\nAs we saw in 2 for lists, strings are indexed, starting from zero.\\nWhen we index a string, we get one of its characters (or letters).  A single character is nothing special — it\\'s just\\na string of length 1.\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; monty[0]\\n\\'M\\'\\n&gt;&gt;&gt; monty[3]\\n\\'t\\'\\n&gt;&gt;&gt; monty[5]\\n\\' \\'\\n\\n\\n\\nAs with lists, if we try to access an index that is outside of the string we get an error:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; monty[20]\\nTraceback (most recent call last):\\n  File \"&lt;stdin&gt;\", line 1, in ?\\nIndexError: string index out of range\\n\\n\\n\\n<!-- XXX I don\\'t think it works very well here to have these two observations\\nfollowed by two code examples - - it\\'s hard to see what the point of\\nthe ``5 = len(monty) - 7`` remark is in this context. Since you\\nprobably don\\'t want to split up the two examples, callouts might\\nameliorate it. -->\\nAgain as with lists, we can use negative indexes for strings,\\nwhere -1 is the index of the last character .\\nPositive and negative indexes give us two ways to refer to\\nany position in a string.  In this case, when the string had a length of 12,\\nindexes 5 and -7 both refer to the same character (a space).\\n(Notice that 5 = len(monty) - 7.)\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; monty[-1] \\n\\'n\\'\\n&gt;&gt;&gt; monty[5]\\n\\' \\'\\n&gt;&gt;&gt; monty[-7]\\n\\' \\'\\n\\n\\n\\nWe can write for loops to iterate over the characters\\nin strings.  This print function includes the optional end=\\' \\'\\nparameter, which is how we tell Python to print a space instead of a newline at the end.\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; sent = \\'colorless green ideas sleep furiously\\'\\n&gt;&gt;&gt; for char in sent:\\n...     print(char, end=\\' \\')\\n...\\nc o l o r l e s s   g r e e n   i d e a s   s l e e p   f u r i o u s l y\\n\\n\\n\\nWe can count individual characters as well.  We should ignore the case\\ndistinction by normalizing everything to lowercase, and filter out non-alphabetic characters:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; from nltk.corpus import gutenberg\\n&gt;&gt;&gt; raw = gutenberg.raw(\\'melville-moby_dick.txt\\')\\n&gt;&gt;&gt; fdist = nltk.FreqDist(ch.lower() for ch in raw if ch.isalpha())\\n&gt;&gt;&gt; fdist.most_common(5)\\n[(\\'e\\', 117092), (\\'t\\', 87996), (\\'a\\', 77916), (\\'o\\', 69326), (\\'n\\', 65617)]\\n&gt;&gt;&gt; [char for (char, count) in fdist.most_common()]\\n[\\'e\\', \\'t\\', \\'a\\', \\'o\\', \\'n\\', \\'i\\', \\'s\\', \\'h\\', \\'r\\', \\'l\\', \\'d\\', \\'u\\', \\'m\\', \\'c\\', \\'w\\',\\n\\'f\\', \\'g\\', \\'p\\', \\'b\\', \\'y\\', \\'v\\', \\'k\\', \\'q\\', \\'j\\', \\'x\\', \\'z\\']\\n\\n\\n\\n\\n\\n\\n[sb]explain this tuple unpacking somewhere?\\n\\n\\nThis gives us the letters of the alphabet, with the most frequently occurring letters\\nlisted first (this is quite complicated and we\\'ll explain it more carefully below).\\nYou might like to visualize the distribution using fdist.plot().\\nThe relative character frequencies of a text can be used in automatically identifying\\nthe language of the text.\\n\\n\\nAccessing Substrings\\n\\n\\nFigure 3.2: String Slicing: The string \"Monty Python\" is shown along with its positive and\\nnegative indexes; two substrings are selected using \"slice\" notation.\\nThe slice [m,n] contains the characters from position m through n-1.\\n\\nA substring is any continuous section of a string that we want to pull out for\\nfurther processing.  We can easily access substrings using the same slice notation\\nwe used for lists (see 3.2).\\nFor example, the following code accesses the substring starting at index 6,\\nup to (but not including) index 10:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; monty[6:10]\\n\\'Pyth\\'\\n\\n\\n\\nHere we see the characters are \\'P\\', \\'y\\', \\'t\\', and \\'h\\' which correspond\\nto monty[6] ... monty[9] but not monty[10]. This is because\\na slice starts at the first index but finishes one before the end index.\\nWe can also slice with negative indexes — the same basic rule of starting\\nfrom the start index and stopping one before the end index applies;\\nhere we stop before the space character.\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; monty[-12:-7]\\n\\'Monty\\'\\n\\n\\n\\nAs with list slices, if we omit the first value, the substring begins at the start\\nof the string.  If we omit the second value, the substring continues to the end\\nof the string:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; monty[:5]\\n\\'Monty\\'\\n&gt;&gt;&gt; monty[6:]\\n\\'Python\\'\\n\\n\\n\\nWe test if a string contains a particular substring using the in operator, as follows:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; phrase = \\'And now for something completely different\\'\\n&gt;&gt;&gt; if \\'thing\\' in phrase:\\n...     print(\\'found \"thing\"\\')\\nfound \"thing\"\\n\\n\\n\\nWe can also find the position of a substring within a string, using find():\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; monty.find(\\'Python\\')\\n6\\n\\n\\n\\n\\nNote\\nYour Turn:\\nMake up a sentence and assign it to a variable, e.g. sent = \\'my sentence...\\'.\\nNow write slice expressions to pull out individual words.  (This is obviously\\nnot a convenient way to process the words of a text!)\\n\\n\\n\\nMore operations on strings\\nPython has comprehensive support for processing strings.  A summary, including some operations\\nwe haven\\'t seen yet, is shown in 3.2.  For more information on strings, type\\nhelp(str) at the Python prompt.\\nTable 3.2: Useful String Methods: operations on strings in addition to the string tests\\nshown in 4.2; all methods produce a new string or list\\n\\n\\n\\n\\n\\nMethod\\nFunctionality\\n\\n\\n\\ns.find(t)\\nindex of first instance of string t inside s (-1 if not found)\\n\\ns.rfind(t)\\nindex of last instance of string t inside s (-1 if not found)\\n\\ns.index(t)\\nlike s.find(t) except it raises ValueError if not found\\n\\ns.rindex(t)\\nlike s.rfind(t) except it raises ValueError if not found\\n\\ns.join(text)\\ncombine the words of the text into a string using s as the glue\\n\\ns.split(t)\\nsplit s into a list wherever a t is found (whitespace by default)\\n\\ns.splitlines()\\nsplit s into a list of strings, one per line\\n\\ns.lower()\\na lowercased version of the string s\\n\\ns.upper()\\nan uppercased version of the string s\\n\\ns.title()\\na titlecased version of the string s\\n\\ns.strip()\\na copy of s without leading or trailing whitespace\\n\\ns.replace(t, u)\\nreplace instances of t with u inside s\\n\\n\\n\\n\\n\\n\\n\\nThe Difference between Lists and Strings\\nStrings and lists are both kinds of sequence.  We can pull them\\napart by indexing and slicing them, and we can join them together\\nby concatenating them.  However, we cannot join strings and lists:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; query = \\'Who knows?\\'\\n&gt;&gt;&gt; beatles = [\\'John\\', \\'Paul\\', \\'George\\', \\'Ringo\\']\\n&gt;&gt;&gt; query[2]\\n\\'o\\'\\n&gt;&gt;&gt; beatles[2]\\n\\'George\\'\\n&gt;&gt;&gt; query[:2]\\n\\'Wh\\'\\n&gt;&gt;&gt; beatles[:2]\\n[\\'John\\', \\'Paul\\']\\n&gt;&gt;&gt; query + \" I don\\'t\"\\n\"Who knows? I don\\'t\"\\n&gt;&gt;&gt; beatles + \\'Brian\\'\\nTraceback (most recent call last):\\n  File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\\nTypeError: can only concatenate list (not \"str\") to list\\n&gt;&gt;&gt; beatles + [\\'Brian\\']\\n[\\'John\\', \\'Paul\\', \\'George\\', \\'Ringo\\', \\'Brian\\']\\n\\n\\n\\nWhen we open a file\\nfor reading into a Python program, we get a string\\ncorresponding to the contents of the whole file. If we use a for loop to\\nprocess the elements of this string, all we can pick out are the\\nindividual characters — we don\\'t get to choose the\\ngranularity. By contrast, the elements of a list can be as big or\\nsmall as we like: for example, they could be paragraphs, sentences,\\nphrases, words, characters. So lists have the advantage that we\\ncan be flexible about the elements they contain, and\\ncorrespondingly flexible about any downstream processing.\\nConsequently, one of the first things we are likely to do in a piece of NLP\\ncode is tokenize a string into a list of strings (3.7).\\nConversely, when we want to write our results to a file, or to a terminal,\\nwe will usually format them as a string (3.9).\\nLists and strings do not have exactly the same functionality.\\nLists have the added power that you can change their elements:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; beatles[0] = \"John Lennon\"\\n&gt;&gt;&gt; del beatles[-1]\\n&gt;&gt;&gt; beatles\\n[\\'John Lennon\\', \\'Paul\\', \\'George\\']\\n\\n\\n\\nOn the other hand if we try to do that with a string\\n— changing the 0th character in query to \\'F\\' — we get:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; query[0] = \\'F\\'\\nTraceback (most recent call last):\\n  File \"&lt;stdin&gt;\", line 1, in ?\\nTypeError: object does not support item assignment\\n\\n\\n\\n This is because strings are immutable — you can\\'t change a\\nstring once you have created it.  However, lists are mutable,\\nand their contents can be modified at any time.  As a result, lists\\nsupport operations that modify the original value rather than producing a new value.\\n\\nNote\\nYour Turn:\\nConsolidate your knowledge of strings by trying some of the exercises on\\nstrings at the end of this chapter.\\n\\n\\n\\n\\n3.3&nbsp;&nbsp;&nbsp;Text Processing with Unicode\\nOur programs will often need to deal with different languages, and\\ndifferent character sets.  The concept of \"plain text\" is a fiction.\\nIf you live in the English-speaking world you probably use ASCII,\\npossibly without realizing it.  If you live in Europe you might use\\none of the extended Latin character sets, containing such characters\\nas \"ø\" for Danish and Norwegian, \"ő\" for Hungarian,\\n\"ñ\" for Spanish and Breton, and \"ň\" for Czech and\\nSlovak. In this section, we will give an overview of how to use\\nUnicode for processing texts that use non-ASCII character sets.\\n\\nWhat is Unicode?\\nUnicode supports over a million characters.  Each\\ncharacter is assigned a number, called a code point.  In Python, code\\npoints are written in the form \\\\uXXXX, where XXXX is the number\\nin 4-digit hexadecimal form.\\nWithin a program, we can manipulate Unicode strings just like normal strings.\\nHowever, when Unicode characters are stored in files or displayed on a terminal,\\nthey must be encoded as a stream of bytes.  Some encodings (such\\nas ASCII and Latin-2) use a single byte per code point, so they can only support a\\nsmall subset of Unicode, enough for a single language.  Other encodings\\n(such as UTF-8) use multiple bytes and can represent the full range of\\nUnicode characters.\\nText in files will be in a particular encoding, so we need some\\nmechanism for translating it into Unicode — translation into\\nUnicode is called decoding. Conversely, to write out Unicode to a\\nfile or a terminal, we first need to translate it into a suitable\\nencoding — this translation out of Unicode is called encoding,\\nand is illustrated in 3.3.\\n\\n\\nFigure 3.3: Unicode Decoding and Encoding\\n\\nFrom a Unicode perspective, characters are abstract entities which can\\nbe realized as one or more glyphs. Only glyphs can appear on a\\nscreen or be printed on paper. A font is a mapping from characters to glyphs.\\n\\n\\nExtracting encoded text from files\\nLet\\'s assume that we have a small text file, and that we know how it\\nis encoded. For example, polish-lat2.txt, as the name suggests, is\\na snippet of Polish text (from the Polish Wikipedia; see\\nhttp://pl.wikipedia.org/wiki/Biblioteka_Pruska).  This file is encoded as Latin-2,\\nalso known as ISO-8859-2. The function nltk.data.find() locates the\\nfile for us.\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; path = nltk.data.find(\\'corpora/unicode_samples/polish-lat2.txt\\')\\n\\n\\n\\nThe Python open() function can read encoded data\\ninto Unicode strings, and write out Unicode strings in encoded\\nform.  It takes a parameter to\\nspecify the encoding of the file being read or written. So let\\'s open\\nour Polish file\\nwith the encoding \\'latin2\\' and  inspect the contents of the file:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; f = open(path, encoding=\\'latin2\\')\\n&gt;&gt;&gt; for line in f:\\n...    line = line.strip()\\n...    print(line)\\nPruska Biblioteka Państwowa. Jej dawne zbiory znane pod nazwą\\n\"Berlinka\" to skarb kultury i sztuki niemieckiej. Przewiezione przez\\nNiemców pod koniec II wojny światowej na Dolny Śląsk, zostały\\nodnalezione po 1945 r. na terytorium Polski. Trafiły do Biblioteki\\nJagiellońskiej w Krakowie, obejmują ponad 500 tys. zabytkowych\\narchiwaliów, m.in. manuskrypty Goethego, Mozarta, Beethovena, Bacha.\\n\\n\\n\\nIf this does not display correctly on your terminal, or if we want\\nto see the underlying numerical values (or \"codepoints\") of the characters,\\nthen we can convert all non-ASCII characters into their two-digit \\\\xXX\\nand four-digit \\\\uXXXX representations:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; f = open(path, encoding=\\'latin2\\')\\n&gt;&gt;&gt; for line in f:\\n...     line = line.strip()\\n...     print(line.encode(\\'unicode_escape\\'))\\nb\\'Pruska Biblioteka Pa\\\\\\\\u0144stwowa. Jej dawne zbiory znane pod nazw\\\\\\\\u0105\\'\\nb\\'\"Berlinka\" to skarb kultury i sztuki niemieckiej. Przewiezione przez\\'\\nb\\'Niemc\\\\\\\\xf3w pod koniec II wojny \\\\\\\\u015bwiatowej na Dolny \\\\\\\\u015al\\\\\\\\u0105sk, zosta\\\\\\\\u0142y\\'\\nb\\'odnalezione po 1945 r. na terytorium Polski. Trafi\\\\\\\\u0142y do Biblioteki\\'\\nb\\'Jagiello\\\\\\\\u0144skiej w Krakowie, obejmuj\\\\\\\\u0105 ponad 500 tys. zabytkowych\\'\\nb\\'archiwali\\\\\\\\xf3w, m.in. manuskrypty Goethego, Mozarta, Beethovena, Bacha.\\'\\n\\n\\n\\nThe first line above illustrates a Unicode escape string\\npreceded by the \\\\u escape string, namely \\\\u0144 . The relevant\\nUnicode character will be dislayed on the screen as the glyph\\nń.  In the third line of the preceding example, we see\\n\\\\xf3, which corresponds to the glyph ó, and is within the\\n128-255 range.\\nIn Python 3, source code is encoded using UTF-8 by default, and you can\\ninclude Unicode characters in strings if you are using IDLE or another program editor\\nthat supports Unicode.\\nArbitrary Unicode characters can be included using the\\n\\\\uXXXX escape sequence.\\nWe find the integer ordinal of a character using ord(). For example:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; ord(\\'ń\\')\\n324\\n\\n\\n\\nThe hexadecimal 4 digit notation for 324 is 0144 (type hex(324) to discover this),\\nand we can define a string with the appropriate escape sequence.\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; nacute = \\'\\\\u0144\\'\\n&gt;&gt;&gt; nacute\\n\\'ń\\'\\n\\n\\n\\n\\nNote\\nThere are many factors determining what glyphs are rendered\\non your screen. If you are sure that you have the correct encoding,\\nbut your Python code is still failing to produce the glyphs you\\nexpected, you should also check that you have the necessary fonts\\ninstalled on your system. It may be necessary to configure your locale\\nto render UTF-8 encoded characters, then use print(nacute.encode(\\'utf8\\'))\\nin order to see the ń displayed in your terminal.\\n\\nWe can also see how this character is represented as a sequence of bytes inside\\na text file:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; nacute.encode(\\'utf8\\')\\nb\\'\\\\xc5\\\\x84\\'\\n\\n\\n\\nThe module unicodedata lets us inspect the properties of Unicode\\ncharacters. In the following example, we select all characters in the\\nthird line of our Polish text outside the ASCII range and print their\\nUTF-8 byte sequence, followed by their code point integer using the\\nstandard Unicode convention (i.e., prefixing the hex digits with\\nU+), followed by their Unicode name.\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; import unicodedata\\n&gt;&gt;&gt; lines = open(path, encoding=\\'latin2\\').readlines()\\n&gt;&gt;&gt; line = lines[2]\\n&gt;&gt;&gt; print(line.encode(\\'unicode_escape\\'))\\nb\\'Niemc\\\\\\\\xf3w pod koniec II wojny \\\\\\\\u015bwiatowej na Dolny \\\\\\\\u015al\\\\\\\\u0105sk, zosta\\\\\\\\u0142y\\\\\\\\n\\'\\n&gt;&gt;&gt; for c in line: \\n...     if ord(c) &gt; 127:\\n...         print(\\'{} U+{:04x} {}\\'.format(c.encode(\\'utf8\\'), ord(c), unicodedata.name(c)))\\nb\\'\\\\xc3\\\\xb3\\' U+00f3 LATIN SMALL LETTER O WITH ACUTE\\nb\\'\\\\xc5\\\\x9b\\' U+015b LATIN SMALL LETTER S WITH ACUTE\\nb\\'\\\\xc5\\\\x9a\\' U+015a LATIN CAPITAL LETTER S WITH ACUTE\\nb\\'\\\\xc4\\\\x85\\' U+0105 LATIN SMALL LETTER A WITH OGONEK\\nb\\'\\\\xc5\\\\x82\\' U+0142 LATIN SMALL LETTER L WITH STROKE\\n\\n\\n\\nIf you replace\\nc.encode(\\'utf8\\') in  with c, and if your system supports UTF-8,\\nyou should see an output like the following:\\n\\nó U+00f3 LATIN SMALL LETTER O WITH ACUTE\\nś U+015b LATIN SMALL LETTER S WITH ACUTE\\nŚ U+015a LATIN CAPITAL LETTER S WITH ACUTE\\ną U+0105 LATIN SMALL LETTER A WITH OGONEK\\nł U+0142 LATIN SMALL LETTER L WITH STROKE\\n\\nAlternatively, you may need to replace the encoding \\'utf8\\' in the\\nexample by \\'latin2\\', again depending on the details of your system.\\nThe next examples illustrate how Python string methods and the re\\nmodule can work with Unicode characters. (We will take a close look at\\nthe re module in the following section. The \\\\w matches a \"word\\ncharacter\", cf 3.4).\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; line.find(\\'zosta\\\\u0142y\\')\\n54\\n&gt;&gt;&gt; line = line.lower()\\n&gt;&gt;&gt; line\\n\\'niemców pod koniec ii wojny światowej na dolny śląsk, zostały\\\\n\\'\\n&gt;&gt;&gt; line.encode(\\'unicode_escape\\')\\nb\\'niemc\\\\\\\\xf3w pod koniec ii wojny \\\\\\\\u015bwiatowej na dolny \\\\\\\\u015bl\\\\\\\\u0105sk, zosta\\\\\\\\u0142y\\\\\\\\n\\'\\n&gt;&gt;&gt; import re\\n&gt;&gt;&gt; m = re.search(\\'\\\\u015b\\\\w*\\', line)\\n&gt;&gt;&gt; m.group()\\n\\'\\\\u015bwiatowej\\'\\n\\n\\n\\nNLTK tokenizers allow Unicode strings as input, and\\ncorrespondingly yield Unicode strings as output.\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; word_tokenize(line)\\n[\\'niemców\\', \\'pod\\', \\'koniec\\', \\'ii\\', \\'wojny\\', \\'światowej\\', \\'na\\', \\'dolny\\', \\'śląsk\\', \\',\\', \\'zostały\\']\\n\\n\\n\\n\\n\\nUsing your local encoding in Python\\nIf you are used to working with characters in a particular local\\nencoding, you probably want to be able to use your standard methods\\nfor inputting and editing strings in a Python file. In order to do this,\\nyou need to include the string \\'# -*- coding: &lt;coding&gt; -*-\\' as the\\nfirst or second line of your file. Note that &lt;coding&gt; has to be a\\nstring like \\'latin-1\\', \\'big5\\' or \\'utf-8\\' (see 3.4).\\n\\n\\nFigure 3.4: Unicode and IDLE: UTF-8 encoded string literals in the IDLE editor;\\nthis requires that an appropriate font is set in IDLE\\'s preferences;\\nhere we have chosen Courier CE.\\n\\n<!-- cf http://mail.python.org/pipermail/python-list/2004-February/247783.html\\n\\nIt is also possible to enter non-ASCII characters in interactive\\nmode. IDLE will convert them to the locale\\'s encoding before\\nevaluating the source code; if that fails, you get the message you\\nsee. So where you trying to enter hangul characters in interactive\\nmode in a locale that does not support hangul, or are you lacking\\na codec for your locale?\\n\\nIn interactive mode, UTF-8 is never used (unless you have an\\nUTF-8 locale). -->\\nThe above example also illustrates how regular expressions can use\\nencoded strings.\\n<!-- If you are using Emacs as your editor, the coding specification\\nwill also be interpreted as a specification of the editor\\'s coding\\nfor the file. Not all of the valid Python names for codings are\\naccepted by Emacs. -->\\n\\n\\n\\n3.4&nbsp;&nbsp;&nbsp;Regular Expressions for Detecting Word Patterns\\nMany linguistic processing tasks involve pattern matching.\\nFor example, we can find words ending with ed using\\nendswith(\\'ed\\').  We saw a variety of such \"word tests\"\\nin 4.2.\\nRegular expressions give us a more powerful and flexible\\nmethod for describing the character patterns we are interested in.\\n\\nNote\\nThere are many other published introductions to regular expressions,\\norganized around the syntax of regular expressions and applied to searching\\ntext files.  Instead of doing this again, we focus on the use of regular expressions\\nat different stages of linguistic processing.  As usual, we\\'ll adopt\\na problem-based approach and present new features only as they are\\nneeded to solve practical problems.  In our discussion we will mark\\nregular expressions using chevrons like this: «patt».\\n\\nTo use regular expressions in Python we need to import the re\\nlibrary using: import re.  We also need a list of words to search;\\nwe\\'ll use the Words Corpus again (4).  We\\nwill preprocess it to remove any proper names.\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; import re\\n&gt;&gt;&gt; wordlist = [w for w in nltk.corpus.words.words(\\'en\\') if w.islower()]\\n\\n\\n\\n\\nUsing Basic Meta-Characters\\nLet\\'s find words ending with ed using the regular expression «ed$».\\nWe will use the re.search(p, s) function to check whether the pattern p can be found\\nsomewhere inside the string s.\\nWe need to specify the characters of interest, and use the dollar sign which has a\\nspecial behavior in the context of regular expressions in that it matches\\nthe end of the word:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; [w for w in wordlist if re.search(\\'ed$\\', w)]\\n[\\'abaissed\\', \\'abandoned\\', \\'abased\\', \\'abashed\\', \\'abatised\\', \\'abed\\', \\'aborted\\', ...]\\n\\n\\n\\nThe . wildcard symbol matches any single character.\\nSuppose we have room in a crossword puzzle for an 8-letter word\\nwith j as its third letter and t as its sixth letter.\\nIn place of each blank cell we use a period:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; [w for w in wordlist if re.search(\\'^..j..t..$\\', w)]\\n[\\'abjectly\\', \\'adjuster\\', \\'dejected\\', \\'dejectly\\', \\'injector\\', \\'majestic\\', ...]\\n\\n\\n\\n\\nNote\\nYour Turn:\\nThe caret symbol ^ matches the start of a string, just like the $ matches\\nthe end.  What results do we get with the above example if we leave out\\nboth of these, and search for «..j..t..»?\\n\\nFinally, the ? symbol specifies that the previous character is optional.\\nThus «^e-?mail$» will match both email and e-mail.\\nWe could count the total number of occurrences of this word (in either spelling)\\nin a text using sum(1 for w in text if re.search(\\'^e-?mail$\\', w)).\\n\\n\\nRanges and Closures\\n\\n\\nFigure 3.5: T9: Text on 9 Keys\\n\\nThe T9 system is used for entering text on mobile phones (see 3.5).  Two or more words that\\nare entered with the same sequence of keystrokes are known as textonyms.\\nFor example, both hole and golf are entered by pressing\\nthe sequence 4653.  What other words\\ncould be produced with the same sequence?  Here we use the regular expression\\n«^[ghi][mno][jlk][def]$»:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; [w for w in wordlist if re.search(\\'^[ghi][mno][jlk][def]$\\', w)]\\n[\\'gold\\', \\'golf\\', \\'hold\\', \\'hole\\']\\n\\n\\n\\nThe first part of the expression, «^[ghi]», matches the start of\\na word followed by g, h, or i.  The next part of the expression,\\n«[mno]», constrains the second character to be\\nm, n, or o.  The third and fourth characters are also constrained.\\nOnly four words satisfy all these constraints.\\nNote that the order of characters inside the square brackets is not significant, so we\\ncould have written «^[hig][nom][ljk][fed]$» and matched the same\\nwords.\\n\\nNote\\nYour Turn:\\nLook for some \"finger-twisters\", by searching for words that only use part\\nof the number-pad.  For example «^[ghijklmno]+$», or\\nmore concisely, «^[g-o]+$», will match words\\nthat only use keys 4, 5, 6 in the center row, and «^[a-fj-o]+$»\\nwill match words that use keys 2, 3, 5, 6 in the top-right corner.\\nWhat do - and + mean?\\n\\nLet\\'s explore the + symbol a bit further.  Notice that it can be applied to\\nindividual letters, or to bracketed sets of letters:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; chat_words = sorted(set(w for w in nltk.corpus.nps_chat.words()))\\n&gt;&gt;&gt; [w for w in chat_words if re.search(\\'^m+i+n+e+$\\', w)]\\n[\\'miiiiiiiiiiiiinnnnnnnnnnneeeeeeeeee\\', \\'miiiiiinnnnnnnnnneeeeeeee\\', \\'mine\\',\\n\\'mmmmmmmmiiiiiiiiinnnnnnnnneeeeeeee\\']\\n&gt;&gt;&gt; [w for w in chat_words if re.search(\\'^[ha]+$\\', w)]\\n[\\'a\\', \\'aaaaaaaaaaaaaaaaa\\', \\'aaahhhh\\', \\'ah\\', \\'ahah\\', \\'ahahah\\', \\'ahh\\',\\n\\'ahhahahaha\\', \\'ahhh\\', \\'ahhhh\\', \\'ahhhhhh\\', \\'ahhhhhhhhhhhhhh\\', \\'h\\', \\'ha\\', \\'haaa\\',\\n\\'hah\\', \\'haha\\', \\'hahaaa\\', \\'hahah\\', \\'hahaha\\', \\'hahahaa\\', \\'hahahah\\', \\'hahahaha\\', ...]\\n\\n\\n\\nIt should be clear that + simply means \"one or more instances of the preceding item\",\\nwhich could be an individual character like m, a set like [fed] or a range like [d-f].\\nNow let\\'s replace + with *, which means \"zero or more instances of the preceding item\".\\nThe regular expression «^m*i*n*e*$» will match everything that we found using\\n«^m+i+n+e+$», but also words where some of the letters don\\'t appear at all,\\ne.g. me, min, and mmmmm.\\nNote that the + and * symbols are sometimes referred to as Kleene closures,\\nor simply closures.\\nThe ^ operator has another function when it appears as the first\\ncharacter inside square brackets.  For\\nexample «[^aeiouAEIOU]» matches any character other than a vowel.\\nWe can search the NPS Chat Corpus for words that are made up entirely of non-vowel\\ncharacters using «^[^aeiouAEIOU]+$» to find items like these:\\n:):):), grrr, cyb3r and zzzzzzzz.  Notice this includes\\nnon-alphabetic characters.\\nHere are some more examples of regular expressions being used to find tokens\\nthat match a particular pattern, illustrating the use of some new symbols:\\n\\\\, {}, (), and |:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; wsj = sorted(set(nltk.corpus.treebank.words()))\\n&gt;&gt;&gt; [w for w in wsj if re.search(\\'^[0-9]+\\\\.[0-9]+$\\', w)]\\n[\\'0.0085\\', \\'0.05\\', \\'0.1\\', \\'0.16\\', \\'0.2\\', \\'0.25\\', \\'0.28\\', \\'0.3\\', \\'0.4\\', \\'0.5\\',\\n\\'0.50\\', \\'0.54\\', \\'0.56\\', \\'0.60\\', \\'0.7\\', \\'0.82\\', \\'0.84\\', \\'0.9\\', \\'0.95\\', \\'0.99\\',\\n\\'1.01\\', \\'1.1\\', \\'1.125\\', \\'1.14\\', \\'1.1650\\', \\'1.17\\', \\'1.18\\', \\'1.19\\', \\'1.2\\', ...]\\n&gt;&gt;&gt; [w for w in wsj if re.search(\\'^[A-Z]+\\\\$$\\', w)]\\n[\\'C$\\', \\'US$\\']\\n&gt;&gt;&gt; [w for w in wsj if re.search(\\'^[0-9]{4}$\\', w)]\\n[\\'1614\\', \\'1637\\', \\'1787\\', \\'1901\\', \\'1903\\', \\'1917\\', \\'1925\\', \\'1929\\', \\'1933\\', ...]\\n&gt;&gt;&gt; [w for w in wsj if re.search(\\'^[0-9]+-[a-z]{3,5}$\\', w)]\\n[\\'10-day\\', \\'10-lap\\', \\'10-year\\', \\'100-share\\', \\'12-point\\', \\'12-year\\', ...]\\n&gt;&gt;&gt; [w for w in wsj if re.search(\\'^[a-z]{5,}-[a-z]{2,3}-[a-z]{,6}$\\', w)]\\n[\\'black-and-white\\', \\'bread-and-butter\\', \\'father-in-law\\', \\'machine-gun-toting\\',\\n\\'savings-and-loan\\']\\n&gt;&gt;&gt; [w for w in wsj if re.search(\\'(ed|ing)$\\', w)]\\n[\\'62%-owned\\', \\'Absorbed\\', \\'According\\', \\'Adopting\\', \\'Advanced\\', \\'Advancing\\', ...]\\n\\n\\n\\n\\nNote\\nYour Turn:\\nStudy the above examples and try to work out what the\\n\\\\, {}, (), and | notations mean before\\nyou read on.\\n\\nYou probably worked out that a backslash means that the following character is\\ndeprived of its special powers and must literally match a specific character in the\\nword.  Thus, while . is special, \\\\. only matches a period.\\nThe braced expressions, like {3,5}, specify the number of repeats of the previous item.\\nThe pipe character indicates a choice between the material on its left or its right.\\nParentheses indicate the scope of an operator: they can be used together with\\nthe pipe (or disjunction) symbol like this: «w(i|e|ai|oo)t», matching wit,\\nwet, wait, and woot.  It is instructive to see what happens when\\nyou omit the parentheses from the last expression above, and search for\\n«ed|ing$».\\nThe meta-characters we have seen are summarized in 3.3.\\nTable 3.3: Basic Regular Expression Meta-Characters, Including Wildcards, Ranges and Closures\\n\\n\\n\\n\\n\\nOperator\\nBehavior\\n\\n\\n\\n.\\nWildcard, matches any character\\n\\n^abc\\nMatches some pattern abc at the start of a string\\n\\nabc$\\nMatches some pattern abc at the end of a string\\n\\n[abc]\\nMatches one of a set of characters\\n\\n[A-Z0-9]\\nMatches one of a range of characters\\n\\ned|ing|s\\nMatches one of the specified strings (disjunction)\\n\\n*\\nZero or more of previous item, e.g. a*, [a-z]* (also known as Kleene Closure)\\n\\n+\\nOne or more of previous item, e.g. a+, [a-z]+\\n\\n?\\nZero or one of the previous item (i.e. optional), e.g. a?, [a-z]?\\n\\n{n}\\nExactly n repeats where n is a non-negative integer\\n\\n{n,}\\nAt least n repeats\\n\\n{,n}\\nNo more than n repeats\\n\\n{m,n}\\nAt least m and no more than n repeats\\n\\na(b|c)+\\nParentheses that indicate the scope of the operators\\n\\n\\n\\n\\n\\nTo the Python interpreter, a regular expression is just like any other string.\\nIf the string contains a backslash followed by particular characters, it will\\ninterpret these specially.  For example \\\\b would be interpreted as the\\nbackspace character.  In general, when using regular expressions containing\\nbackslash, we should instruct the interpreter not to look inside the string\\nat all, but simply to pass it directly to the re library for processing.\\nWe do this by prefixing the string with the letter r, to indicate that\\nit is a raw string.  For example, the raw string r\\'\\\\band\\\\b\\'\\ncontains two \\\\b symbols that are interpreted by the re library\\nas matching word boundaries instead of backspace characters.\\nIf you get into the habit of using r\\'...\\' for regular expressions\\n— as we will do from now on — you will avoid having to think about\\nthese complications.\\n\\n\\n\\n3.5&nbsp;&nbsp;&nbsp;Useful Applications of Regular Expressions\\nThe above examples all involved searching for words w\\nthat match some regular expression regexp using re.search(regexp, w).\\nApart from checking if a regular expression matches a word, we can use\\nregular expressions to extract material from words, or to modify words\\nin specific ways.\\n\\nExtracting Word Pieces\\nThe re.findall() (\"find all\") method finds all (non-overlapping)\\nmatches of the given regular expression.  Let\\'s find all the vowels in\\na word, then count them:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; word = \\'supercalifragilisticexpialidocious\\'\\n&gt;&gt;&gt; re.findall(r\\'[aeiou]\\', word)\\n[\\'u\\', \\'e\\', \\'a\\', \\'i\\', \\'a\\', \\'i\\', \\'i\\', \\'i\\', \\'e\\', \\'i\\', \\'a\\', \\'i\\', \\'o\\', \\'i\\', \\'o\\', \\'u\\']\\n&gt;&gt;&gt; len(re.findall(r\\'[aeiou]\\', word))\\n16\\n\\n\\n\\nLet\\'s look for all sequences of two or more vowels in some text,\\nand determine their relative frequency:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; wsj = sorted(set(nltk.corpus.treebank.words()))\\n&gt;&gt;&gt; fd = nltk.FreqDist(vs for word in wsj\\n...                       for vs in re.findall(r\\'[aeiou]{2,}\\', word))\\n&gt;&gt;&gt; fd.most_common(12)\\n[(\\'io\\', 549), (\\'ea\\', 476), (\\'ie\\', 331), (\\'ou\\', 329), (\\'ai\\', 261), (\\'ia\\', 253),\\n(\\'ee\\', 217), (\\'oo\\', 174), (\\'ua\\', 109), (\\'au\\', 106), (\\'ue\\', 105), (\\'ui\\', 95)]\\n\\n\\n\\n\\nNote\\nYour Turn:\\nIn the W3C Date Time Format, dates are represented like this: 2009-12-31.\\nReplace the ? in the following Python code with a regular expression,\\nin order to convert the string \\'2009-12-31\\' to a list of integers\\n[2009, 12, 31]:\\n[int(n) for n in re.findall(?, \\'2009-12-31\\')]\\n\\n\\n\\nDoing More with Word Pieces\\nOnce we can use re.findall() to extract material from words, there\\'s\\ninteresting things to do with the pieces, like glue them back together or\\nplot them.\\nIt is sometimes noted that English text is highly redundant, and it is still\\neasy to read when word-internal vowels are left out.  For example,\\ndeclaration becomes dclrtn, and inalienable becomes inlnble,\\nretaining any initial or final vowel sequences.   The regular expression\\nin our next example matches initial vowel sequences, final vowel sequences, and all consonants;\\neverything else is ignored.  This three-way disjunction is processed left-to-right,\\nif one of the three parts matches the word, any later parts of the regular\\nexpression are ignored.\\nWe use re.findall() to extract all the matching\\npieces, and \\'\\'.join() to join them together (see 3.9 for\\nmore about the join operation).\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; regexp = r\\'^[AEIOUaeiou]+|[AEIOUaeiou]+$|[^AEIOUaeiou]\\'\\n&gt;&gt;&gt; def compress(word):\\n...     pieces = re.findall(regexp, word)\\n...     return \\'\\'.join(pieces)\\n...\\n&gt;&gt;&gt; english_udhr = nltk.corpus.udhr.words(\\'English-Latin1\\')\\n&gt;&gt;&gt; print(nltk.tokenwrap(compress(w) for w in english_udhr[:75]))\\nUnvrsl Dclrtn of Hmn Rghts Prmble Whrs rcgntn of the inhrnt dgnty and\\nof the eql and inlnble rghts of all mmbrs of the hmn fmly is the fndtn\\nof frdm , jstce and pce in the wrld , Whrs dsrgrd and cntmpt fr hmn\\nrghts hve rsltd in brbrs acts whch hve outrgd the cnscnce of mnknd ,\\nand the advnt of a wrld in whch hmn bngs shll enjy frdm of spch and\\n\\n\\n\\nNext, let\\'s combine regular expressions with conditional frequency\\ndistributions.  Here we will extract all consonant-vowel sequences\\nfrom the words of Rotokas, such as ka and si.  Since each of\\nthese is a pair, it can be used to initialize a conditional frequency\\ndistribution.  We then tabulate the frequency of each pair:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; rotokas_words = nltk.corpus.toolbox.words(\\'rotokas.dic\\')\\n&gt;&gt;&gt; cvs = [cv for w in rotokas_words for cv in re.findall(r\\'[ptksvr][aeiou]\\', w)]\\n&gt;&gt;&gt; cfd = nltk.ConditionalFreqDist(cvs)\\n&gt;&gt;&gt; cfd.tabulate()\\n    a    e    i    o    u\\nk  418  148   94  420  173\\np   83   31  105   34   51\\nr  187   63   84   89   79\\ns    0    0  100    2    1\\nt   47    8    0  148   37\\nv   93   27  105   48   49\\n\\n\\n\\nExamining the rows for s and t, we see they are in partial\\n\"complementary distribution\", which is evidence that they are not\\ndistinct phonemes in the language.  Thus, we could conceivably drop\\ns from the Rotokas alphabet and simply have a pronunciation rule\\nthat the letter t is pronounced s when followed by\\ni.  (Note that the single entry having su, namely kasuari,\\n\\'cassowary\\' is borrowed from English.)\\nIf we want to be able to inspect the words behind the numbers in the above table,\\nit would be helpful to have an index, allowing us to quickly find the list of words\\nthat contains a given consonant-vowel pair, e.g. cv_index[\\'su\\'] should give us\\nall words containing su.  Here\\'s how we can do this:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; cv_word_pairs = [(cv, w) for w in rotokas_words\\n...                          for cv in re.findall(r\\'[ptksvr][aeiou]\\', w)]\\n&gt;&gt;&gt; cv_index = nltk.Index(cv_word_pairs)\\n&gt;&gt;&gt; cv_index[\\'su\\']\\n[\\'kasuari\\']\\n&gt;&gt;&gt; cv_index[\\'po\\']\\n[\\'kaapo\\', \\'kaapopato\\', \\'kaipori\\', \\'kaiporipie\\', \\'kaiporivira\\', \\'kapo\\', \\'kapoa\\',\\n\\'kapokao\\', \\'kapokapo\\', \\'kapokapo\\', \\'kapokapoa\\', \\'kapokapoa\\', \\'kapokapora\\', ...]\\n\\n\\n\\nThis program processes each word w in turn, and for each one, finds every\\nsubstring that matches the regular expression «[ptksvr][aeiou]».\\nIn the case of the word kasuari, it finds ka, su and ri.\\nTherefore, the cv_word_pairs list will contain (\\'ka\\', \\'kasuari\\'),\\n(\\'su\\', \\'kasuari\\') and (\\'ri\\', \\'kasuari\\').  One further step, using\\nnltk.Index(), converts this into a useful index.\\n\\n\\nFinding Word Stems\\nWhen we use a web search engine, we usually don\\'t mind (or even notice)\\nif the words in the document differ from our search terms in having\\ndifferent endings.  A query for laptops finds documents\\ncontaining laptop and vice versa.\\nIndeed, laptop and laptops are just two forms of the\\nsame dictionary word (or lemma).\\nFor some language processing tasks we want to ignore word endings, and just\\ndeal with word stems.\\nThere are various ways we can pull out the stem of a word.  Here\\'s a simple-minded\\napproach which just strips off anything that looks like a suffix:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; def stem(word):\\n...     for suffix in [\\'ing\\', \\'ly\\', \\'ed\\', \\'ious\\', \\'ies\\', \\'ive\\', \\'es\\', \\'s\\', \\'ment\\']:\\n...         if word.endswith(suffix):\\n...             return word[:-len(suffix)]\\n...     return word\\n\\n\\n\\nAlthough we will ultimately use NLTK\\'s built-in stemmers, it\\'s interesting\\nto see how we can use regular expressions for this task.  Our first step is\\nto build up a disjunction of all the suffixes.  We need to enclose it in parentheses\\nin order to limit the scope of the disjunction.\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; re.findall(r\\'^.*(ing|ly|ed|ious|ies|ive|es|s|ment)$\\', \\'processing\\')\\n[\\'ing\\']\\n\\n\\n\\nHere, re.findall() just gave us the suffix even though the regular expression\\nmatched the entire word.  This is because the parentheses have a second function,\\nto select substrings to be extracted.  If we want to use the parentheses to\\nspecify the scope of the disjunction, but not to select the material to be output,\\nwe have to add ?:,\\nwhich is just one of many arcane subtleties of regular expressions.\\nHere\\'s the revised version.\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; re.findall(r\\'^.*(?:ing|ly|ed|ious|ies|ive|es|s|ment)$\\', \\'processing\\')\\n[\\'processing\\']\\n\\n\\n\\nHowever, we\\'d actually like to split the word into stem and suffix.\\nSo we should just parenthesize both parts of the regular expression:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; re.findall(r\\'^(.*)(ing|ly|ed|ious|ies|ive|es|s|ment)$\\', \\'processing\\')\\n[(\\'process\\', \\'ing\\')]\\n\\n\\n\\nThis looks promising, but still has a problem.  Let\\'s look at a different\\nword, processes:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; re.findall(r\\'^(.*)(ing|ly|ed|ious|ies|ive|es|s|ment)$\\', \\'processes\\')\\n[(\\'processe\\', \\'s\\')]\\n\\n\\n\\nThe regular expression incorrectly found an -s suffix instead of\\nan -es suffix.  This demonstrates another subtlety: the star operator\\nis \"greedy\" and the .* part of the expression tries to consume as much of the input\\nas possible.  If we use the \"non-greedy\" version of the star operator, written *?,\\nwe get what we want:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; re.findall(r\\'^(.*?)(ing|ly|ed|ious|ies|ive|es|s|ment)$\\', \\'processes\\')\\n[(\\'process\\', \\'es\\')]\\n\\n\\n\\nThis works even when we allow an empty suffix, by making the content of the\\nsecond parentheses optional:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; re.findall(r\\'^(.*?)(ing|ly|ed|ious|ies|ive|es|s|ment)?$\\', \\'language\\')\\n[(\\'language\\', \\'\\')]\\n\\n\\n\\nThis approach still has many problems (can you spot them?) but we will move\\non to define a function to perform stemming, and apply it to a whole text:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; def stem(word):\\n...     regexp = r\\'^(.*?)(ing|ly|ed|ious|ies|ive|es|s|ment)?$\\'\\n...     stem, suffix = re.findall(regexp, word)[0]\\n...     return stem\\n...\\n&gt;&gt;&gt; raw = \"\"\"DENNIS: Listen, strange women lying in ponds distributing swords\\n... is no basis for a system of government.  Supreme executive power derives from\\n... a mandate from the masses, not from some farcical aquatic ceremony.\"\"\"\\n&gt;&gt;&gt; tokens = word_tokenize(raw)\\n&gt;&gt;&gt; [stem(t) for t in tokens]\\n[\\'DENNIS\\', \\':\\', \\'Listen\\', \\',\\', \\'strange\\', \\'women\\', \\'ly\\', \\'in\\', \\'pond\\', \\'distribut\\',\\n\\'sword\\', \\'i\\', \\'no\\', \\'basi\\', \\'for\\', \\'a\\', \\'system\\', \\'of\\', \\'govern\\', \\'.\\', \\'Supreme\\',\\n\\'execut\\', \\'power\\', \\'deriv\\', \\'from\\', \\'a\\', \\'mandate\\', \\'from\\', \\'the\\', \\'mass\\', \\',\\',\\n\\'not\\', \\'from\\', \\'some\\', \\'farcical\\', \\'aquatic\\', \\'ceremony\\', \\'.\\']\\n\\n\\n\\nNotice that our regular expression removed the s from ponds but also from is\\nand basis.  It produced some non-words like distribut and deriv, but these\\nare acceptable stems in some applications.\\n\\n\\nSearching Tokenized Text\\nYou can use a special kind of regular expression for searching across multiple words\\nin a text (where a text is a list of tokens).  For example, \"&lt;a&gt; &lt;man&gt;\" finds all\\ninstances of a man in the text.  The angle brackets are used to mark token boundaries,\\nand any whitespace between the angle brackets is ignored (behaviors that are unique\\nto NLTK\\'s findall() method for texts).  In the following example, we include\\n&lt;.*&gt;  which will match any single token, and enclose it in parentheses so only the\\nmatched word (e.g. monied) and not the matched phrase (e.g. a monied man)\\nis produced.  The second example finds three-word phrases ending with the word bro\\n.  The last example finds sequences of three or more words starting with\\nthe letter l .\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; from nltk.corpus import gutenberg, nps_chat\\n&gt;&gt;&gt; moby = nltk.Text(gutenberg.words(\\'melville-moby_dick.txt\\'))\\n&gt;&gt;&gt; moby.findall(r\"&lt;a&gt; (&lt;.*&gt;) &lt;man&gt;\") \\nmonied; nervous; dangerous; white; white; white; pious; queer; good;\\nmature; white; Cape; great; wise; wise; butterless; white; fiendish;\\npale; furious; better; certain; complete; dismasted; younger; brave;\\nbrave; brave; brave\\n&gt;&gt;&gt; chat = nltk.Text(nps_chat.words())\\n&gt;&gt;&gt; chat.findall(r\"&lt;.*&gt; &lt;.*&gt; &lt;bro&gt;\") \\nyou rule bro; telling you bro; u twizted bro\\n&gt;&gt;&gt; chat.findall(r\"&lt;l.*&gt;{3,}\") \\nlol lol lol; lmao lol lol; lol lol lol; la la la la la; la la la; la\\nla la; lovely lol lol love; lol lol lol.; la la la; la la la\\n\\n\\n\\n\\nNote\\nYour Turn:\\nConsolidate your understanding of regular expression patterns and substitutions using\\nnltk.re_show(p, s) which annotates the string s to show every place where\\npattern p was matched, and nltk.app.nemo() which provides a graphical\\ninterface for exploring regular expressions.  For more practice, try some\\nof the exercises on regular expressions at the end of this chapter.\\n\\n\\nIt is easy to build search patterns when the linguistic phenomenon we\\'re\\nstudying is tied to particular words.  In some cases, a little creativity\\nwill go a long way.  For instance, searching a large text corpus for\\nexpressions of the form x and other ys allows us to discover\\nhypernyms (cf 5):\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; from nltk.corpus import brown\\n&gt;&gt;&gt; hobbies_learned = nltk.Text(brown.words(categories=[\\'hobbies\\', \\'learned\\']))\\n&gt;&gt;&gt; hobbies_learned.findall(r\"&lt;\\\\w*&gt; &lt;and&gt; &lt;other&gt; &lt;\\\\w*s&gt;\")\\nspeed and other activities; water and other liquids; tomb and other\\nlandmarks; Statues and other monuments; pearls and other jewels;\\ncharts and other items; roads and other features; figures and other\\nobjects; military and other areas; demands and other factors;\\nabstracts and other compilations; iron and other metals\\n\\n\\n\\nWith enough text, this approach would give us a useful store\\nof information about the taxonomy of objects, without the need for\\nany manual labor.  However, our search results will usually\\ncontain false positives, i.e. cases that we would want to exclude.\\nFor example, the result: demands and other factors suggests\\nthat demand is an instance of the type factor, but this\\nsentence is actually about wage demands.  Nevertheless, we could\\nconstruct our own ontology of English concepts by manually correcting\\nthe output of such searches.\\n\\nNote\\nThis combination of automatic and manual processing is the most common\\nway for new corpora to be constructed.  We will return to this in\\n11..\\n\\nSearching corpora also suffers from the problem of false negatives,\\ni.e. omitting cases that we would want to include.  It is risky to\\nconclude that some linguistic phenomenon doesn\\'t exist in a corpus\\njust because we couldn\\'t find any instances of a search pattern.\\nPerhaps we just didn\\'t think carefully enough about suitable patterns.\\n\\nNote\\nYour Turn:\\nLook for instances of the pattern as x as y to discover\\ninformation about entities and their properties.\\n\\n<!-- searching for doubled final consonants: .*([bdgptk])\\\\1ed\\ntransforming date strings\\nHTML stripping\\nspelling correction\\ntextonyms -->\\n\\n\\n\\n3.6&nbsp;&nbsp;&nbsp;Normalizing Text\\nIn earlier program examples we have often converted text to lowercase before\\ndoing anything with its words, e.g. set(w.lower() for w in text).\\nBy using lower(), we have normalized the text to lowercase so that\\nthe distinction between The and the is ignored.  Often we want\\nto go further than this, and strip off any affixes, a task known as stemming.\\nA further step is to make sure that the resulting form is a known word in a dictionary,\\na task known as lemmatization.  We discuss each of these in turn.  First, we need\\nto define the data we will use in this section:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; raw = \"\"\"DENNIS: Listen, strange women lying in ponds distributing swords\\n... is no basis for a system of government.  Supreme executive power derives from\\n... a mandate from the masses, not from some farcical aquatic ceremony.\"\"\"\\n&gt;&gt;&gt; tokens = word_tokenize(raw)\\n\\n\\n\\n\\nStemmers\\nNLTK includes several off-the-shelf stemmers, and if you ever need\\na stemmer you should use one of these in preference to crafting your own\\nusing regular expressions, since these handle a wide range of irregular cases.\\nThe Porter and Lancaster stemmers follow their own rules for stripping affixes.\\nObserve that the Porter stemmer correctly handles the word lying\\n(mapping it to lie), while the Lancaster stemmer does not.\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; porter = nltk.PorterStemmer()\\n&gt;&gt;&gt; lancaster = nltk.LancasterStemmer()\\n&gt;&gt;&gt; [porter.stem(t) for t in tokens]\\n[\\'denni\\', \\':\\', \\'listen\\', \\',\\', \\'strang\\', \\'women\\', \\'lie\\', \\'in\\', \\'pond\\',\\n\\'distribut\\', \\'sword\\', \\'is\\', \\'no\\', \\'basi\\', \\'for\\', \\'a\\', \\'system\\', \\'of\\', \\'govern\\',\\n\\'.\\', \\'suprem\\', \\'execut\\', \\'power\\', \\'deriv\\', \\'from\\', \\'a\\', \\'mandat\\', \\'from\\',\\n\\'the\\', \\'mass\\', \\',\\', \\'not\\', \\'from\\', \\'some\\', \\'farcic\\', \\'aquat\\', \\'ceremoni\\', \\'.\\']\\n&gt;&gt;&gt; [lancaster.stem(t) for t in tokens]\\n[\\'den\\', \\':\\', \\'list\\', \\',\\', \\'strange\\', \\'wom\\', \\'lying\\', \\'in\\', \\'pond\\', \\'distribut\\',\\n\\'sword\\', \\'is\\', \\'no\\', \\'bas\\', \\'for\\', \\'a\\', \\'system\\', \\'of\\', \\'govern\\', \\'.\\', \\'suprem\\',\\n\\'execut\\', \\'pow\\', \\'der\\', \\'from\\', \\'a\\', \\'mand\\', \\'from\\', \\'the\\', \\'mass\\', \\',\\', \\'not\\',\\n\\'from\\', \\'som\\', \\'farc\\', \\'aqu\\', \\'ceremony\\', \\'.\\']\\n\\n\\n\\nStemming is not a well-defined process, and we typically pick the stemmer that best\\nsuits the application we have in mind.  The Porter Stemmer is a good choice if you\\nare indexing some texts and want to support search using alternative forms of\\nwords (illustrated in 3.6, which uses object oriented\\nprogramming techniques that are outside the scope of this book, string formatting\\ntechniques to be covered in 3.9, and the enumerate() function\\nto be explained in 4.2).\\n\\n\\n\\n\\n&nbsp;\\nclass IndexedText(object):\\n\\n    def __init__(self, stemmer, text):\\n        self._text = text\\n        self._stemmer = stemmer\\n        self._index = nltk.Index((self._stem(word), i)\\n                                 for (i, word) in enumerate(text))\\n\\n    def concordance(self, word, width=40):\\n        key = self._stem(word)\\n        wc = int(width/4)                # words of context\\n        for i in self._index[key]:\\n            lcontext = \\' \\'.join(self._text[i-wc:i])\\n            rcontext = \\' \\'.join(self._text[i:i+wc])\\n            ldisplay = \\'{:&gt;{width}}\\'.format(lcontext[-width:], width=width)\\n            rdisplay = \\'{:{width}}\\'.format(rcontext[:width], width=width)\\n            print(ldisplay, rdisplay)\\n\\n    def _stem(self, word):\\n        return self._stemmer.stem(word).lower()\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; porter = nltk.PorterStemmer()\\n&gt;&gt;&gt; grail = nltk.corpus.webtext.words(\\'grail.txt\\')\\n&gt;&gt;&gt; text = IndexedText(porter, grail)\\n&gt;&gt;&gt; text.concordance(\\'lie\\')\\nr king ! DENNIS : Listen , strange women lying in ponds distributing swords is no\\n beat a very brave retreat . ROBIN : All lies ! MINSTREL : [ singing ] Bravest of\\n       Nay . Nay . Come . Come . You may lie here . Oh , but you are wounded !\\ndoctors immediately ! No , no , please ! Lie down . [ clap clap ] PIGLET : Well\\nere is much danger , for beyond the cave lies the Gorge of Eternal Peril , which\\n   you . Oh ... TIM : To the north there lies a cave -- the cave of Caerbannog --\\nh it and lived ! Bones of full fifty men lie strewn about its lair . So , brave k\\nnot stop our fight \\' til each one of you lies dead , and the Holy Grail returns t\\n\\n\\nExample 3.6 (code_stemmer_indexing.py): Figure 3.6: Indexing a Text Using a Stemmer\\n\\n\\n\\nLemmatization\\nThe WordNet lemmatizer only removes affixes if the resulting word is in its dictionary.\\nThis additional checking process makes the lemmatizer slower than the above stemmers.\\nNotice that it doesn\\'t handle lying, but it converts women to woman.\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; wnl = nltk.WordNetLemmatizer()\\n&gt;&gt;&gt; [wnl.lemmatize(t) for t in tokens]\\n[\\'DENNIS\\', \\':\\', \\'Listen\\', \\',\\', \\'strange\\', \\'woman\\', \\'lying\\', \\'in\\', \\'pond\\',\\n\\'distributing\\', \\'sword\\', \\'is\\', \\'no\\', \\'basis\\', \\'for\\', \\'a\\', \\'system\\', \\'of\\',\\n\\'government\\', \\'.\\', \\'Supreme\\', \\'executive\\', \\'power\\', \\'derives\\', \\'from\\', \\'a\\',\\n\\'mandate\\', \\'from\\', \\'the\\', \\'mass\\', \\',\\', \\'not\\', \\'from\\', \\'some\\', \\'farcical\\',\\n\\'aquatic\\', \\'ceremony\\', \\'.\\']\\n\\n\\n\\nThe WordNet lemmatizer is a good choice if you want to compile the vocabulary\\nof some texts and want a list of valid lemmas (or lexicon headwords).\\n\\nNote\\nAnother normalization task involves identifying non-standard words\\nincluding numbers, abbreviations, and dates, and mapping any such tokens\\nto a special vocabulary.  For example, every decimal number could be\\nmapped to a single token 0.0, and every acronym could be mapped to AAA.\\nThis keeps the vocabulary small and improves the accuracy of many\\nlanguage modeling tasks.\\n\\n<!-- Non-Standard Words\\n- - - - - - - - - - - - - - - - - -\\n\\n[Discuss the practice of mapping words such as numbers, abbreviations, dates to\\na special vocabulary, based on Sproat et al 2001; new NLTK support planned...] -->\\n\\n\\n\\n3.7&nbsp;&nbsp;&nbsp;Regular Expressions for Tokenizing Text\\nTokenization is the task of cutting a string into\\nidentifiable linguistic units that constitute a piece of language data.\\nAlthough it is a fundamental task, we have been able to\\ndelay it until now because many corpora are already tokenized,\\nand because NLTK includes some tokenizers.\\nNow that you are familiar with regular expressions,\\nyou can learn how to use them to tokenize text, and to\\nhave much more control over the process.\\n\\nSimple Approaches to Tokenization\\nThe very simplest method for tokenizing text is to split on whitespace.\\nConsider the following text from Alice\\'s Adventures in Wonderland:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; raw = \"\"\"\\'When I\\'M a Duchess,\\' she said to herself, (not in a very hopeful tone\\n... though), \\'I won\\'t have any pepper in my kitchen AT ALL. Soup does very\\n... well without--Maybe it\\'s always pepper that makes people hot-tempered,\\'...\"\"\"\\n\\n\\n\\nWe could split this raw text on whitespace using raw.split().\\nTo do the same using a regular expression, it is not enough to match\\nany space characters in the string  since this results\\nin tokens that contain a \\\\n newline character; instead we need\\nto match any number of spaces, tabs, or newlines :\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; re.split(r\\' \\', raw) \\n[\"\\'When\", \"I\\'M\", \\'a\\', \"Duchess,\\'\", \\'she\\', \\'said\\', \\'to\\', \\'herself,\\', \\'(not\\', \\'in\\',\\n\\'a\\', \\'very\\', \\'hopeful\\', \\'tone\\\\nthough),\\', \"\\'I\", \"won\\'t\", \\'have\\', \\'any\\', \\'pepper\\',\\n\\'in\\', \\'my\\', \\'kitchen\\', \\'AT\\', \\'ALL.\\', \\'Soup\\', \\'does\\', \\'very\\\\nwell\\', \\'without--Maybe\\',\\n\"it\\'s\", \\'always\\', \\'pepper\\', \\'that\\', \\'makes\\', \\'people\\', \"hot-tempered,\\'...\"]\\n&gt;&gt;&gt; re.split(r\\'[ \\\\t\\\\n]+\\', raw) \\n[\"\\'When\", \"I\\'M\", \\'a\\', \"Duchess,\\'\", \\'she\\', \\'said\\', \\'to\\', \\'herself,\\', \\'(not\\', \\'in\\',\\n\\'a\\', \\'very\\', \\'hopeful\\', \\'tone\\', \\'though),\\', \"\\'I\", \"won\\'t\", \\'have\\', \\'any\\', \\'pepper\\',\\n\\'in\\', \\'my\\', \\'kitchen\\', \\'AT\\', \\'ALL.\\', \\'Soup\\', \\'does\\', \\'very\\', \\'well\\', \\'without--Maybe\\',\\n\"it\\'s\", \\'always\\', \\'pepper\\', \\'that\\', \\'makes\\', \\'people\\', \"hot-tempered,\\'...\"]\\n\\n\\n\\nThe regular expression «[ \\\\t\\\\n]+» matches one or more space, tab (\\\\t)\\nor newline (\\\\n).  Other whitespace characters, such as carriage-return and\\nform-feed should really be included too.  Instead, we will use a built-in\\nre abbreviation, \\\\s, which means any whitespace character.  The above\\nstatement can be rewritten as re.split(r\\'\\\\s+\\', raw).\\n\\nNote\\nImportant:\\nRemember to prefix regular expressions with the letter r\\n(meaning \"raw\"), which instructs the Python\\ninterpreter to treat the string literally, rather than\\nprocessing any backslashed characters it contains.\\n\\nSplitting on whitespace gives us tokens like \\'(not\\' and \\'herself,\\'.\\nAn alternative is to use the fact that Python provides us with a\\ncharacter class \\\\w for word characters, equivalent to [a-zA-Z0-9_].\\nIt also defines the complement of this class \\\\W, i.e. all characters\\nother than letters, digits or underscore.  We can use \\\\W in a simple\\nregular expression to split the input on anything other than a word character:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; re.split(r\\'\\\\W+\\', raw)\\n[\\'\\', \\'When\\', \\'I\\', \\'M\\', \\'a\\', \\'Duchess\\', \\'she\\', \\'said\\', \\'to\\', \\'herself\\', \\'not\\', \\'in\\',\\n\\'a\\', \\'very\\', \\'hopeful\\', \\'tone\\', \\'though\\', \\'I\\', \\'won\\', \\'t\\', \\'have\\', \\'any\\', \\'pepper\\',\\n\\'in\\', \\'my\\', \\'kitchen\\', \\'AT\\', \\'ALL\\', \\'Soup\\', \\'does\\', \\'very\\', \\'well\\', \\'without\\',\\n\\'Maybe\\', \\'it\\', \\'s\\', \\'always\\', \\'pepper\\', \\'that\\', \\'makes\\', \\'people\\', \\'hot\\', \\'tempered\\',\\n\\'\\']\\n\\n\\n\\nObserve that this gives us empty strings at the start and the end (to understand\\nwhy, try doing \\'xx\\'.split(\\'x\\')).  We get the same tokens, but without the empty strings,\\nwith re.findall(r\\'\\\\w+\\', raw), using a pattern that matches the words instead of the spaces.\\nNow that we\\'re matching the words, we\\'re in a position to extend the regular expression\\nto cover a wider range of cases.\\nThe regular expression «\\\\w+|\\\\S\\\\w*» will first try to match any sequence\\nof word characters.  If no match is found, it will try to match any\\nnon-whitespace character (\\\\S is the complement of \\\\s) followed by\\nfurther word characters.  This means that punctuation is grouped with any following\\nletters (e.g. \\'s) but that sequences of two or more punctuation\\ncharacters are separated.\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; re.findall(r\\'\\\\w+|\\\\S\\\\w*\\', raw)\\n[\"\\'When\", \\'I\\', \"\\'M\", \\'a\\', \\'Duchess\\', \\',\\', \"\\'\", \\'she\\', \\'said\\', \\'to\\', \\'herself\\', \\',\\',\\n\\'(not\\', \\'in\\', \\'a\\', \\'very\\', \\'hopeful\\', \\'tone\\', \\'though\\', \\')\\', \\',\\', \"\\'I\", \\'won\\', \"\\'t\",\\n\\'have\\', \\'any\\', \\'pepper\\', \\'in\\', \\'my\\', \\'kitchen\\', \\'AT\\', \\'ALL\\', \\'.\\', \\'Soup\\', \\'does\\',\\n\\'very\\', \\'well\\', \\'without\\', \\'-\\', \\'-Maybe\\', \\'it\\', \"\\'s\", \\'always\\', \\'pepper\\', \\'that\\',\\n\\'makes\\', \\'people\\', \\'hot\\', \\'-tempered\\', \\',\\', \"\\'\", \\'.\\', \\'.\\', \\'.\\']\\n\\n\\n\\nLet\\'s generalize the \\\\w+ in the above expression\\nto permit word-internal hyphens and apostrophes: «\\\\w+([-\\']\\\\w+)*».\\nThis expression means \\\\w+ followed by zero or more instances of [-\\']\\\\w+;\\nit would match hot-tempered and it\\'s.\\n(We need to include ?: in this expression for reasons discussed earlier.)\\nWe\\'ll also add a pattern to match quote characters so these are kept separate\\nfrom the text they enclose.\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; print(re.findall(r\"\\\\w+(?:[-\\']\\\\w+)*|\\'|[-.(]+|\\\\S\\\\w*\", raw))\\n[\"\\'\", \\'When\\', \"I\\'M\", \\'a\\', \\'Duchess\\', \\',\\', \"\\'\", \\'she\\', \\'said\\', \\'to\\', \\'herself\\', \\',\\',\\n\\'(\\', \\'not\\', \\'in\\', \\'a\\', \\'very\\', \\'hopeful\\', \\'tone\\', \\'though\\', \\')\\', \\',\\', \"\\'\", \\'I\\',\\n\"won\\'t\", \\'have\\', \\'any\\', \\'pepper\\', \\'in\\', \\'my\\', \\'kitchen\\', \\'AT\\', \\'ALL\\', \\'.\\', \\'Soup\\',\\n\\'does\\', \\'very\\', \\'well\\', \\'without\\', \\'--\\', \\'Maybe\\', \"it\\'s\", \\'always\\', \\'pepper\\',\\n\\'that\\', \\'makes\\', \\'people\\', \\'hot-tempered\\', \\',\\', \"\\'\", \\'...\\']\\n\\n\\n\\nThe above expression also included «[-.(]+» which causes the double hyphen,\\nellipsis, and open parenthesis to be tokenized separately.\\n3.4 lists the regular expression character class symbols we have\\nseen in this section, in addition to some other useful symbols.\\nTable 3.4: Regular Expression Symbols\\n\\n\\n\\n\\n\\nSymbol\\nFunction\\n\\n\\n\\n\\\\b\\nWord boundary (zero width)\\n\\n\\\\d\\nAny decimal digit (equivalent to [0-9])\\n\\n\\\\D\\nAny non-digit character (equivalent to [^0-9])\\n\\n\\\\s\\nAny whitespace character (equivalent to [ \\\\t\\\\n\\\\r\\\\f\\\\v])\\n\\n\\\\S\\nAny non-whitespace character (equivalent to [^ \\\\t\\\\n\\\\r\\\\f\\\\v])\\n\\n\\\\w\\nAny alphanumeric character (equivalent to [a-zA-Z0-9_])\\n\\n\\\\W\\nAny non-alphanumeric character (equivalent to [^a-zA-Z0-9_])\\n\\n\\\\t\\nThe tab character\\n\\n\\\\n\\nThe newline character\\n\\n\\n\\n\\n\\n\\n\\nNLTK\\'s Regular Expression Tokenizer\\nThe function nltk.regexp_tokenize() is similar to re.findall() (as\\nwe\\'ve been using it for tokenization).  However, nltk.regexp_tokenize()\\nis more efficient for this task, and avoids the need for special treatment of parentheses.\\nFor readability we break up the regular expression over several lines\\nand add a comment about each line.  The special (?x) \"verbose flag\"\\ntells Python to strip out the embedded whitespace and comments.\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; text = \\'That U.S.A. poster-print costs $12.40...\\'\\n&gt;&gt;&gt; pattern = r\\'\\'\\'(?x)     # set flag to allow verbose regexps\\n...     (?:[A-Z]\\\\.)+       # abbreviations, e.g. U.S.A.\\n...   | \\\\w+(?:-\\\\w+)*       # words with optional internal hyphens\\n...   | \\\\$?\\\\d+(?:\\\\.\\\\d+)?%? # currency and percentages, e.g. $12.40, 82%\\n...   | \\\\.\\\\.\\\\.             # ellipsis\\n...   | [][.,;\"\\'?():-_`]   # these are separate tokens; includes ], [\\n... \\'\\'\\'\\n&gt;&gt;&gt; nltk.regexp_tokenize(text, pattern)\\n[\\'That\\', \\'U.S.A.\\', \\'poster-print\\', \\'costs\\', \\'$12.40\\', \\'...\\']\\n\\n\\n\\nWhen using the verbose flag, you can no longer use \\' \\' to match\\na space character; use \\\\s instead.\\nThe regexp_tokenize() function has an optional gaps parameter.\\nWhen set to True, the regular expression specifies the gaps\\nbetween tokens, as with re.split().\\n\\nNote\\nWe can evaluate a tokenizer by comparing the resulting tokens with a\\nwordlist, and reporting any tokens that don\\'t appear in the wordlist,\\nusing set(tokens).difference(wordlist).  You\\'ll probably want to\\nlowercase all the tokens first.\\n\\n\\n\\nFurther Issues with Tokenization\\nTokenization turns out to be a far more difficult task than you might have expected.\\nNo single solution works well across-the-board, and we\\nmust decide what counts as a token depending on the application domain.\\nWhen developing a tokenizer it helps to have access to raw text which\\nhas been manually tokenized, in order to compare the output of your tokenizer\\nwith high-quality (or \"gold-standard\") tokens.  The NLTK corpus\\ncollection includes a sample of Penn Treebank data, including the raw\\nWall Street Journal text (nltk.corpus.treebank_raw.raw()) and\\nthe tokenized version (nltk.corpus.treebank.words()).\\nA final issue for tokenization is the presence of contractions, such\\nas didn\\'t.  If we are analyzing the meaning\\nof a sentence, it would probably be more useful to normalize this\\nform to two separate forms: did and n\\'t (or not).\\nWe can do this work with the help of a lookup table.\\n\\n\\n\\n3.8&nbsp;&nbsp;&nbsp;Segmentation\\nThis section discusses more advanced concepts, which you may prefer to skip on the\\nfirst time through this chapter.\\nTokenization is an instance of a more general problem of segmentation.\\nIn this section we will look at two other instances of this problem,\\nwhich use radically different techniques to the ones we have seen so far\\nin this chapter.\\n\\nSentence Segmentation\\nManipulating texts at the level of individual words often presupposes\\nthe ability to divide a text into individual sentences.  As we have\\nseen, some corpora already provide access at the sentence level.  In\\nthe following example, we compute the average number of words per\\nsentence in the Brown Corpus:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; len(nltk.corpus.brown.words()) / len(nltk.corpus.brown.sents())\\n20.250994070456922\\n\\n\\n\\nIn other cases, the text is only available as a stream of characters.  Before\\ntokenizing the text into words, we need to segment it into sentences.  NLTK facilitates\\nthis by including the Punkt sentence segmenter (Kiss &amp; Strunk, 2006).\\nHere is an example of its use in segmenting the text of a novel.\\n(Note that if the segmenter\\'s internal data has been updated by the time you read this,\\nyou will see different output):\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; text = nltk.corpus.gutenberg.raw(\\'chesterton-thursday.txt\\')\\n&gt;&gt;&gt; sents = nltk.sent_tokenize(text)\\n&gt;&gt;&gt; pprint.pprint(sents[79:89])\\n[\\'\"Nonsense!\"\\',\\n \\'said Gregory, who was very rational when anyone else\\\\nattempted paradox.\\',\\n \\'\"Why do all the clerks and navvies in the\\\\n\\'\\n \\'railway trains look so sad and tired, so very sad and tired?\\',\\n \\'I will\\\\ntell you.\\',\\n \\'It is because they know that the train is going right.\\',\\n \\'It\\\\n\\'\\n \\'is because they know that whatever place they have taken a ticket\\\\n\\'\\n \\'for that place they will reach.\\',\\n \\'It is because after they have\\\\n\\'\\n \\'passed Sloane Square they know that the next station must be\\\\n\\'\\n \\'Victoria, and nothing but Victoria.\\',\\n \\'Oh, their wild rapture!\\',\\n \\'oh,\\\\n\\'\\n \\'their eyes like stars and their souls again in Eden, if the next\\\\n\\'\\n \\'station were unaccountably Baker Street!\"\\',\\n \\'\"It is you who are unpoetical,\" replied the poet Syme.\\']\\n\\n\\n\\nNotice that this example is really a single sentence, reporting the speech of Mr Lucian Gregory.\\nHowever, the quoted speech contains several sentences, and these have been split into individual\\nstrings.  This is reasonable behavior for most applications.\\nSentence segmentation is difficult because period is used to mark abbreviations,\\nand some periods simultaneously mark an abbreviation and terminate a sentence,\\nas often happens with acronyms like U.S.A.\\nFor another approach to sentence segmentation, see 2.\\n\\n\\nWord Segmentation\\nFor some writing systems, tokenizing text is made more difficult by the fact that there\\nis no visual representation of word boundaries.\\nFor example, in Chinese, the three-character string: 爱国人\\n(ai4 \"love\" (verb), guo2 \"country\", ren2 \"person\") could\\nbe tokenized as 爱国 / 人, \"country-loving person\"\\nor as 爱 / 国人, \"love country-person.\"\\nA similar problem arises in the processing of spoken language, where the\\nhearer must segment a continuous speech stream into individual words.\\nA particularly challenging version of this problem arises when we don\\'t\\nknow the words in advance.  This is the problem faced by a language learner,\\nsuch as a child hearing utterances from a parent.  Consider the\\nfollowing artificial example, where word boundaries have been removed:\\n\\n  (1)\\n  a.doyouseethekitty\\n\\n  b.seethedoggy\\n\\n  c.doyoulikethekitty\\n\\n  d.likethedoggy\\n\\nOur first challenge is simply to represent the problem: we need to find\\na way to separate text content from the segmentation.  We can do this by\\nannotating each character with a boolean value to indicate whether or\\nnot a word-break appears after the character (an idea that will be used\\nheavily for \"chunking\" in 7.).\\nLet\\'s assume that the learner is given the utterance breaks,\\nsince these often correspond to extended pauses.  Here is a possible representation,\\nincluding the initial and target segmentations:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; text = \"doyouseethekittyseethedoggydoyoulikethekittylikethedoggy\"\\n&gt;&gt;&gt; seg1 = \"0000000000000001000000000010000000000000000100000000000\"\\n&gt;&gt;&gt; seg2 = \"0100100100100001001001000010100100010010000100010010000\"\\n\\n\\n\\nObserve that the segmentation strings consist of zeros and ones.  They\\nare one character shorter than the source text, since a text of length\\nn can only be broken up in n-1 places.\\nThe segment() function in 3.7 demonstrates that we can\\nget back to the original segmented text from the above representation.\\n\\n\\n\\n\\n&nbsp;\\ndef segment(text, segs):\\n    words = []\\n    last = 0\\n    for i in range(len(segs)):\\n        if segs[i] == \\'1\\':\\n            words.append(text[last:i+1])\\n            last = i+1\\n    words.append(text[last:])\\n    return words\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; text = \"doyouseethekittyseethedoggydoyoulikethekittylikethedoggy\"\\n&gt;&gt;&gt; seg1 = \"0000000000000001000000000010000000000000000100000000000\"\\n&gt;&gt;&gt; seg2 = \"0100100100100001001001000010100100010010000100010010000\"\\n&gt;&gt;&gt; segment(text, seg1)\\n[\\'doyouseethekitty\\', \\'seethedoggy\\', \\'doyoulikethekitty\\', \\'likethedoggy\\']\\n&gt;&gt;&gt; segment(text, seg2)\\n[\\'do\\', \\'you\\', \\'see\\', \\'the\\', \\'kitty\\', \\'see\\', \\'the\\', \\'doggy\\', \\'do\\', \\'you\\',\\n\\'like\\', \\'the\\', \\'kitty\\', \\'like\\', \\'the\\', \\'doggy\\']\\n\\n\\nExample 3.7 (code_segment.py): Figure 3.7: Reconstruct Segmented Text from String Representation:\\nseg1 and seg2 represent the initial and final\\nsegmentations of some hypothetical child-directed speech;\\nthe segment() function can use them to reproduce the\\nsegmented text.\\n\\nNow the segmentation task becomes a search problem: find the bit string that causes\\nthe text string to be correctly segmented into words.\\nWe assume the learner is acquiring words and storing them in an internal lexicon.\\nGiven a suitable lexicon, it is possible to reconstruct the source text as a sequence of\\nlexical items.  Following (Brent, 1995), we can define an objective function,\\na scoring function whose value we will try to optimize, based on\\nthe size of the lexicon (number of characters in the words plus an extra\\ndelimiter character to mark the end of each word) and the amount of information needed to\\nreconstruct the source text from the lexicon.  We illustrate this in\\n3.8.\\n\\n\\nFigure 3.8: Calculation of Objective Function: Given a hypothetical segmentation\\nof the source text (on the left), derive a lexicon and a derivation\\ntable that permit the source text to be reconstructed, then total\\nup the number of characters used by each lexical item (including a boundary\\nmarker) and the number of lexical items used by each derivation, to serve as a score of the quality of\\nthe segmentation; smaller values of the score indicate a better segmentation.\\n\\nIt is a simple matter to implement this objective function, as shown in\\n3.9.\\n\\n\\n\\n\\n&nbsp;\\ndef evaluate(text, segs):\\n    words = segment(text, segs)\\n    text_size = len(words)\\n    lexicon_size = sum(len(word) + 1 for word in set(words))\\n    return text_size + lexicon_size\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; text = \"doyouseethekittyseethedoggydoyoulikethekittylikethedoggy\"\\n&gt;&gt;&gt; seg1 = \"0000000000000001000000000010000000000000000100000000000\"\\n&gt;&gt;&gt; seg2 = \"0100100100100001001001000010100100010010000100010010000\"\\n&gt;&gt;&gt; seg3 = \"0000100100000011001000000110000100010000001100010000001\"\\n&gt;&gt;&gt; segment(text, seg3)\\n[\\'doyou\\', \\'see\\', \\'thekitt\\', \\'y\\', \\'see\\', \\'thedogg\\', \\'y\\', \\'doyou\\', \\'like\\',\\n \\'thekitt\\', \\'y\\', \\'like\\', \\'thedogg\\', \\'y\\']\\n&gt;&gt;&gt; evaluate(text, seg3)\\n47\\n&gt;&gt;&gt; evaluate(text, seg2)\\n48\\n&gt;&gt;&gt; evaluate(text, seg1)\\n64\\n\\n\\nExample 3.9 (code_evaluate.py): Figure 3.9: Computing the Cost of Storing the Lexicon and Reconstructing the Source Text\\n\\nThe final step is to search for the pattern of zeros and ones that minimizes this objective\\nfunction, shown in 3.10.  Notice that the best segmentation includes \"words\" like\\nthekitty, since there\\'s not enough evidence in the data to split this any further.\\n\\n\\n\\n\\n&nbsp;\\nfrom random import randint\\n\\ndef flip(segs, pos):\\n    return segs[:pos] + str(1-int(segs[pos])) + segs[pos+1:]\\n\\ndef flip_n(segs, n):\\n    for i in range(n):\\n        segs = flip(segs, randint(0, len(segs)-1))\\n    return segs\\n\\ndef anneal(text, segs, iterations, cooling_rate):\\n    temperature = float(len(segs))\\n    while temperature &gt; 0.5:\\n        best_segs, best = segs, evaluate(text, segs)\\n        for i in range(iterations):\\n            guess = flip_n(segs, round(temperature))\\n            score = evaluate(text, guess)\\n            if score &lt; best:\\n                best, best_segs = score, guess\\n        score, segs = best, best_segs\\n        temperature = temperature / cooling_rate\\n        print(evaluate(text, segs), segment(text, segs))\\n    print()\\n    return segs\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; text = \"doyouseethekittyseethedoggydoyoulikethekittylikethedoggy\"\\n&gt;&gt;&gt; seg1 = \"0000000000000001000000000010000000000000000100000000000\"\\n&gt;&gt;&gt; anneal(text, seg1, 5000, 1.2)\\n61 [\\'doyouseetheki\\', \\'tty\\', \\'see\\', \\'thedoggy\\', \\'doyouliketh\\', \\'ekittylike\\', \\'thedoggy\\']\\n59 [\\'doy\\', \\'ouseetheki\\', \\'ttysee\\', \\'thedoggy\\', \\'doy\\', \\'o\\', \\'ulikethekittylike\\', \\'thedoggy\\']\\n57 [\\'doyou\\', \\'seetheki\\', \\'ttysee\\', \\'thedoggy\\', \\'doyou\\', \\'liketh\\', \\'ekittylike\\', \\'thedoggy\\']\\n55 [\\'doyou\\', \\'seethekit\\', \\'tysee\\', \\'thedoggy\\', \\'doyou\\', \\'likethekittylike\\', \\'thedoggy\\']\\n54 [\\'doyou\\', \\'seethekit\\', \\'tysee\\', \\'thedoggy\\', \\'doyou\\', \\'like\\', \\'thekitty\\', \\'like\\', \\'thedoggy\\']\\n52 [\\'doyou\\', \\'seethekittysee\\', \\'thedoggy\\', \\'doyou\\', \\'like\\', \\'thekitty\\', \\'like\\', \\'thedoggy\\']\\n43 [\\'doyou\\', \\'see\\', \\'thekitty\\', \\'see\\', \\'thedoggy\\', \\'doyou\\', \\'like\\', \\'thekitty\\', \\'like\\', \\'thedoggy\\']\\n\\'0000100100000001001000000010000100010000000100010000000\\'\\n\\n\\nExample 3.10 (code_anneal.py): Figure 3.10: Non-Deterministic Search Using Simulated Annealing: begin searching\\nwith phrase segmentations only; randomly perturb the zeros and ones\\nproportional to the \"temperature\"; with each iteration the temperature\\nis lowered and the perturbation of boundaries is reduced. As this\\nsearch algorithm is non-deterministic, you may see a slightly different\\nresult.\\n\\nWith enough data, it is possible to automatically segment text into words with a reasonable\\ndegree of accuracy.  Such methods can be applied to tokenization for writing systems that\\ndon\\'t have any visual representation of word boundaries.\\n\\n\\n\\n3.9&nbsp;&nbsp;&nbsp;Formatting: From Lists to Strings\\nOften we write a program to report a single data item, such as a particular element\\nin a corpus that meets some complicated criterion, or a single summary statistic\\nsuch as a word-count or the performance of a tagger.  More often, we write a program\\nto produce a structured result; for example, a tabulation of numbers or linguistic forms,\\nor a reformatting of the original data.  When the results to be presented are linguistic,\\ntextual output is usually the most natural choice.  However, when the results are numerical,\\nit may be preferable to produce graphical output.  In this section you will learn about\\na variety of ways to present program output.\\n\\nFrom Lists to Strings\\nThe simplest kind of structured object we use for text processing is lists of words.\\nWhen we want to output these to a display or a file, we must convert\\nthese lists into strings.  To do this in Python we use the join() method, and specify\\nthe string to be used as the \"glue\".\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; silly = [\\'We\\', \\'called\\', \\'him\\', \\'Tortoise\\', \\'because\\', \\'he\\', \\'taught\\', \\'us\\', \\'.\\']\\n&gt;&gt;&gt; \\' \\'.join(silly)\\n\\'We called him Tortoise because he taught us .\\'\\n&gt;&gt;&gt; \\';\\'.join(silly)\\n\\'We;called;him;Tortoise;because;he;taught;us;.\\'\\n&gt;&gt;&gt; \\'\\'.join(silly)\\n\\'WecalledhimTortoisebecausehetaughtus.\\'\\n\\n\\n\\nSo \\' \\'.join(silly) means: take all the items in silly and\\nconcatenate them as one big string, using \\' \\' as a spacer between\\nthe items.  I.e. join() is a method of the string that you want to\\nuse as the glue.  (Many people find this notation for join() counter-intuitive.)\\nThe join() method only works on a list of strings — what we have been calling a text\\n— a complex type that enjoys some privileges in Python.\\n\\n\\nStrings and Formats\\nWe have seen that there are two ways to display the contents of an object:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; word = \\'cat\\'\\n&gt;&gt;&gt; sentence = \"\"\"hello\\n... world\"\"\"\\n&gt;&gt;&gt; print(word)\\ncat\\n&gt;&gt;&gt; print(sentence)\\nhello\\nworld\\n&gt;&gt;&gt; word\\n\\'cat\\'\\n&gt;&gt;&gt; sentence\\n\\'hello\\\\nworld\\'\\n\\n\\n\\nThe print command yields Python\\'s attempt to produce the most human-readable form of an object.\\nThe second method — naming the variable at a prompt — shows us a string\\nthat can be used to recreate this object.  It is important to keep in mind that both of\\nthese are just strings, displayed for the benefit of you, the user.  They do not give\\nus any clue as to the actual internal representation of the object.\\nThere are many other useful ways to display an object as a string of\\ncharacters.  This may be for the benefit of a human reader, or because\\nwe want to export our data to a particular file format for use\\nin an external program.\\nFormatted output typically contains a combination of variables and\\npre-specified strings, e.g. given a frequency distribution fdist\\nwe could do:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; fdist = nltk.FreqDist([\\'dog\\', \\'cat\\', \\'dog\\', \\'cat\\', \\'dog\\', \\'snake\\', \\'dog\\', \\'cat\\'])\\n&gt;&gt;&gt; for word in sorted(fdist):\\n...     print(word, \\'-&gt;\\', fdist[word], end=\\'; \\')\\ncat -&gt; 3; dog -&gt; 4; snake -&gt; 1;\\n\\n\\n\\nPrint statements that contain alternating variables and constants can be difficult to read and\\nmaintain.  Another solution is to use string formatting.\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; for word in sorted(fdist):\\n...    print(\\'{}-&gt;{};\\'.format(word, fdist[word]), end=\\' \\')\\ncat-&gt;3; dog-&gt;4; snake-&gt;1;\\n\\n\\n\\nTo understand what is going on here, let\\'s test out the\\nformat string on its own.  (By now this will be\\nyour usual method of exploring new syntax.)\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; \\'{}-&gt;{};\\'.format (\\'cat\\', 3)\\n\\'cat-&gt;3;\\'\\n\\n\\n\\nThe curly brackets \\'{}\\'  mark the presence of a replacement\\nfield: this acts as a\\nplaceholder for the string values of objects that are\\npassed to the str.format() method. We can embed occurrences of \\'{}\\'\\ninside a string, then replace them with strings by calling format() with\\nappropriate arguments.  A string containing\\nreplacement fields is called a format string.\\nLet\\'s unpack the above code further, in order to see this\\nbehavior up close:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; \\'{}-&gt;\\'.format(\\'cat\\')\\n\\'cat-&gt;\\'\\n&gt;&gt;&gt; \\'{}\\'.format(3)\\n\\'3\\'\\n&gt;&gt;&gt; \\'I want a {} right now\\'.format(\\'coffee\\')\\n\\'I want a coffee right now\\'\\n\\n\\n\\nWe can have any number of placeholders, but  the str.format method\\nmust be called with exactly the same number of arguments.\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; \\'{} wants a {} {}\\'.format (\\'Lee\\', \\'sandwich\\', \\'for lunch\\')\\n\\'Lee wants a sandwich for lunch\\'\\n&gt;&gt;&gt; \\'{} wants a {} {}\\'.format (\\'sandwich\\', \\'for lunch\\')\\nTraceback (most recent call last):\\n...\\n    \\'{} wants a {} {}\\'.format (\\'sandwich\\', \\'for lunch\\')\\nIndexError: tuple index out of range\\n\\n\\n\\nArguments to format() are consumed left to right, and any\\nsuperfluous arguments are simply ignored.\\n\\nSystem Message: ERROR/3 (ch03.rst2, line 2262)\\nUnexpected indentation.\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; \\'{} wants a {}\\'.format (\\'Lee\\', \\'sandwich\\', \\'for lunch\\')\\n\\'Lee wants a sandwich\\'\\n\\n\\n\\nThe field name in a format string can start with a number, which\\nrefers to a positional argument of format(). Something like\\n\\'from {} to {}\\'\\nis equivalent to  \\'from {0} to {1}\\', but we can use the numbers to get\\nnon-default orders:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; \\'from {1} to {0}\\'.format(\\'A\\', \\'B\\')\\n\\'from B to A\\'\\n\\n\\n\\nWe can also provide the values for the placeholders indirectly. Here\\'s\\nan example using a for loop:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; template = \\'Lee wants a {} right now\\'\\n&gt;&gt;&gt; menu = [\\'sandwich\\', \\'spam fritter\\', \\'pancake\\']\\n&gt;&gt;&gt; for snack in menu:\\n...     print(template.format(snack))\\n...\\nLee wants a sandwich right now\\nLee wants a spam fritter right now\\nLee wants a pancake right now\\n\\n\\n\\n\\n\\nLining Things Up\\n<!-- In case we don\\'t know in advance how wide a displayed value should be,\\nthe width value can be replaced with a star in the formatting string,\\nthen specified using a variable width-variable_.\\nwidth = 6\\n\\'%-*s\\' % (width, \\'dog\\') # [_width-variable]\\n \\'dog   \\' -->\\nSo far our format strings generated output of arbitrary width\\non the page (or screen).  We can add padding to obtain output of a given\\nwidth by inserting into the brackets a colon \\':\\' followed by an integer. So {:6}\\nspecifies that we want a string that is\\npadded to width 6.  It is right-justified by default for numbers ,\\nbut we can precede the width specifier with a \\'&lt;\\' alignment option to make numbers left-justified .\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; \\'{:6}\\'.format(41) \\n\\'    41\\'\\n&gt;&gt;&gt; \\'{:&lt;6}\\' .format(41) \\n\\'41    \\'\\n\\n\\n\\nStrings are left-justified by default, but can be right-justified with\\nthe \\'&gt;\\' alignment option.\\n\\nSystem Message: ERROR/3 (ch03.rst2, line 2310)\\nUnexpected indentation.\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; \\'{:6}\\'.format(\\'dog\\') \\n\\'dog   \\'\\n&gt;&gt;&gt; \\'{:&gt;6}\\'.format(\\'dog\\') \\n \\'   dog\\'\\n\\n\\n\\nOther control characters can be used to specify the sign and precision\\nof floating point numbers; for example {:.4f} indicates that four\\ndigits should be displayed after the decimal point for a floating\\npoint number.\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; import math\\n&gt;&gt;&gt; \\'{:.4f}\\'.format(math.pi)\\n\\'3.1416\\'\\n\\n\\n\\nThe string formatting is smart enough to know that if you include a\\n\\'%\\' in your format specification, then you want to represent the\\nvalue as a percentage; there\\'s no need to multiply by 100.\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; count, total = 3205, 9375\\n&gt;&gt;&gt; \"accuracy for {} words: {:.4%}\".format(total, count / total)\\n\\'accuracy for 9375 words: 34.1867%\\'\\n\\n\\n\\nAn important use of formatting strings is for tabulating data.\\nRecall that in 1 we saw\\ndata being tabulated from a conditional frequency distribution.\\nLet\\'s perform the tabulation ourselves, exercising full control\\nof headings and column widths, as shown in 3.11.\\nNote the clear separation between the language processing work,\\nand the tabulation of results.\\n\\n\\n\\n\\n&nbsp;\\ndef tabulate(cfdist, words, categories):\\n    print(\\'{:16}\\'.format(\\'Category\\'), end=\\' \\')                    # column headings\\n    for word in words:\\n        print(\\'{:&gt;6}\\'.format(word), end=\\' \\')\\n    print()\\n    for category in categories:\\n        print(\\'{:16}\\'.format(category), end=\\' \\')                  # row heading\\n        for word in words:                                        # for each word\\n            print(\\'{:6}\\'.format(cfdist[category][word]), end=\\' \\') # print table cell\\n        print()                                                   # end the row\\n\\n&gt;&gt;&gt; from nltk.corpus import brown\\n&gt;&gt;&gt; cfd = nltk.ConditionalFreqDist(\\n...           (genre, word)\\n...           for genre in brown.categories()\\n...           for word in brown.words(categories=genre))\\n&gt;&gt;&gt; genres = [\\'news\\', \\'religion\\', \\'hobbies\\', \\'science_fiction\\', \\'romance\\', \\'humor\\']\\n&gt;&gt;&gt; modals = [\\'can\\', \\'could\\', \\'may\\', \\'might\\', \\'must\\', \\'will\\']\\n&gt;&gt;&gt; tabulate(cfd, modals, genres)\\nCategory            can  could    may  might   must   will\\nnews                 93     86     66     38     50    389\\nreligion             82     59     78     12     54     71\\nhobbies             268     58    131     22     83    264\\nscience_fiction      16     49      4     12      8     16\\nromance              74    193     11     51     45     43\\nhumor                16     30      8      8      9     13\\n\\n\\nExample 3.11 (code_modal_tabulate.py): Figure 3.11: Frequency of Modals in Different Sections of the Brown Corpus\\n\\nRecall from the listing in 3.6 that we used a format string\\n\\'{:{width}}\\' and bound a value to the width parameter in\\nformat().  This allows us to specify the width of a field using a\\nvariable.\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; \\'{:{width}}\\'.format(\\'Monty Python\\', width=15)\\n\\'Monty Python   \\'\\n\\n\\n\\nWe could use this to automatically customize the column to be\\njust wide enough to accommodate all the words, using\\nwidth = max(len(w) for w in words).\\n\\n\\nWriting Results to a File\\nWe have seen how to read text from files (3.1).\\nIt is often useful to write output to files as well.  The following\\ncode opens a file output.txt for writing, and saves the program\\noutput to the file.\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; output_file = open(\\'output.txt\\', \\'w\\')\\n&gt;&gt;&gt; words = set(nltk.corpus.genesis.words(\\'english-kjv.txt\\'))\\n&gt;&gt;&gt; for word in sorted(words):\\n...     print(word, file=output_file)\\n\\n\\n\\nWhen we write non-text data to a file we must convert it to a string first.\\nWe can do this conversion using formatting strings, as we saw above.\\nLet\\'s write the total number of words to our file:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; len(words)\\n2789\\n&gt;&gt;&gt; str(len(words))\\n\\'2789\\'\\n&gt;&gt;&gt; print(str(len(words)), file=output_file)\\n\\n\\n\\n\\nCaution!\\nYou should avoid filenames that contain space characters like\\noutput file.txt, or that are identical except for case\\ndistinctions, e.g. Output.txt and output.TXT.\\n\\n\\n\\nText Wrapping\\nWhen the output of our program is text-like, instead of tabular,\\nit will usually be necessary to wrap it so that it can be displayed\\nconveniently.  Consider the following output, which overflows its line,\\nand which uses a complicated print statement:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; saying = [\\'After\\', \\'all\\', \\'is\\', \\'said\\', \\'and\\', \\'done\\', \\',\\',\\n...           \\'more\\', \\'is\\', \\'said\\', \\'than\\', \\'done\\', \\'.\\']\\n&gt;&gt;&gt; for word in saying:\\n...     print(word, \\'(\\' + str(len(word)) + \\'),\\', end=\\' \\')\\nAfter (5), all (3), is (2), said (4), and (3), done (4), , (1), more (4), is (2), said (4), than (4), done (4), . (1),\\n\\n\\n\\nWe can take care of line wrapping with the help of Python\\'s textwrap module.\\nFor maximum clarity we will separate each step onto its own line:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; from textwrap import fill\\n&gt;&gt;&gt; pieces = [\"{} {}\".format(word, len(word)) for word in saying]\\n&gt;&gt;&gt; output = \\' \\'.join(pieces)\\n&gt;&gt;&gt; wrapped = fill(output)\\n&gt;&gt;&gt; print(wrapped)\\nAfter (5), all (3), is (2), said (4), and (3), done (4), , (1), more\\n(4), is (2), said (4), than (4), done (4), . (1),\\n\\n\\n\\nNotice that there is a linebreak between more and its following number.\\nIf we wanted to avoid this, we could\\nredefine the formatting string so that it contained no spaces,\\ne.g. \\'%s_(%d),\\', then instead of printing the value of wrapped,\\nwe could print wrapped.replace(\\'_\\', \\' \\').\\n\\n\\n\\n3.10&nbsp;&nbsp;&nbsp;Summary\\n\\nIn this book we view a text as a list of words.  A \"raw text\" is a potentially\\nlong string containing words and whitespace formatting, and is how we\\ntypically store and visualize a text.\\nA string is specified in Python using single or double quotes: \\'Monty Python\\', \"Monty Python\".\\nThe characters of a string are accessed using indexes, counting from zero:\\n\\'Monty Python\\'[0] gives the value M.  The length of a string is\\nfound using len().\\nSubstrings are accessed using slice notation: \\'Monty Python\\'[1:5]\\ngives the value onty.  If the start index is omitted, the\\nsubstring begins at the start of the string; if the end index is omitted,\\nthe slice continues to the end of the string.\\nStrings can be split into lists: \\'Monty Python\\'.split() gives\\n[\\'Monty\\', \\'Python\\'].  Lists can be joined into strings:\\n\\'/\\'.join([\\'Monty\\', \\'Python\\']) gives \\'Monty/Python\\'.\\nWe can read text from a file input.txt using text = open(\\'input.txt\\').read().\\nWe can read text from url using text = request.urlopen(url).read().decode(\\'utf8\\').\\nWe can iterate over the lines of a text file using for line in open(f).\\nWe can write text to a file by opening the file for writing\\noutput_file = open(\\'output.txt\\', \\'w\\'), then adding content to the\\nfile print(\"Monty Python\", file=output_file).\\nTexts found on the web may contain unwanted material (such as headers, footers, markup),\\nthat need to be removed before we do any linguistic processing.\\nTokenization is the segmentation of a text into basic units — or tokens —\\nsuch as words and punctuation.\\nTokenization based on whitespace is inadequate for many applications because it\\nbundles punctuation together with words.\\nNLTK provides an off-the-shelf tokenizer nltk.word_tokenize().\\nLemmatization is a process that maps the various forms of a word (such as appeared, appears)\\nto the canonical or citation form of the word, also known as the lexeme or lemma (e.g. appear).\\nRegular expressions are a powerful and flexible method of specifying\\npatterns. Once we have imported the re module, we can use\\nre.findall() to find all substrings in a string that match a pattern.\\nIf a regular expression string includes a backslash, you should tell Python not to\\npreprocess the string, by using a raw string with an r prefix: r\\'regexp\\'.\\nWhen backslash is used before certain characters, e.g. \\\\n, this takes on\\na special meaning (newline character); however, when backslash is used\\nbefore regular expression wildcards and operators, e.g. \\\\., \\\\|, \\\\$,\\nthese characters lose their special meaning and are matched literally.\\nA string formatting expression template % arg_tuple consists of a\\nformat string template that contains conversion specifiers\\nlike %-6s and %0.2d.\\n\\n\\n\\n3.11&nbsp;&nbsp;&nbsp;Further Reading\\nExtra materials for this chapter are posted at http://nltk.org/, including links to freely\\navailable resources on the web.  Remember to consult the Python reference materials\\nat http://docs.python.org/.  (For example, this documentation covers \"universal newline support,\"\\nexplaining how to work with the different newline conventions used by various\\noperating systems.)\\nFor more examples of processing words with NLTK, see the\\ntokenization, stemming and corpus HOWTOs at http://nltk.org/howto.\\nChapters 2 and 3 of (Jurafsky &amp; Martin, 2008) contain more advanced\\nmaterial on regular expressions and morphology.  For more extensive\\ndiscussion of text processing with Python see (Mertz, 2003).\\nFor information about normalizing non-standard words see (Sproat et al, 2001)\\nThere are many references for regular expressions, both practical and\\ntheoretical.  For an introductory\\ntutorial to using regular expressions in Python,\\nsee Kuchling\\'s Regular Expression HOWTO,\\nhttp://www.amk.ca/python/howto/regex/.\\nFor a comprehensive and detailed manual\\nin using regular expressions, covering their syntax in most major\\nprogramming languages, including Python, see (Friedl, 2002).\\nOther presentations include Section 2.1 of (Jurafsky &amp; Martin, 2008),\\nand Chapter 3 of (Mertz, 2003).\\nThere are many online resources for Unicode.  Useful discussions\\nof Python\\'s facilities for handling Unicode are:\\n\\nNed Batchelder, Pragmatic Unicode, http://nedbatchelder.com/text/unipain.html\\nUnicode HOWTO, Python Documentation,\\nhttp://docs.python.org/3/howto/unicode.html\\nDavid Beazley, Mastering Python 3 I/O,\\nhttp://pyvideo.org/video/289/pycon-2010--mastering-python-3-i-o\\nJoel Spolsky, The Absolute Minimum Every Software Developer\\nAbsolutely, Positively Must Know About Unicode and Character Sets\\n(No Excuses!), http://www.joelonsoftware.com/articles/Unicode.html\\n\\nThe problem of tokenizing Chinese text is a major focus of SIGHAN,\\nthe ACL Special Interest Group on Chinese Language Processing\\nhttp://sighan.org/.  Our method for segmenting English text\\nfollows (Brent, 1995); this work falls in the area of\\nlanguage acquisition (Niyogi, 2006).\\nCollocations are a special case of multiword expressions.\\nA multiword expression is a small phrase whose meaning\\nand other properties cannot be predicted from its words alone,\\ne.g. part of speech (Baldwin &amp; Kim, 2010).\\nSimulated annealing is a heuristic for finding\\na good approximation to the optimum value of\\na function in a large, discrete search space,\\nbased on an analogy with annealing in metallurgy.\\nThe technique is described in many Artificial Intelligence texts.\\nThe approach to discovering hyponyms in text using search\\npatterns like x and other ys is described by (Hearst, 1992).\\n\\n\\n3.12&nbsp;&nbsp;&nbsp;Exercises\\n\\n☼ Define a string s = \\'colorless\\'.  Write a Python statement\\nthat changes this to \"colourless\" using only the slice and\\nconcatenation operations.\\n\\n☼ We can use the slice notation to remove morphological endings on\\nwords.  For example, \\'dogs\\'[:-1] removes the last character of\\ndogs, leaving dog.  Use slice notation to remove the\\naffixes from these words (we\\'ve inserted a hyphen to\\nindicate the affix boundary, but omit this from your strings):\\ndish-es, run-ning, nation-ality, un-do,\\npre-heat.\\n\\n☼ We saw how we can generate an IndexError by indexing beyond the end\\nof a string.  Is it possible to construct an index that goes too far to\\nthe left, before the start of the string?\\n\\n☼ We can specify a \"step\" size for the slice. The following\\nreturns every second character within the slice: monty[6:11:2].\\nIt also works in the reverse direction: monty[10:5:-2]\\nTry these for yourself, then experiment with different step values.\\n\\n☼ What happens if you ask the interpreter to evaluate monty[::-1]?\\nExplain why this is a reasonable result.\\n\\n☼ Describe the class of strings matched by the following regular\\nexpressions.\\n\\n[a-zA-Z]+\\n[A-Z][a-z]*\\np[aeiou]{,2}t\\n\\\\d+(\\\\.\\\\d+)?\\n([^aeiou][aeiou][^aeiou])*\\n\\\\w+|[^\\\\w\\\\s]+\\n\\nTest your answers using nltk.re_show().\\n\\n☼ Write regular expressions to match the following classes of strings:\\n\\n\\nA single determiner (assume that a, an, and the\\nare the only determiners).\\nAn arithmetic expression using integers, addition, and\\nmultiplication, such as 2*3+8.\\n\\n\\n\\n☼ Write a utility function that takes a URL as its argument, and returns\\nthe contents of the URL, with all HTML markup removed.  Use from urllib import request\\nand then request.urlopen(\\'http://nltk.org/\\').read().decode(\\'utf8\\') to access the contents of the URL.\\n\\n☼\\nSave some text into a file corpus.txt.  Define a function load(f)\\nthat reads from the file named in its sole argument, and returns a string\\ncontaining the text of the file.\\n\\nUse nltk.regexp_tokenize() to create a tokenizer that tokenizes\\nthe various kinds of punctuation in this text.  Use one multi-line\\nregular expression, with inline comments, using the verbose flag (?x).\\nUse nltk.regexp_tokenize() to create a tokenizer that tokenizes\\nthe following kinds of expression: monetary amounts; dates; names\\nof people and organizations.\\n\\n\\n☼ Rewrite the following loop as a list comprehension:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; sent = [\\'The\\', \\'dog\\', \\'gave\\', \\'John\\', \\'the\\', \\'newspaper\\']\\n&gt;&gt;&gt; result = []\\n&gt;&gt;&gt; for word in sent:\\n...     word_len = (word, len(word))\\n...     result.append(word_len)\\n&gt;&gt;&gt; result\\n[(\\'The\\', 3), (\\'dog\\', 3), (\\'gave\\', 4), (\\'John\\', 4), (\\'the\\', 3), (\\'newspaper\\', 9)]\\n\\n\\n\\n\\n☼ Define a string raw containing a sentence of your own choosing.\\nNow, split raw on some character other than space, such as \\'s\\'.\\n\\n☼ Write a for loop to print out the characters of a string, one per line.\\n\\n☼ What is the difference between calling split on a string\\nwith no argument or with \\' \\' as the argument,\\ne.g. sent.split() versus sent.split(\\' \\')?  What happens\\nwhen the string being split contains tab characters, consecutive\\nspace characters, or a sequence of tabs and spaces?  (In IDLE you\\nwill need to use \\'\\\\t\\' to enter a tab character.)\\n\\n☼ Create a variable words containing a list of words.\\nExperiment with words.sort() and sorted(words).\\nWhat is the difference?\\n\\n☼ Explore the difference between strings and integers by typing\\nthe following at a Python prompt: \"3\" * 7 and 3 * 7.\\nTry converting between strings and integers using\\nint(\"3\") and str(3).\\n\\n☼ Use a text editor to create a file\\ncalled prog.py containing the single line monty = \\'Monty Python\\'.\\nNext, start up a new session with the\\nPython interpreter, and enter the expression monty at the prompt.\\nYou will get an error from the interpreter. Now, try the following\\n(note that you have to leave off the .py part of the filename):\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; from prog import monty\\n&gt;&gt;&gt; monty\\n\\n\\n\\nThis time, Python should return with a value. You can also try\\nimport prog, in which case Python should be able to\\nevaluate the expression prog.monty at the prompt.\\n\\n☼ What happens when the formatting strings %6s and %-6s\\nare used to display strings that are longer than six characters?\\n\\n◑ Read in some text from a corpus, tokenize it, and print the list of\\nall wh-word types that occur.  (wh-words in English\\nare used in questions, relative clauses and exclamations:\\nwho, which, what, and so on.) Print\\nthem in order.  Are any words duplicated in this list, because of\\nthe presence of case distinctions or punctuation?\\n\\n◑ Create a file consisting of words and (made up) frequencies, where each\\nline consists of a word, the space character, and a positive integer,\\ne.g. fuzzy 53.  Read the file into a Python list using open(filename).readlines().\\nNext, break each line into its two fields using split(), and\\nconvert the number into an integer using int().  The result should\\nbe a list of the form: [[\\'fuzzy\\', 53], ...].\\n\\n◑ Write code to access a favorite webpage and extract some text from it.\\nFor example, access a weather site and extract the forecast top\\ntemperature for your town or city today.\\n\\n◑ Write a function unknown() that takes a URL as its argument,\\nand returns a list of unknown words that occur on that webpage.\\nIn order to do this, extract all substrings consisting of lowercase letters\\n(using re.findall()) and remove any items from this set that occur\\nin the Words Corpus (nltk.corpus.words).  Try to categorize these words\\nmanually and discuss your findings.\\n\\n◑ Examine the results of processing the URL\\nhttp://news.bbc.co.uk/ using the regular expressions suggested\\nabove. You will see that there is still a fair amount of\\nnon-textual data there, particularly Javascript commands. You may\\nalso find that sentence breaks have not been properly\\npreserved. Define further regular expressions that improve the\\nextraction of text from this web page.\\n\\n◑ Are you able to write a regular expression to tokenize text in such\\na way that the word don\\'t is tokenized into do and n\\'t?\\nExplain why this regular expression won\\'t work: «n\\'t|\\\\w+».\\n\\n◑ Try to write code to convert text into hAck3r, using regular expressions\\nand substitution, where\\ne → 3,\\ni → 1,\\no → 0,\\nl → |,\\ns → 5,\\n. → 5w33t!,\\nate → 8.\\nNormalize the text to lowercase before converting it.\\nAdd more substitutions of your own.  Now try to map\\ns to two different values: $ for word-initial s,\\nand 5 for word-internal s.\\n\\n◑ Pig Latin is a simple transformation of English text.  Each word of the\\ntext is converted as follows: move any consonant (or consonant cluster)\\nthat appears at the start of the word to the end,\\nthen append ay, e.g. string → ingstray,\\nidle → idleay.  http://en.wikipedia.org/wiki/Pig_Latin\\n\\nWrite a function to convert a word to Pig Latin.\\nWrite code that converts text, instead of individual words.\\nExtend it further to preserve capitalization, to keep qu together\\n(i.e. so that quiet becomes ietquay), and to detect when y\\nis used as a consonant (e.g. yellow) vs a vowel (e.g. style).\\n\\n\\n◑ Download some text from a language that has vowel harmony (e.g. Hungarian),\\nextract the vowel sequences of words, and create a vowel bigram table.\\n\\n◑ Python\\'s random module includes a function choice() which\\nrandomly chooses an item from a sequence, e.g. choice(\"aehh \") will\\nproduce one of four possible characters, with the letter h being\\ntwice as frequent as the others.  Write a generator expression\\nthat produces a sequence of 500 randomly chosen letters drawn from the\\nstring \"aehh \", and put this\\nexpression inside a call to the \\'\\'.join() function, to concatenate\\nthem into one long string.  You should get a result that looks like\\nuncontrolled sneezing or maniacal laughter: he  haha ee  heheeh eha.\\nUse split() and join() again to normalize the whitespace in\\nthis string.\\n\\n◑ Consider the numeric expressions in the following sentence from\\nthe MedLine Corpus: The corresponding free cortisol fractions in these\\nsera were 4.53 +/- 0.15% and 8.16 +/- 0.23%, respectively.\\nShould we say that the numeric expression 4.53 +/- 0.15% is three\\nwords?  Or should we say that it\\'s a single compound word?  Or should\\nwe say that it is actually nine words, since it\\'s read \"four point\\nfive three, plus or minus zero point fifteen percent\"?  Or should we say that\\nit\\'s not a \"real\" word at all, since it wouldn\\'t appear in any dictionary?\\nDiscuss these different possibilities.  Can you think of application domains\\nthat motivate at least two of these answers?\\n\\n◑ Readability measures are used to score the reading difficulty of a\\ntext, for the purposes of selecting texts of appropriate difficulty\\nfor language learners.  Let us define\\nμw to be the average number of letters per word, and\\nμs to be the average number of words per sentence, in\\na given text.  The Automated Readability Index (ARI) of the text\\nis defined to be:\\n4.71 μw + 0.5 μs - 21.43.\\nCompute the ARI score for various sections of the Brown Corpus, including\\nsection f (lore) and j (learned).  Make use of the fact that\\nnltk.corpus.brown.words() produces a sequence of words, while\\nnltk.corpus.brown.sents() produces a sequence of sentences.\\n\\n◑ Use the Porter Stemmer to normalize some tokenized text, calling\\nthe stemmer on each word.  Do the same thing with the Lancaster Stemmer\\nand see if you observe any differences.\\n\\n◑ Define the variable saying to contain the list\\n[\\'After\\', \\'all\\', \\'is\\', \\'said\\', \\'and\\', \\'done\\', \\',\\', \\'more\\',\\n\\'is\\', \\'said\\', \\'than\\', \\'done\\', \\'.\\'].  Process this list using a for loop, and store the\\nlength of each word in a new list lengths.  Hint: begin by assigning the\\nempty list to lengths, using lengths = []. Then each time\\nthrough the loop, use append() to add another length value to\\nthe list. Now do the same thing using a list comprehension.\\n\\n◑ Define a variable silly to contain the string:\\n\\'newly formed bland ideas are inexpressible in an infuriating\\nway\\'.  (This happens to be the legitimate interpretation that\\nbilingual English-Spanish speakers can assign to Chomsky\\'s\\nfamous nonsense phrase, colorless green ideas sleep furiously\\naccording to Wikipedia).  Now write code to perform the following tasks:\\n\\nSplit silly into a list of strings, one per\\nword, using Python\\'s split() operation, and save\\nthis to a variable called bland.\\nExtract the second letter of each word in silly and join\\nthem into a string, to get \\'eoldrnnnna\\'.\\nCombine the words in bland back into a single string, using join().\\nMake sure the words in the resulting string are separated with\\nwhitespace.\\nPrint the words of silly in alphabetical order, one per line.\\n\\n\\n◑ The index() function can be used to look up items in sequences.\\nFor example, \\'inexpressible\\'.index(\\'e\\') tells us the index of the\\nfirst position of the letter e.\\n\\nWhat happens when you look up a substring, e.g. \\'inexpressible\\'.index(\\'re\\')?\\nDefine a variable words containing a list of words.  Now use words.index()\\nto look up the position of an individual word.\\nDefine a variable silly as in the exercise above.\\nUse the index() function in combination with list slicing to\\nbuild a list phrase consisting of all the words up to (but not\\nincluding) in in silly.\\n\\n\\n◑ Write code to convert nationality adjectives like Canadian and\\nAustralian to their corresponding nouns Canada and Australia\\n(see http://en.wikipedia.org/wiki/List_of_adjectival_forms_of_place_names).\\n\\n◑ Read the LanguageLog post on phrases of the form as best as p can\\nand as best p can, where p is a pronoun.   Investigate this\\nphenomenon with the help of a corpus and the findall() method\\nfor searching tokenized text described in\\n3.5.\\nhttp://itre.cis.upenn.edu/~myl/languagelog/archives/002733.html\\n\\n◑ Study the lolcat version of the book of Genesis,\\naccessible as nltk.corpus.genesis.words(\\'lolcat.txt\\'), and\\nthe rules for converting text into lolspeak at\\nhttp://www.lolcatbible.com/index.php?title=How_to_speak_lolcat.\\nDefine regular expressions to convert English words into\\ncorresponding lolspeak words.\\n\\n◑ Read about the re.sub() function for string substitution\\nusing regular expressions, using help(re.sub) and by consulting\\nthe further readings for this chapter.  Use re.sub in writing code\\nto remove HTML tags from an HTML file, and to normalize whitespace.\\n\\n★ An interesting challenge for tokenization is words that have been\\nsplit across a line-break.  E.g. if long-term is split, then we\\nhave the string long-\\\\nterm.\\n\\nWrite a regular expression that identifies words that are\\nhyphenated at a line-break.  The expression will need to include the\\n\\\\n character.\\nUse re.sub() to remove the \\\\n character from these\\nwords.\\nHow might you identify words that should not remain hyphenated\\nonce the newline is removed, e.g. \\'encyclo-\\\\npedia\\'?x\\n\\n\\n★ Read the Wikipedia entry on Soundex.  Implement this\\nalgorithm in Python.\\n\\n★ Obtain raw texts from two or more genres and compute their respective\\nreading difficulty scores as in the earlier exercise on reading difficulty.\\nE.g. compare ABC Rural News and ABC Science News (nltk.corpus.abc).\\nUse Punkt to perform sentence segmentation.\\n\\n★ Rewrite the following nested loop as a nested list comprehension:\\n\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; words = [\\'attribution\\', \\'confabulation\\', \\'elocution\\',\\n...          \\'sequoia\\', \\'tenacious\\', \\'unidirectional\\']\\n&gt;&gt;&gt; vsequences = set()\\n&gt;&gt;&gt; for word in words:\\n...     vowels = []\\n...     for char in word:\\n...         if char in \\'aeiou\\':\\n...             vowels.append(char)\\n...     vsequences.add(\\'\\'.join(vowels))\\n&gt;&gt;&gt; sorted(vsequences)\\n[\\'aiuio\\', \\'eaiou\\', \\'eouio\\', \\'euoia\\', \\'oauaio\\', \\'uiieioa\\']\\n\\n\\n\\n\\n\\n\\n★ Use WordNet to create a semantic index for a text collection.\\nExtend the concordance search program in 3.6,\\nindexing each word using the offset of its first synset,\\ne.g. wn.synsets(\\'dog\\')[0].offset (and optionally the\\noffset of some of its ancestors in the hypernym hierarchy).\\n\\n★ With the help of a multilingual corpus such as the\\nUniversal Declaration of Human Rights Corpus (nltk.corpus.udhr),\\nand NLTK\\'s frequency distribution and rank correlation functionality\\n(nltk.FreqDist, nltk.spearman_correlation),\\ndevelop a system that guesses the language of a previously unseen text.\\nFor simplicity, work with a single character encoding and just a few\\nlanguages.\\n\\n★ Write a program that processes a text and discovers\\ncases where a word has been used with a novel sense.\\nFor each word, compute the WordNet similarity\\nbetween all synsets of the word and all synsets of the\\nwords in its context.  (Note that this is a crude\\napproach; doing it well is a difficult, open research problem.)\\n\\n★ Read the article on normalization of non-standard words\\n(Sproat et al, 2001), and implement a similar system for text normalization.\\n\\n\\n<!-- no longer works, need UserAgent spoofing\\n#. |soso| Define a function ``ghits()`` that takes a word as its argument and\\n   builds a Google query string of the form ``http://www.google.com/search?q=word``.\\n   Strip the |HTML| markup and normalize whitespace.  Search for a substring\\n   of the form ``Results 1 - 10 of about``, followed by some number\\n   `n`:math:,  and extract `n`:math:.\\n   Convert this to an integer and return it. -->\\n\\n\\nAbout this document...\\nUPDATED FOR NLTK 3.0.\\nThis is a chapter from Natural Language Processing with Python,\\nby Steven Bird, Ewan Klein and Edward Loper,\\nCopyright © 2019 the authors.\\nIt is distributed with the Natural Language Toolkit [http://nltk.org/],\\nVersion 3.0, under the terms of the\\nCreative Commons Attribution-Noncommercial-No Derivative Works 3.0 United States License\\n[http://creativecommons.org/licenses/by-nc-nd/3.0/us/].\\nThis document was built on\\nWed  4 Sep 2019 11:40:48 ACST\\n\\n\\n\\n\\n\\n\\n',\n",
       " '\\n\\n\\n\\nfunction astext(node)\\n{\\n    return node.innerHTML.replace(/(]+)>)/ig,\"\")\\n                         .replace(/&gt;/ig, \">\")\\n                         .replace(/&lt;/ig, \"<\")\\n                         .replace(/&quot;/ig, \\'\"\\')\\n                         .replace(/&amp;/ig, \"&\");\\n}\\n\\nfunction copy_notify(node, bar_color, data)\\n{\\n    // The outer box: relative + inline positioning.\\n    var box1 = document.createElement(\"div\");\\n    box1.style.position = \"relative\";\\n    box1.style.display = \"inline\";\\n    box1.style.top = \"2em\";\\n    box1.style.left = \"1em\";\\n  \\n    // A shadow for fun\\n    var shadow = document.createElement(\"div\");\\n    shadow.style.position = \"absolute\";\\n    shadow.style.left = \"-1.3em\";\\n    shadow.style.top = \"-1.3em\";\\n    shadow.style.background = \"#404040\";\\n    \\n    // The inner box: absolute positioning.\\n    var box2 = document.createElement(\"div\");\\n    box2.style.position = \"relative\";\\n    box2.style.border = \"1px solid #a0a0a0\";\\n    box2.style.left = \"-.2em\";\\n    box2.style.top = \"-.2em\";\\n    box2.style.background = \"white\";\\n    box2.style.padding = \".3em .4em .3em .4em\";\\n    box2.style.fontStyle = \"normal\";\\n    box2.style.background = \"#f0e0e0\";\\n\\n    node.insertBefore(box1, node.childNodes.item(0));\\n    box1.appendChild(shadow);\\n    shadow.appendChild(box2);\\n    box2.innerHTML=\"Copied&nbsp;to&nbsp;the&nbsp;clipboard: \" +\\n                   \"\"+\\n                   data+\"\";\\n    setTimeout(function() { node.removeChild(box1); }, 1000);\\n\\n    var elt = node.parentNode.firstChild;\\n    elt.style.background = \"#ffc0c0\";\\n    setTimeout(function() { elt.style.background = bar_color; }, 200);\\n}\\n\\nfunction copy_codeblock_to_clipboard(node)\\n{\\n    var data = astext(node)+\"\\\\n\";\\n    if (copy_text_to_clipboard(data)) {\\n        copy_notify(node, \"#40a060\", data);\\n    }\\n}\\n\\nfunction copy_doctest_to_clipboard(node)\\n{\\n    var s = astext(node)+\"\\\\n   \";\\n    var data = \"\";\\n\\n    var start = 0;\\n    var end = s.indexOf(\"\\\\n\");\\n    while (end >= 0) {\\n        if (s.substring(start, start+4) == \">>> \") {\\n            data += s.substring(start+4, end+1);\\n        }\\n        else if (s.substring(start, start+4) == \"... \") {\\n            data += s.substring(start+4, end+1);\\n        }\\n        /*\\n        else if (end-start > 1) {\\n            data += \"# \" + s.substring(start, end+1);\\n        }*/\\n        // Grab the next line.\\n        start = end+1;\\n        end = s.indexOf(\"\\\\n\", start);\\n    }\\n    \\n    if (copy_text_to_clipboard(data)) {\\n        copy_notify(node, \"#4060a0\", data);\\n    }\\n}\\n    \\nfunction copy_text_to_clipboard(data)\\n{\\n    if (window.clipboardData) {\\n        window.clipboardData.setData(\"Text\", data);\\n        return true;\\n     }\\n    else if (window.netscape) {\\n        // w/ default firefox settings, permission will be denied for this:\\n        netscape.security.PrivilegeManager\\n                      .enablePrivilege(\"UniversalXPConnect\");\\n    \\n        var clip = Components.classes[\"@mozilla.org/widget/clipboard;1\"]\\n                      .createInstance(Components.interfaces.nsIClipboard);\\n        if (!clip) return;\\n    \\n        var trans = Components.classes[\"@mozilla.org/widget/transferable;1\"]\\n                       .createInstance(Components.interfaces.nsITransferable);\\n        if (!trans) return;\\n    \\n        trans.addDataFlavor(\"text/unicode\");\\n    \\n        var str = new Object();\\n        var len = new Object();\\n    \\n        var str = Components.classes[\"@mozilla.org/supports-string;1\"]\\n                     .createInstance(Components.interfaces.nsISupportsString);\\n        var datacopy=data;\\n        str.data=datacopy;\\n        trans.setTransferData(\"text/unicode\",str,datacopy.length*2);\\n        var clipid=Components.interfaces.nsIClipboard;\\n    \\n        if (!clip) return false;\\n    \\n        clip.setData(trans,null,clipid.kGlobalClipboard);\\n        return true;\\n    }\\n    return false;\\n}\\n//-->\\n\\n\\n\\n\\n\\n\\nch04.rst2\\n\\n\\n/*\\n:Author: Edward Loper, James Curran\\n:Copyright: This stylesheet has been placed in the public domain.\\n\\nStylesheet for use with Docutils.\\n\\nThis stylesheet defines new css classes used by NLTK.\\n\\nIt uses a Python syntax highlighting scheme that matches\\nthe colour scheme used by IDLE, which makes it easier for\\nbeginners to check they are typing things in correctly.\\n*/\\n\\n/* Include the standard docutils stylesheet. */\\n@import url(default.css);\\n\\n/* Custom inline roles */\\nspan.placeholder    { font-style: italic; font-family: monospace; }\\nspan.example        { font-style: italic; }\\nspan.emphasis       { font-style: italic; }\\nspan.termdef        { font-weight: bold; }\\n/*span.term           { font-style: italic; }*/\\nspan.category       { font-variant: small-caps; }\\nspan.feature        { font-variant: small-caps; }\\nspan.fval           { font-style: italic; }\\nspan.math           { font-style: italic; }\\nspan.mathit         { font-style: italic; }\\nspan.lex            { font-variant: small-caps; }\\nspan.guide-linecount{ text-align: right; display: block;}\\n\\n/* Python souce code listings */\\nspan.pysrc-prompt   { color: #9b0000; }\\nspan.pysrc-more     { color: #9b00ff; }\\nspan.pysrc-keyword  { color: #e06000; }\\nspan.pysrc-builtin  { color: #940094; }\\nspan.pysrc-string   { color: #00aa00; }\\nspan.pysrc-comment  { color: #ff0000; }\\nspan.pysrc-output   { color: #0000ff; }\\nspan.pysrc-except   { color: #ff0000; }\\nspan.pysrc-defname  { color: #008080; }\\n\\n\\n/* Doctest blocks */\\npre.doctest         { margin: 0; padding: 0; font-weight: bold; }\\ndiv.doctest         { margin: 0 1em 1em 1em; padding: 0; }\\ntable.doctest       { margin: 0; padding: 0;\\n                      border-top: 1px solid gray;\\n                      border-bottom: 1px solid gray; }\\npre.copy-notify     { margin: 0; padding: 0.2em; font-weight: bold;\\n                      background-color: #ffffff; }\\n\\n/* Python source listings */\\ndiv.pylisting       { margin: 0 1em 1em 1em; padding: 0; }\\ntable.pylisting     { margin: 0; padding: 0;\\n                      border-top: 1px solid gray; }\\ntd.caption { border-top: 1px solid black; margin: 0; padding: 0; }\\n.caption-label { font-weight: bold;  }\\ntd.caption p { margin: 0; padding: 0; font-style: normal;}\\n\\ntable tr td.codeblock { \\n  padding: 0.2em ! important; margin: 0;\\n  border-left: 1px solid gray;\\n  border-right: 2px solid gray;\\n  border-top: 0px solid gray;\\n  border-bottom: 1px solid gray;\\n  font-weight: bold; background-color: #eeffee;\\n}\\ntable pre span {\\n  white-space: pre-wrap;\\n}\\ntable tr td.doctest  { \\n  padding: 0.2em; margin: 0;\\n  border-left: 1px solid gray;\\n  border-right: 2px solid gray;\\n  border-top: 0px solid gray;\\n  border-bottom: 1px solid gray;\\n  font-weight: bold; background-color: #eeeeff;\\n}\\n\\ntd.codeblock table tr td.copybar {\\n    background: #40a060; border: 1px solid gray;\\n    font-family: monospace; padding: 0; margin: 0; }\\ntd.doctest table tr td.copybar {\\n    background: #4060a0; border: 1px solid gray;\\n    font-family: monospace; padding: 0; margin: 0; }\\n\\ntd.pysrc { padding-left: 0.5em; }\\n\\nimg.callout { border-width: 0px; }\\n\\ntable.docutils {\\n    border-style: solid;\\n    border-width: 1px;\\n    margin-top: 6px;\\n    border-color: grey;\\n    border-collapse: collapse; }\\n\\ntable.docutils th {\\n    border-style: none;\\n    border-width: 1px;\\n    border-color: grey;\\n    padding: 0 .5em 0 .5em; }\\n\\ntable.docutils td {\\n    border-style: none;\\n    border-width: 1px;\\n    border-color: grey; \\n    padding: 0 .5em 0 .5em; }\\n\\ntable.footnote td { padding: 0; }\\ntable.footnote { border-width: 0; }\\ntable.footnote td { border-width: 0; }\\ntable.footnote th { border-width: 0; }\\n\\ntable.noborder { border-width: 0; }\\n\\ntable.example pre { margin-top: 4px; margin-bottom: 0; }\\n\\n/* For figures & tables */\\np.caption { margin-bottom: 0; }\\ndiv.figure { text-align: center; }\\n\\n/* The index */\\ndiv.index { border: 1px solid black;\\n            background-color: #eeeeee; }\\ndiv.index h1 { padding-left: 0.5em; margin-top: 0.5ex;\\n               border-bottom: 1px solid black; }\\nul.index { margin-left: 0.5em; padding-left: 0; }\\nli.index { list-style-type: none; }\\np.index-heading { font-size: 120%; font-style: italic; margin: 0; }\\nli.index ul { margin-left: 2em; padding-left: 0; }\\n\\n/* \\'Note\\' callouts */\\ndiv.note\\n{\\n  border-right:   #87ceeb 1px solid;\\n  padding-right: 4px;\\n  border-top: #87ceeb 1px solid;\\n  padding-left: 4px;\\n  padding-bottom: 4px;\\n  margin: 2px 5% 10px;\\n  border-left: #87ceeb 1px solid;\\n  padding-top: 4px;\\n  border-bottom: #87ceeb 1px solid;\\n  font-style: normal;\\n  font-family: verdana, arial;\\n  background-color: #b0c4de;\\n}\\n\\ntable.avm { border: 0px solid black; width: 0; }\\ntable.avm tbody tr {border: 0px solid black; }\\ntable.avm tbody tr td { padding: 2px; }\\ntable.avm tbody tr td.avm-key { padding: 5px; font-variant: small-caps; }\\ntable.avm tbody tr td.avm-eq { padding: 5px; }\\ntable.avm tbody tr td.avm-val { padding: 5px; font-style: italic; }\\np.avm-empty { font-style: normal; }\\ntable.avm colgroup col { border: 0px solid black; }\\ntable.avm tbody tr td.avm-topleft \\n    { border-left: 2px solid #000080; border-top: 2px solid #000080; }\\ntable.avm tbody tr td.avm-botleft \\n    { border-left: 2px solid #000080; border-bottom: 2px solid #000080; }\\ntable.avm tbody tr td.avm-topright\\n    { border-right: 2px solid #000080; border-top: 2px solid #000080; }\\ntable.avm tbody tr td.avm-botright\\n    { border-right: 2px solid #000080; border-bottom: 2px solid #000080; }\\ntable.avm tbody tr td.avm-left\\n    { border-left: 2px solid #000080; }\\ntable.avm tbody tr td.avm-right\\n    { border-right: 2px solid #000080; }\\ntable.avm tbody tr td.avm-topbotleft\\n    { border: 2px solid #000080; border-right: 0px solid black; }\\ntable.avm tbody tr td.avm-topbotright\\n    { border: 2px solid #000080; border-left: 0px solid black; }\\ntable.avm tbody tr td.avm-ident\\n    { font-size: 80%; padding: 0; padding-left: 2px; vertical-align: top; }\\n.avm-pointer\\n{ border: 1px solid #008000; padding: 1px; color: #008000; \\n  background: #c0ffc0; font-style: normal; }\\n\\ntable.gloss { border: 0px solid black; width: 0; }\\ntable.gloss tbody tr { border: 0px solid black; }\\ntable.gloss tbody tr td { border: 0px solid black; }\\ntable.gloss colgroup col { border: 0px solid black; }\\ntable.gloss p { margin: 0; padding: 0; }\\n\\ntable.rst-example { border: 1px solid black; }\\ntable.rst-example tbody tr td { background: #eeeeee; }\\ntable.rst-example thead tr th { background: #c0ffff; }\\ntd.rst-raw { width: 0; }\\n\\n/* Used by nltk.org/doc/test: */\\ndiv.doctest-list { text-align: center; }\\ntable.doctest-list { border: 1px solid black;\\n  margin-left: auto; margin-right: auto;\\n}\\ntable.doctest-list tbody tr td { background: #eeeeee;\\n  border: 1px solid #cccccc; text-align: left; }\\ntable.doctest-list thead tr th { background: #304050; color: #ffffff;\\n  border: 1px solid #000000;}\\ntable.doctest-list thead tr a { color: #ffffff; }\\nspan.doctest-passed { color: #008000; }\\nspan.doctest-failed { color: #800000; }\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n<!-- Grammatical Category - e.g. NP and verb as technical terms\\n.. role:: gc\\n   :class: category -->\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n<!-- standard global imports\\n\\n>>> import nltk, re, pprint\\n>>> from nltk import word_tokenize -->\\n\\n\\n\\n\\n<!-- TODO: general technique for computing transitive closures, e.g.\\nadjectives connected by synonymy to a particular word, cf.\\nhttp://www.aclweb.org/anthology/W04-3253 -->\\n\\n\\n\\n\\n\\n\\n4&nbsp;&nbsp;&nbsp;Writing Structured Programs\\nBy now you will have a sense of the capabilities of the Python programming language\\nfor processing natural language.  However, if you\\'re new to Python or to programming, you may\\nstill be wrestling with Python and not feel like you are in full control yet.  In this chapter we\\'ll\\naddress the following questions:\\n\\nHow can you write well-structured, readable programs that you and others will be able to re-use easily?\\nHow do the fundamental building blocks work, such as loops, functions and assignment?\\nWhat are some of the pitfalls with Python programming and how can you avoid them?\\n\\nAlong the way, you will consolidate your knowledge of fundamental programming\\nconstructs, learn more about using features of the Python language in a natural\\nand concise way, and learn some useful techniques in visualizing natural language data.\\nAs before, this chapter contains many examples and\\nexercises (and as before, some exercises introduce new material).\\nReaders new to programming should work through them carefully\\nand consult other introductions to programming if necessary;\\nexperienced programmers can quickly skim this chapter.\\nIn the other chapters of this book, we have organized the programming\\nconcepts as dictated by the needs of NLP.  Here we revert to a more\\nconventional approach where the material is more closely tied to\\nthe structure of the programming language.  There\\'s not room for\\na complete presentation of the language, so we\\'ll just focus on\\nthe language constructs and idioms that are most important for NLP.\\n\\n4.1&nbsp;&nbsp;&nbsp;Back to the Basics\\n\\nAssignment\\nAssignment would seem to be the most elementary programming concept, not\\ndeserving a separate discussion.  However, there are some surprising subtleties\\nhere.  Consider the following code fragment:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; foo = \\'Monty\\'\\n&gt;&gt;&gt; bar = foo \\n&gt;&gt;&gt; foo = \\'Python\\' \\n&gt;&gt;&gt; bar\\n\\'Monty\\'\\n\\n\\n\\nThis behaves exactly as expected.  When we write bar = foo in the above\\ncode ,\\nthe value of foo (the string \\'Monty\\') is assigned to bar.\\nThat is, bar is a copy of foo, so when we overwrite\\nfoo with a new string \\'Python\\' on line , the value\\nof bar is not affected.\\nHowever, assignment statements do not always involve making copies in this way.\\nAssignment always copies the value of an expression, but a value is not\\nalways what you might expect it to be.  In particular,\\nthe \"value\" of a structured object such as a list is actually just a\\nreference to the object.  In the following example,\\n assigns the reference of foo to the new variable bar.\\nNow when we modify something inside foo on line , we can see\\nthat the contents of bar have also been changed.\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; foo = [\\'Monty\\', \\'Python\\']\\n&gt;&gt;&gt; bar = foo \\n&gt;&gt;&gt; foo[1] = \\'Bodkin\\' \\n&gt;&gt;&gt; bar\\n[\\'Monty\\', \\'Bodkin\\']\\n\\n\\n\\n\\n\\nFigure 4.1: List Assignment and Computer Memory: Two list objects foo and bar reference\\nthe same location in the computer\\'s memory; updating foo will also modify bar,\\nand vice versa.\\n\\nThe line bar = foo  does not copy the contents of the\\nvariable, only its \"object reference\".\\nTo understand what is going on here, we need to\\nknow how lists are stored in the computer\\'s memory.\\nIn 4.1, we see that a list foo is\\na reference to an object stored at location 3133 (which is\\nitself a series of pointers to other locations holding strings).\\nWhen we assign bar = foo, it is just the object reference\\n3133 that gets copied.\\nThis behavior extends to other aspects of the language, such as\\nparameter passing (4.4).\\n\\nLet\\'s experiment some more, by creating a variable empty holding the\\nempty list, then using it three times on the next line.\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; empty = []\\n&gt;&gt;&gt; nested = [empty, empty, empty]\\n&gt;&gt;&gt; nested\\n[[], [], []]\\n&gt;&gt;&gt; nested[1].append(\\'Python\\')\\n&gt;&gt;&gt; nested\\n[[\\'Python\\'], [\\'Python\\'], [\\'Python\\']]\\n\\n\\n\\nObserve that changing one of the items inside our nested list of lists changed them all.\\nThis is because each of the three elements is actually just a reference to one and the\\nsame list in memory.\\n\\nNote\\nYour Turn:\\nUse multiplication to create a list of lists: nested = [[]] * 3.\\nNow modify one of the elements of the list, and observe that all the\\nelements are changed.  Use Python\\'s id() function to find out\\nthe numerical identifier for any object, and verify that\\nid(nested[0]), id(nested[1]), and id(nested[2]) are\\nall the same.\\n\\nNow, notice that when we assign a new value to one of the elements of the list,\\nit does not propagate to the others:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; nested = [[]] * 3\\n&gt;&gt;&gt; nested[1].append(\\'Python\\')\\n&gt;&gt;&gt; nested[1] = [\\'Monty\\']\\n&gt;&gt;&gt; nested\\n[[\\'Python\\'], [\\'Monty\\'], [\\'Python\\']]\\n\\n\\n\\n\\n\\n\\n\\nWe began with a list containing three references to a single empty list object.  Then we\\nmodified that object by appending \\'Python\\' to it, resulting in a list containing\\nthree references to a single list object [\\'Python\\'].\\nNext, we overwrote one of those references with a reference to a new object [\\'Monty\\'].\\nThis last step modified one of the three object references inside the nested list.\\nHowever, the [\\'Python\\'] object wasn\\'t changed, and is still referenced from two places in\\nour nested list of lists.  It is crucial to appreciate this difference between\\nmodifying an object via an object reference, and overwriting an object reference.\\n\\nNote\\nImportant:\\nTo copy the items from a list foo to a new list bar, you can write\\nbar = foo[:].  This copies the object references inside the list.\\nTo copy a structure without copying any object references, use copy.deepcopy().\\n\\n\\n\\nEquality\\nPython provides two ways to check that a pair of items are the same.\\nThe is operator tests for object identity.  We can use it to\\nverify our earlier observations about objects.  First we create\\na list containing several copies of the same object, and demonstrate\\nthat they are not only identical according to ==, but also\\nthat they are one and the same object:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; size = 5\\n&gt;&gt;&gt; python = [\\'Python\\']\\n&gt;&gt;&gt; snake_nest = [python] * size\\n&gt;&gt;&gt; snake_nest[0] == snake_nest[1] == snake_nest[2] == snake_nest[3] == snake_nest[4]\\nTrue\\n&gt;&gt;&gt; snake_nest[0] is snake_nest[1] is snake_nest[2] is snake_nest[3] is snake_nest[4]\\nTrue\\n\\n\\n\\nNow let\\'s put a new python in this nest.  We can easily show that the objects are not\\nall identical:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; import random\\n&gt;&gt;&gt; position = random.choice(range(size))\\n&gt;&gt;&gt; snake_nest[position] = [\\'Python\\']\\n&gt;&gt;&gt; snake_nest\\n[[\\'Python\\'], [\\'Python\\'], [\\'Python\\'], [\\'Python\\'], [\\'Python\\']]\\n&gt;&gt;&gt; snake_nest[0] == snake_nest[1] == snake_nest[2] == snake_nest[3] == snake_nest[4]\\nTrue\\n&gt;&gt;&gt; snake_nest[0] is snake_nest[1] is snake_nest[2] is snake_nest[3] is snake_nest[4]\\nFalse\\n\\n\\n\\nYou can do several pairwise tests to discover which position contains the interloper,\\nbut the id() function makes detection easier:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; [id(snake) for snake in snake_nest]\\n[4557855488, 4557854763, 4557855488, 4557855488, 4557855488]\\n\\n\\n\\nThis reveals that the second item of the list has a distinct identifier.  If you try\\nrunning this code snippet yourself, expect to see different numbers in the resulting list,\\nand also the interloper may be in a different position.\\n\\n\\nHaving two kinds of equality might seem strange.  However, it\\'s really just the\\ntype-token distinction, familiar from natural language, here showing up in\\na programming language.\\n\\n\\nConditionals\\nIn the condition part of an if statement, a\\nnonempty string or list is evaluated as true, while an empty string or\\nlist evaluates as false.\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; mixed = [\\'cat\\', \\'\\', [\\'dog\\'], []]\\n&gt;&gt;&gt; for element in mixed:\\n...     if element:\\n...         print(element)\\n...\\ncat\\n[\\'dog\\']\\n\\n\\n\\nThat is, we don\\'t need to say if len(element) &gt; 0: in the\\ncondition.\\nWhat\\'s the difference between using if...elif as opposed to using\\na couple of if statements in a row? Well, consider the following\\nsituation:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; animals = [\\'cat\\', \\'dog\\']\\n&gt;&gt;&gt; if \\'cat\\' in animals:\\n...     print(1)\\n... elif \\'dog\\' in animals:\\n...     print(2)\\n...\\n1\\n\\n\\n\\nSince the if clause of the statement is satisfied, Python never\\ntries to evaluate the elif clause, so we never get to print out\\n2. By contrast, if we replaced the elif by an if, then we\\nwould print out both 1 and 2. So an elif clause\\npotentially gives us more information than a bare if clause; when\\nit evaluates to true, it tells us not only that the condition is\\nsatisfied, but also that the condition of the main if clause was\\nnot satisfied.\\n\\n>> def cond1(l):\\n...    if \\'cat\\' in l:\\n...        print(1)\\n...    elif \\'dog\\' in l:\\n...        print(2)\\n\\n>>> def cond2(l):\\n...    if \\'cat\\' in l:\\n...        print(1)\\n...    if \\'dog\\' in l:\\n...        print(2)\\n\\n>>> animals = [\\'cat\\', \\'dog\\']\\n>>> cond1(animals)\\n1\\n>>> cond2(animals)\\n1\\n2 -->\\nThe functions all() and any() can be applied to a list (or other sequence) to\\ncheck whether all or any items meet some condition:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; sent = [\\'No\\', \\'good\\', \\'fish\\', \\'goes\\', \\'anywhere\\', \\'without\\', \\'a\\', \\'porpoise\\', \\'.\\']\\n&gt;&gt;&gt; all(len(w) &gt; 4 for w in sent)\\nFalse\\n&gt;&gt;&gt; any(len(w) &gt; 4 for w in sent)\\nTrue\\n\\n\\n\\n\\n\\n\\n4.2&nbsp;&nbsp;&nbsp;Sequences\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSo far, we have seen two kinds of sequence object: strings and lists.  Another\\nkind of sequence is called a tuple.\\nTuples are formed with the comma operator , and typically enclosed\\nusing parentheses.  We\\'ve actually seen them in the\\nprevious chapters, and sometimes referred to them as \"pairs\", since\\nthere were always two members.  However, tuples can have any number\\nof members.  Like lists and strings, tuples can be indexed \\nand sliced , and have a length .\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; t = \\'walk\\', \\'fem\\', 3 \\n&gt;&gt;&gt; t\\n(\\'walk\\', \\'fem\\', 3)\\n&gt;&gt;&gt; t[0] \\n\\'walk\\'\\n&gt;&gt;&gt; t[1:] \\n(\\'fem\\', 3)\\n&gt;&gt;&gt; len(t) \\n3\\n\\n\\n\\n\\nCaution!\\nTuples are constructed using the comma operator.  Parentheses are a more\\ngeneral feature of Python syntax, designed for grouping.\\nA tuple containing the single element \\'snark\\' is defined by adding a\\ntrailing comma, like this: \"\\'snark\\',\".  The empty tuple is a special\\ncase, and is defined using empty parentheses ().\\n\\n\\n>> type((\\'snark\\')) -->\\n -->\\n>> type((\\'snark\\',)) -->\\n -->\\n\\n\\n\\nLet\\'s compare strings, lists and tuples directly, and do the indexing, slice, and length\\noperation on each type:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; raw = \\'I turned off the spectroroute\\'\\n&gt;&gt;&gt; text = [\\'I\\', \\'turned\\', \\'off\\', \\'the\\', \\'spectroroute\\']\\n&gt;&gt;&gt; pair = (6, \\'turned\\')\\n&gt;&gt;&gt; raw[2], text[3], pair[1]\\n(\\'t\\', \\'the\\', \\'turned\\')\\n&gt;&gt;&gt; raw[-3:], text[-3:], pair[-3:]\\n(\\'ute\\', [\\'off\\', \\'the\\', \\'spectroroute\\'], (6, \\'turned\\'))\\n&gt;&gt;&gt; len(raw), len(text), len(pair)\\n(29, 5, 2)\\n\\n\\n\\nNotice in this code sample that we computed multiple values on a\\nsingle line, separated by commas.  These comma-separated expressions\\nare actually just tuples — Python allows us to omit the\\nparentheses around tuples if there is no ambiguity. When we print a\\ntuple, the parentheses are always displayed. By using tuples in this\\nway, we are implicitly aggregating items together.\\n\\nOperating on Sequence Types\\nWe can iterate over the items in a sequence s in a variety of useful ways,\\nas shown in 4.1.\\nTable 4.1: Various ways to iterate over sequences\\n\\n\\n\\n\\n\\nPython Expression\\nComment\\n\\n\\n\\nfor item in s\\niterate over the items of s\\n\\nfor item in sorted(s)\\niterate over the items of s in order\\n\\nfor item in set(s)\\niterate over unique elements of s\\n\\nfor item in reversed(s)\\niterate over elements of s in reverse\\n\\nfor item in set(s).difference(t)\\niterate over elements of s not in t\\n\\n\\n\\n\\n\\nThe sequence functions illustrated in 4.1 can be combined\\nin various ways; for example, to get unique elements of s sorted\\nin reverse, use reversed(sorted(set(s))).\\nWe can randomize the contents of a list s before iterating over\\nthem, using random.shuffle(s).\\nWe can convert between these sequence types.  For example,\\ntuple(s) converts any kind of sequence into a tuple, and\\nlist(s) converts any kind of sequence into a list.\\nWe can convert a list of strings to a single string using the\\njoin() function, e.g. \\':\\'.join(words).\\nSome other objects, such as a FreqDist, can be converted into a\\nsequence (using list() or sorted()) and support iteration, e.g.\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; raw = \\'Red lorry, yellow lorry, red lorry, yellow lorry.\\'\\n&gt;&gt;&gt; text = word_tokenize(raw)\\n&gt;&gt;&gt; fdist = nltk.FreqDist(text)\\n&gt;&gt;&gt; sorted(fdist)\\n[\\',\\', \\'.\\', \\'Red\\', \\'lorry\\', \\'red\\', \\'yellow\\']\\n&gt;&gt;&gt; for key in fdist:\\n...     print(key + \\':\\', fdist[key], end=\\'; \\')\\n...\\nlorry: 4; red: 1; .: 1; ,: 3; Red: 1; yellow: 2\\n\\n\\n\\nIn the next example, we use tuples to re-arrange the\\ncontents of our list.  (We can omit the parentheses\\nbecause the comma has higher precedence than assignment.)\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; words = [\\'I\\', \\'turned\\', \\'off\\', \\'the\\', \\'spectroroute\\']\\n&gt;&gt;&gt; words[2], words[3], words[4] = words[3], words[4], words[2]\\n&gt;&gt;&gt; words\\n[\\'I\\', \\'turned\\', \\'the\\', \\'spectroroute\\', \\'off\\']\\n\\n\\n\\nThis is an idiomatic and readable way to move items inside a list.\\nIt is equivalent to the following traditional way of doing such\\ntasks that does not use tuples (notice that this method needs a\\ntemporary variable tmp).\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; tmp = words[2]\\n&gt;&gt;&gt; words[2] = words[3]\\n&gt;&gt;&gt; words[3] = words[4]\\n&gt;&gt;&gt; words[4] = tmp\\n\\n\\n\\nAs we have seen, Python has sequence functions such as sorted() and reversed()\\nthat rearrange the items of a sequence.  There are also functions that\\nmodify the structure of a sequence and which can be handy for\\nlanguage processing.  Thus, zip() takes\\nthe items of two or more sequences and \"zips\" them together into a single list of tuples.\\nGiven a sequence s, enumerate(s) returns pairs consisting of\\nan index and the item at that index.\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; words = [\\'I\\', \\'turned\\', \\'off\\', \\'the\\', \\'spectroroute\\']\\n&gt;&gt;&gt; tags = [\\'noun\\', \\'verb\\', \\'prep\\', \\'det\\', \\'noun\\']\\n&gt;&gt;&gt; zip(words, tags)\\n&lt;zip object at ...&gt;\\n&gt;&gt;&gt; list(zip(words, tags))\\n[(\\'I\\', \\'noun\\'), (\\'turned\\', \\'verb\\'), (\\'off\\', \\'prep\\'),\\n(\\'the\\', \\'det\\'), (\\'spectroroute\\', \\'noun\\')]\\n&gt;&gt;&gt; list(enumerate(words))\\n[(0, \\'I\\'), (1, \\'turned\\'), (2, \\'off\\'), (3, \\'the\\'), (4, \\'spectroroute\\')]\\n\\n\\n\\n\\nNote\\nIt is a widespread feature of Python 3 and NLTK 3 to only perform\\ncomputation when required (a feature known as \"lazy evaluation\").\\nIf you ever see a result like &lt;zip object at 0x10d005448&gt; when\\nyou expect to see a sequence, you can force the object to be\\nevaluated just by putting it in a context that expects a sequence,\\nlike list(x), or for item in x.\\n\\nFor some NLP tasks it is necessary to cut up a sequence into two or more parts.\\nFor instance, we might want to \"train\" a system on 90% of the data and test it\\non the remaining 10%.  To do this we decide the location where we want to\\ncut the data , then cut the sequence at that location .\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; text = nltk.corpus.nps_chat.words()\\n&gt;&gt;&gt; cut = int(0.9 * len(text)) \\n&gt;&gt;&gt; training_data, test_data = text[:cut], text[cut:] \\n&gt;&gt;&gt; text == training_data + test_data \\nTrue\\n&gt;&gt;&gt; len(training_data) / len(test_data) \\n9.0\\n\\n\\n\\nWe can verify that none of the original data is lost during this process, nor is it duplicated\\n.  We can also verify that the ratio of the sizes of the two pieces is what\\nwe intended .\\n\\n\\nCombining Different Sequence Types\\nLet\\'s combine our knowledge of these three sequence types, together with list\\ncomprehensions, to perform the task of sorting the words in a string by\\ntheir length.\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; words = \\'I turned off the spectroroute\\'.split() \\n&gt;&gt;&gt; wordlens = [(len(word), word) for word in words] \\n&gt;&gt;&gt; wordlens.sort() \\n&gt;&gt;&gt; \\' \\'.join(w for (_, w) in wordlens) \\n\\'I off the turned spectroroute\\'\\n\\n\\n\\n\\nEach of the above lines of code contains a significant feature.\\nA simple string is actually an object with methods defined on it such as split() .\\nWe use a list comprehension to build a list of tuples ,\\nwhere each tuple consists of a number (the word length) and the\\nword, e.g. (3, \\'the\\').  We use the sort() method \\nto sort the list in-place.  Finally, we discard the length\\ninformation and join the words back into a single string .\\n(The underscore  is just a regular Python variable,\\nbut we can use underscore by convention to indicate that we will\\nnot use its value.)\\nWe began by talking about the commonalities in these sequence types,\\nbut the above code illustrates important differences in their\\nroles.  First, strings appear at the beginning and the end: this is\\ntypical in the context where our program is reading in some text and\\nproducing output for us to read.  Lists and tuples are used in the\\nmiddle, but for different purposes.  A list is typically a sequence of\\nobjects all having the same type, of arbitrary length.  We often\\nuse lists to hold sequences of words.  In contrast,\\na tuple is typically a collection of objects of different types, of\\nfixed length.  We often use a tuple to hold a record,\\na collection of different fields relating to some entity.\\nThis distinction between the use of lists and tuples takes some\\ngetting used to,\\nso here is another example:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; lexicon = [\\n...     (\\'the\\', \\'det\\', [\\'Di:\\', \\'D@\\']),\\n...     (\\'off\\', \\'prep\\', [\\'Qf\\', \\'O:f\\'])\\n... ]\\n\\n\\n\\nHere, a lexicon is represented as a list because it is a\\ncollection of objects of a single type — lexical entries —\\nof no predetermined length.  An individual entry is represented as a\\ntuple because it is a collection of objects with different\\ninterpretations, such as the orthographic form, the part of speech,\\nand the pronunciations (represented in the SAMPA computer-readable\\nphonetic alphabet http://www.phon.ucl.ac.uk/home/sampa/).\\nNote that these pronunciations are stored using a list. (Why?)\\n\\nNote\\nA good way to decide when to use tuples vs lists is to ask whether\\nthe interpretation of an item depends on its position.  For example,\\na tagged token combines two strings having different interpretation,\\nand we choose to interpret the first item as the token and the\\nsecond item as the tag.  Thus we use tuples like this: (\\'grail\\', \\'noun\\');\\na tuple of the form (\\'noun\\', \\'grail\\') would be nonsensical since\\nit would be a word noun tagged grail.\\nIn contrast, the elements of a text are all tokens, and position is\\nnot significant.  Thus we use lists like this: [\\'venetian\\', \\'blind\\'];\\na list of the form [\\'blind\\', \\'venetian\\'] would be equally valid.\\nThe linguistic meaning of the words might be different, but the\\ninterpretation of list items as tokens is unchanged.\\n\\nThe distinction between lists and tuples has been described in terms of\\nusage.  However, there is a more fundamental difference: in Python,\\nlists are mutable, while tuples are immutable.  In other\\nwords, lists can be modified, while tuples cannot.  Here are some of\\nthe operations on lists that do in-place modification of the list.\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; lexicon.sort()\\n&gt;&gt;&gt; lexicon[1] = (\\'turned\\', \\'VBD\\', [\\'t3:nd\\', \\'t3`nd\\'])\\n&gt;&gt;&gt; del lexicon[0]\\n\\n\\n\\n\\nNote\\nYour Turn:\\nConvert lexicon to a tuple, using lexicon = tuple(lexicon),\\nthen try each of the above operations, to confirm that none of\\nthem is permitted on tuples.\\n\\n\\n\\nGenerator Expressions\\nWe\\'ve been making heavy use of list comprehensions, for compact and readable\\nprocessing of texts.  Here\\'s an example where we tokenize and normalize a text:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; text = \\'\\'\\'\"When I use a word,\" Humpty Dumpty said in rather a scornful tone,\\n... \"it means just what I choose it to mean - neither more nor less.\"\\'\\'\\'\\n&gt;&gt;&gt; [w.lower() for w in word_tokenize(text)]\\n[\\'``\\', \\'when\\', \\'i\\', \\'use\\', \\'a\\', \\'word\\', \\',\\', \"\\'\\'\", \\'humpty\\', \\'dumpty\\', \\'said\\', ...]\\n\\n\\n\\nSuppose we now want to process these words further.  We can do this by inserting the above\\nexpression inside a call to some other function ,\\nbut Python allows us to omit the brackets .\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; max([w.lower() for w in word_tokenize(text)]) \\n\\'word\\'\\n&gt;&gt;&gt; max(w.lower() for w in word_tokenize(text)) \\n\\'word\\'\\n\\n\\n\\nThe second line uses a generator expression.  This is more than a notational convenience:\\nin many language processing situations, generator expressions will be more efficient.\\nIn , storage for the list object must be allocated\\nbefore the value of max() is computed.  If the text is\\nvery large, this could be slow.  In , the data is streamed to the calling\\nfunction.  Since the calling function simply has to find the maximum value — the\\nword which comes latest in lexicographic sort order — it can process the stream\\nof data without having to store anything more than the maximum value seen so far.\\n\\n\\n\\n4.3&nbsp;&nbsp;&nbsp;Questions of Style\\nProgramming is as much an art as a science.  The undisputed \"bible\" of programming,\\na 2,500 page multi-volume work by Donald Knuth, is called\\nThe Art of Computer Programming.  Many books have been written on\\nLiterate Programming, recognizing that humans, not just computers,\\nmust read and understand programs.  Here we pick up on some issues of\\nprogramming style that have important ramifications for the readability\\nof your code, including code layout, procedural vs declarative style,\\nand the use of loop variables.\\n\\nPython Coding Style\\nWhen writing programs you make many subtle choices about names,\\nspacing, comments, and so on.  When you look at code written by\\nother people, needless differences in style make it harder\\nto interpret the code.  Therefore, the designers of the Python\\nlanguage have published a style guide for Python code, available\\nat http://www.python.org/dev/peps/pep-0008/.\\nThe underlying value presented in the style guide is consistency,\\nfor the purpose of maximizing the readability of code.\\nWe briefly review some of its key recommendations here, and refer\\nreaders to the full guide for detailed discussion with examples.\\n\\nCode layout should use four spaces per indentation level.  You should make sure that\\nwhen you write Python code in a file, you\\navoid tabs for indentation, since these can be misinterpreted by\\ndifferent text editors and the indentation can be messed up.\\nLines should be less than 80 characters long; if necessary you can\\nbreak a line inside parentheses, brackets, or braces, because\\nPython is able to detect that the line continues over to the next line.\\nIf you need to break a line outside parentheses, brackets, or braces,\\nyou can often add extra parentheses, and you can always add a backslash at\\nthe end of the line that is broken:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; if (len(syllables) &gt; 4 and len(syllables[2]) == 3 and\\n...    syllables[2][2] in [aeiou] and syllables[2][3] == syllables[1][3]):\\n...     process(syllables)\\n&gt;&gt;&gt; if len(syllables) &gt; 4 and len(syllables[2]) == 3 and \\\\\\n...    syllables[2][2] in [aeiou] and syllables[2][3] == syllables[1][3]:\\n...     process(syllables)\\n\\n\\n\\n\\nNote\\nTyping spaces instead of tabs soon becomes a chore.  Many programming\\neditors have built-in support for Python, and can automatically indent\\ncode and highlight any syntax errors (including indentation errors).\\nFor a list of Python-aware editors, please see\\nhttp://wiki.python.org/moin/PythonEditors.\\n\\n\\n\\nProcedural vs Declarative Style\\n\\n\\n\\n\\n\\n\\nWe have just seen how the same task can be performed in different\\nways, with implications for efficiency.  Another factor influencing\\nprogram development is programming style.  Consider the following\\nprogram to compute the average length of words in the Brown Corpus:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; tokens = nltk.corpus.brown.words(categories=\\'news\\')\\n&gt;&gt;&gt; count = 0\\n&gt;&gt;&gt; total = 0\\n&gt;&gt;&gt; for token in tokens:\\n...     count += 1\\n...     total += len(token)\\n&gt;&gt;&gt; total / count\\n4.401545438271973\\n\\n\\n\\n\\nIn this program we use the variable count to keep track of the\\nnumber of tokens seen, and total to store the combined length of\\nall words.  This is a low-level style, not far removed from machine\\ncode, the primitive operations performed by the computer\\'s CPU.\\nThe two variables are just like a CPU\\'s registers, accumulating values\\nat many intermediate stages, values that are meaningless until the end.\\nWe say that this program is written in a procedural style, dictating\\nthe machine operations step by step.  Now consider the following\\nprogram that computes the same thing:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; total = sum(len(t) for t in tokens)\\n&gt;&gt;&gt; print(total / len(tokens))\\n4.401...\\n\\n\\n\\n<!-- Monkey patching so that the next example doesn\\'t take forever\\n>>> tokens = \"the cat sat on the mat\".split() -->\\nThe first line uses a generator expression to sum the token lengths,\\nwhile the second line computes the average as before.\\nEach line of code performs a complete, meaningful task, which\\ncan be understood in terms of high-level properties like:\\n\"total is the sum of the lengths of the tokens\".\\nImplementation details are left to the Python interpreter.\\nThe second program uses a built-in function, and constitutes\\nprogramming at a more abstract level; the resulting code is\\nmore declarative.  Let\\'s look at an extreme example:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; word_list = []\\n&gt;&gt;&gt; i = 0\\n&gt;&gt;&gt; while i &lt; len(tokens):\\n...     j = 0\\n...     while j &lt; len(word_list) and word_list[j] &lt;= tokens[i]:\\n...         j += 1\\n...     if j == 0 or tokens[i] != word_list[j-1]:\\n...         word_list.insert(j, tokens[i])\\n...     i += 1\\n...\\n\\n\\n\\nThe equivalent declarative version uses familiar built-in functions,\\nand its purpose is instantly recognizable:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; word_list = sorted(set(tokens))\\n\\n\\n\\n\\nAnother case where a loop variable seems to be necessary is for printing\\na counter with each line of output.  Instead, we can use enumerate(), which\\nprocesses a sequence s and produces a tuple of the form (i, s[i]) for each\\nitem in s, starting with (0, s[0]).  Here we enumerate the key-value pairs of the\\nfrequency distribution, resulting in nested tuples (rank, (word, count)).\\nWe print rank+1 so that the counting appears to start from 1,\\nas required when producing a list of ranked items.\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; fd = nltk.FreqDist(nltk.corpus.brown.words())\\n&gt;&gt;&gt; cumulative = 0.0\\n&gt;&gt;&gt; most_common_words = [word for (word, count) in fd.most_common()]\\n&gt;&gt;&gt; for rank, word in enumerate(most_common_words):\\n...     cumulative += fd.freq(word)\\n...     print(\"%3d %6.2f%% %s\" % (rank + 1, cumulative * 100, word))\\n...     if cumulative &gt; 0.25:\\n...         break\\n...\\n  1   5.40% the\\n  2  10.42% ,\\n  3  14.67% .\\n  4  17.78% of\\n  5  20.19% and\\n  6  22.40% to\\n  7  24.29% a\\n  8  25.97% in\\n\\n\\n\\nIt\\'s sometimes tempting to use loop variables to store a maximum or minimum value\\nseen so far.  Let\\'s use this method to find the longest word in a text.\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; text = nltk.corpus.gutenberg.words(\\'milton-paradise.txt\\')\\n&gt;&gt;&gt; longest = \\'\\'\\n&gt;&gt;&gt; for word in text:\\n...     if len(word) &gt; len(longest):\\n...         longest = word\\n&gt;&gt;&gt; longest\\n\\'unextinguishable\\'\\n\\n\\n\\nHowever, a more transparent solution uses two list comprehensions,\\nboth having forms that should be familiar by now:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; maxlen = max(len(word) for word in text)\\n&gt;&gt;&gt; [word for word in text if len(word) == maxlen]\\n[\\'unextinguishable\\', \\'transubstantiate\\', \\'inextinguishable\\', \\'incomprehensible\\']\\n\\n\\n\\nNote that our first solution found the first word having the longest length, while the\\nsecond solution found all of the longest words (which is usually what we would want).\\nAlthough there\\'s a theoretical efficiency difference between the two solutions,\\nthe main overhead is reading the data into main memory; once it\\'s there, a second pass\\nthrough the data is effectively instantaneous.  We also need to balance our concerns about\\nprogram efficiency with programmer efficiency.  A fast but cryptic solution\\nwill be harder to understand and maintain.\\n\\n\\nSome Legitimate Uses for Counters\\n\\n\\nThere are cases where we still want to use loop variables in a list comprehension.\\nFor example, we need to use a loop variable to extract successive overlapping n-grams\\nfrom a list:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; sent = [\\'The\\', \\'dog\\', \\'gave\\', \\'John\\', \\'the\\', \\'newspaper\\']\\n&gt;&gt;&gt; n = 3\\n&gt;&gt;&gt; [sent[i:i+n] for i in range(len(sent)-n+1)]\\n[[\\'The\\', \\'dog\\', \\'gave\\'],\\n [\\'dog\\', \\'gave\\', \\'John\\'],\\n [\\'gave\\', \\'John\\', \\'the\\'],\\n [\\'John\\', \\'the\\', \\'newspaper\\']]\\n\\n\\n\\nIt is quite tricky to get the range of the loop variable right.\\nSince this is a common operation in NLP, NLTK\\nsupports it with functions bigrams(text) and trigrams(text), and\\na general purpose ngrams(text, n).\\nHere\\'s an example of how we can use loop variables in\\nbuilding multidimensional structures.\\nFor example, to build an array with m rows and n columns,\\nwhere each cell is a set, we could use a nested list comprehension:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; m, n = 3, 7\\n&gt;&gt;&gt; array = [[set() for i in range(n)] for j in range(m)]\\n&gt;&gt;&gt; array[2][5].add(\\'Alice\\')\\n&gt;&gt;&gt; pprint.pprint(array)\\n[[set(), set(), set(), set(), set(), set(), set()],\\n [set(), set(), set(), set(), set(), set(), set()],\\n [set(), set(), set(), set(), set(), {\\'Alice\\'}, set()]]\\n\\n\\n\\nObserve that the loop variables i and j are not used\\nanywhere in the resulting object, they are just needed for a syntactically\\ncorrect for statement.  As another example of this usage, observe\\nthat the expression [\\'very\\' for i in range(3)] produces a list\\ncontaining three instances of \\'very\\', with no integers in sight.\\nNote that it would be incorrect to do this work using multiplication,\\nfor reasons concerning object copying that were discussed earlier in this section.\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; array = [[set()] * n] * m\\n&gt;&gt;&gt; array[2][5].add(7)\\n&gt;&gt;&gt; pprint.pprint(array)\\n[[{7}, {7}, {7}, {7}, {7}, {7}, {7}],\\n [{7}, {7}, {7}, {7}, {7}, {7}, {7}],\\n [{7}, {7}, {7}, {7}, {7}, {7}, {7}]]\\n\\n\\n\\n\\n\\nIteration is an important programming device.\\nIt is tempting to adopt idioms from other languages.\\nHowever, Python offers some elegant and highly readable alternatives,\\nas we have seen.\\n\\n\\n\\n4.4&nbsp;&nbsp;&nbsp;Functions: The Foundation of Structured Programming\\n\\n\\n\\nFunctions provide an effective way to package and re-use program code,\\nas already explained in 3.\\nFor example, suppose we find that we often want to read text from an HTML file.\\nThis involves several steps: opening the file, reading it in, normalizing\\nwhitespace, and stripping HTML markup.  We can collect these steps into a\\nfunction, and give it a name such as get_text(), as shown in 4.2.\\n\\n\\n\\n\\n&nbsp;\\nimport re\\ndef get_text(file):\\n    \"\"\"Read text from a file, normalizing whitespace and stripping HTML markup.\"\"\"\\n    text = open(file).read()\\n    text = re.sub(r\\'&lt;.*?&gt;\\', \\' \\', text)\\n    text = re.sub(\\'\\\\s+\\', \\' \\', text)\\n    return text\\n\\n\\nExample 4.2 (code_get_text.py): Figure 4.2: Read text from a file\\n\\nNow, any time we want to get cleaned-up text from an HTML file, we can just call\\nget_text() with the name of the file as its only argument.  It will return\\na string, and we can assign this to a variable, e.g.:\\ncontents = get_text(\"test.html\").  Each time we want to use this series of\\nsteps we only have to call the function.\\nUsing functions has the benefit of saving space in our program.  More\\nimportantly, our choice of name for the function helps make the program readable.\\nIn the case of the above example, whenever our program needs to read cleaned-up\\ntext from a file we don\\'t have to clutter the program with four lines of code, we\\nsimply need to call get_text().  This naming helps to provide some \"semantic\\ninterpretation\" — it helps a reader of our program to see what the program \"means\".\\nNotice that the above function definition contains a string.  The first string inside\\na function definition is called a docstring.  Not only does it document the\\npurpose of the function to someone reading the code, it is accessible to a programmer\\nwho has loaded the code from a file:\\n|   &gt;&gt;&gt; help(get_text)\\n|   Help on function get_text in module __main__:\\n|\\n|   get_text(file)\\n|       Read text from a file, normalizing whitespace and stripping HTML markup.\\n\\nWe have seen that functions help to make our work reusable and readable.  They\\nalso help make it reliable.  When we re-use code that has already been developed\\nand tested, we can be more confident that it handles a variety of cases correctly.\\nWe also remove the risk that we forget some important step, or introduce a bug.\\nThe program that calls our function also has increased reliability.  The author\\nof that program is dealing with a shorter program, and its components behave\\ntransparently.\\nTo summarize, as its name suggests, a function captures functionality.\\nIt is a segment of code that can be given a meaningful name and which performs\\na well-defined task.  Functions allow us to abstract away from the details,\\nto see a bigger picture, and to program more effectively.\\nThe rest of this section takes a closer look at functions, exploring the\\nmechanics and discussing ways to make your programs easier to read.\\n\\nFunction Inputs and Outputs\\nWe pass information to functions using a function\\'s parameters,\\nthe parenthesized list of variables and constants following\\nthe function\\'s name in the function definition.  Here\\'s a complete example:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; def repeat(msg, num):  \\n...     return \\' \\'.join([msg] * num)\\n&gt;&gt;&gt; monty = \\'Monty Python\\'\\n&gt;&gt;&gt; repeat(monty, 3) \\n\\'Monty Python Monty Python Monty Python\\'\\n\\n\\n\\nWe first define the function to take two parameters, msg and num\\n. Then we call the function and pass it two arguments, monty and 3\\n; these arguments fill the \"placeholders\" provided by the parameters and\\nprovide values for the occurrences of msg and num in the function body.\\nIt is not necessary to have any parameters, as we see in the following example:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; def monty():\\n...     return \"Monty Python\"\\n&gt;&gt;&gt; monty()\\n\\'Monty Python\\'\\n\\n\\n\\nA function usually communicates its results back to the calling program via the return statement,\\nas we have just seen.  To the calling program, it looks as if the function call had been replaced\\nwith the function\\'s result, e.g.:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; repeat(monty(), 3)\\n\\'Monty Python Monty Python Monty Python\\'\\n&gt;&gt;&gt; repeat(\\'Monty Python\\', 3)\\n\\'Monty Python Monty Python Monty Python\\'\\n\\n\\n\\nA Python function is not required to have a return statement.\\nSome functions do their work as a side effect, printing a result,\\nmodifying a file, or updating the contents of a parameter to the function\\n(such functions are called \"procedures\" in some other programming languages).\\n\\n\\n\\n\\nConsider the following three sort functions.\\nThe third one is dangerous because a programmer could\\nuse it without realizing that it had modified its input.\\nIn general, functions should modify the contents of a parameter\\n(my_sort1()), or return a value (my_sort2()),\\nnot both (my_sort3()).\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; def my_sort1(mylist):      # good: modifies its argument, no return value\\n...     mylist.sort()\\n&gt;&gt;&gt; def my_sort2(mylist):      # good: doesn\\'t touch its argument, returns value\\n...     return sorted(mylist)\\n&gt;&gt;&gt; def my_sort3(mylist):      # bad: modifies its argument and also returns it\\n...     mylist.sort()\\n...     return mylist\\n\\n\\n\\n\\n\\nParameter Passing\\nBack in 4.1 you saw that assignment works on values,\\nbut that the value of a structured object is a reference to that object.  The same\\nis true for functions.  Python interprets function parameters as values (this is\\nknown as call-by-value).  In the following code, set_up() has two parameters,\\nboth of which are modified inside the function.  We begin by assigning an empty string\\nto w and an empty list to p.  After calling the function, w is unchanged,\\nwhile p is changed:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; def set_up(word, properties):\\n...     word = \\'lolcat\\'\\n...     properties.append(\\'noun\\')\\n...     properties = 5\\n...\\n&gt;&gt;&gt; w = \\'\\'\\n&gt;&gt;&gt; p = []\\n&gt;&gt;&gt; set_up(w, p)\\n&gt;&gt;&gt; w\\n\\'\\'\\n&gt;&gt;&gt; p\\n[\\'noun\\']\\n\\n\\n\\nNotice that w was not changed by the function.\\nWhen we called set_up(w, p), the value of w (an empty string) was assigned to\\na new variable word.  Inside the function, the value of word was modified.\\nHowever, that change did not propagate to w.  This parameter passing is\\nidentical to the following sequence of assignments:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; w = \\'\\'\\n&gt;&gt;&gt; word = w\\n&gt;&gt;&gt; word = \\'lolcat\\'\\n&gt;&gt;&gt; w\\n\\'\\'\\n\\n\\n\\nLet\\'s look at what happened with the list p.\\nWhen we called set_up(w, p), the value of p (a reference to an empty\\nlist) was assigned to a new local variable properties,\\nso both variables now reference the same memory location.\\nThe function modifies properties, and this change is also\\nreflected in the value of p as we saw.  The function also\\nassigned a new value to properties (the number 5); this\\ndid not modify the contents at that memory location, but\\ncreated a new local variable.\\nThis behavior is just as if we had done the following sequence of assignments:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; p = []\\n&gt;&gt;&gt; properties = p\\n&gt;&gt;&gt; properties.append(\\'noun\\')\\n&gt;&gt;&gt; properties = 5\\n&gt;&gt;&gt; p\\n[\\'noun\\']\\n\\n\\n\\nThus, to understand Python\\'s call-by-value parameter passing,\\nit is enough to understand how assignment works.  Remember that you\\ncan use the id() function and is operator to check your\\nunderstanding of object identity after each statement.\\n\\n\\nVariable Scope\\nFunction definitions create a new, local scope for variables.\\nWhen you assign to a new variable inside the body of a function,\\nthe name is only defined within that function.  The name is not\\nvisible outside the function, or in other functions.  This behavior\\nmeans you can choose variable names without being concerned about\\ncollisions with names used in your other function definitions.\\nWhen you refer to an existing name from within the body\\nof a function, the Python interpreter first tries to resolve\\nthe name with respect to the names that are local to the function.\\nIf nothing is found, the interpreter checks if it is a global\\nname within the module.  Finally, if that does not succeed, the\\ninterpreter checks if the name is a Python built-in.  This is\\nthe so-called LGB rule of name resolution: local,\\nthen global, then built-in.\\n\\nCaution!\\nA function can enable access to a global variable using the\\nglobal declaration.  However, this practice should be\\navoided as much as possible.  Defining global variables\\ninside a function introduces dependencies on context\\nand limits the portability (or reusability) of the function.\\nIn general you should use parameters for function inputs\\nand return values for function outputs.\\n\\n\\n\\nChecking Parameter Types\\nPython does not allow us to declare the type of a variable when we write a program,\\nand this permits us to define functions that are flexible\\nabout the type of their arguments.  For example, a tagger might expect\\na sequence of words, but it wouldn\\'t care whether this sequence is expressed\\nas a list or a tuple (or an iterator, another sequence type that is\\noutside the scope of the current discussion).\\nHowever, often we want to write programs for later use by others, and want\\nto program in a defensive style, providing useful warnings when functions\\nhave not been invoked correctly.  The author of the following tag()\\nfunction assumed that its argument would always be a string.\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; def tag(word):\\n...     if word in [\\'a\\', \\'the\\', \\'all\\']:\\n...         return \\'det\\'\\n...     else:\\n...         return \\'noun\\'\\n...\\n&gt;&gt;&gt; tag(\\'the\\')\\n\\'det\\'\\n&gt;&gt;&gt; tag(\\'knight\\')\\n\\'noun\\'\\n&gt;&gt;&gt; tag([\"\\'Tis\", \\'but\\', \\'a\\', \\'scratch\\']) \\n\\'noun\\'\\n\\n\\n\\nThe function returns sensible values for the arguments \\'the\\' and \\'knight\\',\\nbut look what happens when it is passed a list  — it fails to\\ncomplain, even though the result which it returns is clearly incorrect.\\nThe author of this function could take some extra steps to\\nensure that the word parameter of the tag() function is a string.\\nA naive approach would be to check the type of the argument using\\nif not type(word) is str, and if word is not a string, to simply\\nreturn Python\\'s special empty value, None. This is a slight improvement, because\\nthe function is checking the type of the argument, and trying to return a \"special\", diagnostic\\nvalue for the wrong input.\\nHowever, it is also dangerous because the calling program\\nmay not detect that None is intended as a \"special\" value, and this diagnostic\\nreturn value may then be\\npropagated to other parts of the program with unpredictable consequences.\\nThis approach also fails if the word is a Unicode string, which has\\ntype unicode, not str.\\nHere\\'s a better solution, using an assert statement together with Python\\'s basestring\\ntype that generalizes over both unicode and str.\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; def tag(word):\\n...     assert isinstance(word, basestring), \"argument to tag() must be a string\"\\n...     if word in [\\'a\\', \\'the\\', \\'all\\']:\\n...         return \\'det\\'\\n...     else:\\n...         return \\'noun\\'\\n\\n\\n\\nIf the assert statement fails, it will produce an error that cannot be ignored,\\nsince it halts program execution.\\nAdditionally, the error message is easy to interpret.  Adding assertions to\\na program helps you find logical errors, and is a kind of defensive programming.\\nA more fundamental approach is to document the parameters to each function\\nusing docstrings as described later in this section.\\n\\n\\n\\nFunctional Decomposition\\nWell-structured programs usually make extensive use of functions.\\nWhen a block of program code grows longer than 10-20 lines, it is a\\ngreat help to readability if the code is broken up into one or more\\nfunctions, each one having a clear purpose.  This is analogous to\\nthe way a good essay is divided into paragraphs, each expressing one main idea.\\n\\n\\n\\n\\n\\nFunctions provide an important kind of abstraction.\\nThey allow us to group multiple actions into a single, complex action,\\nand associate a name with it.\\n(Compare this with the way we combine the actions of\\ngo and bring back into a single more complex action fetch.)\\nWhen we use functions, the main program can be written at a higher level\\nof abstraction, making its structure transparent, e.g.\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; data = load_corpus()\\n&gt;&gt;&gt; results = analyze(data)\\n&gt;&gt;&gt; present(results)\\n\\n\\n\\nAppropriate use of functions makes programs more readable and maintainable.\\nAdditionally, it becomes possible to reimplement a function\\n— replacing the function\\'s body with more efficient code —\\nwithout having to be concerned with the rest of the program.\\nConsider the freq_words function in 4.3.\\nIt updates the contents of a frequency distribution that is\\npassed in as a parameter, and it also prints a list of the\\nn most frequent words.\\n\\n\\n\\n\\n&nbsp;\\nfrom urllib import request\\nfrom bs4 import BeautifulSoup\\n\\ndef freq_words(url, freqdist, n):\\n    html = request.urlopen(url).read().decode(\\'utf8\\')\\n    raw = BeautifulSoup(html, \\'html.parser\\').get_text()\\n    for word in word_tokenize(raw):\\n        freqdist[word.lower()] += 1\\n    result = []\\n    for word, count in freqdist.most_common(n):\\n        result = result + [word]\\n    print(result)\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; constitution = \"http://www.archives.gov/exhibits/charters/constitution_transcript.html\"\\n&gt;&gt;&gt; fd = nltk.FreqDist()\\n&gt;&gt;&gt; freq_words(constitution, fd, 30)\\n[\\'the\\', \\',\\', \\'of\\', \\'and\\', \\'shall\\', \\'.\\', \\'be\\', \\'to\\', \\';\\', \\'in\\', \\'states\\',\\n\\'or\\', \\'united\\', \\'a\\', \\'state\\', \\'by\\', \\'for\\', \\'any\\', \\'=\\', \\'which\\', \\'president\\',\\n\\'all\\', \\'on\\', \\'may\\', \\'such\\', \\'as\\', \\'have\\', \\')\\', \\'(\\', \\'congress\\']\\n\\n\\nExample 4.3 (code_freq_words1.py): Figure 4.3: Poorly Designed Function to Compute Frequent Words\\n\\nThis function has a number of problems.\\nThe function has two side-effects: it modifies the contents of its second\\nparameter, and it prints a selection of the results it has computed.\\nThe function would be easier to understand and to reuse elsewhere if we\\ninitialize the FreqDist() object inside the function (in the same place\\nit is populated), and if we moved the selection and display of results to the\\ncalling program. Given that its task is to identify frequent words, it\\nshould probably just return a list, not the whole frequency distribution.\\nIn 4.4 we refactor this function,\\nand simplify its interface by dropping the freqdist parameter.\\n\\n\\n\\n\\n&nbsp;\\nfrom urllib import request\\nfrom bs4 import BeautifulSoup\\n\\ndef freq_words(url, n):\\n    html = request.urlopen(url).read().decode(\\'utf8\\')\\n    text = BeautifulSoup(html, \\'html.parser\\').get_text()\\n    freqdist = nltk.FreqDist(word.lower() for word in word_tokenize(text))\\n    return [word for (word, _) in fd.most_common(n)]\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; freq_words(constitution, 30)\\n[\\'the\\', \\',\\', \\'of\\', \\'and\\', \\'shall\\', \\'.\\', \\'be\\', \\'to\\', \\';\\', \\'in\\', \\'states\\',\\n\\'or\\', \\'united\\', \\'a\\', \\'state\\', \\'by\\', \\'for\\', \\'any\\', \\'=\\', \\'which\\', \\'president\\',\\n\\'all\\', \\'on\\', \\'may\\', \\'such\\', \\'as\\', \\'have\\', \\')\\', \\'(\\', \\'congress\\']\\n\\n\\nExample 4.4 (code_freq_words2.py): Figure 4.4: Well-Designed Function to Compute Frequent Words\\n\\nThe readability and usability of the freq_words function is improved.\\n\\nNote\\nWe have used _ as a variable name. This is no different to any other\\nvariable except it signals to the reader that we don\\'t have a use\\nfor the information it holds.\\n\\n\\n\\nDocumenting Functions\\nIf we have done a good job at decomposing our program into functions, then it should\\nbe easy to describe the purpose of each function in plain language, and provide\\nthis in the docstring at the top of the function definition.  This statement\\nshould not explain how the functionality is implemented; in fact it should be possible\\nto re-implement the function using a different method without changing this\\nstatement.\\nFor the simplest functions, a one-line docstring is usually adequate (see 4.2).\\nYou should provide a triple-quoted string containing a complete sentence on a single line.\\nFor non-trivial functions, you should still provide a one sentence summary on the first line,\\nsince many docstring processing tools index this string.  This should be followed by\\na blank line, then a more detailed description of the functionality\\n(see http://www.python.org/dev/peps/pep-0257/ for more information in docstring\\nconventions).\\n\\nDocstrings can include a doctest block, illustrating the use of\\nthe function and the expected output.  These can be tested automatically\\nusing Python\\'s docutils module.\\nDocstrings should document the type of each parameter to the function, and the return\\ntype.  At a minimum, that can be done in plain text.  However, note that NLTK uses\\nthe Sphinx markup language to document parameters.  This format\\ncan be automatically converted into richly structured\\nAPI documentation (see http://nltk.org/), and includes special handling of certain\\n\"fields\" such as param which allow the inputs and outputs of functions to be\\nclearly documented.  4.5 illustrates\\na complete docstring.\\n\\n\\n\\n\\n&nbsp;\\ndef accuracy(reference, test):\\n    \"\"\"\\n    Calculate the fraction of test items that equal the corresponding reference items.\\n\\n    Given a list of reference values and a corresponding list of test values,\\n    return the fraction of corresponding values that are equal.\\n    In particular, return the fraction of indexes\\n    {0&lt;i&lt;=len(test)} such that C{test[i] == reference[i]}.\\n\\n        &gt;&gt;&gt; accuracy([\\'ADJ\\', \\'N\\', \\'V\\', \\'N\\'], [\\'N\\', \\'N\\', \\'V\\', \\'ADJ\\'])\\n        0.5\\n\\n    :param reference: An ordered list of reference values\\n    :type reference: list\\n    :param test: A list of values to compare against the corresponding\\n        reference values\\n    :type test: list\\n    :return: the accuracy score\\n    :rtype: float\\n    :raises ValueError: If reference and length do not have the same length\\n    \"\"\"\\n\\n    if len(reference) != len(test):\\n        raise ValueError(\"Lists must have the same length.\")\\n    num_correct = 0\\n    for x, y in zip(reference, test):\\n        if x == y:\\n            num_correct += 1\\n    return float(num_correct) / len(reference)\\n\\n\\nExample 4.5 (code_sphinx.py): Figure 4.5: Illustration of a complete docstring, consisting of a one-line summary,\\na more detailed explanation, a doctest example, and Sphinx markup\\nspecifying the parameters, types, return type, and exceptions.\\n\\n\\n\\n\\n4.5&nbsp;&nbsp;&nbsp;Doing More with Functions\\nThis section discusses more advanced features, which you may prefer to skip on the\\nfirst time through this chapter.\\n\\nFunctions as Arguments\\nSo far the arguments we have passed into functions have been simple objects like\\nstrings, or structured objects like lists.  Python also lets us pass a function as\\nan argument to another function.  Now we can abstract out the operation, and apply\\na different operation on the same data.  As the following examples show,\\nwe can pass the built-in function len() or a user-defined function last_letter()\\nas arguments to another function:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; sent = [\\'Take\\', \\'care\\', \\'of\\', \\'the\\', \\'sense\\', \\',\\', \\'and\\', \\'the\\',\\n...         \\'sounds\\', \\'will\\', \\'take\\', \\'care\\', \\'of\\', \\'themselves\\', \\'.\\']\\n&gt;&gt;&gt; def extract_property(prop):\\n...     return [prop(word) for word in sent]\\n...\\n&gt;&gt;&gt; extract_property(len)\\n[4, 4, 2, 3, 5, 1, 3, 3, 6, 4, 4, 4, 2, 10, 1]\\n&gt;&gt;&gt; def last_letter(word):\\n...     return word[-1]\\n&gt;&gt;&gt; extract_property(last_letter)\\n[\\'e\\', \\'e\\', \\'f\\', \\'e\\', \\'e\\', \\',\\', \\'d\\', \\'e\\', \\'s\\', \\'l\\', \\'e\\', \\'e\\', \\'f\\', \\'s\\', \\'.\\']\\n\\n\\n\\nThe objects len and last_letter can be\\npassed around like lists and dictionaries.  Notice that parentheses\\nare only used after a function name if we are invoking the function;\\nwhen we are simply treating the function as an object these are omitted.\\nPython provides us with one more way to define functions as arguments\\nto other functions, so-called lambda expressions.  Supposing there\\nwas no need to use the above last_letter() function in multiple places,\\nand thus no need to give it a name.  We can equivalently write the following:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; extract_property(lambda w: w[-1])\\n[\\'e\\', \\'e\\', \\'f\\', \\'e\\', \\'e\\', \\',\\', \\'d\\', \\'e\\', \\'s\\', \\'l\\', \\'e\\', \\'e\\', \\'f\\', \\'s\\', \\'.\\']\\n\\n\\n\\nOur next example illustrates passing a function to the sorted() function.\\nWhen we call the latter with a single argument (the list to be sorted),\\nit uses the built-in comparison function cmp().\\nHowever, we can supply our own sort function, e.g. to sort by decreasing\\nlength.\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; sorted(sent)\\n[\\',\\', \\'.\\', \\'Take\\', \\'and\\', \\'care\\', \\'care\\', \\'of\\', \\'of\\', \\'sense\\', \\'sounds\\',\\n\\'take\\', \\'the\\', \\'the\\', \\'themselves\\', \\'will\\']\\n&gt;&gt;&gt; sorted(sent, cmp)\\n[\\',\\', \\'.\\', \\'Take\\', \\'and\\', \\'care\\', \\'care\\', \\'of\\', \\'of\\', \\'sense\\', \\'sounds\\',\\n\\'take\\', \\'the\\', \\'the\\', \\'themselves\\', \\'will\\']\\n&gt;&gt;&gt; sorted(sent, lambda x, y: cmp(len(y), len(x)))\\n[\\'themselves\\', \\'sounds\\', \\'sense\\', \\'Take\\', \\'care\\', \\'will\\', \\'take\\', \\'care\\',\\n\\'the\\', \\'and\\', \\'the\\', \\'of\\', \\'of\\', \\',\\', \\'.\\']\\n\\n\\n\\n\\n\\nAccumulative Functions\\nThese functions start by initializing some storage, and iterate over\\ninput to build it up, before returning some final object (a large structure\\nor aggregated result).  A standard way to do this is to initialize an\\nempty list, accumulate the material, then return the list, as shown\\nin function search1() in 4.6.\\n\\n\\n\\n\\n&nbsp;\\ndef search1(substring, words):\\n    result = []\\n    for word in words:\\n        if substring in word:\\n            result.append(word)\\n    return result\\n\\ndef search2(substring, words):\\n    for word in words:\\n        if substring in word:\\n            yield word\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; for item in search1(\\'zz\\', nltk.corpus.brown.words()):\\n...     print(item, end=\" \")\\nGrizzlies\\' fizzled Rizzuto huzzahs dazzler jazz Pezza ...\\n&gt;&gt;&gt; for item in search2(\\'zz\\', nltk.corpus.brown.words()):\\n...     print(item, end=\" \")\\nGrizzlies\\' fizzled Rizzuto huzzahs dazzler jazz Pezza ...\\n\\n\\nExample 4.6 (code_search_examples.py): Figure 4.6: Accumulating Output into a List\\n\\nThe function search2() is a generator.\\nThe first time this function is called, it gets as far as the yield\\nstatement and pauses.  The calling program gets the first word and does\\nany necessary processing.  Once the calling program is ready for another\\nword, execution of the function is continued from where it stopped, until\\nthe next time it encounters a yield statement.  This approach is\\ntypically more efficient, as the function only generates the data as it is\\nrequired by the calling program, and does not need to allocate additional\\nmemory to store the output (cf. our discussion of generator expressions above).\\nHere\\'s a more sophisticated example of a generator which produces\\nall permutations of a list of words.  In order to force the permutations()\\nfunction to generate all its output, we wrap it with a call to list() .\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; def permutations(seq):\\n...     if len(seq) &lt;= 1:\\n...         yield seq\\n...     else:\\n...         for perm in permutations(seq[1:]):\\n...             for i in range(len(perm)+1):\\n...                 yield perm[:i] + seq[0:1] + perm[i:]\\n...\\n&gt;&gt;&gt; list(permutations([\\'police\\', \\'fish\\', \\'buffalo\\'])) \\n[[\\'police\\', \\'fish\\', \\'buffalo\\'], [\\'fish\\', \\'police\\', \\'buffalo\\'],\\n [\\'fish\\', \\'buffalo\\', \\'police\\'], [\\'police\\', \\'buffalo\\', \\'fish\\'],\\n [\\'buffalo\\', \\'police\\', \\'fish\\'], [\\'buffalo\\', \\'fish\\', \\'police\\']]\\n\\n\\n\\n\\nNote\\nThe permutations function uses a technique called recursion,\\ndiscussed below in 4.7.\\nThe ability to generate permutations of a set of words is\\nuseful for creating data to test a grammar (8.).\\n\\n\\n\\nHigher-Order Functions\\nPython provides some higher-order functions that are standard\\nfeatures of functional programming languages such as Haskell.\\nWe illustrate them here, alongside the equivalent expression\\nusing list comprehensions.\\nLet\\'s start by defining a function is_content_word()\\nwhich checks whether a word is from the open class of content words.\\nWe use this function as the first parameter of filter(),\\nwhich applies the function to each item in the sequence contained\\nin its second parameter, and only retains the items for which\\nthe function returns True.\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; def is_content_word(word):\\n...     return word.lower() not in [\\'a\\', \\'of\\', \\'the\\', \\'and\\', \\'will\\', \\',\\', \\'.\\']\\n&gt;&gt;&gt; sent = [\\'Take\\', \\'care\\', \\'of\\', \\'the\\', \\'sense\\', \\',\\', \\'and\\', \\'the\\',\\n...         \\'sounds\\', \\'will\\', \\'take\\', \\'care\\', \\'of\\', \\'themselves\\', \\'.\\']\\n&gt;&gt;&gt; list(filter(is_content_word, sent))\\n[\\'Take\\', \\'care\\', \\'sense\\', \\'sounds\\', \\'take\\', \\'care\\', \\'themselves\\']\\n&gt;&gt;&gt; [w for w in sent if is_content_word(w)]\\n[\\'Take\\', \\'care\\', \\'sense\\', \\'sounds\\', \\'take\\', \\'care\\', \\'themselves\\']\\n\\n\\n\\nAnother higher-order function is map(), which applies a function\\nto every item in a sequence.  It is a general version of the\\nextract_property() function we saw in 4.5.\\nHere is a simple way to find the average length of a sentence in the news\\nsection of the Brown Corpus, followed by an equivalent version with list comprehension\\ncalculation:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; lengths = list(map(len, nltk.corpus.brown.sents(categories=\\'news\\')))\\n&gt;&gt;&gt; sum(lengths) / len(lengths)\\n21.75081116158339\\n&gt;&gt;&gt; lengths = [len(sent) for sent in nltk.corpus.brown.sents(categories=\\'news\\')]\\n&gt;&gt;&gt; sum(lengths) / len(lengths)\\n21.75081116158339\\n\\n\\n\\nIn the above examples we specified a user-defined function is_content_word()\\nand a built-in function len().  We can also provide a lambda expression.\\nHere\\'s a pair of equivalent examples which count the number of vowels in each word.\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; list(map(lambda w: len(filter(lambda c: c.lower() in \"aeiou\", w)), sent))\\n[2, 2, 1, 1, 2, 0, 1, 1, 2, 1, 2, 2, 1, 3, 0]\\n&gt;&gt;&gt; [len(c for c in w if c.lower() in \"aeiou\") for w in sent]\\n[2, 2, 1, 1, 2, 0, 1, 1, 2, 1, 2, 2, 1, 3, 0]\\n\\n\\n\\nThe solutions based on list comprehensions are usually more readable than the\\nsolutions based on higher-order functions, and we have favored the former\\napproach throughout this book.\\n\\n\\nNamed Arguments\\nWhen there are a lot of parameters it is easy to get confused about the\\ncorrect order.  Instead we can refer to parameters by name, and even assign\\nthem a default value just in case one was not provided by the calling\\nprogram.  Now the parameters can be specified in any order, and can be omitted.\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; def repeat(msg=\\'&lt;empty&gt;\\', num=1):\\n...     return msg * num\\n&gt;&gt;&gt; repeat(num=3)\\n\\'&lt;empty&gt;&lt;empty&gt;&lt;empty&gt;\\'\\n&gt;&gt;&gt; repeat(msg=\\'Alice\\')\\n\\'Alice\\'\\n&gt;&gt;&gt; repeat(num=5, msg=\\'Alice\\')\\n\\'AliceAliceAliceAliceAlice\\'\\n\\n\\n\\n\\n\\nThese are called keyword arguments.\\nIf we mix these two kinds of parameters, then we must ensure that the unnamed parameters precede the named ones.\\nIt has to be this way, since unnamed parameters are defined by position.  We can define a function that takes\\nan arbitrary number of unnamed and named parameters, and access them via an in-place list of arguments *args and\\nan \"in-place dictionary\" of keyword arguments **kwargs.\\n(Dictionaries will be presented in 3.)\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; def generic(*args, **kwargs):\\n...     print(args)\\n...     print(kwargs)\\n...\\n&gt;&gt;&gt; generic(1, \"African swallow\", monty=\"python\")\\n(1, \\'African swallow\\')\\n{\\'monty\\': \\'python\\'}\\n\\n\\n\\nWhen *args appears as a function parameter, it actually corresponds to all the unnamed parameters of\\nthe function.  Here\\'s another illustration of this aspect of Python syntax, for the zip() function which\\noperates on a variable number of arguments.  We\\'ll use the variable name *song to demonstrate that\\nthere\\'s nothing special about the name *args.\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; song = [[\\'four\\', \\'calling\\', \\'birds\\'],\\n...         [\\'three\\', \\'French\\', \\'hens\\'],\\n...         [\\'two\\', \\'turtle\\', \\'doves\\']]\\n&gt;&gt;&gt; list(zip(song[0], song[1], song[2]))\\n[(\\'four\\', \\'three\\', \\'two\\'), (\\'calling\\', \\'French\\', \\'turtle\\'), (\\'birds\\', \\'hens\\', \\'doves\\')]\\n&gt;&gt;&gt; list(zip(*song))\\n[(\\'four\\', \\'three\\', \\'two\\'), (\\'calling\\', \\'French\\', \\'turtle\\'), (\\'birds\\', \\'hens\\', \\'doves\\')]\\n\\n\\n\\nIt should be clear from the above example that typing *song is just a convenient\\nshorthand, and equivalent to typing out song[0], song[1], song[2].\\nHere\\'s another example of the use of keyword arguments in a function\\ndefinition, along with three equivalent ways to call the function:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; def freq_words(file, min=1, num=10):\\n...     text = open(file).read()\\n...     tokens = word_tokenize(text)\\n...     freqdist = nltk.FreqDist(t for t in tokens if len(t) &gt;= min)\\n...     return freqdist.most_common(num)\\n&gt;&gt;&gt; fw = freq_words(\\'ch01.rst\\', 4, 10)\\n&gt;&gt;&gt; fw = freq_words(\\'ch01.rst\\', min=4, num=10)\\n&gt;&gt;&gt; fw = freq_words(\\'ch01.rst\\', num=10, min=4)\\n\\n\\n\\nA side-effect of having named arguments is that they permit optionality.  Thus we\\ncan leave out any arguments where we are happy with the default value:\\nfreq_words(\\'ch01.rst\\', min=4), freq_words(\\'ch01.rst\\', 4).\\nAnother common use of optional arguments is to permit a flag.\\nHere\\'s a revised version of the same function that reports its\\nprogress if a verbose flag is set:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; def freq_words(file, min=1, num=10, verbose=False):\\n...     freqdist = FreqDist()\\n...     if verbose: print(\"Opening\", file)\\n...     text = open(file).read()\\n...     if verbose: print(\"Read in %d characters\" % len(file))\\n...     for word in word_tokenize(text):\\n...         if len(word) &gt;= min:\\n...             freqdist[word] += 1\\n...             if verbose and freqdist.N() % 100 == 0: print(\".\", sep=\"\")\\n...     if verbose: print\\n...     return freqdist.most_common(num)\\n\\n\\n\\n\\nCaution!\\nTake care not to use a mutable object as the default value of\\na parameter.  A series of calls to the function will use the\\nsame object, sometimes with bizarre results as we will see in\\nthe discussion of debugging below.\\n\\n\\nCaution!\\nIf your program will work with a lot of files, it is a good idea to\\nclose any open files once they are no longer required. Python will\\nclose open files automatically if you use the with statement:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; with open(\"lexicon.txt\") as f:\\n...     data = f.read()\\n...     # process the data\\n\\n\\n\\n\\n\\n\\n\\n4.6&nbsp;&nbsp;&nbsp;Program Development\\nProgramming is a skill that is acquired over several years of\\nexperience with a variety of programming languages and tasks.  Key\\nhigh-level abilities are algorithm design and its manifestation in\\nstructured programming.  Key low-level abilities include familiarity\\nwith the syntactic constructs of the language, and knowledge of a\\nvariety of diagnostic methods for trouble-shooting a program which\\ndoes not exhibit the expected behavior.\\nThis section describes the internal structure of a program module and\\nhow to organize a multi-module program.  Then it describes various\\nkinds of error that arise during program development, what you can\\ndo to fix them and, better still, to avoid them in the first place.\\n\\nStructure of a Python Module\\nThe purpose of a program module is to bring logically-related definitions and functions\\ntogether in order to facilitate re-use and abstraction.  Python modules are nothing\\nmore than individual .py files.  For example, if you were working\\nwith a particular corpus format, the functions to read and write the format could be\\nkept together.  Constants used by both formats, such as field separators,\\nor a EXTN = \".inf\" filename extension, could be shared.  If the format was updated,\\nyou would know that only one file needed to be changed.  Similarly, a module could\\ncontain code for creating and manipulating a particular data structure such as\\nsyntax trees, or code for performing a particular processing task such as\\nplotting corpus statistics.\\nWhen you start writing Python modules, it helps to have some\\nexamples to emulate.  You can locate the code for any NLTK module on your\\nsystem using the __file__ variable, e.g.:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; nltk.metrics.distance.__file__\\n\\'/usr/lib/python2.5/site-packages/nltk/metrics/distance.pyc\\'\\n\\n\\n\\nThis returns the location of the compiled .pyc file for the module, and\\nyou\\'ll probably see a different location on your machine. The file that you will need\\nto open is the corresponding .py source file, and this will be in the same\\ndirectory as the .pyc file.\\nAlternatively, you can view the latest version of this module on the web\\nat http://code.google.com/p/nltk/source/browse/trunk/nltk/nltk/metrics/distance.py.\\nLike every other NLTK module, distance.py begins with a group of comment\\nlines giving a one-line title of the module and identifying the authors.\\n(Since the code is distributed, it also includes the URL where the\\ncode is available, a copyright statement, and license information.)\\nNext is the module-level docstring, a triple-quoted multiline string\\ncontaining information about the module that will be printed when\\nsomeone types help(nltk.metrics.distance).\\n\\n# Natural Language Toolkit: Distance Metrics\\n#\\n# Copyright (C) 2001-2019 NLTK Project\\n# Author: Edward Loper &lt;edloper@gmail.com&gt;\\n#         Steven Bird &lt;stevenbird1@gmail.com&gt;\\n#         Tom Lippincott &lt;tom@cs.columbia.edu&gt;\\n# URL: &lt;http://nltk.org/&gt;\\n# For license information, see LICENSE.TXT\\n#\\n\\n\"\"\"\\nDistance Metrics.\\n\\nCompute the distance between two items (usually strings).\\nAs metrics, they must satisfy the following three requirements:\\n\\n1. d(a, a) = 0\\n2. d(a, b) &gt;= 0\\n3. d(a, c) &lt;= d(a, b) + d(b, c)\\n\"\"\"\\n\\nAfter this comes all the import statements required for the module,\\nthen any global variables,\\nfollowed by a series of function definitions that make up most\\nof the module.  Other modules define \"classes,\" the main building block\\nof object-oriented programming, which falls outside the scope of this book.\\n(Most NLTK modules also include a demo() function which can be used\\nto see examples of the module in use.)\\n\\nNote\\nSome module variables and functions are only used within the module.\\nThese should have names beginning with an underscore, e.g. _helper(),\\nsince this will hide the name.  If another module imports this one,\\nusing the idiom: from module import *, these names will not be imported.\\nYou can optionally list the externally accessible names of a module using\\na special built-in variable like this: __all__ = [\\'edit_distance\\', \\'jaccard_distance\\'].\\n\\n\\n\\nMulti-Module Programs\\nSome programs bring together a diverse range of tasks, such as loading data from\\na corpus, performing some analysis tasks on the data, then visualizing it.\\nWe may already have stable modules that take care of loading data and producing visualizations.\\nOur work might involve coding up the analysis task, and just invoking functions\\nfrom the existing modules.  This scenario is depicted in 4.7.\\n\\n\\nFigure 4.7: Structure of a Multi-Module Program: The main program my_program.py imports functions\\nfrom two other modules; unique analysis tasks are localized to the main program, while\\ncommon loading and visualization tasks are kept apart to facilitate re-use and abstraction.\\n\\nBy dividing our work into several modules and using import statements to\\naccess functions defined elsewhere, we can keep the individual modules simple\\nand easy to maintain.  This approach will also result in a growing collection\\nof modules, and make it possible for us to build sophisticated systems involving\\na hierarchy of modules.  Designing such systems well is a\\ncomplex software engineering task, and beyond the scope of this book.\\n\\n\\nSources of Error\\nMastery of programming depends on having a variety of problem-solving skills to\\ndraw upon when the program doesn\\'t work as expected.  Something as trivial as\\na mis-placed symbol might cause the program to behave very differently.\\nWe call these \"bugs\" because they are tiny in comparison to the damage\\nthey can cause.  They creep into our code unnoticed, and it\\'s only much later\\nwhen we\\'re running the program on some new data that their presence is detected.\\nSometimes, fixing one bug only reveals another, and we get the distinct impression\\nthat the bug is on the move.  The only reassurance we have is that bugs are\\nspontaneous and not the fault of the programmer.\\nFlippancy aside, debugging code is hard because there are so many ways for\\nit to be faulty.  Our understanding of the input data, the algorithm, or\\neven the programming language, may be at fault.  Let\\'s look at examples\\nof each of these.\\nFirst, the input data may contain some unexpected characters.\\nFor example, WordNet synset names have the form tree.n.01, with three\\ncomponents separated using periods.  The NLTK WordNet module initially\\ndecomposed these names using split(\\'.\\').  However, this method broke when\\nsomeone tried to look up the word PhD, which has the synset\\nname ph.d..n.01, containing four periods instead of the expected two.\\nThe solution was to use rsplit(\\'.\\', 2) to do at most two splits, using\\nthe rightmost instances of the period, and leaving the ph.d. string intact.\\nAlthough several people had tested\\nthe module before it was released, it was some weeks before someone detected\\nthe problem (see http://code.google.com/p/nltk/issues/detail?id=297).\\nSecond, a supplied function might not behave as expected.\\nFor example, while testing NLTK\\'s interface to WordNet, one of the\\nauthors noticed that no synsets had any antonyms defined, even though\\nthe underlying database provided a large quantity of antonym information.\\nWhat looked like a bug in the WordNet interface turned out to\\nbe a misunderstanding about WordNet itself: antonyms are defined for\\nlemmas, not for synsets.  The only \"bug\" was a misunderstanding\\nof the interface (see http://code.google.com/p/nltk/issues/detail?id=98).\\n\\n\\nThird, our understanding of Python\\'s semantics may be at fault.\\nIt is easy to make the wrong assumption about the relative\\nscope of two operators.\\nFor example, \"%s.%s.%02d\" % \"ph.d.\", \"n\", 1 produces a run-time\\nerror TypeError: not enough arguments for format string.\\nThis is because the percent operator has higher precedence than\\nthe comma operator.  The fix is to add parentheses in order to\\nforce the required scope.  As another example, suppose we are\\ndefining a function to collect all tokens of a text having a\\ngiven length.  The function has parameters for the text and\\nthe word length, and an extra parameter that allows the initial\\nvalue of the result to be given as a parameter:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; def find_words(text, wordlength, result=[]):\\n...     for word in text:\\n...         if len(word) == wordlength:\\n...             result.append(word)\\n...     return result\\n&gt;&gt;&gt; find_words([\\'omg\\', \\'teh\\', \\'lolcat\\', \\'sitted\\', \\'on\\', \\'teh\\', \\'mat\\'], 3) \\n[\\'omg\\', \\'teh\\', \\'teh\\', \\'mat\\']\\n&gt;&gt;&gt; find_words([\\'omg\\', \\'teh\\', \\'lolcat\\', \\'sitted\\', \\'on\\', \\'teh\\', \\'mat\\'], 2, [\\'ur\\']) \\n[\\'ur\\', \\'on\\']\\n&gt;&gt;&gt; find_words([\\'omg\\', \\'teh\\', \\'lolcat\\', \\'sitted\\', \\'on\\', \\'teh\\', \\'mat\\'], 3) \\n[\\'omg\\', \\'teh\\', \\'teh\\', \\'mat\\', \\'omg\\', \\'teh\\', \\'teh\\', \\'mat\\']\\n\\n\\n\\nThe first time we call find_words() , we get all three-letter\\nwords as expected.  The second time we specify an initial value for the result,\\na one-element list [\\'ur\\'], and as expected, the result has this word along with the\\nother two-letter word in our text.  Now, the next time we call find_words() \\nwe use the same parameters as in , but we get a different result!\\nEach time we call find_words() with no third parameter, the result will\\nsimply extend the result of the previous call, rather than start with the\\nempty result list as specified in the function definition.  The program\\'s\\nbehavior is not as expected because we incorrectly assumed that the default\\nvalue was created at the time the function was invoked.  However, it is\\ncreated just once, at the time the Python interpreter loads the function.\\nThis one list object is used whenever no explicit value is provided to the function.\\n\\n\\nDebugging Techniques\\nSince most code errors result from the programmer making incorrect assumptions,\\nthe first thing to do when you detect a bug is to check your assumptions.\\nLocalize the problem by adding print statements to the program, showing the\\nvalue of important variables, and showing how far the program has progressed.\\nIf the program produced an \"exception\" — a run-time error —\\nthe interpreter will print a stack trace,\\npinpointing the location of program execution at the time of the error.\\nIf the program depends on input data, try to reduce this to the smallest\\nsize while still producing the error.\\nOnce you have localized the problem to a particular function, or to a line\\nof code, you need to work out what is going wrong.  It is often helpful to\\nrecreate the situation using the interactive command line.  Define some\\nvariables then copy-paste the offending line of code into the session\\nand see what happens.  Check your understanding of the code by reading\\nsome documentation, and examining other code samples that purport to do\\nthe same thing that you are trying to do.  Try explaining your code to\\nsomeone else, in case they can see where things are going wrong.\\nPython provides a debugger which allows you to monitor the execution\\nof your program, specify line numbers where execution will stop (i.e. breakpoints),\\nand step through sections of code and inspect the value of variables.\\nYou can invoke the debugger on your code as follows:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; import pdb\\n&gt;&gt;&gt; import mymodule\\n&gt;&gt;&gt; pdb.run(\\'mymodule.myfunction()\\')\\n\\n\\n\\nIt will present you with a prompt (Pdb) where you can type instructions\\nto the debugger.  Type help to see the full list of commands.\\nTyping step (or just s) will execute the current line and\\nstop.  If the current line calls a function, it will enter the function\\nand stop at the first line.  Typing next (or just n) is similar,\\nbut it stops execution at the next line in the current function.  The\\nbreak (or b) command can be used to create or list breakpoints.  Type\\ncontinue (or c) to continue execution as far as the next breakpoint.\\nType the name of any variable to inspect its value.\\nWe can use the Python debugger to locate the problem in our find_words()\\nfunction.  Remember that the problem arose the second time the function was\\ncalled.  We\\'ll start by calling the function without using the debugger ,\\nusing the smallest possible input.  The second time, we\\'ll call it with the\\ndebugger .\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; import pdb\\n&gt;&gt;&gt; find_words([\\'cat\\'], 3) \\n[\\'cat\\']\\n&gt;&gt;&gt; pdb.run(\"find_words([\\'dog\\'], 3)\") \\n&gt; &lt;string&gt;(1)&lt;module&gt;()\\n(Pdb) step\\n--Call--\\n&gt; &lt;stdin&gt;(1)find_words()\\n(Pdb) args\\ntext = [\\'dog\\']\\nwordlength = 3\\nresult = [\\'cat\\']\\n\\n\\n\\nHere we typed just two commands into the debugger: step took us inside\\nthe function, and args showed the values of its arguments (or parameters).\\nWe see immediately that result has an initial value of [\\'cat\\'], and not\\nthe empty list as expected.  The debugger has helped us to localize the problem,\\nprompting us to check our understanding of Python functions.\\n\\n\\nDefensive Programming\\nIn order to avoid some of the pain of debugging, it helps to adopt\\nsome defensive programming habits.  Instead of writing a 20-line\\nprogram then testing it, build the program bottom-up out of\\nsmall pieces that are known to work.  Each time you combine these\\npieces to make a larger unit, test it carefully to see that it works\\nas expected.  Consider adding assert statements to your code,\\nspecifying properties of a variable, e.g. assert(isinstance(text, list)).\\nIf the value of the text variable later becomes a string when your\\ncode is used in some larger context, this will raise an AssertionError\\nand you will get immediate notification of the problem.\\nOnce you think you\\'ve found the bug, view your solution as a hypothesis.\\nTry to predict the effect of your bugfix before re-running the program.\\nIf the bug isn\\'t fixed, don\\'t fall into the trap of blindly changing\\nthe code in the hope that it will magically start working again.\\nInstead, for each change, try to articulate a hypothesis about what\\nis wrong and why the change will fix the problem.  Then undo the change\\nif the problem was not resolved.\\nAs you develop your program, extend its functionality, and fix any bugs,\\nit helps to maintain a suite of test cases.\\nThis is called regression testing, since it is meant to detect\\nsituations where the code \"regresses\" — where a change to the\\ncode has an unintended side-effect of breaking something that\\nused to work.  Python provides a simple regression testing framework\\nin the form of the doctest module.  This module searches a file\\nof code or documentation for blocks of text that look like\\nan interactive Python session, of the form you have already seen\\nmany times in this book.  It executes the Python commands it finds,\\nand tests that their output matches the output supplied in the original\\nfile.  Whenever there is a mismatch, it reports the expected and actual\\nvalues.  For details please consult the doctest documentation at\\nhttp://docs.python.org/library/doctest.html.  Apart from its\\nvalue for regression testing, the doctest module is useful for\\nensuring that your software documentation stays in sync with your\\ncode.\\nPerhaps the most important defensive programming strategy is to\\nset out your code clearly, choose meaningful variable and function\\nnames, and simplify the code wherever possible by decomposing it into\\nfunctions and modules with well-documented interfaces.\\n\\n\\n\\n4.7&nbsp;&nbsp;&nbsp;Algorithm Design\\nThis section discusses more advanced concepts, which you may prefer to skip on the\\nfirst time through this chapter.\\nA major part of algorithmic problem solving is selecting or adapting\\nan appropriate algorithm for the problem at hand.  Sometimes there are\\nseveral alternatives, and choosing the best one depends on knowledge\\nabout how each alternative performs as the size of the data grows.\\nWhole books are written on this topic, and we only have space to introduce\\nsome key concepts and elaborate on the approaches that are most prevalent\\nin natural language processing.\\nThe best known strategy is known as divide-and-conquer.\\nWe attack a problem of size n by dividing it into two problems of size n/2,\\nsolve these problems, and combine their results into a solution of the original problem.\\nFor example, suppose that we had a pile of cards with a single word written on each card.\\nWe could sort this pile by splitting it in half and giving it to two other people\\nto sort (they could do the same in turn).  Then, when two sorted piles come back, it\\nis an easy task to merge them into a single sorted pile.\\nSee 4.8 for an illustration of this process.\\n\\n\\nFigure 4.8: Sorting by Divide-and-Conquer: to sort an array, we split it in half and\\nsort each half (recursively); we merge each sorted half back into a whole\\nlist (again recursively); this algorithm is known as \"Merge Sort\".\\n\\nAnother example is the process of looking up a word in a dictionary.  We open\\nthe book somewhere around the middle and compare our word with the current\\npage.  If it\\'s earlier in the dictionary we repeat the process on the first\\nhalf; if its later we use the second half.  This search method is called\\nbinary search since it splits the problem in half at every step.\\nIn another approach to algorithm design, we attack a problem\\nby transforming it into an instance of a problem we already know how to solve.\\nFor example, in order to detect duplicate entries in a list, we can pre-sort\\nthe list, then scan through it once to check if any adjacent pairs of elements\\nare identical.\\n\\nRecursion\\nThe above examples of sorting and searching have a striking property:\\nto solve a problem of size n, we have to break it in half and\\nthen work on one or more problems of size n/2.\\nA common way to implement such methods uses recursion.\\nWe define a function f which simplifies the problem,\\nand calls itself to solve one or more easier instances\\nof the same problem.  It then combines the results into a solution\\nfor the original problem.\\nFor example, suppose we have a set of n words, and want to\\ncalculate how many different ways they can be combined to make a\\nsequence of words.  If we have only one word (n=1), there is\\njust one way to make it into a sequence.  If we have a set of two\\nwords, there are two ways to put them into a sequence.  For three\\nwords there are six possibilities.  In general, for n words,\\nthere are n × n-1 × … × 2 × 1\\nways (i.e. the factorial of n).  We can code this up as follows:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; def factorial1(n):\\n...     result = 1\\n...     for i in range(n):\\n...         result *= (i+1)\\n...     return result\\n\\n\\n\\nHowever, there is also a recursive algorithm for solving this problem,\\nbased on the following observation.  Suppose we have a way to\\nconstruct all orderings for n-1 distinct words.  Then\\nfor each such ordering, there are n places where we can\\ninsert a new word: at the start, the end, or any of the n-2\\nboundaries between the words.  Thus we simply multiply the number\\nof solutions found for n-1 by the value of n.\\nWe also need the base case, to say that if we have a single\\nword, there\\'s just one ordering.  We can code this up as follows:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; def factorial2(n):\\n...     if n == 1:\\n...         return 1\\n...     else:\\n...         return n * factorial2(n-1)\\n\\n\\n\\n\\nThese two algorithms solve the same problem.  One uses iteration\\nwhile the other uses recursion.\\nWe can use recursion to navigate a deeply-nested object, such as the\\nWordNet hypernym hierarchy.  Let\\'s count the size of the hypernym\\nhierarchy rooted at a given synset s.  We\\'ll do this by finding the\\nsize of each hyponym of s, then adding these together\\n(we will also add 1 for the synset itself).  The following\\nfunction size1() does this work; notice that the body of\\nthe function includes a recursive call to size1():\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; def size1(s):\\n...     return 1 + sum(size1(child) for child in s.hyponyms())\\n\\n\\n\\nWe can also design an iterative solution to this problem which processes\\nthe hierarchy in layers.  The first layer is the synset itself ,\\nthen all the hyponyms of the synset, then all the hyponyms of the\\nhyponyms.  Each time through the loop it computes the next layer\\nby finding the hyponyms of everything in the last layer .\\nIt also maintains a total of the number of synsets encountered so far .\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; def size2(s):\\n...     layer = [s] \\n...     total = 0\\n...     while layer:\\n...         total += len(layer) \\n...         layer = [h for c in layer for h in c.hyponyms()] \\n...     return total\\n\\n\\n\\nNot only is the iterative solution much longer, it is harder to interpret.\\nIt forces us to think procedurally, and keep track of what is happening with\\nthe layer and total variables through time.  Let\\'s satisfy ourselves\\nthat both solutions give the same result.  We\\'ll use another form of the import\\nstatement, allowing us to abbreviate the name wordnet to wn:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; from nltk.corpus import wordnet as wn\\n&gt;&gt;&gt; dog = wn.synset(\\'dog.n.01\\')\\n&gt;&gt;&gt; size1(dog)\\n190\\n&gt;&gt;&gt; size2(dog)\\n190\\n\\n\\n\\nAs a final example of recursion, let\\'s use it to construct\\na deeply-nested object.\\nA letter trie is a data structure that can be used\\nfor indexing a lexicon, one letter at a time.  (The name\\nis based on the word retrieval).\\nFor example, if trie\\ncontained a letter trie, then trie[\\'c\\'] would be a smaller\\ntrie which held all words starting with c.\\n4.9 demonstrates the recursive process of building a trie,\\nusing Python dictionaries (3).\\nTo insert the word chien (French for dog),\\nwe split off the c and recursively insert hien\\ninto the sub-trie trie[\\'c\\'].  The recursion continues\\nuntil there are no letters remaining in the word, when we\\nstore the intended value (in this case, the word dog).\\n\\n\\n\\n\\n&nbsp;\\ndef insert(trie, key, value):\\n    if key:\\n        first, rest = key[0], key[1:]\\n        if first not in trie:\\n            trie[first] = {}\\n        insert(trie[first], rest, value)\\n    else:\\n        trie[\\'value\\'] = value\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; trie = {}\\n&gt;&gt;&gt; insert(trie, \\'chat\\', \\'cat\\')\\n&gt;&gt;&gt; insert(trie, \\'chien\\', \\'dog\\')\\n&gt;&gt;&gt; insert(trie, \\'chair\\', \\'flesh\\')\\n&gt;&gt;&gt; insert(trie, \\'chic\\', \\'stylish\\')\\n&gt;&gt;&gt; trie = dict(trie)               # for nicer printing\\n&gt;&gt;&gt; trie[\\'c\\'][\\'h\\'][\\'a\\'][\\'t\\'][\\'value\\']\\n\\'cat\\'\\n&gt;&gt;&gt; pprint.pprint(trie, width=40)\\n{\\'c\\': {\\'h\\': {\\'a\\': {\\'t\\': {\\'value\\': \\'cat\\'}},\\n                  {\\'i\\': {\\'r\\': {\\'value\\': \\'flesh\\'}}},\\n             \\'i\\': {\\'e\\': {\\'n\\': {\\'value\\': \\'dog\\'}}}\\n                  {\\'c\\': {\\'value\\': \\'stylish\\'}}}}}\\n\\n\\nExample 4.9 (code_trie.py): Figure 4.9: Building a Letter Trie: A recursive function that builds a nested dictionary\\nstructure; each level of nesting contains all words with a given prefix,\\nand a sub-trie containing all possible continuations.\\n\\n\\nCaution!\\nDespite the simplicity of recursive programming, it comes with a cost.\\nEach time a function is called, some state information needs to be\\npushed on a stack, so that once the function has completed, execution\\ncan continue from where it left off.  For this reason, iterative\\nsolutions are often more efficient than recursive solutions.\\n\\n\\n\\nSpace-Time Tradeoffs\\nWe can sometimes significantly speed up the execution of a program by building an auxiliary\\ndata structure, such as an index.  The listing in 4.10 implements a simple\\ntext retrieval system for the Movie Reviews Corpus.  By indexing the document collection it\\nprovides much faster lookup.\\n\\n\\n\\n\\n&nbsp;\\ndef raw(file):\\n    contents = open(file).read()\\n    contents = re.sub(r\\'&lt;.*?&gt;\\', \\' \\', contents)\\n    contents = re.sub(\\'\\\\s+\\', \\' \\', contents)\\n    return contents\\n\\ndef snippet(doc, term):\\n    text = \\' \\'*30 + raw(doc) + \\' \\'*30\\n    pos = text.index(term)\\n    return text[pos-30:pos+30]\\n\\nprint(\"Building Index...\")\\nfiles = nltk.corpus.movie_reviews.abspaths()\\nidx = nltk.Index((w, f) for f in files for w in raw(f).split())\\n\\nquery = \\'\\'\\nwhile query != \"quit\":\\n    query = input(\"query&gt; \")     # use raw_input() in Python 2\\n    if query in idx:\\n        for doc in idx[query]:\\n            print(snippet(doc, query))\\n    else:\\n        print(\"Not found\")\\n\\n\\nExample 4.10 (code_search_documents.py): Figure 4.10: A Simple Text Retrieval System\\n\\nA more subtle example of a space-time tradeoff involves replacing the tokens of a corpus\\nwith integer identifiers.  We create a vocabulary for the corpus, a list in which each\\nword is stored once, then invert this list so that we can look up any word to find its\\nidentifier.  Each document is preprocessed, so that a list of words becomes a list of integers.\\nAny language models can now work with integers.  See the listing in 4.11\\nfor an example of how to do this for a tagged corpus.\\n\\n\\n\\n\\n\\n&nbsp;\\ndef preprocess(tagged_corpus):\\n    words = set()\\n    tags = set()\\n    for sent in tagged_corpus:\\n        for word, tag in sent:\\n            words.add(word)\\n            tags.add(tag)\\n    wm = dict((w, i) for (i, w) in enumerate(words))\\n    tm = dict((t, i) for (i, t) in enumerate(tags))\\n    return [[(wm[w], tm[t]) for (w, t) in sent] for sent in tagged_corpus]\\n\\n\\nExample 4.11 (code_strings_to_ints.py): Figure 4.11: Preprocess tagged corpus data, converting all words and tags to integers\\n\\nAnother example of a space-time tradeoff is maintaining a vocabulary list.\\nIf you need to process an input text to check that all words are in an\\nexisting vocabulary, the vocabulary should be stored as a set, not a list.\\nThe elements of a set are automatically indexed, so testing membership\\nof a large set will be much faster than testing membership of the\\ncorresponding list.\\nWe can test this claim using the timeit module.\\nThe Timer class has two parameters, a statement which\\nis executed multiple times, and setup code that is executed\\nonce at the beginning.  We will simulate a vocabulary of\\n100,000 items using a list  or set \\nof integers.  The test statement will generate a random\\nitem which has a 50% chance of being in the vocabulary .\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; from timeit import Timer\\n&gt;&gt;&gt; vocab_size = 100000\\n&gt;&gt;&gt; setup_list = \"import random; vocab = range(%d)\" % vocab_size \\n&gt;&gt;&gt; setup_set = \"import random; vocab = set(range(%d))\" % vocab_size \\n&gt;&gt;&gt; statement = \"random.randint(0, %d) in vocab\" % (vocab_size * 2) \\n&gt;&gt;&gt; print(Timer(statement, setup_list).timeit(1000))\\n2.78092288971\\n&gt;&gt;&gt; print(Timer(statement, setup_set).timeit(1000))\\n0.0037260055542\\n\\n\\n\\nPerforming 1000 list membership tests takes a total of 2.8 seconds,\\nwhile the equivalent tests on a set take a mere 0.0037 seconds,\\nor three orders of magnitude faster!\\n\\n\\nDynamic Programming\\nDynamic programming is a general technique for designing algorithms\\nwhich is widely used in natural language processing.  The term\\n\\'programming\\' is used in a different sense to what you might expect,\\nto mean planning or scheduling.  Dynamic programming is used when a\\nproblem contains overlapping sub-problems.  Instead of computing\\nsolutions to these sub-problems repeatedly, we simply store them in a\\nlookup table.\\nIn the remainder of this section we will introduce dynamic programming,\\nbut in a rather different context to syntactic parsing.\\nPingala was an Indian author who lived around the 5th century B.C.,\\nand wrote a treatise on Sanskrit prosody called the Chandas Shastra.\\nVirahanka extended this work around the 6th century A.D., studying the\\nnumber of ways of combining short and long syllables to create a meter\\nof length n.  Short syllables, marked S, take up one unit of length, while\\nlong syllables, marked L, take two.\\nPingala found, for example, that there are five ways to\\nconstruct a meter of length 4: V4 = {LL, SSL, SLS,\\nLSS, SSSS}.  Observe that we can split V4 into two\\nsubsets, those starting with L and those starting with\\nS, as shown in (1).\\n\\n  (1)V4 =\\n  LL, LSS\\n    i.e. L prefixed to each item of V2 = {L, SS}\\n  SSL, SLS, SSSS\\n    i.e. S prefixed to each item of V3 = {SL, LS, SSS}\\n\\n\\n\\n\\n\\n\\n&nbsp;\\ndef virahanka1(n):\\n    if n == 0:\\n        return [\"\"]\\n    elif n == 1:\\n        return [\"S\"]\\n    else:\\n        s = [\"S\" + prosody for prosody in virahanka1(n-1)]\\n        l = [\"L\" + prosody for prosody in virahanka1(n-2)]\\n        return s + l\\n\\ndef virahanka2(n):\\n    lookup = [[\"\"], [\"S\"]]\\n    for i in range(n-1):\\n        s = [\"S\" + prosody for prosody in lookup[i+1]]\\n        l = [\"L\" + prosody for prosody in lookup[i]]\\n        lookup.append(s + l)\\n    return lookup[n]\\n\\ndef virahanka3(n, lookup={0:[\"\"], 1:[\"S\"]}):\\n    if n not in lookup:\\n        s = [\"S\" + prosody for prosody in virahanka3(n-1)]\\n        l = [\"L\" + prosody for prosody in virahanka3(n-2)]\\n        lookup[n] = s + l\\n    return lookup[n]\\n\\nfrom nltk import memoize\\n@memoize\\ndef virahanka4(n):\\n    if n == 0:\\n        return [\"\"]\\n    elif n == 1:\\n        return [\"S\"]\\n    else:\\n        s = [\"S\" + prosody for prosody in virahanka4(n-1)]\\n        l = [\"L\" + prosody for prosody in virahanka4(n-2)]\\n        return s + l\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; virahanka1(4)\\n[\\'SSSS\\', \\'SSL\\', \\'SLS\\', \\'LSS\\', \\'LL\\']\\n&gt;&gt;&gt; virahanka2(4)\\n[\\'SSSS\\', \\'SSL\\', \\'SLS\\', \\'LSS\\', \\'LL\\']\\n&gt;&gt;&gt; virahanka3(4)\\n[\\'SSSS\\', \\'SSL\\', \\'SLS\\', \\'LSS\\', \\'LL\\']\\n&gt;&gt;&gt; virahanka4(4)\\n[\\'SSSS\\', \\'SSL\\', \\'SLS\\', \\'LSS\\', \\'LL\\']\\n\\n\\nExample 4.12 (code_virahanka.py): Figure 4.12: Four Ways to Compute Sanskrit Meter: (i) recursive; (ii) bottom-up dynamic programming;\\n(iii) top-down dynamic programming; and (iv) built-in memoization.\\n\\nWith this observation, we can write a little recursive function called\\nvirahanka1() to compute these meters, shown in 4.12.\\nNotice that, in order to compute V4 we first compute\\nV3 and V2.  But to compute V3,\\nwe need to first compute V2 and V1.  This call\\nstructure is depicted in (2).\\n\\n  (2)\\nAs you can see, V2 is computed twice.\\nThis might not seem like a significant problem, but\\nit turns out to be rather wasteful as n gets large:\\nto compute V20 using this recursive technique, we\\nwould compute V2 4,181 times;\\nand for V40 we would compute V2 63,245,986 times!\\nA much better alternative is to store the value of V2 in a table\\nand look it up whenever we need it.  The same goes for other values, such\\nas V3 and so on.  Function virahanka2() implements a\\ndynamic programming approach to the problem.  It works by filling up a\\ntable (called lookup) with solutions to all smaller instances of the\\nproblem, stopping as soon as we reach the value we\\'re interested in.\\nAt this point we read off the value and return it.  Crucially, each\\nsub-problem is only ever solved once.\\nNotice that the approach taken in virahanka2() is to solve smaller\\nproblems on the way to solving larger problems.  Accordingly, this is known as the\\nbottom-up approach to dynamic programming.  Unfortunately it turns out\\nto be quite wasteful for some applications, since it\\nmay compute solutions to sub-problems that are never required for\\nsolving the main problem.  This wasted computation can be avoided\\nusing the top-down approach to dynamic programming, which is\\nillustrated in the function virahanka3() in 4.12.\\nUnlike the bottom-up approach, this approach is recursive.  It avoids\\nthe huge wastage of virahanka1() by checking whether it has\\npreviously stored the result.  If not, it computes the result\\nrecursively and stores it in the table.  The last step is to return\\nthe stored result.  The final method, in virahanka4(),\\nis to use a Python \"decorator\" called memoize,\\nwhich takes care of the housekeeping work done\\nby virahanka3() without cluttering up the program.\\nThis \"memoization\" process stores the result of each previous\\ncall to the function along with the parameters that were used.\\nIf the function is subsequently called with the same parameters,\\nit returns the stored result instead of recalculating it.\\n(This aspect of Python syntax is beyond the scope of this book.)\\nThis concludes our brief introduction to dynamic programming.\\nWe will encounter it again in 4.\\n\\n\\n\\n4.8&nbsp;&nbsp;&nbsp;A Sample of Python Libraries\\nPython has hundreds of third-party libraries, specialized software packages that extend\\nthe functionality of Python.  NLTK is one such library.  To realize the full power\\nof Python programming, you should become familiar with several other libraries.\\nMost of these will need to be manually installed on your computer.\\n\\nMatplotlib\\nPython has some libraries that are useful for visualizing language data.\\nThe Matplotlib package supports sophisticated\\nplotting functions with a MATLAB-style interface, and is available from\\nhttp://matplotlib.sourceforge.net/.\\nSo far we have focused on textual presentation and the use of formatted print\\nstatements to get output lined up in columns.  It is often very useful to display\\nnumerical data in graphical form, since this often makes it easier to detect\\npatterns.  For example, in 3.7 we saw a table of numbers\\nshowing the frequency of particular modal verbs in the Brown Corpus, classified\\nby genre.  The program in 4.13 presents the same information in graphical\\nformat.  The output is shown in 4.14 (a color figure in the graphical display).\\n\\n\\n\\n\\n&nbsp;\\nfrom numpy import arange\\nfrom matplotlib import pyplot\\n\\ncolors = \\'rgbcmyk\\' # red, green, blue, cyan, magenta, yellow, black\\n\\ndef bar_chart(categories, words, counts):\\n    \"Plot a bar chart showing counts for each word by category\"\\n    ind = arange(len(words))\\n    width = 1 / (len(categories) + 1)\\n    bar_groups = []\\n    for c in range(len(categories)):\\n        bars = pyplot.bar(ind+c*width, counts[categories[c]], width,\\n                         color=colors[c % len(colors)])\\n        bar_groups.append(bars)\\n    pyplot.xticks(ind+width, words)\\n    pyplot.legend([b[0] for b in bar_groups], categories, loc=\\'upper left\\')\\n    pyplot.ylabel(\\'Frequency\\')\\n    pyplot.title(\\'Frequency of Six Modal Verbs by Genre\\')\\n    pyplot.show()\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; genres = [\\'news\\', \\'religion\\', \\'hobbies\\', \\'government\\', \\'adventure\\']\\n&gt;&gt;&gt; modals = [\\'can\\', \\'could\\', \\'may\\', \\'might\\', \\'must\\', \\'will\\']\\n&gt;&gt;&gt; cfdist = nltk.ConditionalFreqDist(\\n...              (genre, word)\\n...              for genre in genres\\n...              for word in nltk.corpus.brown.words(categories=genre)\\n...              if word in modals)\\n...\\n&gt;&gt;&gt; counts = {}\\n&gt;&gt;&gt; for genre in genres:\\n...     counts[genre] = [cfdist[genre][word] for word in modals]\\n&gt;&gt;&gt; bar_chart(genres, modals, counts)\\n\\n\\nExample 4.13 (code_modal_plot.py): Figure 4.13: Frequency of Modals in Different Sections of the Brown Corpus\\n\\n\\n\\nFigure 4.14: Bar Chart Showing Frequency of Modals in Different Sections of Brown Corpus: this\\nvisualization was produced by the program in 4.13.\\n\\n<!-- def count_words_by_tag(t, genres):\\n    cfdist = nltk.ConditionalFreqDist()\\n    for genre in genres:\\n        for (word,tag) in nltk.corpus.brown.tagged_words(categories=genre):\\n            if tag == t:\\n                 cfdist[genre][word.lower()] += 1\\n    return cfdist -->\\nFrom the bar chart it is immediately obvious that may and must have\\nalmost identical relative frequencies.  The same goes for could and might.\\nIt is also possible to generate such data visualizations on the fly.\\nFor example, a web page with form input could permit visitors to specify\\nsearch parameters, submit the form, and see a dynamically generated\\nvisualization.\\nTo do this we have to specify the Agg backend for matplotlib,\\nwhich is a library for producing raster (pixel) images .\\nNext, we use all the same Matplotlib methods as before, but instead of displaying the\\nresult on a graphical terminal using pyplot.show(), we save it to a file\\nusing pyplot.savefig() .  We specify the filename\\nthen print HTML markup that directs the web browser to load the file.\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; from matplotlib import use, pyplot\\n&gt;&gt;&gt; use(\\'Agg\\') \\n&gt;&gt;&gt; pyplot.savefig(\\'modals.png\\') \\n&gt;&gt;&gt; print(\\'Content-Type: text/html\\')\\n&gt;&gt;&gt; print()\\n&gt;&gt;&gt; print(\\'&lt;html&gt;&lt;body&gt;\\')\\n&gt;&gt;&gt; print(\\'&lt;img src=\"modals.png\"/&gt;\\')\\n&gt;&gt;&gt; print(\\'&lt;/body&gt;&lt;/html&gt;\\')\\n\\n\\n\\n\\n\\nNetworkX\\nThe NetworkX package is for defining and manipulating structures consisting of\\nnodes and edges, known as graphs.  It is\\navailable from https://networkx.lanl.gov/.\\nNetworkX can be used in conjunction with Matplotlib to\\nvisualize networks, such as WordNet (the semantic network we\\nintroduced in 5).  The program in 4.15\\ninitializes an empty graph  then traverses\\nthe WordNet hypernym hierarchy adding edges to\\nthe graph .\\nNotice that the traversal is recursive ,\\napplying the programming technique discussed in\\n4.7.  The resulting display is shown in 4.16.\\n\\n\\n\\n\\n&nbsp;\\nimport networkx as nx\\nimport matplotlib\\nfrom nltk.corpus import wordnet as wn\\n\\ndef traverse(graph, start, node):\\n    graph.depth[node.name] = node.shortest_path_distance(start)\\n    for child in node.hyponyms():\\n        graph.add_edge(node.name, child.name) \\n        traverse(graph, start, child) \\n\\ndef hyponym_graph(start):\\n    G = nx.Graph() \\n    G.depth = {}\\n    traverse(G, start, start)\\n    return G\\n\\ndef graph_draw(graph):\\n    nx.draw_graphviz(graph,\\n         node_size = [16 * graph.degree(n) for n in graph],\\n         node_color = [graph.depth[n] for n in graph],\\n         with_labels = False)\\n    matplotlib.pyplot.show()\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; dog = wn.synset(\\'dog.n.01\\')\\n&gt;&gt;&gt; graph = hyponym_graph(dog)\\n&gt;&gt;&gt; graph_draw(graph)\\n\\n\\nExample 4.15 (code_networkx.py): Figure 4.15: Using the NetworkX and Matplotlib Libraries\\n\\n\\n\\nFigure 4.16: Visualization with NetworkX and Matplotlib: Part of the WordNet hypernym\\nhierarchy is displayed, starting with dog.n.01 (the darkest node in the middle);\\nnode size is based on the number of children of the node, and color is based on\\nthe distance of the node from dog.n.01; this visualization was produced\\nby the program in 4.15.\\n\\n\\n\\ncsv\\nLanguage analysis work often involves data tabulations, containing information\\nabout lexical items, or the participants in an empirical study, or the linguistic\\nfeatures extracted from a corpus.  Here\\'s a fragment of a simple lexicon, in CSV format:\\n\\nsleep, sli:p, v.i, a condition of body and mind ...\\nwalk, wo:k, v.intr, progress by lifting and setting down each foot ...\\nwake, weik, intrans, cease to sleep\\n\\nWe can use Python\\'s CSV library to read and write files stored in this format.\\nFor example, we can open a CSV file called lexicon.csv \\nand iterate over its rows :\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; import csv\\n&gt;&gt;&gt; input_file = open(\"lexicon.csv\", \"rb\") \\n&gt;&gt;&gt; for row in csv.reader(input_file): \\n...     print(row)\\n[\\'sleep\\', \\'sli:p\\', \\'v.i\\', \\'a condition of body and mind ...\\']\\n[\\'walk\\', \\'wo:k\\', \\'v.intr\\', \\'progress by lifting and setting down each foot ...\\']\\n[\\'wake\\', \\'weik\\', \\'intrans\\', \\'cease to sleep\\']\\n\\n\\n\\nEach row is just a list of strings.  If any fields contain numerical\\ndata, they will appear as strings, and will have to be converted using\\nint() or float().\\n\\n\\nNumPy\\nThe NumPy package provides substantial support for numerical processing in Python.\\nNumPy has a multi-dimensional array object, which is easy to initialize and access:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; from numpy import array\\n&gt;&gt;&gt; cube = array([ [[0,0,0], [1,1,1], [2,2,2]],\\n...                [[3,3,3], [4,4,4], [5,5,5]],\\n...                [[6,6,6], [7,7,7], [8,8,8]] ])\\n&gt;&gt;&gt; cube[1,1,1]\\n4\\n&gt;&gt;&gt; cube[2].transpose()\\narray([[6, 7, 8],\\n       [6, 7, 8],\\n       [6, 7, 8]])\\n&gt;&gt;&gt; cube[2,1:]\\narray([[7, 7, 7],\\n       [8, 8, 8]])\\n\\n\\n\\nNumPy includes linear algebra functions.  Here we perform\\nsingular value decomposition on a matrix, an operation used\\nin latent semantic analysis to help identify implicit\\nconcepts in a document collection.\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; from numpy import linalg\\n&gt;&gt;&gt; a=array([[4,0], [3,-5]])\\n&gt;&gt;&gt; u,s,vt = linalg.svd(a)\\n&gt;&gt;&gt; u\\narray([[-0.4472136 , -0.89442719],\\n       [-0.89442719,  0.4472136 ]])\\n&gt;&gt;&gt; s\\narray([ 6.32455532,  3.16227766])\\n&gt;&gt;&gt; vt\\narray([[-0.70710678,  0.70710678],\\n       [-0.70710678, -0.70710678]])\\n\\n\\n\\nNLTK\\'s clustering package nltk.cluster makes extensive use of NumPy arrays,\\nand includes support for k-means clustering, Gaussian EM clustering,\\ngroup average agglomerative clustering, and dendrogram plots.\\nFor details, type help(nltk.cluster).\\n\\n\\nOther Python Libraries\\nThere are many other Python libraries, and you can search for them with the\\nhelp of the Python Package Index http://pypi.python.org/.\\nMany libraries provide an interface to external software, such\\nas relational databases (e.g. mysql-python)\\nand large document collections (e.g. PyLucene).\\nMany other libraries give access to file formats\\nsuch as PDF, MSWord, and XML (pypdf, pywin32, xml.etree),\\nRSS feeds (e.g. feedparser),\\nand electronic mail (e.g. imaplib, email).\\n\\n\\n\\n4.9&nbsp;&nbsp;&nbsp;Summary\\n\\nPython\\'s assignment and parameter passing use object references;\\ne.g. if a is a list and we assign b = a, then any operation\\non a will modify b, and vice versa.\\nThe is operation tests if two objects are identical internal objects,\\nwhile == tests if two objects are equivalent.  This distinction\\nparallels the type-token distinction.\\nStrings, lists and tuples are different kinds of sequence object, supporting\\ncommon operations such as indexing, slicing, len(), sorted(), and\\nmembership testing using in.\\nA declarative programming style usually produces more compact,\\nreadable code; manually-incremented loop variables are usually\\nunnecessary; when a sequence must be enumerated, use enumerate().\\nFunctions are an essential programming abstraction: key concepts\\nto understand are parameter passing, variable scope, and docstrings.\\nA function serves as a namespace: names defined inside a function are not visible\\noutside that function, unless those names are declared to be global.\\nModules permit logically-related material to be localized in a file.\\nA module serves as a namespace: names defined in a module — such as variables\\nand functions — are not visible to other modules, unless those names are imported.\\nDynamic programming is an algorithm design technique used widely in NLP\\nthat stores the results of previous computations in order to avoid\\nunnecessary recomputation.\\n\\n\\n\\n4.10&nbsp;&nbsp;&nbsp;Further Reading\\nThis chapter has touched on many topics in programming, some specific to Python,\\nand some quite general.  We\\'ve just scratched the surface, and you may want\\nto read more about these topics, starting with the further materials for\\nthis chapter available at http://nltk.org/.\\nThe Python website provides extensive documentation.  It is important to\\nunderstand the built-in functions and standard types, described at\\nhttp://docs.python.org/library/functions.html and\\nhttp://docs.python.org/library/stdtypes.html.\\nWe have learnt about generators and their importance for efficiency;\\nfor information about iterators, a closely related topic,\\nsee http://docs.python.org/library/itertools.html.\\nConsult your favorite Python book for more information on such topics.\\nAn excellent resource for using Python for multimedia processing,\\nincluding working with sound files, is (Guzdial, 2005).\\nWhen using the online Python documentation, be aware that\\nyour installed version might be different from the version\\nof the documentation you are reading.  You can easily\\ncheck what version you have, with import sys; sys.version.\\nVersion-specific documentation is available at\\nhttp://www.python.org/doc/versions/.\\nAlgorithm design is a rich field within computer science.  Some\\ngood starting points are (Harel, 2004), (Levitin, 2004), (Knuth, 2006).\\nUseful guidance on the practice of software development is provided\\nin (Hunt &amp; Thomas, 2000) and (McConnell, 2004).\\n\\n\\n4.11&nbsp;&nbsp;&nbsp;Exercises\\n\\n☼ Find out more about sequence objects using Python\\'s help facility.\\nIn the interpreter, type help(str), help(list), and help(tuple).\\nThis will give you a full list of the functions supported by each type.\\nSome functions have special names flanked with underscore; as the\\nhelp documentation shows, each such function corresponds to something\\nmore familiar.  For example x.__getitem__(y) is just a long-winded\\nway of saying x[y].\\n\\n☼ Identify three operations that can be performed on both tuples\\nand lists.  Identify three list operations that cannot be performed on\\ntuples.  Name a context where using a list instead of a tuple generates\\na Python error.\\n\\n☼ Find out how to create a tuple consisting of a single item.\\nThere are at least two ways to do this.\\n\\n☼ Create a list words = [\\'is\\', \\'NLP\\', \\'fun\\', \\'?\\'].  Use\\na series of assignment statements (e.g. words[1] = words[2])\\nand a temporary variable tmp to transform this list into the\\nlist [\\'NLP\\', \\'is\\', \\'fun\\', \\'!\\'].  Now do the same transformation\\nusing tuple assignment.\\n\\n☼ Read about the built-in comparison function cmp, by\\ntyping help(cmp).  How does it differ in behavior from\\nthe comparison operators?\\n\\n☼ Does the method for creating a sliding window of n-grams\\nbehave correctly for the two limiting cases: n = 1, and n = len(sent)?\\n\\n☼ We pointed out that when empty strings and empty lists occur\\nin the condition part of an if clause, they evaluate to\\nFalse. In this case, they are said to be occurring in a\\nBoolean context.\\nExperiment with different kind of non-Boolean expressions in Boolean\\ncontexts, and see whether they evaluate as True or False.\\n\\n☼ Use the inequality operators to compare strings, e.g.\\n\\'Monty\\' &lt; \\'Python\\'.  What happens when you do \\'Z\\' &lt; \\'a\\'?\\nTry pairs of strings which have a common prefix, e.g. \\'Monty\\' &lt; \\'Montague\\'.\\nRead up on \"lexicographical sort\" in order to understand what is\\ngoing on here.  Try comparing structured objects, e.g.\\n(\\'Monty\\', 1) &lt; (\\'Monty\\', 2).  Does this behave as expected?\\n\\n☼ Write code that removes whitespace at the beginning and end of a\\nstring, and normalizes whitespace between words to be a single\\nspace character.\\n\\ndo this task using split() and join()\\ndo this task using regular expression substitutions\\n\\n\\n☼ Write a program to sort words by length.  Define a helper function\\ncmp_len which uses the cmp comparison function on word lengths.\\n\\n◑ Create a list of words and store it in a variable sent1.\\nNow assign sent2 = sent1.  Modify one of the items in sent1\\nand verify that sent2 has changed.\\n\\nNow try the same exercise but instead assign sent2 = sent1[:].\\nModify sent1 again and see what happens to sent2.  Explain.\\nNow define text1 to be a list of lists of strings (e.g. to\\nrepresent a text consisting of multiple sentences.  Now assign\\ntext2 = text1[:], assign a new value to one of the words,\\ne.g. text1[1][1] = \\'Monty\\'.  Check what this did to text2.\\nExplain.\\nLoad Python\\'s deepcopy() function (i.e. from copy import deepcopy),\\nconsult its documentation, and test that it makes a fresh copy of any\\nobject.\\n\\n\\n◑ Initialize an n-by-m list of lists of empty strings using list\\nmultiplication, e.g. word_table = [[\\'\\'] * n] * m.  What happens\\nwhen you set one of its values, e.g. word_table[1][2] = \"hello\"?\\nExplain why this happens.  Now write an expression using range()\\nto construct a list of lists, and show that it does not have this problem.\\n\\n◑ Write code to initialize a two-dimensional array of sets called\\nword_vowels and process a list of words, adding each\\nword to word_vowels[l][v] where l is the length of the word and v is\\nthe number of vowels it contains.\\n\\n◑ Write a function novel10(text) that prints any word that\\nappeared in the last 10% of a text that had not been encountered earlier.\\n\\n◑ Write a program that takes a sentence expressed as a single string,\\nsplits it and counts up the words.  Get it to print out each word and the\\nword\\'s frequency, one per line, in alphabetical order.\\n\\n◑ Read up on Gematria, a method for assigning numbers to words, and for\\nmapping between words having the same number to discover the hidden meaning of\\ntexts (http://en.wikipedia.org/wiki/Gematria, http://essenes.net/gemcal.htm).\\n\\nWrite a function gematria() that sums the numerical values of\\nthe letters of a word, according to the letter values in letter_vals:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; letter_vals = {\\'a\\':1, \\'b\\':2, \\'c\\':3, \\'d\\':4, \\'e\\':5, \\'f\\':80, \\'g\\':3, \\'h\\':8,\\n... \\'i\\':10, \\'j\\':10, \\'k\\':20, \\'l\\':30, \\'m\\':40, \\'n\\':50, \\'o\\':70, \\'p\\':80, \\'q\\':100,\\n... \\'r\\':200, \\'s\\':300, \\'t\\':400, \\'u\\':6, \\'v\\':6, \\'w\\':800, \\'x\\':60, \\'y\\':10, \\'z\\':7}\\n\\n\\n\\n\\nProcess a corpus (e.g. nltk.corpus.state_union) and for each document, count how\\nmany of its words have the number 666.\\n\\nWrite a function decode() to process a text, randomly replacing words with\\ntheir Gematria equivalents, in order to discover the \"hidden meaning\" of the text.\\n\\n\\n\\n◑ Write a function shorten(text, n) to process a text, omitting the n\\nmost frequently occurring words of the text.  How readable is it?\\n\\n◑ Write code to print out an index for a lexicon, allowing someone\\nto look up words according to their meanings (or pronunciations; whatever\\nproperties are contained in lexical entries).\\n\\n◑ Write a list comprehension that sorts a list of WordNet synsets for\\nproximity to a given synset.  For example, given the synsets\\nminke_whale.n.01, orca.n.01, novel.n.01, and tortoise.n.01,\\nsort them according to their shortest_path_distance() from right_whale.n.01.\\n\\n◑ Write a function that takes a list of words (containing duplicates) and\\nreturns a list of words (with no duplicates) sorted by decreasing frequency.\\nE.g. if the input list contained 10 instances of the word table and 9 instances\\nof the word chair, then table would appear before chair in the output\\nlist.\\n\\n◑ Write a function that takes a text and a vocabulary as its arguments\\nand returns the set of words that appear in the text but not in the\\nvocabulary.  Both arguments can be represented as lists of strings.\\nCan you do this in a single line, using set.difference()?\\n\\n◑ Import the itemgetter() function from the operator module in Python\\'s\\nstandard library (i.e. from operator import itemgetter).  Create a list\\nwords containing several words.  Now try calling:\\nsorted(words, key=itemgetter(1)), and sorted(words, key=itemgetter(-1)).\\nExplain what itemgetter() is doing.\\n\\n◑ Write a recursive function lookup(trie, key) that looks up a key in a trie,\\nand returns the value it finds.  Extend the function to return a word when it is uniquely\\ndetermined by its prefix (e.g. vanguard is the only word that starts with vang-,\\nso lookup(trie, \\'vang\\') should return the same thing as lookup(trie, \\'vanguard\\')).\\n\\n◑ Read up on \"keyword linkage\" (chapter 5 of (Scott &amp; Tribble, 2006)).  Extract keywords from\\nNLTK\\'s Shakespeare Corpus and using the NetworkX package, plot keyword linkage networks.\\n\\n◑ Read about string edit distance and the Levenshtein Algorithm.\\nTry the implementation provided in nltk.edit_distance().\\nIn what way is this using dynamic programming?  Does it use the bottom-up or\\ntop-down approach?\\n[See also http://norvig.com/spell-correct.html]\\n\\n◑ The Catalan numbers arise in many applications of combinatorial mathematics,\\nincluding the counting of parse trees (6).  The series\\ncan be defined as follows: C0 = 1, and\\nCn+1 = Σ0..n (CiCn-i).\\n\\nWrite a recursive function to compute nth Catalan number Cn.\\nNow write another function that does this computation using dynamic programming.\\nUse the timeit module to compare the performance of these functions as n\\nincreases.\\n\\n\\n★\\nReproduce some of the results of (Zhao &amp; Zobel, 2007) concerning authorship identification.\\n\\n★ Study gender-specific lexical choice, and see if you can\\nreproduce some of the results of http://www.clintoneast.com/articles/words.php\\n\\n★ Write a recursive function that pretty prints a trie in alphabetically\\nsorted order, e.g.:\\nchair: \\'flesh\\'\\n---t: \\'cat\\'\\n--ic: \\'stylish\\'\\n---en: \\'dog\\'\\n\\n\\n★ With the help of the trie data structure, write a recursive\\nfunction that processes text, locating the uniqueness point in\\neach word, and discarding the remainder of each word.  How much compression does this\\ngive?  How readable is the resulting text?\\n\\n★ Obtain some raw text, in the form of a single, long string.\\nUse Python\\'s textwrap module to break it up into multiple lines.\\nNow write code to add extra spaces between words, in order to justify\\nthe output.  Each line must have the same width, and spaces must be\\napproximately evenly distributed across each line.  No line can\\nbegin or end with a space.\\n\\n★ Develop a simple extractive summarization tool, that prints the\\nsentences of a document which contain the highest total word\\nfrequency.  Use FreqDist() to count word frequencies, and use\\nsum to sum the frequencies of the words in each sentence.\\nRank the sentences according to their score.  Finally, print the n\\nhighest-scoring sentences in document order.  Carefully review the\\ndesign of your program, especially your approach to this double\\nsorting.  Make sure the program is written as clearly as possible.\\n\\n★\\nRead the following article on semantic orientation of adjectives.\\nUse the NetworkX package to visualize\\na network of adjectives with edges to indicate same vs different\\nsemantic orientation.  http://www.aclweb.org/anthology/P97-1023\\n\\n★\\nDesign an algorithm to find the \"statistically improbable\\nphrases\" of a document collection.\\nhttp://www.amazon.com/gp/search-inside/sipshelp.html\\n\\n★ Write a program to implement a brute-force algorithm for\\ndiscovering word squares, a kind of n × n crossword\\nin which the entry in the nth row is the same as the entry\\nin the nth column.  For discussion, see\\nhttp://itre.cis.upenn.edu/~myl/languagelog/archives/002679.html\\n\\n\\n<!-- #. |hard| Extend the program in Example compound-keys_ in the following ways:\\n\\na) Define two sets ``verbs`` and ``preps``, and add each verb and preposition\\n   as they are encountered.  (Note that you can add an item to a set without\\n   bothering to check whether it is already present.)\\n\\nb) Create nested loops to display the results, iterating over verbs and\\n   prepositions in sorted order.  Generate one line of output per verb,\\n   listing prepositions and attachment ratios as follows:\\n   ``raised: about 0:3, at 1:0, by 9:0, for 3:6, from 5:0, in 5:5...``\\n\\nc) We used a tuple to represent a compound key consisting of two strings.\\n   However, we could have simply concatenated the strings, e.g.\\n   ``key = verb + \":\" + prep``, resulting in a simple string key.\\n   Why is it better to use tuples for compound keys? -->\\n\\n\\nAbout this document...\\nUPDATED FOR NLTK 3.0.\\nThis is a chapter from Natural Language Processing with Python,\\nby Steven Bird, Ewan Klein and Edward Loper,\\nCopyright © 2019 the authors.\\nIt is distributed with the Natural Language Toolkit [http://nltk.org/],\\nVersion 3.0, under the terms of the\\nCreative Commons Attribution-Noncommercial-No Derivative Works 3.0 United States License\\n[http://creativecommons.org/licenses/by-nc-nd/3.0/us/].\\nThis document was built on\\nWed  4 Sep 2019 11:40:48 ACST\\n\\n\\n\\n\\n\\n\\n',\n",
       " '\\n\\n\\n\\nfunction astext(node)\\n{\\n    return node.innerHTML.replace(/(]+)>)/ig,\"\")\\n                         .replace(/&gt;/ig, \">\")\\n                         .replace(/&lt;/ig, \"<\")\\n                         .replace(/&quot;/ig, \\'\"\\')\\n                         .replace(/&amp;/ig, \"&\");\\n}\\n\\nfunction copy_notify(node, bar_color, data)\\n{\\n    // The outer box: relative + inline positioning.\\n    var box1 = document.createElement(\"div\");\\n    box1.style.position = \"relative\";\\n    box1.style.display = \"inline\";\\n    box1.style.top = \"2em\";\\n    box1.style.left = \"1em\";\\n  \\n    // A shadow for fun\\n    var shadow = document.createElement(\"div\");\\n    shadow.style.position = \"absolute\";\\n    shadow.style.left = \"-1.3em\";\\n    shadow.style.top = \"-1.3em\";\\n    shadow.style.background = \"#404040\";\\n    \\n    // The inner box: absolute positioning.\\n    var box2 = document.createElement(\"div\");\\n    box2.style.position = \"relative\";\\n    box2.style.border = \"1px solid #a0a0a0\";\\n    box2.style.left = \"-.2em\";\\n    box2.style.top = \"-.2em\";\\n    box2.style.background = \"white\";\\n    box2.style.padding = \".3em .4em .3em .4em\";\\n    box2.style.fontStyle = \"normal\";\\n    box2.style.background = \"#f0e0e0\";\\n\\n    node.insertBefore(box1, node.childNodes.item(0));\\n    box1.appendChild(shadow);\\n    shadow.appendChild(box2);\\n    box2.innerHTML=\"Copied&nbsp;to&nbsp;the&nbsp;clipboard: \" +\\n                   \"\"+\\n                   data+\"\";\\n    setTimeout(function() { node.removeChild(box1); }, 1000);\\n\\n    var elt = node.parentNode.firstChild;\\n    elt.style.background = \"#ffc0c0\";\\n    setTimeout(function() { elt.style.background = bar_color; }, 200);\\n}\\n\\nfunction copy_codeblock_to_clipboard(node)\\n{\\n    var data = astext(node)+\"\\\\n\";\\n    if (copy_text_to_clipboard(data)) {\\n        copy_notify(node, \"#40a060\", data);\\n    }\\n}\\n\\nfunction copy_doctest_to_clipboard(node)\\n{\\n    var s = astext(node)+\"\\\\n   \";\\n    var data = \"\";\\n\\n    var start = 0;\\n    var end = s.indexOf(\"\\\\n\");\\n    while (end >= 0) {\\n        if (s.substring(start, start+4) == \">>> \") {\\n            data += s.substring(start+4, end+1);\\n        }\\n        else if (s.substring(start, start+4) == \"... \") {\\n            data += s.substring(start+4, end+1);\\n        }\\n        /*\\n        else if (end-start > 1) {\\n            data += \"# \" + s.substring(start, end+1);\\n        }*/\\n        // Grab the next line.\\n        start = end+1;\\n        end = s.indexOf(\"\\\\n\", start);\\n    }\\n    \\n    if (copy_text_to_clipboard(data)) {\\n        copy_notify(node, \"#4060a0\", data);\\n    }\\n}\\n    \\nfunction copy_text_to_clipboard(data)\\n{\\n    if (window.clipboardData) {\\n        window.clipboardData.setData(\"Text\", data);\\n        return true;\\n     }\\n    else if (window.netscape) {\\n        // w/ default firefox settings, permission will be denied for this:\\n        netscape.security.PrivilegeManager\\n                      .enablePrivilege(\"UniversalXPConnect\");\\n    \\n        var clip = Components.classes[\"@mozilla.org/widget/clipboard;1\"]\\n                      .createInstance(Components.interfaces.nsIClipboard);\\n        if (!clip) return;\\n    \\n        var trans = Components.classes[\"@mozilla.org/widget/transferable;1\"]\\n                       .createInstance(Components.interfaces.nsITransferable);\\n        if (!trans) return;\\n    \\n        trans.addDataFlavor(\"text/unicode\");\\n    \\n        var str = new Object();\\n        var len = new Object();\\n    \\n        var str = Components.classes[\"@mozilla.org/supports-string;1\"]\\n                     .createInstance(Components.interfaces.nsISupportsString);\\n        var datacopy=data;\\n        str.data=datacopy;\\n        trans.setTransferData(\"text/unicode\",str,datacopy.length*2);\\n        var clipid=Components.interfaces.nsIClipboard;\\n    \\n        if (!clip) return false;\\n    \\n        clip.setData(trans,null,clipid.kGlobalClipboard);\\n        return true;\\n    }\\n    return false;\\n}\\n//-->\\n\\n\\n\\n\\n\\n\\n5. Categorizing and Tagging Words\\n\\n\\n/*\\n:Author: Edward Loper, James Curran\\n:Copyright: This stylesheet has been placed in the public domain.\\n\\nStylesheet for use with Docutils.\\n\\nThis stylesheet defines new css classes used by NLTK.\\n\\nIt uses a Python syntax highlighting scheme that matches\\nthe colour scheme used by IDLE, which makes it easier for\\nbeginners to check they are typing things in correctly.\\n*/\\n\\n/* Include the standard docutils stylesheet. */\\n@import url(default.css);\\n\\n/* Custom inline roles */\\nspan.placeholder    { font-style: italic; font-family: monospace; }\\nspan.example        { font-style: italic; }\\nspan.emphasis       { font-style: italic; }\\nspan.termdef        { font-weight: bold; }\\n/*span.term           { font-style: italic; }*/\\nspan.category       { font-variant: small-caps; }\\nspan.feature        { font-variant: small-caps; }\\nspan.fval           { font-style: italic; }\\nspan.math           { font-style: italic; }\\nspan.mathit         { font-style: italic; }\\nspan.lex            { font-variant: small-caps; }\\nspan.guide-linecount{ text-align: right; display: block;}\\n\\n/* Python souce code listings */\\nspan.pysrc-prompt   { color: #9b0000; }\\nspan.pysrc-more     { color: #9b00ff; }\\nspan.pysrc-keyword  { color: #e06000; }\\nspan.pysrc-builtin  { color: #940094; }\\nspan.pysrc-string   { color: #00aa00; }\\nspan.pysrc-comment  { color: #ff0000; }\\nspan.pysrc-output   { color: #0000ff; }\\nspan.pysrc-except   { color: #ff0000; }\\nspan.pysrc-defname  { color: #008080; }\\n\\n\\n/* Doctest blocks */\\npre.doctest         { margin: 0; padding: 0; font-weight: bold; }\\ndiv.doctest         { margin: 0 1em 1em 1em; padding: 0; }\\ntable.doctest       { margin: 0; padding: 0;\\n                      border-top: 1px solid gray;\\n                      border-bottom: 1px solid gray; }\\npre.copy-notify     { margin: 0; padding: 0.2em; font-weight: bold;\\n                      background-color: #ffffff; }\\n\\n/* Python source listings */\\ndiv.pylisting       { margin: 0 1em 1em 1em; padding: 0; }\\ntable.pylisting     { margin: 0; padding: 0;\\n                      border-top: 1px solid gray; }\\ntd.caption { border-top: 1px solid black; margin: 0; padding: 0; }\\n.caption-label { font-weight: bold;  }\\ntd.caption p { margin: 0; padding: 0; font-style: normal;}\\n\\ntable tr td.codeblock { \\n  padding: 0.2em ! important; margin: 0;\\n  border-left: 1px solid gray;\\n  border-right: 2px solid gray;\\n  border-top: 0px solid gray;\\n  border-bottom: 1px solid gray;\\n  font-weight: bold; background-color: #eeffee;\\n}\\ntable pre span {\\n  white-space: pre-wrap;\\n}\\ntable tr td.doctest  { \\n  padding: 0.2em; margin: 0;\\n  border-left: 1px solid gray;\\n  border-right: 2px solid gray;\\n  border-top: 0px solid gray;\\n  border-bottom: 1px solid gray;\\n  font-weight: bold; background-color: #eeeeff;\\n}\\n\\ntd.codeblock table tr td.copybar {\\n    background: #40a060; border: 1px solid gray;\\n    font-family: monospace; padding: 0; margin: 0; }\\ntd.doctest table tr td.copybar {\\n    background: #4060a0; border: 1px solid gray;\\n    font-family: monospace; padding: 0; margin: 0; }\\n\\ntd.pysrc { padding-left: 0.5em; }\\n\\nimg.callout { border-width: 0px; }\\n\\ntable.docutils {\\n    border-style: solid;\\n    border-width: 1px;\\n    margin-top: 6px;\\n    border-color: grey;\\n    border-collapse: collapse; }\\n\\ntable.docutils th {\\n    border-style: none;\\n    border-width: 1px;\\n    border-color: grey;\\n    padding: 0 .5em 0 .5em; }\\n\\ntable.docutils td {\\n    border-style: none;\\n    border-width: 1px;\\n    border-color: grey; \\n    padding: 0 .5em 0 .5em; }\\n\\ntable.footnote td { padding: 0; }\\ntable.footnote { border-width: 0; }\\ntable.footnote td { border-width: 0; }\\ntable.footnote th { border-width: 0; }\\n\\ntable.noborder { border-width: 0; }\\n\\ntable.example pre { margin-top: 4px; margin-bottom: 0; }\\n\\n/* For figures & tables */\\np.caption { margin-bottom: 0; }\\ndiv.figure { text-align: center; }\\n\\n/* The index */\\ndiv.index { border: 1px solid black;\\n            background-color: #eeeeee; }\\ndiv.index h1 { padding-left: 0.5em; margin-top: 0.5ex;\\n               border-bottom: 1px solid black; }\\nul.index { margin-left: 0.5em; padding-left: 0; }\\nli.index { list-style-type: none; }\\np.index-heading { font-size: 120%; font-style: italic; margin: 0; }\\nli.index ul { margin-left: 2em; padding-left: 0; }\\n\\n/* \\'Note\\' callouts */\\ndiv.note\\n{\\n  border-right:   #87ceeb 1px solid;\\n  padding-right: 4px;\\n  border-top: #87ceeb 1px solid;\\n  padding-left: 4px;\\n  padding-bottom: 4px;\\n  margin: 2px 5% 10px;\\n  border-left: #87ceeb 1px solid;\\n  padding-top: 4px;\\n  border-bottom: #87ceeb 1px solid;\\n  font-style: normal;\\n  font-family: verdana, arial;\\n  background-color: #b0c4de;\\n}\\n\\ntable.avm { border: 0px solid black; width: 0; }\\ntable.avm tbody tr {border: 0px solid black; }\\ntable.avm tbody tr td { padding: 2px; }\\ntable.avm tbody tr td.avm-key { padding: 5px; font-variant: small-caps; }\\ntable.avm tbody tr td.avm-eq { padding: 5px; }\\ntable.avm tbody tr td.avm-val { padding: 5px; font-style: italic; }\\np.avm-empty { font-style: normal; }\\ntable.avm colgroup col { border: 0px solid black; }\\ntable.avm tbody tr td.avm-topleft \\n    { border-left: 2px solid #000080; border-top: 2px solid #000080; }\\ntable.avm tbody tr td.avm-botleft \\n    { border-left: 2px solid #000080; border-bottom: 2px solid #000080; }\\ntable.avm tbody tr td.avm-topright\\n    { border-right: 2px solid #000080; border-top: 2px solid #000080; }\\ntable.avm tbody tr td.avm-botright\\n    { border-right: 2px solid #000080; border-bottom: 2px solid #000080; }\\ntable.avm tbody tr td.avm-left\\n    { border-left: 2px solid #000080; }\\ntable.avm tbody tr td.avm-right\\n    { border-right: 2px solid #000080; }\\ntable.avm tbody tr td.avm-topbotleft\\n    { border: 2px solid #000080; border-right: 0px solid black; }\\ntable.avm tbody tr td.avm-topbotright\\n    { border: 2px solid #000080; border-left: 0px solid black; }\\ntable.avm tbody tr td.avm-ident\\n    { font-size: 80%; padding: 0; padding-left: 2px; vertical-align: top; }\\n.avm-pointer\\n{ border: 1px solid #008000; padding: 1px; color: #008000; \\n  background: #c0ffc0; font-style: normal; }\\n\\ntable.gloss { border: 0px solid black; width: 0; }\\ntable.gloss tbody tr { border: 0px solid black; }\\ntable.gloss tbody tr td { border: 0px solid black; }\\ntable.gloss colgroup col { border: 0px solid black; }\\ntable.gloss p { margin: 0; padding: 0; }\\n\\ntable.rst-example { border: 1px solid black; }\\ntable.rst-example tbody tr td { background: #eeeeee; }\\ntable.rst-example thead tr th { background: #c0ffff; }\\ntd.rst-raw { width: 0; }\\n\\n/* Used by nltk.org/doc/test: */\\ndiv.doctest-list { text-align: center; }\\ntable.doctest-list { border: 1px solid black;\\n  margin-left: auto; margin-right: auto;\\n}\\ntable.doctest-list tbody tr td { background: #eeeeee;\\n  border: 1px solid #cccccc; text-align: left; }\\ntable.doctest-list thead tr th { background: #304050; color: #ffffff;\\n  border: 1px solid #000000;}\\ntable.doctest-list thead tr a { color: #ffffff; }\\nspan.doctest-passed { color: #008000; }\\nspan.doctest-failed { color: #800000; }\\n\\n\\n\\n\\n\\n\\n5. Categorizing and Tagging Words\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n<!-- Grammatical Category - e.g. NP and verb as technical terms\\n.. role:: gc\\n   :class: category -->\\n\\n\\n\\n\\n\\n\\n\\n\\n<!-- standard global imports\\n\\n>>> import nltk, re, pprint\\n>>> from nltk import word_tokenize -->\\n\\n\\n\\n<!-- TODO: * outstanding problems:\\n- what are we doing with ConditionalFreqDist?\\n- nltk.tag contains all of math library\\n- nltk.corpus.brown.tagged_sents() is too verbose? -->\\n\\n\\n\\n\\n\\n<!-- TODO: how POS tagging disambiguates the word \"like\" and this can be\\nuseful for sentiment detection -->\\n\\nBack in elementary school you learnt the difference between nouns, verbs,\\nadjectives, and adverbs.  These \"word classes\" are not just\\nthe idle invention of grammarians, but are useful categories for many\\nlanguage processing tasks.  As we will see, they arise from simple analysis\\nof the distribution of words in text.  The goal of this chapter is to\\nanswer the following questions:\\n\\nWhat are lexical categories and how are they used in natural language processing?\\nWhat is a good Python data structure for storing words and their categories?\\nHow can we automatically tag each word of a text with its word class?\\n\\nAlong the way, we\\'ll cover some fundamental techniques in NLP, including\\nsequence labeling, n-gram models, backoff, and evaluation.  These techniques\\nare useful in many areas, and tagging gives us a simple context in which\\nto present them.  We will also see how tagging is the second step in the typical\\nNLP pipeline, following tokenization.\\nThe process of classifying words into their parts of speech and\\nlabeling them accordingly is known as part-of-speech tagging,\\nPOS-tagging, or simply tagging.  Parts of speech\\nare also known as word classes or lexical categories.\\nThe collection of tags\\nused for a particular task is known as a tagset.  Our emphasis\\nin this chapter is on exploiting tags, and tagging text automatically.\\n\\n1&nbsp;&nbsp;&nbsp;Using a Tagger\\nA part-of-speech tagger, or POS-tagger, processes a sequence of words, and attaches a\\npart of speech tag to each word (don\\'t forget to import nltk):\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; text = word_tokenize(\"And now for something completely different\")\\n&gt;&gt;&gt; nltk.pos_tag(text)\\n[(\\'And\\', \\'CC\\'), (\\'now\\', \\'RB\\'), (\\'for\\', \\'IN\\'), (\\'something\\', \\'NN\\'),\\n(\\'completely\\', \\'RB\\'), (\\'different\\', \\'JJ\\')]\\n\\n\\n\\nHere we see that and is CC, a coordinating conjunction;\\nnow and completely are RB, or adverbs;\\nfor is IN, a preposition;\\nsomething is NN, a noun; and\\ndifferent is JJ, an adjective.\\n\\nNote\\nNLTK provides documentation for each tag, which can be queried using\\nthe tag, e.g. nltk.help.upenn_tagset(\\'RB\\'), or a regular\\nexpression, e.g. nltk.help.upenn_tagset(\\'NN.*\\').\\nSome corpora have README files with tagset documentation,\\nsee nltk.corpus.???.readme(), substituting in the name\\nof the corpus.\\n\\nLet\\'s look at another example, this time including some homonyms:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; text = word_tokenize(\"They refuse to permit us to obtain the refuse permit\")\\n&gt;&gt;&gt; nltk.pos_tag(text)\\n[(\\'They\\', \\'PRP\\'), (\\'refuse\\', \\'VBP\\'), (\\'to\\', \\'TO\\'), (\\'permit\\', \\'VB\\'), (\\'us\\', \\'PRP\\'),\\n(\\'to\\', \\'TO\\'), (\\'obtain\\', \\'VB\\'), (\\'the\\', \\'DT\\'), (\\'refuse\\', \\'NN\\'), (\\'permit\\', \\'NN\\')]\\n\\n\\n\\nNotice that refuse and permit both appear as a\\npresent tense verb (VBP) and a noun (NN).\\nE.g. refUSE is a verb meaning \"deny,\" while REFuse is\\na noun meaning \"trash\" (i.e. they are not homophones).\\nThus, we need to know which word is being used in order to pronounce\\nthe text correctly.  (For this reason,\\ntext-to-speech systems usually perform POS-tagging.)\\n\\nNote\\nYour Turn:\\nMany words, like ski and race, can be used as nouns\\nor verbs with no difference in pronunciation.  Can you think of\\nothers?  Hint: think of a commonplace object and try to put\\nthe word to before it to see if it can also be a verb, or\\nthink of an action and try to put the before it to see if\\nit can also be a noun.  Now make up a sentence with both uses\\nof this word, and run the POS-tagger on this sentence.\\n\\nLexical categories like \"noun\" and part-of-speech tags like NN seem to have\\ntheir uses, but the details will be obscure to many readers.  You might wonder what\\njustification there is for introducing this extra level of information.\\nMany of these categories arise from superficial analysis the distribution\\nof words in text.  Consider the following analysis involving\\nwoman (a noun), bought (a verb),\\nover (a preposition), and the (a determiner).\\nThe text.similar() method takes a word w, finds all contexts\\nw1w w2,\\nthen finds all words w\\' that appear in the same context,\\ni.e. w1w\\'w2.\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; text = nltk.Text(word.lower() for word in nltk.corpus.brown.words())\\n&gt;&gt;&gt; text.similar(\\'woman\\')\\nBuilding word-context index...\\nman day time year car moment world family house boy child country job\\nstate girl place war way case question\\n&gt;&gt;&gt; text.similar(\\'bought\\')\\nmade done put said found had seen given left heard been brought got\\nset was called felt in that told\\n&gt;&gt;&gt; text.similar(\\'over\\')\\nin on to of and for with from at by that into as up out down through\\nabout all is\\n&gt;&gt;&gt; text.similar(\\'the\\')\\na his this their its her an that our any all one these my in your no\\nsome other and\\n\\n\\n\\nObserve that searching for woman finds nouns;\\nsearching for bought mostly finds verbs;\\nsearching for over generally finds prepositions;\\nsearching for the finds several determiners.\\nA tagger can correctly identify the tags on these words\\nin the context of a sentence, e.g. The woman bought over $150,000\\nworth of clothes.\\nA tagger can also model our knowledge of unknown words,\\ne.g. we can guess that scrobbling is probably a verb,\\nwith the root scrobble,\\nand likely to occur in contexts like he was scrobbling.\\n\\n\\n2&nbsp;&nbsp;&nbsp;Tagged Corpora\\n\\n2.1&nbsp;&nbsp;&nbsp;Representing Tagged Tokens\\nBy convention in NLTK, a tagged token is represented using a\\ntuple consisting of the token and the tag.\\nWe can create one of these special tuples from the standard string\\nrepresentation of a tagged token, using the function str2tuple():\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; tagged_token = nltk.tag.str2tuple(\\'fly/NN\\')\\n&gt;&gt;&gt; tagged_token\\n(\\'fly\\', \\'NN\\')\\n&gt;&gt;&gt; tagged_token[0]\\n\\'fly\\'\\n&gt;&gt;&gt; tagged_token[1]\\n\\'NN\\'\\n\\n\\n\\nWe can construct a list of tagged tokens directly from a string.  The first\\nstep is to tokenize the string\\nto access the individual word/tag strings, and then to convert\\neach of these into a tuple (using str2tuple()).\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; sent = \\'\\'\\'\\n... The/AT grand/JJ jury/NN commented/VBD on/IN a/AT number/NN of/IN\\n... other/AP topics/NNS ,/, AMONG/IN them/PPO the/AT Atlanta/NP and/CC\\n... Fulton/NP-tl County/NN-tl purchasing/VBG departments/NNS which/WDT it/PPS\\n... said/VBD ``/`` ARE/BER well/QL operated/VBN and/CC follow/VB generally/RB\\n... accepted/VBN practices/NNS which/WDT inure/VB to/IN the/AT best/JJT\\n... interest/NN of/IN both/ABX governments/NNS \\'\\'/\\'\\' ./.\\n... \\'\\'\\'\\n&gt;&gt;&gt; [nltk.tag.str2tuple(t) for t in sent.split()]\\n[(\\'The\\', \\'AT\\'), (\\'grand\\', \\'JJ\\'), (\\'jury\\', \\'NN\\'), (\\'commented\\', \\'VBD\\'),\\n(\\'on\\', \\'IN\\'), (\\'a\\', \\'AT\\'), (\\'number\\', \\'NN\\'), ... (\\'.\\', \\'.\\')]\\n\\n\\n\\n\\n\\n2.2&nbsp;&nbsp;&nbsp;Reading Tagged Corpora\\nSeveral of the corpora included with NLTK have been tagged for\\ntheir part-of-speech. Here\\'s an example of what you might see if you\\nopened a file from the Brown Corpus with a text editor:\\n\\nThe/at Fulton/np-tl County/nn-tl Grand/jj-tl Jury/nn-tl\\nsaid/vbd Friday/nr an/at investigation/nn of/in Atlanta\\'s/np$\\nrecent/jj primary/nn election/nn produced/vbd / no/at\\nevidence/nn \\'\\'/\\'\\' that/cs any/dti irregularities/nns took/vbd\\nplace/nn ./.\\nOther corpora use a variety of formats for storing part-of-speech tags.\\nNLTK\\'s corpus readers provide a uniform interface so that you\\ndon\\'t have to be concerned with the different file formats.\\nIn contrast with the file fragment shown above,\\nthe corpus reader for the Brown Corpus represents the data as shown below.\\nNote that part-of-speech tags have been converted to uppercase, since this has\\nbecome standard practice since the Brown Corpus was published.\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; nltk.corpus.brown.tagged_words()\\n[(\\'The\\', \\'AT\\'), (\\'Fulton\\', \\'NP-TL\\'), ...]\\n&gt;&gt;&gt; nltk.corpus.brown.tagged_words(tagset=\\'universal\\')\\n[(\\'The\\', \\'DET\\'), (\\'Fulton\\', \\'NOUN\\'), ...]\\n\\n\\n\\nWhenever a corpus contains tagged text, the NLTK corpus interface\\nwill have a tagged_words() method.\\nHere are some more examples, again using the output format\\nillustrated for the Brown Corpus:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; print(nltk.corpus.nps_chat.tagged_words())\\n[(\\'now\\', \\'RB\\'), (\\'im\\', \\'PRP\\'), (\\'left\\', \\'VBD\\'), ...]\\n&gt;&gt;&gt; nltk.corpus.conll2000.tagged_words()\\n[(\\'Confidence\\', \\'NN\\'), (\\'in\\', \\'IN\\'), (\\'the\\', \\'DT\\'), ...]\\n&gt;&gt;&gt; nltk.corpus.treebank.tagged_words()\\n[(\\'Pierre\\', \\'NNP\\'), (\\'Vinken\\', \\'NNP\\'), (\\',\\', \\',\\'), ...]\\n\\n\\n\\nNot all corpora employ the same set of tags; see the\\ntagset help functionality and the readme() methods\\nmentioned above for documentation.\\nInitially we want to avoid the complications of these tagsets,\\nso we use a built-in mapping to the \"Universal Tagset\":\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; nltk.corpus.brown.tagged_words(tagset=\\'universal\\')\\n[(\\'The\\', \\'DET\\'), (\\'Fulton\\', \\'NOUN\\'), ...]\\n&gt;&gt;&gt; nltk.corpus.treebank.tagged_words(tagset=\\'universal\\')\\n[(\\'Pierre\\', \\'NOUN\\'), (\\'Vinken\\', \\'NOUN\\'), (\\',\\', \\'.\\'), ...]\\n\\n\\n\\nTagged corpora for several other languages are distributed with NLTK,\\nincluding Chinese, Hindi, Portuguese, Spanish, Dutch and Catalan.\\nThese usually contain non-ASCII text,\\nand Python always displays this in hexadecimal when printing a larger structure\\nsuch as a list.\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; nltk.corpus.sinica_treebank.tagged_words()\\n[(\\'ä\\', \\'Neu\\'), (\\'åæ\\', \\'Nad\\'), (\\'åç\\', \\'Nba\\'), ...]\\n&gt;&gt;&gt; nltk.corpus.indian.tagged_words()\\n[(\\'মহিষের\\', \\'NN\\'), (\\'সন্তান\\', \\'NN\\'), (\\':\\', \\'SYM\\'), ...]\\n&gt;&gt;&gt; nltk.corpus.mac_morpho.tagged_words()\\n[(\\'Jersei\\', \\'N\\'), (\\'atinge\\', \\'V\\'), (\\'m\\\\xe9dia\\', \\'N\\'), ...]\\n&gt;&gt;&gt; nltk.corpus.conll2002.tagged_words()\\n[(\\'Sao\\', \\'NC\\'), (\\'Paulo\\', \\'VMI\\'), (\\'(\\', \\'Fpa\\'), ...]\\n&gt;&gt;&gt; nltk.corpus.cess_cat.tagged_words()\\n[(\\'El\\', \\'da0ms0\\'), (\\'Tribunal_Suprem\\', \\'np0000o\\'), ...]\\n\\n\\n\\nIf your environment is set up correctly, with appropriate editors and fonts,\\nyou should be able to display individual strings in a human-readable way.\\nFor example, 2.1 shows data accessed using\\nnltk.corpus.indian.\\n\\n\\nFigure 2.1: POS-Tagged Data from Four Indian Languages: Bangla, Hindi, Marathi, and Telugu\\n\\n\\nIf the corpus is also segmented into sentences, it will have\\na tagged_sents() method that divides up the tagged words into\\nsentences rather than presenting them as one big list.\\nThis will be useful when we come to developing automatic taggers,\\nas they are trained and tested on lists of sentences, not words.\\n\\n\\n2.3&nbsp;&nbsp;&nbsp;A Universal Part-of-Speech Tagset\\nTagged corpora use many different conventions for tagging words.\\nTo help us get started, we will be looking at a simplified tagset\\n(shown in 2.1).\\nTable 2.1: Universal Part-of-Speech Tagset\\n\\n\\n\\n\\n\\n\\nTag\\nMeaning\\nEnglish Examples\\n\\n\\n\\nADJ\\nadjective\\nnew, good, high, special, big, local\\n\\nADP\\nadposition\\non, of, at, with, by, into, under\\n\\nADV\\nadverb\\nreally, already, still, early, now\\n\\nCONJ\\nconjunction\\nand, or, but, if, while, although\\n\\nDET\\ndeterminer, article\\nthe, a, some, most, every, no, which\\n\\nNOUN\\nnoun\\nyear, home, costs, time, Africa\\n\\nNUM\\nnumeral\\ntwenty-four, fourth, 1991, 14:24\\n\\nPRT\\nparticle\\nat, on, out, over per, that, up, with\\n\\nPRON\\npronoun\\nhe, their, her, its, my, I, us\\n\\nVERB\\nverb\\nis, say, told, given, playing, would\\n\\n.\\npunctuation marks\\n. , ; !\\n\\nX\\nother\\nersatz, esprit, dunno, gr8, univeristy\\n\\n\\n\\n\\n\\nLet\\'s see which of these tags are the most common in the news\\ncategory of the Brown corpus:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; from nltk.corpus import brown\\n&gt;&gt;&gt; brown_news_tagged = brown.tagged_words(categories=\\'news\\', tagset=\\'universal\\')\\n&gt;&gt;&gt; tag_fd = nltk.FreqDist(tag for (word, tag) in brown_news_tagged)\\n&gt;&gt;&gt; tag_fd.most_common()\\n[(\\'NOUN\\', 30640), (\\'VERB\\', 14399), (\\'ADP\\', 12355), (\\'.\\', 11928), (\\'DET\\', 11389),\\n (\\'ADJ\\', 6706), (\\'ADV\\', 3349), (\\'CONJ\\', 2717), (\\'PRON\\', 2535), (\\'PRT\\', 2264),\\n (\\'NUM\\', 2166), (\\'X\\', 106)]\\n\\n\\n\\n\\nNote\\nYour Turn:\\nPlot the above frequency distribution using tag_fd.plot(cumulative=True).\\nWhat percentage of words are tagged using the first five tags of the above list?\\n\\nWe can use these tags to do powerful searches using a graphical\\nPOS-concordance tool nltk.app.concordance().  Use it\\nto search for any combination of words and POS tags, e.g.\\nN N N N, hit/VD, hit/VN, or the ADJ man.\\n\\n\\n\\n2.4&nbsp;&nbsp;&nbsp;Nouns\\nNouns generally refer to people, places, things, or concepts, e.g.:\\nwoman, Scotland, book, intelligence.  Nouns can appear after\\ndeterminers and adjectives, and can be the subject or object of the\\nverb, as shown in 2.2.\\nTable 2.2: Syntactic Patterns involving some Nouns\\n\\n\\n\\n\\n\\n\\nWord\\nAfter a determiner\\nSubject of the verb\\n\\n\\n\\nwoman\\nthe woman who I saw yesterday ...\\nthe woman sat down\\n\\nScotland\\nthe Scotland I remember as a child ...\\nScotland has five million people\\n\\nbook\\nthe book I bought yesterday ...\\nthis book recounts the colonization of Australia\\n\\nintelligence\\nthe intelligence displayed by the child ...\\nMary\\'s intelligence impressed her teachers\\n\\n\\n\\n\\n\\nThe simplified noun tags are N for common nouns like book,\\nand NP for proper nouns like Scotland.\\nLet\\'s inspect some tagged text to see what parts of speech occur before a noun,\\nwith the most frequent ones first. To begin with, we construct a list\\nof bigrams whose members are themselves word-tag pairs such as\\n((\\'The\\', \\'DET\\'), (\\'Fulton\\', \\'NP\\')) and  ((\\'Fulton\\', \\'NP\\'), (\\'County\\', \\'N\\')).\\nThen we construct a FreqDist from the tag parts of the bigrams.\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; word_tag_pairs = nltk.bigrams(brown_news_tagged)\\n&gt;&gt;&gt; noun_preceders = [a[1] for (a, b) in word_tag_pairs if b[1] == \\'NOUN\\']\\n&gt;&gt;&gt; fdist = nltk.FreqDist(noun_preceders)\\n&gt;&gt;&gt; [tag for (tag, _) in fdist.most_common()]\\n[\\'NOUN\\', \\'DET\\', \\'ADJ\\', \\'ADP\\', \\'.\\', \\'VERB\\', \\'CONJ\\', \\'NUM\\', \\'ADV\\', \\'PRT\\', \\'PRON\\', \\'X\\']\\n\\n\\n\\nThis confirms our assertion that nouns occur after determiners and\\nadjectives, including numeral adjectives (tagged as NUM).\\n\\n\\n\\n2.5&nbsp;&nbsp;&nbsp;Verbs\\nVerbs are words that describe events and actions, e.g. fall,\\neat in 2.3.\\nIn the context of a sentence, verbs typically express a relation\\ninvolving the referents of one or more noun phrases.\\nTable 2.3: Syntactic Patterns involving some Verbs\\n\\n\\n\\n\\n\\n\\nWord\\nSimple\\nWith modifiers and adjuncts (italicized)\\n\\n\\n\\nfall\\nRome fell\\nDot com stocks suddenly fell like a stone\\n\\neat\\nMice eat cheese\\nJohn ate the pizza with gusto\\n\\n\\n\\n\\n\\nWhat are the most common verbs in news text?  Let\\'s sort all the verbs by frequency:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; wsj = nltk.corpus.treebank.tagged_words(tagset=\\'universal\\')\\n&gt;&gt;&gt; word_tag_fd = nltk.FreqDist(wsj)\\n&gt;&gt;&gt; [wt[0] for (wt, _) in word_tag_fd.most_common() if wt[1] == \\'VERB\\']\\n[\\'is\\', \\'said\\', \\'are\\', \\'was\\', \\'be\\', \\'has\\', \\'have\\', \\'will\\', \\'says\\', \\'would\\',\\n \\'were\\', \\'had\\', \\'been\\', \\'could\\', \"\\'s\", \\'can\\', \\'do\\', \\'say\\', \\'make\\', \\'may\\',\\n \\'did\\', \\'rose\\', \\'made\\', \\'does\\', \\'expected\\', \\'buy\\', \\'take\\', \\'get\\', \\'might\\',\\n \\'sell\\', \\'added\\', \\'sold\\', \\'help\\', \\'including\\', \\'should\\', \\'reported\\', ...]\\n\\n\\n\\nNote that the items being counted in the frequency distribution are word-tag pairs.\\nSince words and tags are paired, we can treat the word as a condition and the tag\\nas an event, and initialize a conditional frequency distribution with a list of\\ncondition-event pairs.  This lets us see a frequency-ordered list of tags given a word:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; cfd1 = nltk.ConditionalFreqDist(wsj)\\n&gt;&gt;&gt; cfd1[\\'yield\\'].most_common()\\n[(\\'VERB\\', 28), (\\'NOUN\\', 20)]\\n&gt;&gt;&gt; cfd1[\\'cut\\'].most_common()\\n[(\\'VERB\\', 25), (\\'NOUN\\', 3)]\\n\\n\\n\\nWe can reverse the order of the pairs, so that the tags are the conditions, and the\\nwords are the events.  Now we can see likely words for a given tag. We\\nwill do this for the WSJ tagset rather than the universal tagset:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; wsj = nltk.corpus.treebank.tagged_words()\\n&gt;&gt;&gt; cfd2 = nltk.ConditionalFreqDist((tag, word) for (word, tag) in wsj)\\n&gt;&gt;&gt; list(cfd2[\\'VBN\\'])\\n[\\'been\\', \\'expected\\', \\'made\\', \\'compared\\', \\'based\\', \\'priced\\', \\'used\\', \\'sold\\',\\n\\'named\\', \\'designed\\', \\'held\\', \\'fined\\', \\'taken\\', \\'paid\\', \\'traded\\', \\'said\\', ...]\\n\\n\\n\\nTo clarify the distinction between VBD (past tense) and VBN\\n(past participle), let\\'s find words which can be both VBD and\\nVBN, and see some surrounding text:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; [w for w in cfd1.conditions() if \\'VBD\\' in cfd1[w] and \\'VBN\\' in cfd1[w]]\\n[\\'Asked\\', \\'accelerated\\', \\'accepted\\', \\'accused\\', \\'acquired\\', \\'added\\', \\'adopted\\', ...]\\n&gt;&gt;&gt; idx1 = wsj.index((\\'kicked\\', \\'VBD\\'))\\n&gt;&gt;&gt; wsj[idx1-4:idx1+1]\\n[(\\'While\\', \\'IN\\'), (\\'program\\', \\'NN\\'), (\\'trades\\', \\'NNS\\'), (\\'swiftly\\', \\'RB\\'),\\n (\\'kicked\\', \\'VBD\\')]\\n&gt;&gt;&gt; idx2 = wsj.index((\\'kicked\\', \\'VBN\\'))\\n&gt;&gt;&gt; wsj[idx2-4:idx2+1]\\n[(\\'head\\', \\'NN\\'), (\\'of\\', \\'IN\\'), (\\'state\\', \\'NN\\'), (\\'has\\', \\'VBZ\\'), (\\'kicked\\', \\'VBN\\')]\\n\\n\\n\\nIn this case, we see that the past participle of kicked is preceded by a form of\\nthe auxiliary verb have. Is this generally true?\\n\\nNote\\nYour Turn:\\nGiven the list of past participles produced by\\nlist(cfd2[\\'VN\\']), try to collect a list of all the word-tag\\npairs that immediately precede items in that list.\\n\\n\\n\\n2.6&nbsp;&nbsp;&nbsp;Adjectives and Adverbs\\nTwo other important word classes are adjectives and adverbs.\\nAdjectives describe nouns, and can be used as modifiers\\n(e.g. large in the large pizza), or in predicates (e.g. the\\npizza is large).  English adjectives can have internal structure\\n(e.g.  fall+ing in the falling\\nstocks).  Adverbs modify verbs to specify the time, manner, place or\\ndirection of the event described by the verb (e.g. quickly in\\nthe stocks fell quickly).  Adverbs may also modify adjectives\\n(e.g. really in Mary\\'s teacher was really nice).\\nEnglish has several categories of closed class words in addition to\\nprepositions, such as articles (also often called determiners)\\n(e.g., the, a), modals (e.g., should,\\nmay), and personal pronouns (e.g., she, they).\\nEach dictionary and grammar classifies these words differently.\\n\\nNote\\nYour Turn:\\nIf you are uncertain about some of these parts of speech, study them using\\nnltk.app.concordance(), or watch some of the Schoolhouse Rock!\\ngrammar videos available at YouTube, or consult the Further Reading\\nsection at the end of this chapter.\\n\\n\\n\\n2.7&nbsp;&nbsp;&nbsp;Unsimplified Tags\\nLet\\'s find the most frequent nouns of each noun part-of-speech type.\\nThe program in 2.2 finds all tags starting with NN,\\nand provides a few example words for each one.  You will see that\\nthere are many variants of NN; the most important contain $\\nfor possessive nouns, S for plural nouns (since plural nouns\\ntypically end in s) and P for proper nouns.  In addition,\\nmost of the tags have suffix modifiers: -NC for citations, -HL\\nfor words in headlines and -TL for titles (a feature of Brown tags).\\n\\n\\n\\n\\n&nbsp;\\ndef findtags(tag_prefix, tagged_text):\\n    cfd = nltk.ConditionalFreqDist((tag, word) for (word, tag) in tagged_text\\n                                  if tag.startswith(tag_prefix))\\n    return dict((tag, cfd[tag].most_common(5)) for tag in cfd.conditions())\\n\\n&gt;&gt;&gt; tagdict = findtags(\\'NN\\', nltk.corpus.brown.tagged_words(categories=\\'news\\'))\\n&gt;&gt;&gt; for tag in sorted(tagdict):\\n...     print(tag, tagdict[tag])\\n...\\nNN [(\\'year\\', 137), (\\'time\\', 97), (\\'state\\', 88), (\\'week\\', 85), (\\'man\\', 72)]\\nNN$ [(\"year\\'s\", 13), (\"world\\'s\", 8), (\"state\\'s\", 7), (\"nation\\'s\", 6), (\"company\\'s\", 6)]\\nNN$-HL [(\"Golf\\'s\", 1), (\"Navy\\'s\", 1)]\\nNN$-TL [(\"President\\'s\", 11), (\"Army\\'s\", 3), (\"Gallery\\'s\", 3), (\"University\\'s\", 3), (\"League\\'s\", 3)]\\nNN-HL [(\\'sp.\\', 2), (\\'problem\\', 2), (\\'Question\\', 2), (\\'business\\', 2), (\\'Salary\\', 2)]\\nNN-NC [(\\'eva\\', 1), (\\'aya\\', 1), (\\'ova\\', 1)]\\nNN-TL [(\\'President\\', 88), (\\'House\\', 68), (\\'State\\', 59), (\\'University\\', 42), (\\'City\\', 41)]\\nNN-TL-HL [(\\'Fort\\', 2), (\\'Dr.\\', 1), (\\'Oak\\', 1), (\\'Street\\', 1), (\\'Basin\\', 1)]\\nNNS [(\\'years\\', 101), (\\'members\\', 69), (\\'people\\', 52), (\\'sales\\', 51), (\\'men\\', 46)]\\nNNS$ [(\"children\\'s\", 7), (\"women\\'s\", 5), (\"janitors\\'\", 3), (\"men\\'s\", 3), (\"taxpayers\\'\", 2)]\\nNNS$-HL [(\"Dealers\\'\", 1), (\"Idols\\'\", 1)]\\nNNS$-TL [(\"Women\\'s\", 4), (\"States\\'\", 3), (\"Giants\\'\", 2), (\"Bros.\\'\", 1), (\"Writers\\'\", 1)]\\nNNS-HL [(\\'comments\\', 1), (\\'Offenses\\', 1), (\\'Sacrifices\\', 1), (\\'funds\\', 1), (\\'Results\\', 1)]\\nNNS-TL [(\\'States\\', 38), (\\'Nations\\', 11), (\\'Masters\\', 10), (\\'Rules\\', 9), (\\'Communists\\', 9)]\\nNNS-TL-HL [(\\'Nations\\', 1)]\\n\\n\\nExample 2.2 (code_findtags.py): Figure 2.2: Program to Find the Most Frequent Noun Tags\\n\\nWhen we come to constructing part-of-speech taggers later in this chapter,\\nwe will use the unsimplified tags.\\n\\n\\n2.8&nbsp;&nbsp;&nbsp;Exploring Tagged Corpora\\nLet\\'s briefly return to the kinds of exploration of corpora we saw in previous chapters,\\nthis time exploiting POS tags.\\nSuppose we\\'re studying the word often and want to see how it is used\\nin text.  We could ask to see the words that follow often\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; brown_learned_text = brown.words(categories=\\'learned\\')\\n&gt;&gt;&gt; sorted(set(b for (a, b) in nltk.bigrams(brown_learned_text) if a == \\'often\\'))\\n[\\',\\', \\'.\\', \\'accomplished\\', \\'analytically\\', \\'appear\\', \\'apt\\', \\'associated\\', \\'assuming\\',\\n\\'became\\', \\'become\\', \\'been\\', \\'began\\', \\'call\\', \\'called\\', \\'carefully\\', \\'chose\\', ...]\\n\\n\\n\\nHowever, it\\'s probably more instructive to use the tagged_words()\\nmethod to look at the part-of-speech tag of the following words:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; brown_lrnd_tagged = brown.tagged_words(categories=\\'learned\\', tagset=\\'universal\\')\\n&gt;&gt;&gt; tags = [b[1] for (a, b) in nltk.bigrams(brown_lrnd_tagged) if a[0] == \\'often\\']\\n&gt;&gt;&gt; fd = nltk.FreqDist(tags)\\n&gt;&gt;&gt; fd.tabulate()\\n PRT  ADV  ADP    . VERB  ADJ\\n   2    8    7    4   37    6\\n\\n\\n\\nNotice that the most high-frequency parts of speech following often are verbs.\\nNouns never appear in this position (in this particular corpus).\\nNext, let\\'s look at some larger context, and find words involving\\nparticular sequences of tags and words (in this case \"&lt;Verb&gt; to &lt;Verb&gt;\").\\nIn code-three-word-phrase we consider each three-word window in the sentence ,\\nand check if they meet our criterion .  If the tags\\nmatch, we print the corresponding words .\\n\\n\\n\\n\\n&nbsp;\\nfrom nltk.corpus import brown\\ndef process(sentence):\\n    for (w1,t1), (w2,t2), (w3,t3) in nltk.trigrams(sentence): \\n        if (t1.startswith(\\'V\\') and t2 == \\'TO\\' and t3.startswith(\\'V\\')): \\n            print(w1, w2, w3) \\n\\n&gt;&gt;&gt; for tagged_sent in brown.tagged_sents():\\n...     process(tagged_sent)\\n...\\ncombined to achieve\\ncontinue to place\\nserve to protect\\nwanted to wait\\nallowed to place\\nexpected to become\\n...\\n\\n\\nExample 2.3 (code_three_word_phrase.py): Figure 2.3: Searching for Three-Word Phrases Using POS Tags\\n\\nFinally, let\\'s look for words that are highly ambiguous as to their part of speech tag.\\nUnderstanding why such words are tagged as they are in each context can help us clarify\\nthe distinctions between the tags.\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; brown_news_tagged = brown.tagged_words(categories=\\'news\\', tagset=\\'universal\\')\\n&gt;&gt;&gt; data = nltk.ConditionalFreqDist((word.lower(), tag)\\n...                                 for (word, tag) in brown_news_tagged)\\n&gt;&gt;&gt; for word in sorted(data.conditions()):\\n...     if len(data[word]) &gt; 3:\\n...         tags = [tag for (tag, _) in data[word].most_common()]\\n...         print(word, \\' \\'.join(tags))\\n...\\nbest ADJ ADV NP V\\nbetter ADJ ADV V DET\\nclose ADV ADJ V N\\ncut V N VN VD\\neven ADV DET ADJ V\\ngrant NP N V -\\nhit V VD VN N\\nlay ADJ V NP VD\\nleft VD ADJ N VN\\nlike CNJ V ADJ P -\\nnear P ADV ADJ DET\\nopen ADJ V N ADV\\npast N ADJ DET P\\npresent ADJ ADV V N\\nread V VN VD NP\\nright ADJ N DET ADV\\nsecond NUM ADV DET N\\nset VN V VD N -\\nthat CNJ V WH DET\\n\\n\\n\\n\\nNote\\nYour Turn:\\nOpen the POS concordance tool nltk.app.concordance() and load the complete\\nBrown Corpus (simplified tagset).  Now pick some of the above words and see how the tag\\nof the word correlates with the context of the word.\\nE.g. search for near to see all forms mixed together, near/ADJ to see it used\\nas an adjective, near N to see just those cases where a noun follows, and so forth.\\nFor a larger set of examples, modify the supplied code so that it lists words having\\nthree distinct tags.\\n\\n\\n\\n\\n3&nbsp;&nbsp;&nbsp;Mapping Words to Properties Using Python Dictionaries\\nAs we have seen, a tagged word of the form (word, tag) is\\nan association between a word and a part-of-speech tag.\\nOnce we start doing part-of-speech tagging, we will be creating\\nprograms that assign a tag to a word, the tag which is most\\nlikely in a given context.  We can think of this process as\\nmapping from words to tags.  The most natural way to\\nstore mappings in Python uses the so-called dictionary data type\\n(also known as an associative array or hash array\\nin other programming languages).\\nIn this section we look at dictionaries and see how they can\\nrepresent a variety of language information, including\\nparts of speech.\\n\\n3.1&nbsp;&nbsp;&nbsp;Indexing Lists vs Dictionaries\\nA text, as we have seen, is treated in Python as a list of words.\\nAn important property of lists is that we can \"look up\" a particular\\nitem by giving its index, e.g. text1[100].  Notice how we specify\\na number, and get back a word.  We can think of a list as a simple\\nkind of table, as shown in 3.1.\\n\\n\\nFigure 3.1: List Look-up: we access the contents of a Python list with the help of an integer index.\\n\\nContrast this situation with frequency distributions (3),\\nwhere we specify a word, and get back a number, e.g. fdist[\\'monstrous\\'], which\\ntells us the number of times a given word has occurred in a text.  Look-up using words is\\nfamiliar to anyone who has used a dictionary.  Some more examples are shown in\\n3.2.\\n\\n\\nFigure 3.2: Dictionary Look-up: we access the entry of a dictionary using a key\\nsuch as someone\\'s name, a web domain, or an English word;\\nother names for dictionary are map, hashmap, hash, and associative array.\\n\\nIn the case of a phonebook, we look up an entry using a name,\\nand get back a number.  When we type a domain name in a web browser,\\nthe computer looks this up to get back an IP address.  A word\\nfrequency table allows us to look up a word and find its frequency in\\na text collection.  In all these cases, we are mapping from names to\\nnumbers, rather than the other way around as with a list.\\nIn general, we would like to be able to map between\\narbitrary types of information.  3.1 lists a variety\\nof linguistic objects, along with what they map.\\nTable 3.1: Linguistic Objects as Mappings from Keys to Values\\n\\n\\n\\n\\n\\n\\nLinguistic Object\\nMaps From\\nMaps To\\n\\n\\n\\nDocument Index\\nWord\\nList of pages (where word is found)\\n\\nThesaurus\\nWord sense\\nList of synonyms\\n\\nDictionary\\nHeadword\\nEntry (part-of-speech, sense definitions, etymology)\\n\\nComparative Wordlist\\nGloss term\\nCognates (list of words, one per language)\\n\\nMorph Analyzer\\nSurface form\\nMorphological analysis (list of component morphemes)\\n\\n\\n\\n\\n\\nMost often, we are mapping from a \"word\" to some structured object.\\nFor example, a document index maps from a word (which we can represent\\nas a string), to a list of pages (represented as a list of integers).\\nIn this section, we will see how to represent such mappings in Python.\\n\\n\\n3.2&nbsp;&nbsp;&nbsp;Dictionaries in Python\\nPython provides a dictionary data type that can be used for\\nmapping between arbitrary types.  It is like a conventional dictionary,\\nin that it gives you an efficient way to look things up.  However,\\nas we see from 3.1, it has a much wider range of uses.\\nTo illustrate, we define pos to be an empty dictionary and then add four\\nentries to it, specifying the part-of-speech of some words.  We add\\nentries to a dictionary using the familiar square bracket notation:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; pos = {}\\n&gt;&gt;&gt; pos\\n{}\\n&gt;&gt;&gt; pos[\\'colorless\\'] = \\'ADJ\\' \\n&gt;&gt;&gt; pos\\n{\\'colorless\\': \\'ADJ\\'}\\n&gt;&gt;&gt; pos[\\'ideas\\'] = \\'N\\'\\n&gt;&gt;&gt; pos[\\'sleep\\'] = \\'V\\'\\n&gt;&gt;&gt; pos[\\'furiously\\'] = \\'ADV\\'\\n&gt;&gt;&gt; pos \\n{\\'furiously\\': \\'ADV\\', \\'ideas\\': \\'N\\', \\'colorless\\': \\'ADJ\\', \\'sleep\\': \\'V\\'}\\n\\n\\n\\nSo, for example,  says that\\nthe part-of-speech of colorless is adjective, or more\\nspecifically, that the key \\'colorless\\'\\nis assigned the value \\'ADJ\\'  in dictionary pos.\\nWhen we inspect the value of pos  we see\\na set of key-value pairs.  Once we have populated the dictionary\\nin this way, we can employ the keys to retrieve values:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; pos[\\'ideas\\']\\n\\'N\\'\\n&gt;&gt;&gt; pos[\\'colorless\\']\\n\\'ADJ\\'\\n\\n\\n\\nOf course, we might accidentally use a key that hasn\\'t been assigned a value.\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; pos[\\'green\\']\\nTraceback (most recent call last):\\n  File \"&lt;stdin&gt;\", line 1, in ?\\nKeyError: \\'green\\'\\n\\n\\n\\nThis raises an important question.  Unlike lists and strings, where we\\ncan use len() to work out which integers will be legal indexes,\\nhow do we work out the legal keys for a dictionary?  If the dictionary\\nis not too big, we can simply inspect its contents by evaluating the\\nvariable pos.  As we saw above (line ), this gives\\nus the key-value pairs.  Notice that they are not in the same order they\\nwere originally entered; this is because dictionaries are not sequences\\nbut mappings (cf. 3.2), and the keys are not inherently\\nordered.\\nAlternatively, to just find the keys, we can convert the\\ndictionary to a list  — or use\\nthe dictionary in a context where a list is expected,\\nas the parameter of sorted() ,\\nor in a for loop .\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; list(pos) \\n[\\'ideas\\', \\'furiously\\', \\'colorless\\', \\'sleep\\']\\n&gt;&gt;&gt; sorted(pos) \\n[\\'colorless\\', \\'furiously\\', \\'ideas\\', \\'sleep\\']\\n&gt;&gt;&gt; [w for w in pos if w.endswith(\\'s\\')] \\n[\\'colorless\\', \\'ideas\\']\\n\\n\\n\\n\\nNote\\nWhen you type list(pos) you might see a different order\\nto the one shown above.  If you want to see the keys in order, just sort them.\\n\\nAs well as iterating over all keys\\nin the dictionary with a for loop, we can use the for loop\\nas we did for printing lists:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; for word in sorted(pos):\\n...     print(word + \":\", pos[word])\\n...\\ncolorless: ADJ\\nfuriously: ADV\\nsleep: V\\nideas: N\\n\\n\\n\\nFinally, the dictionary methods keys(), values() and\\nitems() allow us to access the keys, values, and key-value pairs as separate lists.\\nWe can even sort tuples , which orders them according to their first element\\n(and if the first elements are the same, it uses their second elements).\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; list(pos.keys())\\n[\\'colorless\\', \\'furiously\\', \\'sleep\\', \\'ideas\\']\\n&gt;&gt;&gt; list(pos.values())\\n[\\'ADJ\\', \\'ADV\\', \\'V\\', \\'N\\']\\n&gt;&gt;&gt; list(pos.items())\\n[(\\'colorless\\', \\'ADJ\\'), (\\'furiously\\', \\'ADV\\'), (\\'sleep\\', \\'V\\'), (\\'ideas\\', \\'N\\')]\\n&gt;&gt;&gt; for key, val in sorted(pos.items()): \\n...     print(key + \":\", val)\\n...\\ncolorless: ADJ\\nfuriously: ADV\\nideas: N\\nsleep: V\\n\\n\\n\\nWe want to be sure that when we look something up in a dictionary, we\\nonly get one value for each key. Now\\nsuppose we try to use a dictionary to store the fact that the\\nword sleep can be used as both a verb and a noun:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; pos[\\'sleep\\'] = \\'V\\'\\n&gt;&gt;&gt; pos[\\'sleep\\']\\n\\'V\\'\\n&gt;&gt;&gt; pos[\\'sleep\\'] = \\'N\\'\\n&gt;&gt;&gt; pos[\\'sleep\\']\\n\\'N\\'\\n\\n\\n\\nInitially, pos[\\'sleep\\'] is given the value \\'V\\'. But this is\\nimmediately overwritten with the new value \\'N\\'.\\nIn other words, there can only be one entry in the dictionary for \\'sleep\\'.\\nHowever, there is a way of storing multiple values in\\nthat entry: we use a list value,\\ne.g. pos[\\'sleep\\'] = [\\'N\\', \\'V\\'].  In fact, this is what we\\nsaw in 4 for the CMU Pronouncing Dictionary,\\nwhich stores multiple pronunciations for a single word.\\n\\n\\n3.3&nbsp;&nbsp;&nbsp;Defining Dictionaries\\nWe can use the same key-value pair format to create a dictionary.  There\\'s\\na couple of ways to do this, and we will normally use the first:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; pos = {\\'colorless\\': \\'ADJ\\', \\'ideas\\': \\'N\\', \\'sleep\\': \\'V\\', \\'furiously\\': \\'ADV\\'}\\n&gt;&gt;&gt; pos = dict(colorless=\\'ADJ\\', ideas=\\'N\\', sleep=\\'V\\', furiously=\\'ADV\\')\\n\\n\\n\\nNote that dictionary keys must be immutable types, such as strings and tuples.\\nIf we try to define a dictionary using a mutable key, we get a TypeError:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; pos = {[\\'ideas\\', \\'blogs\\', \\'adventures\\']: \\'N\\'}\\nTraceback (most recent call last):\\n  File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\\nTypeError: list objects are unhashable\\n\\n\\n\\n\\n\\n3.4&nbsp;&nbsp;&nbsp;Default Dictionaries\\nIf we try to access a key that is not in a dictionary, we get an error.\\nHowever, its often useful if a dictionary can automatically create\\nan entry for this new key and give it a default value, such as zero or\\nthe empty list.  For this reason, a special kind of dictionary\\ncalled a defaultdict is available.\\nIn order to use it, we have to supply a parameter which can be used to\\ncreate the default value, e.g. int, float, str, list, dict,\\ntuple.\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; from collections import defaultdict\\n&gt;&gt;&gt; frequency = defaultdict(int)\\n&gt;&gt;&gt; frequency[\\'colorless\\'] = 4\\n&gt;&gt;&gt; frequency[\\'ideas\\']\\n0\\n&gt;&gt;&gt; pos = defaultdict(list)\\n&gt;&gt;&gt; pos[\\'sleep\\'] = [\\'NOUN\\', \\'VERB\\']\\n&gt;&gt;&gt; pos[\\'ideas\\']\\n[]\\n\\n\\n\\n\\nNote\\nThese default values are actually functions that convert other\\nobjects to the specified type (e.g. int(\"2\"), list(\"2\")).\\nWhen they are called with no parameter — int(), list()\\n— they return 0 and [] respectively.\\n\\nThe above examples specified the default value of a dictionary entry to\\nbe the default value of a particular data type.  However, we can specify\\nany default value we like, simply by providing the name of a function\\nthat can be called with no arguments to create the required value.\\nLet\\'s return to our part-of-speech example, and create a dictionary\\nwhose default value for any entry is \\'N\\' .\\nWhen we access a non-existent entry ,\\nit is automatically added to the dictionary .\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; pos = defaultdict(lambda: \\'NOUN\\') \\n&gt;&gt;&gt; pos[\\'colorless\\'] = \\'ADJ\\'\\n&gt;&gt;&gt; pos[\\'blog\\'] \\n\\'NOUN\\'\\n&gt;&gt;&gt; list(pos.items())\\n[(\\'blog\\', \\'NOUN\\'), (\\'colorless\\', \\'ADJ\\')] # [_automatically-added]\\n\\n\\n\\n\\nNote\\nThe above example used a lambda expression, introduced in\\n4.4.  This lambda expression specifies no\\nparameters, so we call it using parentheses with no arguments.\\nThus, the definitions of f and g below are equivalent:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; f = lambda: \\'NOUN\\'\\n&gt;&gt;&gt; f()\\n\\'NOUN\\'\\n&gt;&gt;&gt; def g():\\n...     return \\'NOUN\\'\\n&gt;&gt;&gt; g()\\n\\'NOUN\\'\\n\\n\\n\\n\\nLet\\'s see how default dictionaries could be used in a more substantial\\nlanguage processing task.\\nMany language processing tasks — including tagging — struggle to correctly process\\nthe hapaxes of a text.  They can perform better with a fixed vocabulary and a\\nguarantee that no new words will appear.  We can preprocess a text to replace\\nlow-frequency words with a special \"out of vocabulary\" token UNK, with\\nthe help of a default dictionary.  (Can you work out how to do this without\\nreading on?)\\nWe need to create a default dictionary that maps each word to its replacement.\\nThe most frequent n words will be mapped to themselves.\\nEverything else will be mapped to UNK.\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; alice = nltk.corpus.gutenberg.words(\\'carroll-alice.txt\\')\\n&gt;&gt;&gt; vocab = nltk.FreqDist(alice)\\n&gt;&gt;&gt; v1000 = [word for (word, _) in vocab.most_common(1000)]\\n&gt;&gt;&gt; mapping = defaultdict(lambda: \\'UNK\\')\\n&gt;&gt;&gt; for v in v1000:\\n...     mapping[v] = v\\n...\\n&gt;&gt;&gt; alice2 = [mapping[v] for v in alice]\\n&gt;&gt;&gt; alice2[:100]\\n[\\'UNK\\', \\'Alice\\', \"\\'\", \\'s\\', \\'UNK\\', \\'in\\', \\'UNK\\', \\'by\\', \\'UNK\\', \\'UNK\\', \\'UNK\\',\\n\\'UNK\\', \\'CHAPTER\\', \\'I\\', \\'.\\', \\'UNK\\', \\'the\\', \\'Rabbit\\', \\'-\\', \\'UNK\\', \\'Alice\\',\\n\\'was\\', \\'beginning\\', \\'to\\', \\'get\\', \\'very\\', \\'tired\\', \\'of\\', \\'sitting\\', \\'by\\',\\n\\'her\\', \\'sister\\', \\'on\\', \\'the\\', \\'UNK\\', \\',\\', \\'and\\', \\'of\\', \\'having\\', \\'nothing\\',\\n\\'to\\', \\'do\\', \\':\\', \\'once\\', \\'or\\', \\'twice\\', \\'she\\', \\'had\\', \\'UNK\\', \\'into\\', \\'the\\',\\n\\'book\\', \\'her\\', \\'sister\\', \\'was\\', \\'UNK\\', \\',\\', \\'but\\', \\'it\\', \\'had\\', \\'no\\',\\n\\'pictures\\', \\'or\\', \\'UNK\\', \\'in\\', \\'it\\', \\',\\', \"\\'\", \\'and\\', \\'what\\', \\'is\\', \\'the\\',\\n\\'use\\', \\'of\\', \\'a\\', \\'book\\', \",\\'\", \\'thought\\', \\'Alice\\', \"\\'\", \\'without\\',\\n\\'pictures\\', \\'or\\', \\'conversation\\', \"?\\'\" ...]\\n&gt;&gt;&gt; len(set(alice2))\\n1001\\n\\n\\n\\n<!-- note: |TRY|\\nRepeat the above example for different vocabulary sizes and different texts.\\nHow small a vocabulary can you tolerate while still getting something useful\\nfrom the text? -->\\n\\n\\n3.5&nbsp;&nbsp;&nbsp;Incrementally Updating a Dictionary\\nWe can employ dictionaries to count occurrences, emulating the method\\nfor tallying words shown in fig-tally.\\nWe begin by initializing an empty defaultdict, then process each\\npart-of-speech tag in the text.  If the tag hasn\\'t been seen before,\\nit will have a zero count by default.  Each time we encounter a tag,\\nwe increment its count using the += operator.\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; from collections import defaultdict\\n&gt;&gt;&gt; counts = defaultdict(int)\\n&gt;&gt;&gt; from nltk.corpus import brown\\n&gt;&gt;&gt; for (word, tag) in brown.tagged_words(categories=\\'news\\', tagset=\\'universal\\'):\\n...     counts[tag] += 1\\n...\\n&gt;&gt;&gt; counts[\\'NOUN\\']\\n30640\\n&gt;&gt;&gt; sorted(counts)\\n[\\'ADJ\\', \\'PRT\\', \\'ADV\\', \\'X\\', \\'CONJ\\', \\'PRON\\', \\'VERB\\', \\'.\\', \\'NUM\\', \\'NOUN\\', \\'ADP\\', \\'DET\\']\\n\\n&gt;&gt;&gt; from operator import itemgetter\\n&gt;&gt;&gt; sorted(counts.items(), key=itemgetter(1), reverse=True)\\n[(\\'NOUN\\', 30640), (\\'VERB\\', 14399), (\\'ADP\\', 12355), (\\'.\\', 11928), ...]\\n&gt;&gt;&gt; [t for t, c in sorted(counts.items(), key=itemgetter(1), reverse=True)]\\n[\\'NOUN\\', \\'VERB\\', \\'ADP\\', \\'.\\', \\'DET\\', \\'ADJ\\', \\'ADV\\', \\'CONJ\\', \\'PRON\\', \\'PRT\\', \\'NUM\\', \\'X\\']\\n\\n\\nExample 3.3 (code_dictionary.py): Figure 3.3: Incrementally Updating a Dictionary, and Sorting by Value\\n\\nThe listing in 3.3 illustrates an important idiom for\\nsorting a dictionary by its values, to show words in decreasing\\norder of frequency.  The first parameter of sorted() is the items\\nto sort, a list of tuples consisting of a POS tag and a frequency.\\nThe second parameter specifies the sort key using a function itemgetter().\\nIn general, itemgetter(n) returns a function that can be called on\\nsome other sequence object to obtain the nth element, e.g.:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; pair = (\\'NP\\', 8336)\\n&gt;&gt;&gt; pair[1]\\n8336\\n&gt;&gt;&gt; itemgetter(1)(pair)\\n8336\\n\\n\\n\\nThe last parameter of sorted() specifies that the items should be returned\\nin reverse order, i.e. decreasing values of frequency.\\nThere\\'s a second useful programming idiom at the beginning of\\n3.3, where we initialize a defaultdict and then use a\\nfor loop to update its values. Here\\'s a schematic version:\\n\\n&gt;&gt;&gt; my_dictionary = defaultdict(function to create default value)\\n&gt;&gt;&gt; for item in sequence:\\n...      my_dictionary[item_key] is updated with information about item\\n\\nHere\\'s another instance of this pattern, where we index words according to their last two letters:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; last_letters = defaultdict(list)\\n&gt;&gt;&gt; words = nltk.corpus.words.words(\\'en\\')\\n&gt;&gt;&gt; for word in words:\\n...     key = word[-2:]\\n...     last_letters[key].append(word)\\n...\\n&gt;&gt;&gt; last_letters[\\'ly\\']\\n[\\'abactinally\\', \\'abandonedly\\', \\'abasedly\\', \\'abashedly\\', \\'abashlessly\\', \\'abbreviately\\',\\n\\'abdominally\\', \\'abhorrently\\', \\'abidingly\\', \\'abiogenetically\\', \\'abiologically\\', ...]\\n&gt;&gt;&gt; last_letters[\\'zy\\']\\n[\\'blazy\\', \\'bleezy\\', \\'blowzy\\', \\'boozy\\', \\'breezy\\', \\'bronzy\\', \\'buzzy\\', \\'Chazy\\', ...]\\n\\n\\n\\nThe following example uses the same pattern to create an anagram dictionary.\\n(You might experiment with the third line to get an idea of why this program works.)\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; anagrams = defaultdict(list)\\n&gt;&gt;&gt; for word in words:\\n...     key = \\'\\'.join(sorted(word))\\n...     anagrams[key].append(word)\\n...\\n&gt;&gt;&gt; anagrams[\\'aeilnrt\\']\\n[\\'entrail\\', \\'latrine\\', \\'ratline\\', \\'reliant\\', \\'retinal\\', \\'trenail\\']\\n\\n\\n\\nSince accumulating words like this is such a common task,\\nNLTK provides a more convenient way of creating a defaultdict(list),\\nin the form of nltk.Index().\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; anagrams = nltk.Index((\\'\\'.join(sorted(w)), w) for w in words)\\n&gt;&gt;&gt; anagrams[\\'aeilnrt\\']\\n[\\'entrail\\', \\'latrine\\', \\'ratline\\', \\'reliant\\', \\'retinal\\', \\'trenail\\']\\n\\n\\n\\n\\nNote\\nnltk.Index is a defaultdict(list) with extra support for\\ninitialization.  Similarly,\\nnltk.FreqDist is essentially a defaultdict(int) with extra\\nsupport for initialization (along with sorting and plotting methods).\\n\\n\\n\\n3.6&nbsp;&nbsp;&nbsp;Complex Keys and Values\\nWe can use default dictionaries with complex keys and values.\\nLet\\'s study the range of possible tags for a word, given the\\nword itself, and the tag of the previous word.  We will see\\nhow this information can be used by a POS tagger.\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; pos = defaultdict(lambda: defaultdict(int))\\n&gt;&gt;&gt; brown_news_tagged = brown.tagged_words(categories=\\'news\\', tagset=\\'universal\\')\\n&gt;&gt;&gt; for ((w1, t1), (w2, t2)) in nltk.bigrams(brown_news_tagged): \\n...     pos[(t1, w2)][t2] += 1 \\n...\\n&gt;&gt;&gt; pos[(\\'DET\\', \\'right\\')] \\ndefaultdict(&lt;class \\'int\\'&gt;, {\\'ADJ\\': 11, \\'NOUN\\': 5})\\n\\n\\n\\nThis example uses a dictionary whose default value for an entry\\nis a dictionary (whose default value is int(), i.e. zero).\\nNotice how we iterated over the bigrams of the tagged\\ncorpus, processing a pair of word-tag pairs for each iteration .\\nEach time through the loop we updated our pos dictionary\\'s\\nentry for (t1, w2), a tag and its following word .\\nWhen we look up an item in pos we must specify a compound key ,\\nand we get back a dictionary object.\\nA POS tagger could use such information to decide that the\\nword right, when preceded by a determiner, should be tagged as ADJ.\\n\\n\\n3.7&nbsp;&nbsp;&nbsp;Inverting a Dictionary\\nDictionaries support efficient lookup, so long as you want to get the value for\\nany key.  If d is a dictionary and k is a key, we type d[k] and\\nimmediately obtain the value.  Finding a key given a value is slower and more\\ncumbersome:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; counts = defaultdict(int)\\n&gt;&gt;&gt; for word in nltk.corpus.gutenberg.words(\\'milton-paradise.txt\\'):\\n...     counts[word] += 1\\n...\\n&gt;&gt;&gt; [key for (key, value) in counts.items() if value == 32]\\n[\\'brought\\', \\'Him\\', \\'virtue\\', \\'Against\\', \\'There\\', \\'thine\\', \\'King\\', \\'mortal\\',\\n\\'every\\', \\'been\\']\\n\\n\\n\\nIf we expect to do this kind of \"reverse lookup\" often, it helps to construct\\na dictionary that maps values to keys.  In the case that no two keys have\\nthe same value, this is an easy thing to do.  We just get all the key-value\\npairs in the dictionary, and create a new dictionary of value-key\\npairs. The next example also illustrates another way of initializing a\\ndictionary pos with key-value pairs.\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; pos = {\\'colorless\\': \\'ADJ\\', \\'ideas\\': \\'N\\', \\'sleep\\': \\'V\\', \\'furiously\\': \\'ADV\\'}\\n&gt;&gt;&gt; pos2 = dict((value, key) for (key, value) in pos.items())\\n&gt;&gt;&gt; pos2[\\'N\\']\\n\\'ideas\\'\\n\\n\\n\\nLet\\'s first make our part-of-speech dictionary a bit more realistic\\nand add some more words to pos using the dictionary update() method, to\\ncreate the situation where multiple keys have the same value. Then the\\ntechnique just shown for reverse lookup will no longer work (why\\nnot?).  Instead, we have to use append() to accumulate the words\\nfor each part-of-speech, as follows:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; pos.update({\\'cats\\': \\'N\\', \\'scratch\\': \\'V\\', \\'peacefully\\': \\'ADV\\', \\'old\\': \\'ADJ\\'})\\n&gt;&gt;&gt; pos2 = defaultdict(list)\\n&gt;&gt;&gt; for key, value in pos.items():\\n...     pos2[value].append(key)\\n...\\n&gt;&gt;&gt; pos2[\\'ADV\\']\\n[\\'peacefully\\', \\'furiously\\']\\n\\n\\n\\nNow we have inverted the pos dictionary, and can look up any part-of-speech and find\\nall words having that part-of-speech.  We can do the same thing even\\nmore simply using NLTK\\'s support for indexing as follows:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; pos2 = nltk.Index((value, key) for (key, value) in pos.items())\\n&gt;&gt;&gt; pos2[\\'ADV\\']\\n[\\'peacefully\\', \\'furiously\\']\\n\\n\\n\\nA summary of Python\\'s dictionary methods is given in 3.2.\\nTable 3.2: Python\\'s Dictionary Methods: A summary of commonly-used methods and idioms\\ninvolving dictionaries.\\n\\n\\n\\n\\n\\nExample\\nDescription\\n\\n\\n\\nd = {}\\ncreate an empty dictionary and assign it to d\\n\\nd[key] = value\\nassign a value to a given dictionary key\\n\\nd.keys()\\nthe list of keys of the dictionary\\n\\nlist(d)\\nthe list of keys of the dictionary\\n\\nsorted(d)\\nthe keys of the dictionary, sorted\\n\\nkey in d\\ntest whether a particular key is in the dictionary\\n\\nfor key in d\\niterate over the keys of the dictionary\\n\\nd.values()\\nthe list of values in the dictionary\\n\\ndict([(k1,v1), (k2,v2), ...])\\ncreate a dictionary from a list of key-value pairs\\n\\nd1.update(d2)\\nadd all items from d2 to d1\\n\\ndefaultdict(int)\\na dictionary whose default value is zero\\n\\n\\n\\n\\n\\n\\n\\n\\n4&nbsp;&nbsp;&nbsp;Automatic Tagging\\nIn the rest of this chapter we will explore various ways to automatically\\nadd part-of-speech tags to text.  We will see that the tag of a word depends\\non the word and its context within a sentence.  For this reason, we will\\nbe working with data at the level of (tagged) sentences rather than words.\\nWe\\'ll begin by loading the data we will be using.\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; from nltk.corpus import brown\\n&gt;&gt;&gt; brown_tagged_sents = brown.tagged_sents(categories=\\'news\\')\\n&gt;&gt;&gt; brown_sents = brown.sents(categories=\\'news\\')\\n\\n\\n\\n\\n4.1&nbsp;&nbsp;&nbsp;The Default Tagger\\nThe simplest possible tagger assigns the same tag to each token.  This\\nmay seem to be a rather banal step, but it establishes an important\\nbaseline for tagger performance.  In order to get the best result, we\\ntag each word with the most likely tag.  Let\\'s find out which tag is\\nmost likely (now using the unsimplified tagset):\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; tags = [tag for (word, tag) in brown.tagged_words(categories=\\'news\\')]\\n&gt;&gt;&gt; nltk.FreqDist(tags).max()\\n\\'NN\\'\\n\\n\\n\\nNow we can create a tagger that tags everything as NN.\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; raw = \\'I do not like green eggs and ham, I do not like them Sam I am!\\'\\n&gt;&gt;&gt; tokens = nltk.word_tokenize(raw)\\n&gt;&gt;&gt; default_tagger = nltk.DefaultTagger(\\'NN\\')\\n&gt;&gt;&gt; default_tagger.tag(tokens)\\n[(\\'I\\', \\'NN\\'), (\\'do\\', \\'NN\\'), (\\'not\\', \\'NN\\'), (\\'like\\', \\'NN\\'), (\\'green\\', \\'NN\\'),\\n(\\'eggs\\', \\'NN\\'), (\\'and\\', \\'NN\\'), (\\'ham\\', \\'NN\\'), (\\',\\', \\'NN\\'), (\\'I\\', \\'NN\\'),\\n(\\'do\\', \\'NN\\'), (\\'not\\', \\'NN\\'), (\\'like\\', \\'NN\\'), (\\'them\\', \\'NN\\'), (\\'Sam\\', \\'NN\\'),\\n(\\'I\\', \\'NN\\'), (\\'am\\', \\'NN\\'), (\\'!\\', \\'NN\\')]\\n\\n\\n\\nUnsurprisingly, this method performs rather poorly.\\nOn a typical corpus, it will tag only about an eighth of the tokens correctly,\\nas we see below:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; default_tagger.evaluate(brown_tagged_sents)\\n0.13089484257215028\\n\\n\\n\\nDefault taggers assign their tag to every single word, even words that\\nhave never been encountered before.  As it happens, once we have processed\\nseveral thousand words of English text, most new words will be nouns.\\nAs we will see, this means that default taggers can help to improve the\\nrobustness of a language processing system.  We will return to them\\nshortly.\\n\\n\\n4.2&nbsp;&nbsp;&nbsp;The Regular Expression Tagger\\nThe regular expression tagger assigns tags to tokens on the basis of\\nmatching patterns.  For instance, we might guess that any word ending\\nin ed is the past participle of a verb, and any word ending with\\n\\'s is a possessive noun.  We can express these as a list of\\nregular expressions:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; patterns = [\\n...     (r\\'.*ing$\\', \\'VBG\\'),                # gerunds\\n...     (r\\'.*ed$\\', \\'VBD\\'),                 # simple past\\n...     (r\\'.*es$\\', \\'VBZ\\'),                 # 3rd singular present\\n...     (r\\'.*ould$\\', \\'MD\\'),                # modals\\n...     (r\\'.*\\\\\\'s$\\', \\'NN$\\'),                # possessive nouns\\n...     (r\\'.*s$\\', \\'NNS\\'),                  # plural nouns\\n...     (r\\'^-?[0-9]+(\\\\.[0-9]+)?$\\', \\'CD\\'),  # cardinal numbers\\n...     (r\\'.*\\', \\'NN\\')                      # nouns (default)\\n... ]\\n\\n\\n\\nNote that these are processed in order, and the first one that matches is applied.\\nNow we can set up a tagger and use it to tag a sentence.  Now its right about a fifth\\nof the time.\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; regexp_tagger = nltk.RegexpTagger(patterns)\\n&gt;&gt;&gt; regexp_tagger.tag(brown_sents[3])\\n[(\\'``\\', \\'NN\\'), (\\'Only\\', \\'NN\\'), (\\'a\\', \\'NN\\'), (\\'relative\\', \\'NN\\'), (\\'handful\\', \\'NN\\'),\\n(\\'of\\', \\'NN\\'), (\\'such\\', \\'NN\\'), (\\'reports\\', \\'NNS\\'), (\\'was\\', \\'NNS\\'), (\\'received\\', \\'VBD\\'),\\n(\"\\'\\'\", \\'NN\\'), (\\',\\', \\'NN\\'), (\\'the\\', \\'NN\\'), (\\'jury\\', \\'NN\\'), (\\'said\\', \\'NN\\'), (\\',\\', \\'NN\\'),\\n(\\'``\\', \\'NN\\'), (\\'considering\\', \\'VBG\\'), (\\'the\\', \\'NN\\'), (\\'widespread\\', \\'NN\\'), ...]\\n&gt;&gt;&gt; regexp_tagger.evaluate(brown_tagged_sents)\\n0.20326391789486245\\n\\n\\n\\nThe final regular expression «.*» is a catch-all that tags everything as a noun.\\nThis is equivalent to the default tagger (only much less efficient).\\nInstead of re-specifying this as part of the regular expression tagger,\\nis there a way to combine this tagger with the default tagger?  We\\nwill see how to do this shortly.\\n\\nNote\\nYour Turn:\\nSee if you can come up with patterns to improve the performance of the above\\nregular expression tagger.  (Note that 1\\ndescribes a way to partially automate such work.)\\n\\n\\n\\n4.3&nbsp;&nbsp;&nbsp;The Lookup Tagger\\nA lot of high-frequency words do not have the NN tag.\\nLet\\'s find the hundred most frequent words and store their most likely tag.\\nWe can then use this information as the model for a \"lookup tagger\"\\n(an NLTK UnigramTagger):\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; fd = nltk.FreqDist(brown.words(categories=\\'news\\'))\\n&gt;&gt;&gt; cfd = nltk.ConditionalFreqDist(brown.tagged_words(categories=\\'news\\'))\\n&gt;&gt;&gt; most_freq_words = fd.most_common(100)\\n&gt;&gt;&gt; likely_tags = dict((word, cfd[word].max()) for (word, _) in most_freq_words)\\n&gt;&gt;&gt; baseline_tagger = nltk.UnigramTagger(model=likely_tags)\\n&gt;&gt;&gt; baseline_tagger.evaluate(brown_tagged_sents)\\n0.45578495136941344\\n\\n\\n\\nIt should come as no surprise by now that simply\\nknowing the tags for the 100 most frequent words enables us to tag a large fraction of\\ntokens correctly (nearly half in fact).\\nLet\\'s see what it does on some untagged input text:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; sent = brown.sents(categories=\\'news\\')[3]\\n&gt;&gt;&gt; baseline_tagger.tag(sent)\\n[(\\'``\\', \\'``\\'), (\\'Only\\', None), (\\'a\\', \\'AT\\'), (\\'relative\\', None),\\n(\\'handful\\', None), (\\'of\\', \\'IN\\'), (\\'such\\', None), (\\'reports\\', None),\\n(\\'was\\', \\'BEDZ\\'), (\\'received\\', None), (\"\\'\\'\", \"\\'\\'\"), (\\',\\', \\',\\'),\\n(\\'the\\', \\'AT\\'), (\\'jury\\', None), (\\'said\\', \\'VBD\\'), (\\',\\', \\',\\'),\\n(\\'``\\', \\'``\\'), (\\'considering\\', None), (\\'the\\', \\'AT\\'), (\\'widespread\\', None),\\n(\\'interest\\', None), (\\'in\\', \\'IN\\'), (\\'the\\', \\'AT\\'), (\\'election\\', None),\\n(\\',\\', \\',\\'), (\\'the\\', \\'AT\\'), (\\'number\\', None), (\\'of\\', \\'IN\\'),\\n(\\'voters\\', None), (\\'and\\', \\'CC\\'), (\\'the\\', \\'AT\\'), (\\'size\\', None),\\n(\\'of\\', \\'IN\\'), (\\'this\\', \\'DT\\'), (\\'city\\', None), (\"\\'\\'\", \"\\'\\'\"), (\\'.\\', \\'.\\')]\\n\\n\\n\\nMany words have been assigned a tag of None,\\nbecause they were not among the 100 most frequent words.\\nIn these cases we would like to assign the default tag of NN.\\nIn other words, we want to use the lookup table first,\\nand if it is unable to assign a tag, then use the default tagger,\\na process known as backoff (5).\\nWe do this by specifying one tagger as a parameter to the other,\\nas shown below.  Now the lookup tagger will only store word-tag pairs\\nfor words other than nouns, and whenever it cannot assign a tag to a\\nword it will invoke the default tagger.\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; baseline_tagger = nltk.UnigramTagger(model=likely_tags,\\n...                                      backoff=nltk.DefaultTagger(\\'NN\\'))\\n\\n\\n\\nLet\\'s put all this together and write a program to create and\\nevaluate lookup taggers having a range of sizes, in 4.1.\\n\\n\\n\\n\\n&nbsp;\\ndef performance(cfd, wordlist):\\n    lt = dict((word, cfd[word].max()) for word in wordlist)\\n    baseline_tagger = nltk.UnigramTagger(model=lt, backoff=nltk.DefaultTagger(\\'NN\\'))\\n    return baseline_tagger.evaluate(brown.tagged_sents(categories=\\'news\\'))\\n\\ndef display():\\n    import pylab\\n    word_freqs = nltk.FreqDist(brown.words(categories=\\'news\\')).most_common()\\n    words_by_freq = [w for (w, _) in word_freqs]\\n    cfd = nltk.ConditionalFreqDist(brown.tagged_words(categories=\\'news\\'))\\n    sizes = 2 ** pylab.arange(15)\\n    perfs = [performance(cfd, words_by_freq[:size]) for size in sizes]\\n    pylab.plot(sizes, perfs, \\'-bo\\')\\n    pylab.title(\\'Lookup Tagger Performance with Varying Model Size\\')\\n    pylab.xlabel(\\'Model Size\\')\\n    pylab.ylabel(\\'Performance\\')\\n    pylab.show()\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; display()                                  \\n\\n\\nExample 4.1 (code_baseline_tagger.py): Figure 4.1: Lookup Tagger Performance with Varying Model Size\\n\\n\\n\\nFigure 4.2: Lookup Tagger\\n\\nObserve that performance initially increases rapidly as the model size grows, eventually\\nreaching a plateau, when large increases in model size yield little improvement\\nin performance.  (This example used the pylab plotting package, discussed\\nin 4.8.)\\n\\n\\n4.4&nbsp;&nbsp;&nbsp;Evaluation\\nIn the above examples, you will have noticed an emphasis on\\naccuracy scores.  In fact, evaluating the performance of\\nsuch tools is a central theme in NLP.  Recall the processing\\npipeline in fig-sds; any errors in the output of one\\nmodule are greatly multiplied in the downstream modules.\\nWe evaluate the performance of a tagger relative to the tags\\na human expert would assign.  Since we don\\'t usually have access\\nto an expert and impartial human judge, we make do instead with\\ngold standard test data. This is a corpus which has been manually\\nannotated and which is accepted as a standard against which the\\nguesses of an automatic system are assessed. The tagger is regarded as\\nbeing correct if the tag it guesses for a given word is the same as\\nthe gold standard tag.\\nOf course, the humans who designed and carried out the\\noriginal gold standard annotation were only human. Further\\nanalysis might show mistakes in the gold standard, or may\\neventually lead to a revised tagset and more elaborate guidelines.\\nNevertheless, the gold standard is by definition \"correct\"\\nas far as the evaluation of an automatic tagger is concerned.\\n\\nNote\\nDeveloping an annotated corpus is a major undertaking.\\nApart from the data, it generates sophisticated tools,\\ndocumentation, and practices for ensuring high quality\\nannotation.  The tagsets and other coding schemes inevitably\\ndepend on some theoretical position that is not shared by\\nall, however corpus creators often go to great lengths to\\nmake their work as theory-neutral as possible in order to\\nmaximize the usefulness of their work.  We will discuss\\nthe challenges of creating a corpus in 11..\\n\\n\\n\\n\\n5&nbsp;&nbsp;&nbsp;N-Gram Tagging\\n\\n5.1&nbsp;&nbsp;&nbsp;Unigram Tagging\\nUnigram taggers are based on a simple statistical algorithm:\\nfor each token, assign the tag that is most likely for\\nthat particular token. For example, it will assign the tag JJ to any\\noccurrence of the word frequent, since frequent is used as an\\nadjective (e.g. a frequent word) more often than it is used as a\\nverb (e.g. I frequent this cafe).\\nA unigram tagger behaves just like a lookup tagger (4),\\nexcept there is a more convenient technique for setting it up,\\ncalled training.  In the following code sample,\\nwe train a unigram tagger, use it to tag a sentence, then evaluate:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; from nltk.corpus import brown\\n&gt;&gt;&gt; brown_tagged_sents = brown.tagged_sents(categories=\\'news\\')\\n&gt;&gt;&gt; brown_sents = brown.sents(categories=\\'news\\')\\n&gt;&gt;&gt; unigram_tagger = nltk.UnigramTagger(brown_tagged_sents)\\n&gt;&gt;&gt; unigram_tagger.tag(brown_sents[2007])\\n[(\\'Various\\', \\'JJ\\'), (\\'of\\', \\'IN\\'), (\\'the\\', \\'AT\\'), (\\'apartments\\', \\'NNS\\'),\\n(\\'are\\', \\'BER\\'), (\\'of\\', \\'IN\\'), (\\'the\\', \\'AT\\'), (\\'terrace\\', \\'NN\\'), (\\'type\\', \\'NN\\'),\\n(\\',\\', \\',\\'), (\\'being\\', \\'BEG\\'), (\\'on\\', \\'IN\\'), (\\'the\\', \\'AT\\'), (\\'ground\\', \\'NN\\'),\\n(\\'floor\\', \\'NN\\'), (\\'so\\', \\'QL\\'), (\\'that\\', \\'CS\\'), (\\'entrance\\', \\'NN\\'), (\\'is\\', \\'BEZ\\'),\\n(\\'direct\\', \\'JJ\\'), (\\'.\\', \\'.\\')]\\n&gt;&gt;&gt; unigram_tagger.evaluate(brown_tagged_sents)\\n0.9349006503968017\\n\\n\\n\\nWe train a UnigramTagger by specifying tagged sentence data as\\na parameter when we initialize the tagger.  The training process involves\\ninspecting the tag of each word and storing the most likely tag for any word\\nin a dictionary, stored inside the tagger.\\n\\n\\n5.2&nbsp;&nbsp;&nbsp;Separating the Training and Testing Data\\nNow that we are training a tagger on some data, we must be careful not to test it on the\\nsame data, as we did in the above example.  A tagger that simply memorized its training data\\nand made no attempt to construct a general model would get a perfect score, but would also\\nbe useless for tagging new text.  Instead, we should split the data, training on 90% and\\ntesting on the remaining 10%:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; size = int(len(brown_tagged_sents) * 0.9)\\n&gt;&gt;&gt; size\\n4160\\n&gt;&gt;&gt; train_sents = brown_tagged_sents[:size]\\n&gt;&gt;&gt; test_sents = brown_tagged_sents[size:]\\n&gt;&gt;&gt; unigram_tagger = nltk.UnigramTagger(train_sents)\\n&gt;&gt;&gt; unigram_tagger.evaluate(test_sents)\\n0.811721...\\n\\n\\n\\nAlthough the score is worse, we now have a better picture of the usefulness of\\nthis tagger, i.e. its performance on previously unseen text.\\n\\n\\n5.3&nbsp;&nbsp;&nbsp;General N-Gram Tagging\\nWhen we perform a language processing task based on unigrams, we are using\\none item of context.  In the case of tagging, we only consider the current\\ntoken, in isolation from any larger context.  Given such a model, the best\\nwe can do is tag each word with its a priori most likely tag.\\nThis means we would tag a word such as wind with the same tag,\\nregardless of whether it appears in the context the wind or\\nto wind.\\nAn n-gram tagger is a generalization of a unigram tagger whose context is\\nthe current word together with the part-of-speech tags of the\\nn-1 preceding tokens, as shown in 5.1. The tag to be\\nchosen, tn, is circled, and the context is shaded\\nin grey. In the example of an n-gram tagger shown in 5.1,\\nwe have n=3; that is, we consider the tags of the two preceding words in addition\\nto the current word.  An n-gram tagger\\npicks the tag that is most likely in the given context.\\n\\n\\nFigure 5.1: Tagger Context\\n\\n\\nNote\\nA 1-gram tagger is another term for a unigram tagger: i.e.,\\nthe context used to tag a token is just the text of the token itself.\\n2-gram taggers are also called bigram taggers, and 3-gram taggers\\nare called trigram taggers.\\n\\nThe NgramTagger class uses a tagged training corpus to determine which\\npart-of-speech tag is most likely for each context.  Here we see\\na special case of an n-gram tagger, namely a bigram tagger.\\nFirst we train it, then use it to tag untagged sentences:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; bigram_tagger = nltk.BigramTagger(train_sents)\\n&gt;&gt;&gt; bigram_tagger.tag(brown_sents[2007])\\n[(\\'Various\\', \\'JJ\\'), (\\'of\\', \\'IN\\'), (\\'the\\', \\'AT\\'), (\\'apartments\\', \\'NNS\\'),\\n(\\'are\\', \\'BER\\'), (\\'of\\', \\'IN\\'), (\\'the\\', \\'AT\\'), (\\'terrace\\', \\'NN\\'),\\n(\\'type\\', \\'NN\\'), (\\',\\', \\',\\'), (\\'being\\', \\'BEG\\'), (\\'on\\', \\'IN\\'), (\\'the\\', \\'AT\\'),\\n(\\'ground\\', \\'NN\\'), (\\'floor\\', \\'NN\\'), (\\'so\\', \\'CS\\'), (\\'that\\', \\'CS\\'),\\n(\\'entrance\\', \\'NN\\'), (\\'is\\', \\'BEZ\\'), (\\'direct\\', \\'JJ\\'), (\\'.\\', \\'.\\')]\\n&gt;&gt;&gt; unseen_sent = brown_sents[4203]\\n&gt;&gt;&gt; bigram_tagger.tag(unseen_sent)\\n[(\\'The\\', \\'AT\\'), (\\'population\\', \\'NN\\'), (\\'of\\', \\'IN\\'), (\\'the\\', \\'AT\\'), (\\'Congo\\', \\'NP\\'),\\n(\\'is\\', \\'BEZ\\'), (\\'13.5\\', None), (\\'million\\', None), (\\',\\', None), (\\'divided\\', None),\\n(\\'into\\', None), (\\'at\\', None), (\\'least\\', None), (\\'seven\\', None), (\\'major\\', None),\\n(\\'``\\', None), (\\'culture\\', None), (\\'clusters\\', None), (\"\\'\\'\", None), (\\'and\\', None),\\n(\\'innumerable\\', None), (\\'tribes\\', None), (\\'speaking\\', None), (\\'400\\', None),\\n(\\'separate\\', None), (\\'dialects\\', None), (\\'.\\', None)]\\n\\n\\n\\nNotice that the bigram tagger manages to tag every word in a sentence it saw during\\ntraining, but does badly on an unseen sentence.  As soon as it encounters a new word\\n(i.e., 13.5), it is unable to assign a tag.  It cannot tag the following word\\n(i.e., million) even if it was seen during training, simply because it never\\nsaw it during training with a None tag on the previous word.  Consequently, the\\ntagger fails to tag the rest of the sentence.  Its overall accuracy score is very low:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; bigram_tagger.evaluate(test_sents)\\n0.102063...\\n\\n\\n\\n\\nAs n gets larger, the specificity of the contexts increases,\\nas does the chance that the data we wish to tag contains contexts that\\nwere not present in the training data. This is known as the sparse\\ndata problem, and is quite pervasive in NLP. As a consequence, there is a\\ntrade-off between the accuracy and the coverage of our results (and\\nthis is related to the precision/recall trade-off in information\\nretrieval).\\n\\nCaution!\\nn-gram taggers should not consider context that crosses a\\nsentence boundary.  Accordingly, NLTK taggers are designed to work\\nwith lists of sentences, where each sentence is a list of words.  At\\nthe start of a sentence, tn-1 and preceding\\ntags are set to None.\\n\\n\\n\\n5.4&nbsp;&nbsp;&nbsp;Combining Taggers\\nOne way to address the trade-off between accuracy and coverage is to\\nuse the more accurate algorithms when we can, but to fall back on\\nalgorithms with wider coverage when necessary. For example, we could\\ncombine the results of a bigram tagger, a unigram tagger, and\\na default tagger, as follows:\\n\\nTry tagging the token with the bigram tagger.\\nIf the bigram tagger is unable to find a tag for the token, try\\nthe unigram tagger.\\nIf the unigram tagger is also unable to find a tag, use a default tagger.\\n\\nMost NLTK taggers permit a backoff-tagger to be specified.\\nThe backoff-tagger may itself have a backoff tagger:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; t0 = nltk.DefaultTagger(\\'NN\\')\\n&gt;&gt;&gt; t1 = nltk.UnigramTagger(train_sents, backoff=t0)\\n&gt;&gt;&gt; t2 = nltk.BigramTagger(train_sents, backoff=t1)\\n&gt;&gt;&gt; t2.evaluate(test_sents)\\n0.844513...\\n\\n\\n\\n\\nNote\\nYour Turn:\\nExtend the above example by defining a TrigramTagger called\\nt3, which backs off to t2.\\n\\nNote that we specify the backoff tagger when the tagger is\\ninitialized so that training can take advantage of the backoff tagger.\\nThus, if the bigram tagger would assign the same tag\\nas its unigram backoff tagger in a certain context,\\nthe bigram tagger discards the training instance.\\nThis keeps the bigram tagger model as small as possible.  We can\\nfurther specify that a tagger needs to see more than one instance of a\\ncontext in order to retain it, e.g. nltk.BigramTagger(sents, cutoff=2, backoff=t1)\\nwill discard contexts that have only been seen once or twice.\\n\\n\\n5.5&nbsp;&nbsp;&nbsp;Tagging Unknown Words\\nOur approach to tagging unknown words still uses backoff to a regular-expression tagger\\nor a default tagger.  These are unable to make use of context.  Thus, if our tagger\\nencountered the word blog, not seen during training, it would assign it the same tag,\\nregardless of whether this word appeared in the context the blog or to blog.\\nHow can we do better with these unknown words, or out-of-vocabulary items?\\nA useful method to tag unknown words based on context is to limit the vocabulary\\nof a tagger to the most frequent n words, and to replace every other word\\nwith a special word UNK using the method shown in 3.\\nDuring training, a unigram tagger will probably learn that UNK is usually a noun.\\nHowever, the n-gram taggers will detect contexts in which it has some other tag.\\nFor example, if the preceding word is to (tagged TO), then UNK\\nwill probably be tagged as a verb.\\n\\n\\n\\n5.6&nbsp;&nbsp;&nbsp;Storing Taggers\\nTraining a tagger on a large corpus may take a significant time.  Instead of training a tagger\\nevery time we need one, it is convenient to save a trained tagger in a file for later re-use.\\nLet\\'s save our tagger t2 to a file t2.pkl.\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; from pickle import dump\\n&gt;&gt;&gt; output = open(\\'t2.pkl\\', \\'wb\\')\\n&gt;&gt;&gt; dump(t2, output, -1)\\n&gt;&gt;&gt; output.close()\\n\\n\\n\\nNow, in a separate Python process, we can load our saved tagger.\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; from pickle import load\\n&gt;&gt;&gt; input = open(\\'t2.pkl\\', \\'rb\\')\\n&gt;&gt;&gt; tagger = load(input)\\n&gt;&gt;&gt; input.close()\\n\\n\\n\\nNow let\\'s check that it can be used for tagging.\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; text = \"\"\"The board\\'s action shows what free enterprise\\n...     is up against in our complex maze of regulatory laws .\"\"\"\\n&gt;&gt;&gt; tokens = text.split()\\n&gt;&gt;&gt; tagger.tag(tokens)\\n[(\\'The\\', \\'AT\\'), (\"board\\'s\", \\'NN$\\'), (\\'action\\', \\'NN\\'), (\\'shows\\', \\'NNS\\'),\\n(\\'what\\', \\'WDT\\'), (\\'free\\', \\'JJ\\'), (\\'enterprise\\', \\'NN\\'), (\\'is\\', \\'BEZ\\'),\\n(\\'up\\', \\'RP\\'), (\\'against\\', \\'IN\\'), (\\'in\\', \\'IN\\'), (\\'our\\', \\'PP$\\'), (\\'complex\\', \\'JJ\\'),\\n(\\'maze\\', \\'NN\\'), (\\'of\\', \\'IN\\'), (\\'regulatory\\', \\'NN\\'), (\\'laws\\', \\'NNS\\'), (\\'.\\', \\'.\\')]\\n\\n\\n\\n\\n\\n5.7&nbsp;&nbsp;&nbsp;Performance Limitations\\nWhat is the upper limit to the performance of an n-gram tagger?\\nConsider the case of a trigram tagger.  How many cases of part-of-speech ambiguity does it\\nencounter?  We can determine the answer to this question empirically:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; cfd = nltk.ConditionalFreqDist(\\n...            ((x[1], y[1], z[0]), z[1])\\n...            for sent in brown_tagged_sents\\n...            for x, y, z in nltk.trigrams(sent))\\n&gt;&gt;&gt; ambiguous_contexts = [c for c in cfd.conditions() if len(cfd[c]) &gt; 1]\\n&gt;&gt;&gt; sum(cfd[c].N() for c in ambiguous_contexts) / cfd.N()\\n0.049297702068029296\\n\\n\\n\\nThus, one out of twenty trigrams is ambiguous [EXAMPLES].  Given the\\ncurrent word and the previous two tags, in 5% of cases there is more than one tag\\nthat could be legitimately assigned to the current word according to\\nthe training data.  Assuming we always pick the most likely tag in\\nsuch ambiguous contexts, we can derive a lower bound on\\nthe performance of a trigram tagger.\\n\\nAnother way to investigate the performance of a tagger is to study\\nits mistakes.  Some tags may be harder than others to assign, and\\nit might be possible to treat them specially by pre- or post-processing\\nthe data.  A convenient way to look at tagging errors is the\\nconfusion matrix.  It charts expected tags (the gold standard)\\nagainst actual tags generated by a tagger:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; test_tags = [tag for sent in brown.sents(categories=\\'editorial\\')\\n...                  for (word, tag) in t2.tag(sent)]\\n&gt;&gt;&gt; gold_tags = [tag for (word, tag) in brown.tagged_words(categories=\\'editorial\\')]\\n&gt;&gt;&gt; print(nltk.ConfusionMatrix(gold_tags, test_tags))           \\n\\n\\n\\n\\nBased on such analysis we may decide to modify the tagset.  Perhaps\\na distinction between tags that is difficult to make can be dropped,\\nsince it is not important in the context of some larger processing task.\\nAnother way to analyze the performance bound on a tagger comes from\\nthe less than 100% agreement between human annotators.  [MORE]\\nIn general, observe that the tagging process collapses distinctions:\\ne.g. lexical identity is usually lost when all personal pronouns are\\ntagged PRP.  At the same time, the tagging process introduces\\nnew distinctions and removes ambiguities: e.g. deal tagged as VB or NN.\\nThis characteristic of collapsing certain distinctions and introducing new\\ndistinctions is an important feature of tagging which\\nfacilitates classification and prediction.\\nWhen we introduce finer distinctions in a tagset, an n-gram tagger gets\\nmore detailed information about the left-context when it is deciding\\nwhat tag to assign to a particular word.\\nHowever, the tagger simultaneously has to do more work to classify the\\ncurrent token, simply because there are more tags to choose from.\\nConversely, with fewer distinctions (as with the simplified tagset),\\nthe tagger has less information about context, and it has a smaller\\nrange of choices in classifying the current token.\\nWe have seen that ambiguity in the training data leads to an upper limit\\nin tagger performance.  Sometimes more context will resolve the\\nambiguity.  In other cases however, as noted by (Church, Young, &amp; Bloothooft, 1996), the\\nambiguity can only be resolved with reference to syntax, or to world\\nknowledge.  Despite these imperfections, part-of-speech tagging has\\nplayed a central role in the rise of statistical approaches to natural\\nlanguage processing.  In the early 1990s, the surprising accuracy of\\nstatistical taggers was a striking demonstration that it was possible\\nto solve one small part of the language understanding problem, namely\\npart-of-speech disambiguation, without reference to deeper sources of\\nlinguistic knowledge.  Can this idea be pushed further?  In 7.,\\nwe shall see that it can.\\n\\n\\n\\n6&nbsp;&nbsp;&nbsp;Transformation-Based Tagging\\nA potential issue with n-gram taggers is the size of their n-gram\\ntable (or language model).  If tagging is to be employed in a variety\\nof language technologies deployed on mobile computing devices, it is\\nimportant to strike a balance between model size and tagger\\nperformance.  An n-gram tagger with backoff may store trigram and\\nbigram tables, large sparse arrays which may have hundreds of millions\\nof entries.\\nA second issue concerns context.  The only information an n-gram\\ntagger considers from prior context is tags, even though words\\nthemselves might be a useful source of information.  It is simply\\nimpractical for n-gram models to be conditioned on the identities of\\nwords in the context.  In this section we examine Brill tagging,\\nan inductive tagging method which performs very well using models\\nthat are only a tiny fraction of the size of n-gram taggers.\\nBrill tagging is a kind of transformation-based learning, named\\nafter its inventor.  The\\ngeneral idea is very simple: guess the tag of each word, then go back\\nand fix the mistakes.  In this way, a Brill tagger successively\\ntransforms a bad tagging of a text into a better one.  As with n-gram\\ntagging, this is a supervised learning method, since we need\\nannotated training data to figure out whether the tagger\\'s guess is a\\nmistake or not.  However, unlike n-gram tagging, it does\\nnot count observations but compiles a list of transformational\\ncorrection rules.\\nThe process of Brill tagging is usually explained by analogy with\\npainting.  Suppose we were painting a tree, with all its details of\\nboughs, branches, twigs and leaves, against a uniform sky-blue\\nbackground.  Instead of painting the tree first then trying to paint\\nblue in the gaps, it is simpler to paint the whole canvas blue, then\\n\"correct\" the tree section by over-painting the blue background.  In\\nthe same fashion we might paint the trunk a uniform brown before going\\nback to over-paint further details with even finer brushes.  Brill\\ntagging uses the same idea: begin with broad brush strokes then fix up\\nthe details, with successively finer changes.  Let\\'s look at an\\nexample involving the following sentence:\\n\\n  (1)The President said he will ask Congress to increase grants to states\\nfor vocational rehabilitation\\nWe will examine the operation of two rules:\\n(a) Replace NN with VB when the previous word is TO;\\n(b) Replace TO with IN when the next tag is NNS.\\n6.1\\nillustrates this process, first tagging with the unigram tagger, then\\napplying the rules to fix the errors.\\nTable 6.1: Steps in Brill Tagging\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nPhrase\\nto\\nincrease\\ngrants\\nto\\nstates\\nfor\\nvocational\\nrehabilitation\\n\\nUnigram\\nTO\\nNN\\nNNS\\nTO\\nNNS\\nIN\\nJJ\\nNN\\n\\nRule 1\\n&nbsp;\\nVB\\n&nbsp;\\n&nbsp;\\n&nbsp;\\n&nbsp;\\n&nbsp;\\n&nbsp;\\n\\nRule 2\\n&nbsp;\\n&nbsp;\\n&nbsp;\\nIN\\n&nbsp;\\n&nbsp;\\n&nbsp;\\n&nbsp;\\n\\nOutput\\nTO\\nVB\\nNNS\\nIN\\nNNS\\nIN\\nJJ\\nNN\\n\\nGold\\nTO\\nVB\\nNNS\\nIN\\nNNS\\nIN\\nJJ\\nNN\\n\\n\\n\\n\\n\\nIn this table we see two rules.  All such rules are generated from a\\ntemplate of the following form: \"replace T1 with\\nT2 in the context C\".  Typical contexts are the\\nidentity or the tag of the preceding or following word, or the\\nappearance of a specific tag within 2-3 words of the current word.  During\\nits training phase, the tagger guesses values for T1,\\nT2 and C, to create thousands of candidate rules.\\nEach rule is scored according to its net benefit: the\\nnumber of incorrect tags that it corrects, less the number of correct\\ntags it incorrectly modifies.\\n\\nBrill taggers have another interesting property: the rules are\\nlinguistically interpretable.  Compare this with the n-gram taggers,\\nwhich employ a potentially massive table of n-grams.  We cannot learn\\nmuch from direct inspection of such a table, in comparison to the\\nrules learned by the Brill tagger.\\n6.1 demonstrates NLTK\\'s Brill tagger.\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; from nltk.tbl import demo as brill_demo\\n&gt;&gt;&gt; brill_demo.demo()\\nTraining Brill tagger on 80 sentences...\\nFinding initial useful rules...\\n    Found 6555 useful rules.\\n\\n           B      |\\n   S   F   r   O  |        Score = Fixed - Broken\\n   c   i   o   t  |  R     Fixed = num tags changed incorrect -&gt; correct\\n   o   x   k   h  |  u     Broken = num tags changed correct -&gt; incorrect\\n   r   e   e   e  |  l     Other = num tags changed incorrect -&gt; incorrect\\n   e   d   n   r  |  e\\n------------------+-------------------------------------------------------\\n  12  13   1   4  | NN -&gt; VB if the tag of the preceding word is \\'TO\\'\\n   8   9   1  23  | NN -&gt; VBD if the tag of the following word is \\'DT\\'\\n   8   8   0   9  | NN -&gt; VBD if the tag of the preceding word is \\'NNS\\'\\n   6   9   3  16  | NN -&gt; NNP if the tag of words i-2...i-1 is \\'-NONE-\\'\\n   5   8   3   6  | NN -&gt; NNP if the tag of the following word is \\'NNP\\'\\n   5   6   1   0  | NN -&gt; NNP if the text of words i-2...i-1 is \\'like\\'\\n   5   5   0   3  | NN -&gt; VBN if the text of the following word is \\'*-1\\'\\n   ...\\n&gt;&gt;&gt; print(open(\"errors.out\").read())\\n             left context |    word/test-&gt;gold     | right context\\n--------------------------+------------------------+--------------------------\\n                          |      Then/NN-&gt;RB       | ,/, in/IN the/DT guests/N\\n, in/IN the/DT guests/NNS |       \\'/VBD-&gt;POS       | honor/NN ,/, the/DT speed\\n\\'/POS honor/NN ,/, the/DT |    speedway/JJ-&gt;NN     | hauled/VBD out/RP four/CD\\nNN ,/, the/DT speedway/NN |     hauled/NN-&gt;VBD     | out/RP four/CD drivers/NN\\nDT speedway/NN hauled/VBD |      out/NNP-&gt;RP       | four/CD drivers/NNS ,/, c\\ndway/NN hauled/VBD out/RP |      four/NNP-&gt;CD      | drivers/NNS ,/, crews/NNS\\nhauled/VBD out/RP four/CD |    drivers/NNP-&gt;NNS    | ,/, crews/NNS and/CC even\\nP four/CD drivers/NNS ,/, |     crews/NN-&gt;NNS      | and/CC even/RB the/DT off\\nNNS and/CC even/RB the/DT |    official/NNP-&gt;JJ    | Indianapolis/NNP 500/CD a\\n                          |     After/VBD-&gt;IN      | the/DT race/NN ,/, Fortun\\nter/IN the/DT race/NN ,/, |    Fortune/IN-&gt;NNP     | 500/CD executives/NNS dro\\ns/NNS drooled/VBD like/IN |  schoolboys/NNP-&gt;NNS   | over/IN the/DT cars/NNS a\\nolboys/NNS over/IN the/DT |      cars/NN-&gt;NNS      | and/CC drivers/NNS ./.\\n\\n\\nExample 6.1 (code_brill_demo.py): Figure 6.1: Brill Tagger Demonstration: the tagger has a collection of\\ntemplates of the form X -&gt; Y if the preceding word is Z;\\nthe variables in these templates are instantiated to particular\\nwords and tags to create \"rules\"; the score for a rule is the\\nnumber of broken examples it corrects minus the number of\\ncorrect cases it breaks; apart from training a tagger, the\\ndemonstration displays residual errors.\\n\\n\\n\\n\\n\\n7&nbsp;&nbsp;&nbsp;How to Determine the Category of a Word\\nNow that we have examined word classes in detail, we turn to a more\\nbasic question: how do we decide what category a word belongs to in\\nthe first place? In general, linguists use morphological, syntactic,\\nand semantic clues to determine the category of a word.\\n\\n7.1&nbsp;&nbsp;&nbsp;Morphological Clues\\nThe internal structure of a word may give useful clues as to the\\nword\\'s category. For example, -ness is a suffix\\nthat combines with an adjective to produce a noun, e.g.\\nhappy → happiness, ill → illness. So\\nif we encounter a word that ends in -ness, this is very likely\\nto be a noun.  Similarly, -ment is a suffix that combines\\nwith some verbs to produce a noun, e.g.\\ngovern → government and establish → establishment.\\nEnglish verbs can also be morphologically complex.  For instance, the\\npresent participle of a verb ends in -ing, and expresses\\nthe idea of ongoing, incomplete action (e.g. falling, eating).\\nThe -ing suffix also appears on nouns derived from verbs, e.g. the\\nfalling of the leaves (this is known as the gerund).\\n\\n\\n7.2&nbsp;&nbsp;&nbsp;Syntactic Clues\\nAnother source of information is the typical contexts in which a word can\\noccur. For example, assume that we have already determined the\\ncategory of nouns. Then we might say that a syntactic criterion for an\\nadjective in English is that it can occur immediately before a noun,\\nor immediately following the words be or very. According\\nto these tests, near should be categorized as an adjective:\\n\\n  (2)\\n  a.the near window\\n\\n  b.The end is (very) near.\\n\\n\\n\\n7.3&nbsp;&nbsp;&nbsp;Semantic Clues\\nFinally, the meaning of a word is a useful clue as to its lexical\\ncategory.  For example, the best-known definition of a noun is\\nsemantic: \"the name of a person, place or thing\". Within modern linguistics,\\nsemantic criteria for word classes are treated with suspicion, mainly\\nbecause they are hard to formalize. Nevertheless, semantic criteria\\nunderpin many of our intuitions about word classes, and enable us to\\nmake a good guess about the categorization of words in languages that\\nwe are unfamiliar with.  For example, if all we know about the Dutch word\\nverjaardag is that it means the same as the English word\\nbirthday, then we can guess that verjaardag is a noun in\\nDutch. However, some care is needed: although we might translate zij\\nis vandaag jarig as it\\'s her birthday today, the word\\njarig is in fact an adjective in Dutch, and has no exact\\nequivalent in English.\\n\\n\\n7.4&nbsp;&nbsp;&nbsp;New Words\\nAll languages acquire new lexical items. A list of words recently\\nadded to the Oxford Dictionary of English includes cyberslacker,\\nfatoush, blamestorm, SARS, cantopop, bupkis, noughties, muggle, and\\nrobata. Notice that all these new words are nouns, and this is\\nreflected in calling nouns an open class. By contrast, prepositions\\nare regarded as a closed class. That is, there is a limited set of\\nwords belonging to the class (e.g., above, along, at, below, beside,\\nbetween, during, for, from, in, near, on, outside, over, past,\\nthrough, towards, under, up, with), and membership of the set only\\nchanges very gradually over time.\\n\\n\\n7.5&nbsp;&nbsp;&nbsp;Morphology in Part of Speech Tagsets\\n\\nCommon tagsets often capture some morpho-syntactic information;\\nthat is, information about the kind of morphological markings that\\nwords receive by virtue of their syntactic role.  Consider, for\\nexample, the selection of distinct grammatical forms of the word\\ngo illustrated in the following sentences:\\n\\n  (3)\\n  a.Go away!\\n\\n  b.He sometimes goes to the cafe.\\n\\n  c.All the cakes have gone.\\n\\n  d.We went on the excursion.\\n\\nEach of these forms — go, goes, gone, and went —\\nis morphologically distinct from the others. Consider the form,\\ngoes. This occurs in a restricted set of grammatical contexts, and\\nrequires a third person singular subject. Thus, the\\nfollowing sentences are ungrammatical.\\n\\n  (4)\\n  a.*They sometimes goes to the cafe.\\n\\n  b.*I sometimes goes to the cafe.\\n\\nBy contrast, gone is the past participle form; it is required\\nafter have (and cannot be replaced in this context by\\ngoes), and cannot occur as the main verb of a clause.\\n\\n  (5)\\n  a.*All the cakes have goes.\\n\\n  b.*He sometimes gone to the cafe.\\n\\nWe can easily imagine a tagset in which the four distinct\\ngrammatical forms just discussed were all tagged as VB. Although\\nthis would be adequate for some purposes, a more fine-grained tagset\\nprovides useful information about these forms that can help\\nother processors that try to detect patterns in tag\\nsequences.  The Brown tagset captures these distinctions,\\nas summarized in 7.1.\\nTable 7.1: Some morphosyntactic distinctions in the Brown tagset\\n\\n\\n\\n\\n\\n\\nForm\\nCategory\\nTag\\n\\n\\n\\ngo\\nbase\\nVB\\n\\ngoes\\n3rd singular present\\nVBZ\\n\\ngone\\npast participle\\nVBN\\n\\ngoing\\ngerund\\nVBG\\n\\nwent\\nsimple past\\nVBD\\n\\n\\n\\n\\n\\nIn addition to this set of verb tags, the various forms of the verb to be\\nhave special tags:\\nbe/BE, being/BEG, am/BEM, are/BER, is/BEZ, been/BEN, were/BED and\\nwas/BEDZ (plus extra tags for negative forms of the verb).   All told,\\nthis fine-grained tagging of verbs means that an automatic tagger\\nthat uses this tagset is effectively carrying out a limited amount\\nof morphological analysis.\\nMost part-of-speech tagsets make use of the same basic categories,\\nsuch as noun, verb, adjective, and preposition. However, tagsets\\ndiffer both in how finely they divide words into categories, and in\\nhow they define their categories. For example, is might be tagged\\nsimply as a verb in one tagset; but as a distinct form of the lexeme be\\nin another tagset (as in the Brown Corpus).  This variation in tagsets is\\nunavoidable, since part-of-speech tags are used in different ways for\\ndifferent tasks. In other words, there is no one \\'right way\\' to assign\\ntags, only more or less useful ways depending on one\\'s goals.\\n\\n\\n\\n\\n8&nbsp;&nbsp;&nbsp;Summary\\n\\nWords can be grouped into classes, such as nouns, verbs, adjectives, and adverbs.\\nThese classes are known as lexical categories or parts of speech.\\nParts of speech are assigned short labels, or tags, such as NN, VB,\\nThe process of automatically assigning parts of speech to words in text\\nis called part-of-speech tagging, POS tagging, or just tagging.\\nAutomatic tagging is an important step in the NLP pipeline,\\nand is useful in a variety of situations including:\\npredicting the behavior of previously unseen words,\\nanalyzing word usage in corpora, and text-to-speech systems.\\nSome linguistic corpora, such as the Brown Corpus, have been POS tagged.\\nA variety of tagging methods are possible, e.g.\\ndefault tagger, regular expression tagger, unigram tagger and n-gram taggers.\\nThese can be combined using a technique known as backoff.\\nTaggers can be trained and evaluated using tagged corpora.\\nBackoff is a method for combining models: when a more specialized\\nmodel (such as a bigram tagger) cannot assign a tag in a given\\ncontext, we backoff to a more general model (such as a unigram tagger).\\nPart-of-speech tagging is an important, early example of a sequence\\nclassification task in NLP: a classification decision at any one point\\nin the sequence makes use of words and tags in the local context.\\nA dictionary is used to map between arbitrary types of information,\\nsuch as a string and a number: freq[\\'cat\\'] = 12.  We create\\ndictionaries using the brace notation: pos = {},\\npos = {\\'furiously\\': \\'adv\\', \\'ideas\\': \\'n\\', \\'colorless\\': \\'adj\\'}.\\nN-gram taggers can be defined for large values of n, but once\\nn is larger than 3 we usually encounter the sparse data problem;\\neven with a large quantity of training data we only see a tiny\\nfraction of possible contexts.\\nTransformation-based tagging involves learning a series\\nof repair rules of the form \"change tag s to tag\\nt in context c\", where each rule\\nfixes mistakes and possibly introduces a (smaller) number\\nof errors.\\n\\n\\n\\n9&nbsp;&nbsp;&nbsp;Further Reading\\nExtra materials for this chapter are posted at http://nltk.org/, including links to freely\\navailable resources on the web.\\nFor more examples of tagging with NLTK, please see the\\nTagging HOWTO at http://nltk.org/howto.\\nChapters 4 and 5 of (Jurafsky &amp; Martin, 2008) contain more advanced\\nmaterial on n-grams and part-of-speech tagging.\\nThe \"Universal Tagset\" is described by (Petrov, Das, &amp; McDonald, 2012).\\nOther approaches to tagging involve machine learning methods (chap-data-intensive).\\nIn 7. we will see a generalization of tagging called chunking in which a\\ncontiguous sequence of words is assigned a single tag.\\nFor tagset documentation, see\\nnltk.help.upenn_tagset() and nltk.help.brown_tagset().\\nLexical categories are introduced in linguistics textbooks, including those\\nlisted in 1..\\nThere are many other kinds of tagging.\\nWords can be tagged with directives to a speech synthesizer,\\nindicating which words should be emphasized.  Words can be tagged with sense\\nnumbers, indicating which sense of the word was used.  Words can also\\nbe tagged with morphological features.\\nExamples of each of these kinds of tags are shown below.\\nFor space reasons, we only show the tag for a single\\nword. Note also that the first two examples use XML-style\\ntags, where elements in angle brackets enclose the word that is\\ntagged.\\n\\nSpeech Synthesis Markup Language (W3C SSML):\\nThat is a &lt;emphasis&gt;big&lt;/emphasis&gt; car!\\nSemCor: Brown Corpus tagged with WordNet senses:\\nSpace in any &lt;wf pos=\"NN\" lemma=\"form\" wnsn=\"4\"&gt;form&lt;/wf&gt;\\nis completely measured by the three dimensions.\\n(Wordnet form/nn sense 4: \"shape, form, configuration,\\ncontour, conformation\")\\nMorphological tagging, from the Turin University Italian Treebank:\\nE\\' italiano , come progetto e realizzazione , il\\nprimo (PRIMO ADJ ORDIN M SING) porto turistico dell\\' Albania .\\n\\nNote that tagging is also performed at higher levels.  Here is an example\\nof dialogue act tagging, from the NPS Chat Corpus (Forsyth &amp; Martell, 2007) included with\\nNLTK.  Each turn of the dialogue is categorized as to its communicative\\nfunction:\\nStatement  User117 Dude..., I wanted some of that\\nynQuestion User120 m I missing something?\\nBye        User117 I\\'m gonna go fix food, I\\'ll be back later.\\nSystem     User122 JOIN\\nSystem     User2   slaps User122 around a bit with a large trout.\\nStatement  User121 18/m pm me if u tryin to chat\\n\\n\\n\\n10&nbsp;&nbsp;&nbsp;Exercises\\n\\n☼\\nSearch the web for \"spoof newspaper headlines\", to find such gems as:\\nBritish Left Waffles on Falkland Islands, and\\nJuvenile Court to Try Shooting Defendant.\\nManually tag these headlines to see if knowledge of the part-of-speech\\ntags removes the ambiguity.\\n☼\\nWorking with someone else, take turns to pick a word that can be\\neither a noun or a verb (e.g. contest); the opponent has to\\npredict which one is likely to be the most frequent in the Brown corpus; check the\\nopponent\\'s prediction, and tally the score over several turns.\\n☼\\nTokenize and tag the following sentence:\\nThey wind back the clock, while we chase after the wind.\\nWhat different pronunciations and parts of speech are involved?\\n☼ Review the mappings in 3.1.  Discuss any other\\nexamples of mappings you can think of.  What type of information do they map\\nfrom and to?\\n☼ Using the Python interpreter in interactive mode, experiment with\\nthe dictionary examples in this chapter.  Create a dictionary d, and add\\nsome entries.  What happens if you try to access a non-existent\\nentry, e.g. d[\\'xyz\\']?\\n☼ Try deleting an element from a dictionary d, using the syntax\\ndel d[\\'abc\\'].  Check that the item was deleted.\\n☼ Create two dictionaries, d1 and d2, and add some entries to\\neach.  Now issue the command d1.update(d2).  What did this do?\\nWhat might it be useful for?\\n☼ Create a dictionary e, to represent a single lexical entry\\nfor some word of your choice.\\nDefine keys like headword, part-of-speech, sense, and\\nexample, and assign them suitable values.\\n☼ Satisfy yourself that there are\\nrestrictions on the distribution of go and went, in the\\nsense that they cannot be freely interchanged in the kinds of contexts\\nillustrated in (3d) in 7.\\n☼\\nTrain a unigram tagger and run it on some new text.\\nObserve that some words are not assigned a tag.  Why not?\\n☼\\nLearn about the affix tagger (type help(nltk.AffixTagger)).\\nTrain an affix tagger and run it on some new text.\\nExperiment with different settings for the affix length\\nand the minimum word length.  Discuss your findings.\\n☼\\nTrain a bigram tagger with no backoff tagger, and run it on some of the training\\ndata.  Next, run it on some new data.\\nWhat happens to the performance of the tagger?  Why?\\n☼ We can use a dictionary to specify the values to be\\nsubstituted into a formatting string.  Read Python\\'s library\\ndocumentation for formatting strings\\nhttp://docs.python.org/lib/typesseq-strings.html\\nand use this method to display today\\'s date in two\\ndifferent formats.\\n◑ Use sorted() and set() to get a sorted list of tags used in the Brown\\ncorpus, removing duplicates.\\n◑ Write programs to process the Brown Corpus and find answers to the following\\nquestions:\\nWhich nouns are more common in their plural form, rather than their singular\\nform? (Only consider regular plurals, formed with the -s suffix.)\\nWhich word has the greatest number of distinct tags.  What are they, and\\nwhat do they represent?\\nList tags in order of decreasing frequency.  What do the 20 most frequent tags represent?\\nWhich tags are nouns most commonly found after?  What do these tags represent?\\n\\n\\n◑ Explore the following issues that arise in connection with the lookup tagger:\\nWhat happens to the tagger performance for the various\\nmodel sizes when a backoff tagger is omitted?\\nConsider the curve in 4.2; suggest a\\ngood size for a lookup tagger that balances memory and performance.\\nCan you come up with scenarios where it would be preferable to\\nminimize memory usage, or to maximize performance with no regard for memory usage?\\n\\n\\n◑ What is the upper limit of performance for a lookup tagger,\\nassuming no limit to the size of its table?  (Hint: write a program\\nto work out what percentage of tokens of a word are assigned\\nthe most likely tag for that word, on average.)\\n◑ Generate some statistics for tagged data to answer the following questions:\\nWhat proportion of word types are always assigned the same part-of-speech tag?\\nHow many words are ambiguous, in the sense that they appear with at least two tags?\\nWhat percentage of word tokens in the Brown Corpus involve\\nthese ambiguous words?\\n\\n\\n◑ The evaluate() method works out how accurately\\nthe tagger performs on this text.  For example, if the supplied tagged text\\nwas [(\\'the\\', \\'DT\\'), (\\'dog\\', \\'NN\\')] and the tagger produced the output\\n[(\\'the\\', \\'NN\\'), (\\'dog\\', \\'NN\\')], then the score would be 0.5.\\nLet\\'s try to figure out how the evaluation method works:\\nA tagger t takes a list of words as input, and produces a list of tagged words\\nas output.  However, t.evaluate() is given correctly tagged text as its only parameter.\\nWhat must it do with this input before performing the tagging?\\nOnce the tagger has created newly tagged text, how might the evaluate() method\\ngo about comparing it with the original tagged text and computing the accuracy score?\\nNow examine the source code to see how the method is implemented.  Inspect\\nnltk.tag.api.__file__ to discover the location of the source code,\\nand open this file using an editor (be sure to use the api.py file and\\nnot the compiled api.pyc binary file).\\n\\n\\n◑ Write code to search the Brown Corpus for particular words and phrases\\naccording to tags, to answer the following questions:\\nProduce an alphabetically sorted list of the distinct words tagged as MD.\\nIdentify words that can be plural nouns or third person singular verbs\\n(e.g. deals, flies).\\nIdentify three-word prepositional phrases of the form IN + DET + NN\\n(eg. in the lab).\\nWhat is the ratio of masculine to feminine pronouns?\\n\\n\\n◑ In 3.1 we saw a table involving frequency counts for\\nthe verbs adore, love, like, prefer and\\npreceding qualifiers absolutely and definitely.\\nInvestigate the full range of adverbs that appear before these four verbs.\\n◑\\nWe defined the regexp_tagger that can be used\\nas a fall-back tagger for unknown words.  This tagger only checks for\\ncardinal numbers.  By testing for particular prefix or suffix strings,\\nit should be possible to guess other tags.  For example,\\nwe could tag any word that ends with -s as a plural noun.\\nDefine a regular expression tagger (using RegexpTagger())\\nthat tests for at least five other patterns in the spelling of words.\\n(Use inline documentation to explain the rules.)\\n◑\\nConsider the regular expression tagger developed in the exercises in\\nthe previous section.  Evaluate the tagger using its accuracy() method,\\nand try to come up with ways to improve its performance.  Discuss your findings.\\nHow does objective evaluation help in the development process?\\n◑\\nHow serious is the sparse data problem?  Investigate the\\nperformance of n-gram taggers as n increases from 1 to 6.\\nTabulate the accuracy score.  Estimate the training data required\\nfor these taggers, assuming a vocabulary size of\\n105 and a tagset size of 102.\\n◑ Obtain some tagged data for another language, and train and\\nevaluate a variety of taggers on it.  If the language is\\nmorphologically complex, or if there are any orthographic clues\\n(e.g. capitalization) to word classes, consider developing a\\nregular expression tagger for it (ordered after the unigram\\ntagger, and before the default tagger).  How does the accuracy of\\nyour tagger(s) compare with the same taggers run on English data?\\nDiscuss any issues you encounter in applying these methods to the language.\\n◑ 4.1 plotted a curve showing\\nchange in the performance of a lookup tagger as the model size was increased.\\nPlot the performance curve for a unigram tagger, as the amount of training\\ndata is varied.\\n◑\\nInspect the confusion matrix for the bigram tagger t2 defined in 5,\\nand identify one or more sets of tags to collapse.  Define a dictionary to do\\nthe mapping, and evaluate the tagger on the simplified data.\\n◑\\nExperiment with taggers using the simplified tagset (or make one of your\\nown by discarding all but the first character of each tag name).\\nSuch a tagger has fewer distinctions to make, but much less\\ninformation on which to base its work.  Discuss your findings.\\n◑\\nRecall the example of a bigram tagger which encountered a word it hadn\\'t\\nseen during training, and tagged the rest of the sentence as None.\\nIt is possible for a bigram tagger to fail part way through a sentence\\neven if it contains no unseen words (even if the sentence was used during\\ntraining).  In what circumstance can this happen?  Can you write a program\\nto find some examples of this?\\n◑\\nPreprocess the Brown News data by replacing low frequency words with UNK,\\nbut leaving the tags untouched.  Now train and evaluate a bigram tagger\\non this data.  How much does this help?  What is the contribution of the unigram\\ntagger and default tagger now?\\n◑\\nModify the program in 4.1 to use a logarithmic scale on\\nthe x-axis, by replacing pylab.plot() with pylab.semilogx().\\nWhat do you notice about the shape of the resulting plot?  Does the gradient\\ntell you anything?\\n◑\\nConsult the documentation for the Brill tagger demo function,\\nusing help(nltk.tag.brill.demo).\\nExperiment with the tagger by setting different values for the parameters.\\nIs there any trade-off between training time (corpus size) and performance?\\n◑ Write code that builds a dictionary of dictionaries of sets.\\nUse it to store the set of POS tags that can follow a given word having\\na given POS tag, i.e. wordi → tagi\\n→ tagi+1.\\n★ There are 264 distinct words in the Brown Corpus having exactly\\nthree possible tags.\\nPrint a table with the integers 1..10 in one column, and the\\nnumber of distinct words in the corpus having 1..10 distinct tags\\nin the other column.\\nFor the word with the greatest number of distinct tags, print\\nout sentences from the corpus containing the word, one for each\\npossible tag.\\n\\n\\n★ Write a program to classify contexts involving the word must according\\nto the tag of the following word.  Can this be used to discriminate between the\\nepistemic and deontic uses of must?\\n★\\nCreate a regular expression tagger and various unigram and n-gram taggers,\\nincorporating backoff, and train them on part of the Brown corpus.\\nCreate three different combinations of the taggers. Test the\\naccuracy of each combined tagger. Which combination works best?\\nTry varying the size of the training corpus. How does it affect\\nyour results?\\n\\n\\n★\\nOur approach for tagging an unknown word has been to consider the letters of the word\\n(using RegexpTagger()), or to ignore the word altogether and tag\\nit as a noun (using nltk.DefaultTagger()).  These methods will not do well for texts having\\nnew words that are not nouns.\\nConsider the sentence I like to blog on Kim\\'s blog.  If blog is a new\\nword, then looking at the previous tag (TO versus NP$) would probably be helpful.\\nI.e. we need a default tagger that is sensitive to the preceding tag.\\nCreate a new kind of unigram tagger that looks at the tag of the previous word,\\nand ignores the current word.  (The best way to do this is to modify the source\\ncode for UnigramTagger(), which presumes knowledge of object-oriented\\nprogramming in Python.)\\nAdd this tagger to the sequence of backoff taggers (including ordinary trigram\\nand bigram taggers that look at words), right before the usual default tagger.\\nEvaluate the contribution of this new unigram tagger.\\n\\n\\n★\\nConsider the code in 5 which\\ndetermines the upper bound for accuracy of a trigram tagger.\\nReview Abney\\'s discussion concerning the impossibility of\\nexact tagging (Church, Young, &amp; Bloothooft, 1996).  Explain why correct tagging of\\nthese examples requires access to other kinds of information than\\njust words and tags.  How might you estimate the scale of this problem?\\n★\\nUse some of the estimation techniques in nltk.probability,\\nsuch as Lidstone or Laplace estimation, to develop a statistical\\ntagger that does a better job than n-gram backoff taggers in cases where\\ncontexts encountered during testing were not seen during training.\\n★\\nInspect the diagnostic files created by the Brill tagger rules.out and\\nerrors.out.  Obtain the demonstration code by accessing the source code\\n(at http://www.nltk.org/code)\\nand create your own version of the Brill tagger.\\nDelete some of the rule templates, based on what you learned from inspecting rules.out.\\nAdd some new rule templates which employ contexts that might help to\\ncorrect the errors you saw in errors.out.\\n★\\nDevelop an n-gram backoff tagger that permits \"anti-n-grams\" such as\\n[\"the\", \"the\"] to be specified when a tagger is initialized.\\nAn anti-ngram is assigned a count of zero and is used to prevent\\nbackoff for this n-gram (e.g. to avoid\\nestimating P(the | the) as just P(the)).\\n★\\nInvestigate three different ways to define the split between training and\\ntesting data when developing a tagger using the Brown Corpus:\\ngenre (category), source (fileid), and sentence.\\nCompare their relative performance and discuss which method\\nis the most legitimate.  (You might use n-fold cross validation,\\ndiscussed in 3, to improve the accuracy of the evaluations.)\\n★\\nDevelop your own NgramTagger class that inherits from NLTK\\'s class,\\nand which encapsulates the method of collapsing the vocabulary of\\nthe tagged training and testing data that was described in\\nthis chapter.  Make sure that the unigram and default backoff taggers\\nhave access to the full vocabulary.\\n\\n\\n\\nAbout this document...\\nUPDATED FOR NLTK 3.0.\\nThis is a chapter from Natural Language Processing with Python,\\nby Steven Bird, Ewan Klein and Edward Loper,\\nCopyright © 2019 the authors.\\nIt is distributed with the Natural Language Toolkit [http://nltk.org/],\\nVersion 3.0, under the terms of the\\nCreative Commons Attribution-Noncommercial-No Derivative Works 3.0 United States License\\n[http://creativecommons.org/licenses/by-nc-nd/3.0/us/].\\nThis document was built on\\nWed  4 Sep 2019 11:40:48 ACST\\n\\n\\n\\n\\n\\n',\n",
       " '\\n\\n\\n\\nfunction astext(node)\\n{\\n    return node.innerHTML.replace(/(]+)>)/ig,\"\")\\n                         .replace(/&gt;/ig, \">\")\\n                         .replace(/&lt;/ig, \"<\")\\n                         .replace(/&quot;/ig, \\'\"\\')\\n                         .replace(/&amp;/ig, \"&\");\\n}\\n\\nfunction copy_notify(node, bar_color, data)\\n{\\n    // The outer box: relative + inline positioning.\\n    var box1 = document.createElement(\"div\");\\n    box1.style.position = \"relative\";\\n    box1.style.display = \"inline\";\\n    box1.style.top = \"2em\";\\n    box1.style.left = \"1em\";\\n  \\n    // A shadow for fun\\n    var shadow = document.createElement(\"div\");\\n    shadow.style.position = \"absolute\";\\n    shadow.style.left = \"-1.3em\";\\n    shadow.style.top = \"-1.3em\";\\n    shadow.style.background = \"#404040\";\\n    \\n    // The inner box: absolute positioning.\\n    var box2 = document.createElement(\"div\");\\n    box2.style.position = \"relative\";\\n    box2.style.border = \"1px solid #a0a0a0\";\\n    box2.style.left = \"-.2em\";\\n    box2.style.top = \"-.2em\";\\n    box2.style.background = \"white\";\\n    box2.style.padding = \".3em .4em .3em .4em\";\\n    box2.style.fontStyle = \"normal\";\\n    box2.style.background = \"#f0e0e0\";\\n\\n    node.insertBefore(box1, node.childNodes.item(0));\\n    box1.appendChild(shadow);\\n    shadow.appendChild(box2);\\n    box2.innerHTML=\"Copied&nbsp;to&nbsp;the&nbsp;clipboard: \" +\\n                   \"\"+\\n                   data+\"\";\\n    setTimeout(function() { node.removeChild(box1); }, 1000);\\n\\n    var elt = node.parentNode.firstChild;\\n    elt.style.background = \"#ffc0c0\";\\n    setTimeout(function() { elt.style.background = bar_color; }, 200);\\n}\\n\\nfunction copy_codeblock_to_clipboard(node)\\n{\\n    var data = astext(node)+\"\\\\n\";\\n    if (copy_text_to_clipboard(data)) {\\n        copy_notify(node, \"#40a060\", data);\\n    }\\n}\\n\\nfunction copy_doctest_to_clipboard(node)\\n{\\n    var s = astext(node)+\"\\\\n   \";\\n    var data = \"\";\\n\\n    var start = 0;\\n    var end = s.indexOf(\"\\\\n\");\\n    while (end >= 0) {\\n        if (s.substring(start, start+4) == \">>> \") {\\n            data += s.substring(start+4, end+1);\\n        }\\n        else if (s.substring(start, start+4) == \"... \") {\\n            data += s.substring(start+4, end+1);\\n        }\\n        /*\\n        else if (end-start > 1) {\\n            data += \"# \" + s.substring(start, end+1);\\n        }*/\\n        // Grab the next line.\\n        start = end+1;\\n        end = s.indexOf(\"\\\\n\", start);\\n    }\\n    \\n    if (copy_text_to_clipboard(data)) {\\n        copy_notify(node, \"#4060a0\", data);\\n    }\\n}\\n    \\nfunction copy_text_to_clipboard(data)\\n{\\n    if (window.clipboardData) {\\n        window.clipboardData.setData(\"Text\", data);\\n        return true;\\n     }\\n    else if (window.netscape) {\\n        // w/ default firefox settings, permission will be denied for this:\\n        netscape.security.PrivilegeManager\\n                      .enablePrivilege(\"UniversalXPConnect\");\\n    \\n        var clip = Components.classes[\"@mozilla.org/widget/clipboard;1\"]\\n                      .createInstance(Components.interfaces.nsIClipboard);\\n        if (!clip) return;\\n    \\n        var trans = Components.classes[\"@mozilla.org/widget/transferable;1\"]\\n                       .createInstance(Components.interfaces.nsITransferable);\\n        if (!trans) return;\\n    \\n        trans.addDataFlavor(\"text/unicode\");\\n    \\n        var str = new Object();\\n        var len = new Object();\\n    \\n        var str = Components.classes[\"@mozilla.org/supports-string;1\"]\\n                     .createInstance(Components.interfaces.nsISupportsString);\\n        var datacopy=data;\\n        str.data=datacopy;\\n        trans.setTransferData(\"text/unicode\",str,datacopy.length*2);\\n        var clipid=Components.interfaces.nsIClipboard;\\n    \\n        if (!clip) return false;\\n    \\n        clip.setData(trans,null,clipid.kGlobalClipboard);\\n        return true;\\n    }\\n    return false;\\n}\\n//-->\\n\\n\\n\\n\\n\\n\\n6. Learning to Classify Text\\n\\n\\n/*\\n:Author: Edward Loper, James Curran\\n:Copyright: This stylesheet has been placed in the public domain.\\n\\nStylesheet for use with Docutils.\\n\\nThis stylesheet defines new css classes used by NLTK.\\n\\nIt uses a Python syntax highlighting scheme that matches\\nthe colour scheme used by IDLE, which makes it easier for\\nbeginners to check they are typing things in correctly.\\n*/\\n\\n/* Include the standard docutils stylesheet. */\\n@import url(default.css);\\n\\n/* Custom inline roles */\\nspan.placeholder    { font-style: italic; font-family: monospace; }\\nspan.example        { font-style: italic; }\\nspan.emphasis       { font-style: italic; }\\nspan.termdef        { font-weight: bold; }\\n/*span.term           { font-style: italic; }*/\\nspan.category       { font-variant: small-caps; }\\nspan.feature        { font-variant: small-caps; }\\nspan.fval           { font-style: italic; }\\nspan.math           { font-style: italic; }\\nspan.mathit         { font-style: italic; }\\nspan.lex            { font-variant: small-caps; }\\nspan.guide-linecount{ text-align: right; display: block;}\\n\\n/* Python souce code listings */\\nspan.pysrc-prompt   { color: #9b0000; }\\nspan.pysrc-more     { color: #9b00ff; }\\nspan.pysrc-keyword  { color: #e06000; }\\nspan.pysrc-builtin  { color: #940094; }\\nspan.pysrc-string   { color: #00aa00; }\\nspan.pysrc-comment  { color: #ff0000; }\\nspan.pysrc-output   { color: #0000ff; }\\nspan.pysrc-except   { color: #ff0000; }\\nspan.pysrc-defname  { color: #008080; }\\n\\n\\n/* Doctest blocks */\\npre.doctest         { margin: 0; padding: 0; font-weight: bold; }\\ndiv.doctest         { margin: 0 1em 1em 1em; padding: 0; }\\ntable.doctest       { margin: 0; padding: 0;\\n                      border-top: 1px solid gray;\\n                      border-bottom: 1px solid gray; }\\npre.copy-notify     { margin: 0; padding: 0.2em; font-weight: bold;\\n                      background-color: #ffffff; }\\n\\n/* Python source listings */\\ndiv.pylisting       { margin: 0 1em 1em 1em; padding: 0; }\\ntable.pylisting     { margin: 0; padding: 0;\\n                      border-top: 1px solid gray; }\\ntd.caption { border-top: 1px solid black; margin: 0; padding: 0; }\\n.caption-label { font-weight: bold;  }\\ntd.caption p { margin: 0; padding: 0; font-style: normal;}\\n\\ntable tr td.codeblock { \\n  padding: 0.2em ! important; margin: 0;\\n  border-left: 1px solid gray;\\n  border-right: 2px solid gray;\\n  border-top: 0px solid gray;\\n  border-bottom: 1px solid gray;\\n  font-weight: bold; background-color: #eeffee;\\n}\\ntable pre span {\\n  white-space: pre-wrap;\\n}\\ntable tr td.doctest  { \\n  padding: 0.2em; margin: 0;\\n  border-left: 1px solid gray;\\n  border-right: 2px solid gray;\\n  border-top: 0px solid gray;\\n  border-bottom: 1px solid gray;\\n  font-weight: bold; background-color: #eeeeff;\\n}\\n\\ntd.codeblock table tr td.copybar {\\n    background: #40a060; border: 1px solid gray;\\n    font-family: monospace; padding: 0; margin: 0; }\\ntd.doctest table tr td.copybar {\\n    background: #4060a0; border: 1px solid gray;\\n    font-family: monospace; padding: 0; margin: 0; }\\n\\ntd.pysrc { padding-left: 0.5em; }\\n\\nimg.callout { border-width: 0px; }\\n\\ntable.docutils {\\n    border-style: solid;\\n    border-width: 1px;\\n    margin-top: 6px;\\n    border-color: grey;\\n    border-collapse: collapse; }\\n\\ntable.docutils th {\\n    border-style: none;\\n    border-width: 1px;\\n    border-color: grey;\\n    padding: 0 .5em 0 .5em; }\\n\\ntable.docutils td {\\n    border-style: none;\\n    border-width: 1px;\\n    border-color: grey; \\n    padding: 0 .5em 0 .5em; }\\n\\ntable.footnote td { padding: 0; }\\ntable.footnote { border-width: 0; }\\ntable.footnote td { border-width: 0; }\\ntable.footnote th { border-width: 0; }\\n\\ntable.noborder { border-width: 0; }\\n\\ntable.example pre { margin-top: 4px; margin-bottom: 0; }\\n\\n/* For figures & tables */\\np.caption { margin-bottom: 0; }\\ndiv.figure { text-align: center; }\\n\\n/* The index */\\ndiv.index { border: 1px solid black;\\n            background-color: #eeeeee; }\\ndiv.index h1 { padding-left: 0.5em; margin-top: 0.5ex;\\n               border-bottom: 1px solid black; }\\nul.index { margin-left: 0.5em; padding-left: 0; }\\nli.index { list-style-type: none; }\\np.index-heading { font-size: 120%; font-style: italic; margin: 0; }\\nli.index ul { margin-left: 2em; padding-left: 0; }\\n\\n/* \\'Note\\' callouts */\\ndiv.note\\n{\\n  border-right:   #87ceeb 1px solid;\\n  padding-right: 4px;\\n  border-top: #87ceeb 1px solid;\\n  padding-left: 4px;\\n  padding-bottom: 4px;\\n  margin: 2px 5% 10px;\\n  border-left: #87ceeb 1px solid;\\n  padding-top: 4px;\\n  border-bottom: #87ceeb 1px solid;\\n  font-style: normal;\\n  font-family: verdana, arial;\\n  background-color: #b0c4de;\\n}\\n\\ntable.avm { border: 0px solid black; width: 0; }\\ntable.avm tbody tr {border: 0px solid black; }\\ntable.avm tbody tr td { padding: 2px; }\\ntable.avm tbody tr td.avm-key { padding: 5px; font-variant: small-caps; }\\ntable.avm tbody tr td.avm-eq { padding: 5px; }\\ntable.avm tbody tr td.avm-val { padding: 5px; font-style: italic; }\\np.avm-empty { font-style: normal; }\\ntable.avm colgroup col { border: 0px solid black; }\\ntable.avm tbody tr td.avm-topleft \\n    { border-left: 2px solid #000080; border-top: 2px solid #000080; }\\ntable.avm tbody tr td.avm-botleft \\n    { border-left: 2px solid #000080; border-bottom: 2px solid #000080; }\\ntable.avm tbody tr td.avm-topright\\n    { border-right: 2px solid #000080; border-top: 2px solid #000080; }\\ntable.avm tbody tr td.avm-botright\\n    { border-right: 2px solid #000080; border-bottom: 2px solid #000080; }\\ntable.avm tbody tr td.avm-left\\n    { border-left: 2px solid #000080; }\\ntable.avm tbody tr td.avm-right\\n    { border-right: 2px solid #000080; }\\ntable.avm tbody tr td.avm-topbotleft\\n    { border: 2px solid #000080; border-right: 0px solid black; }\\ntable.avm tbody tr td.avm-topbotright\\n    { border: 2px solid #000080; border-left: 0px solid black; }\\ntable.avm tbody tr td.avm-ident\\n    { font-size: 80%; padding: 0; padding-left: 2px; vertical-align: top; }\\n.avm-pointer\\n{ border: 1px solid #008000; padding: 1px; color: #008000; \\n  background: #c0ffc0; font-style: normal; }\\n\\ntable.gloss { border: 0px solid black; width: 0; }\\ntable.gloss tbody tr { border: 0px solid black; }\\ntable.gloss tbody tr td { border: 0px solid black; }\\ntable.gloss colgroup col { border: 0px solid black; }\\ntable.gloss p { margin: 0; padding: 0; }\\n\\ntable.rst-example { border: 1px solid black; }\\ntable.rst-example tbody tr td { background: #eeeeee; }\\ntable.rst-example thead tr th { background: #c0ffff; }\\ntd.rst-raw { width: 0; }\\n\\n/* Used by nltk.org/doc/test: */\\ndiv.doctest-list { text-align: center; }\\ntable.doctest-list { border: 1px solid black;\\n  margin-left: auto; margin-right: auto;\\n}\\ntable.doctest-list tbody tr td { background: #eeeeee;\\n  border: 1px solid #cccccc; text-align: left; }\\ntable.doctest-list thead tr th { background: #304050; color: #ffffff;\\n  border: 1px solid #000000;}\\ntable.doctest-list thead tr a { color: #ffffff; }\\nspan.doctest-passed { color: #008000; }\\nspan.doctest-failed { color: #800000; }\\n\\n\\n\\n\\n\\n6. Learning to Classify Text\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n<!-- Grammatical Category - e.g. NP and verb as technical terms\\n.. role:: gc\\n   :class: category -->\\n\\n\\n\\n\\n\\n\\n\\n\\n<!-- standard global imports\\n\\n>>> from __future__ import division\\n>>> import nltk, re, pprint -->\\n<!-- Organization:\\n- Supervised Classification - - this introduces basic concepts, and\\n  runs through lots of interesting examples\\n- Evaluation - - talks about evaluation :)\\n- Classification Methods (x3)\\n- Generative vs Conditional\\n- Joint Classification\\n- Data modeling - - this talks about abstractly what we can learn\\n  about language as linguists\\n- ML in Python -->\\n\\n\"(ie word enclosed by double quotes) needs to be replaced by -->\\n`:lx: -->\\n\\n\\n<!-- explain that segmentation (e.g. tokenization, sentence segmentation) can\\nbe viewed as a classification task -->\\n<!-- Determinize, for the sake of doctest:\\n\\n>>> import random; random.seed(12345) -->\\nDetecting patterns is a central part of Natural Language Processing.  Words ending in\\n-ed tend to be past tense verbs (5.).  Frequent use of\\nwill is indicative of news text (3).  These observable\\npatterns — word structure and word frequency — happen to\\ncorrelate with particular aspects of meaning, such as tense and topic.\\nBut how did we know where to start looking, which aspects of form to\\nassociate with which aspects of meaning?\\nThe goal of this chapter is to answer the following questions:\\n\\nHow can we identify particular features of language data that\\nare salient for classifying it?\\nHow can we construct models of language that can\\nbe used to perform language processing tasks automatically?\\nWhat can we learn about language from these models?\\n\\nAlong the way we will study some important machine learning\\ntechniques, including decision trees, naive Bayes\\' classifiers,\\nand maximum entropy classifiers.  We will gloss over the mathematical and\\nstatistical underpinnings of these techniques, focusing instead on how\\nand when to use them (see the Further Readings section for more\\ntechnical background).  Before looking at these methods, we first need\\nto appreciate the broad scope of this topic.\\n\\n1&nbsp;&nbsp;&nbsp;Supervised Classification\\nClassification is the task of choosing the correct class\\nlabel for a given input.  In basic classification tasks, each\\ninput is considered in isolation from all other inputs, and the set of\\nlabels is defined in advance.  Some examples of classification tasks\\nare:\\n\\nDeciding whether an email is spam or not.\\nDeciding what the topic of a news article is, from a fixed list of\\ntopic areas such as \"sports,\" \"technology,\" and \"politics.\"\\nDeciding whether a given occurrence of the word bank is used to\\nrefer to a river bank, a financial institution, the act of tilting\\nto the side, or the act of depositing something in a financial\\ninstitution.\\n\\nThe basic classification task has a number of interesting variants.\\nFor example, in multi-class classification, each instance may be\\nassigned multiple labels; in open-class classification, the set of\\nlabels is not defined in advance; and in sequence classification, a\\nlist of inputs are jointly classified.\\nA classifier is called supervised if it is built based on\\ntraining corpora containing the correct label for each input.  The\\nframework used by supervised classification is shown in\\n1.1.\\n\\n\\nFigure 1.1: Supervised Classification.  (a) During training, a feature\\nextractor is used to convert each input value to a feature set.\\nThese feature sets, which capture the basic information about\\neach input that should be used to classify it, are discussed in\\nthe next section.\\nPairs of feature sets and labels are fed into the machine learning\\nalgorithm to generate a model.  (b) During prediction, the same\\nfeature extractor is used to convert unseen inputs to feature sets.\\nThese feature sets are then fed into the model, which generates\\npredicted labels.\\n\\nIn the rest of this section, we will look at how classifiers can be\\nemployed to solve a wide variety of tasks.  Our discussion is not intended\\nto be comprehensive, but to give a representative sample of tasks that\\ncan be performed with the help of text classifiers.\\n\\n1.1&nbsp;&nbsp;&nbsp;Gender Identification\\nIn 4 we saw that male and female names\\nhave some distinctive characteristics.  Names ending in a,\\ne and i are likely to be female, while names ending in\\nk, o, r, s and t are likely to be male.\\nLet\\'s build a classifier to model these differences more precisely.\\nThe first step in creating a classifier is deciding what\\nfeatures of the input are relevant, and how to encode\\nthose features.  For this example, we\\'ll start by just looking at the\\nfinal letter of a given name.  The following feature extractor\\nfunction builds a dictionary containing relevant information about a\\ngiven name:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; def gender_features(word):\\n...     return {\\'last_letter\\': word[-1]}\\n&gt;&gt;&gt; gender_features(\\'Shrek\\')\\n{\\'last_letter\\': \\'k\\'}\\n\\n\\n\\nThe returned dictionary, known as a feature set, maps from\\nfeature names to their values.  Feature names are case-sensitive\\nstrings that typically provide a short human-readable description of\\nthe feature, as in the example \\'last_letter\\'.  Feature values are values with simple types, such as\\nbooleans, numbers, and strings.\\n\\nNote\\nMost classification methods require that features be encoded using\\nsimple value types, such as booleans, numbers, and strings.  But\\nnote that just because a feature has a simple type, this does not\\nnecessarily mean that the feature\\'s value is simple to express or\\ncompute. Indeed, it is even possible to use very complex and\\ninformative values, such as the output of a second supervised\\nclassifier, as features.\\n\\nNow that we\\'ve defined a feature extractor, we need to prepare\\na list of examples and corresponding\\nclass labels.\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; from nltk.corpus import names\\n&gt;&gt;&gt; labeled_names = ([(name, \\'male\\') for name in names.words(\\'male.txt\\')] +\\n... [(name, \\'female\\') for name in names.words(\\'female.txt\\')])\\n&gt;&gt;&gt; import random\\n&gt;&gt;&gt; random.shuffle(labeled_names)\\n\\n\\n\\n\\nNext, we use the feature extractor to process the names data, and\\ndivide the resulting list of feature sets into a training set\\nand a test set.  The training set is used to train a new\\n\"naive Bayes\" classifier.\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; featuresets = [(gender_features(n), gender) for (n, gender) in labeled_names]\\n&gt;&gt;&gt; train_set, test_set = featuresets[500:], featuresets[:500]\\n&gt;&gt;&gt; classifier = nltk.NaiveBayesClassifier.train(train_set)\\n\\n\\n\\nWe will learn more about the naive Bayes classifier later in the\\nchapter.  For now, let\\'s just test it out on some names that did not\\nappear in its training data:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; classifier.classify(gender_features(\\'Neo\\'))\\n\\'male\\'\\n&gt;&gt;&gt; classifier.classify(gender_features(\\'Trinity\\'))\\n\\'female\\'\\n\\n\\n\\nObserve that these character names from The Matrix are correctly\\nclassified.  Although this science fiction movie is set in 2199, it\\nstill conforms with our expectations about names and genders.  We can\\nsystematically evaluate the classifier on a much larger quantity of\\nunseen data:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; print(nltk.classify.accuracy(classifier, test_set))\\n0.77\\n\\n\\n\\nFinally, we can examine the classifier to determine which features it\\nfound most effective for distinguishing the names\\' genders:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; classifier.show_most_informative_features(5)\\nMost Informative Features\\n             last_letter = \\'a\\'            female : male   =     33.2 : 1.0\\n             last_letter = \\'k\\'              male : female =     32.6 : 1.0\\n             last_letter = \\'p\\'              male : female =     19.7 : 1.0\\n             last_letter = \\'v\\'              male : female =     18.6 : 1.0\\n             last_letter = \\'f\\'              male : female =     17.3 : 1.0\\n\\n\\n\\nThis listing shows that the names in the training set that end in \"a\"\\nare female 33 times more often than they are male, but names that end\\nin \"k\" are male 32 times more often than they are female.  These\\nratios are known as likelihood ratios, and can be useful for\\ncomparing different feature-outcome relationships.\\n\\nNote\\nYour Turn:\\nModify the gender_features() function to provide the\\nclassifier with features encoding the length of the name, its first\\nletter, and any other features that seem like they might be\\ninformative.  Retrain the classifier with these new features, and\\ntest its accuracy.\\n\\nWhen working with large corpora, constructing a single list\\nthat contains the features of every instance can use up a large\\namount of memory.  In these cases, use the function\\nnltk.classify.apply_features, which returns an object that acts\\nlike a list but does not store all the feature sets in memory:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; from nltk.classify import apply_features\\n&gt;&gt;&gt; train_set = apply_features(gender_features, labeled_names[500:])\\n&gt;&gt;&gt; test_set = apply_features(gender_features, labeled_names[:500])\\n\\n\\n\\n\\n\\n1.2&nbsp;&nbsp;&nbsp;Choosing The Right Features\\nSelecting relevant features and deciding how to encode them for a\\nlearning method can have an enormous impact on the learning method\\'s\\nability to extract a good model.  Much of the interesting work in\\nbuilding a classifier is deciding what features might be relevant, and\\nhow we can represent them.  Although it\\'s often possible to get decent\\nperformance by using a fairly simple and obvious set of features,\\nthere are usually significant gains to be had by using carefully\\nconstructed features based on a thorough understanding of the task at\\nhand.\\nTypically, feature extractors are built through a process of\\ntrial-and-error, guided by intuitions about what information is\\nrelevant to the problem.  It\\'s common to start with a\\n\"kitchen sink\" approach, including all the features that you can think\\nof, and then checking to see which features actually are\\nhelpful.  We take this approach for name gender features in\\n1.2.\\n<!-- XXX is it worth telling readers not to confuse feature names like \"count(j)\"\\nwith functions? -->\\n<!-- XXX how about mentioning the term \"presence feature\" as the name\\nfor boolean-valued features? -->\\n<!-- XXX briefly explain what\\'s being done in\\nfeatures[\"count(%s)\" % letter] = name.lower().count(letter) -->\\n\\n\\n\\n\\n&nbsp;\\ndef gender_features2(name):\\n    features = {}\\n    features[\"first_letter\"] = name[0].lower()\\n    features[\"last_letter\"] = name[-1].lower()\\n    for letter in \\'abcdefghijklmnopqrstuvwxyz\\':\\n        features[\"count({})\".format(letter)] = name.lower().count(letter)\\n        features[\"has({})\".format(letter)] = (letter in name.lower())\\n    return features\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; gender_features2(\\'John\\') \\n{\\'count(j)\\': 1, \\'has(d)\\': False, \\'count(b)\\': 0, ...}\\n\\n\\nExample 1.2 (code_gender_features_overfitting.py): Figure 1.2: A Feature Extractor that Overfits Gender Features.\\nThe feature sets returned by this feature extractor contain a\\nlarge number of specific features, leading to overfitting for\\nthe relatively small Names Corpus.\\n\\nHowever, there are usually limits to the number of features\\nthat you should use with a given learning algorithm — if you provide\\ntoo many features, then the algorithm will have a higher chance of\\nrelying on idiosyncrasies of your training data that don\\'t generalize\\nwell to new examples.  This problem is known as overfitting, and\\ncan be especially problematic when working with small training sets.  For\\nexample, if we train a naive Bayes classifier using the feature\\nextractor shown in 1.2, it will overfit\\nthe relatively small training set, resulting in a system whose accuracy\\nis about 1% lower than the accuracy of a classifier that only\\npays attention to the final letter of each name:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; featuresets = [(gender_features2(n), gender) for (n, gender) in labeled_names]\\n&gt;&gt;&gt; train_set, test_set = featuresets[500:], featuresets[:500]\\n&gt;&gt;&gt; classifier = nltk.NaiveBayesClassifier.train(train_set)\\n&gt;&gt;&gt; print(nltk.classify.accuracy(classifier, test_set))\\n0.768\\n\\n\\n\\n\\n\\n\\n\\nOnce an initial set of features has been chosen, a very productive\\nmethod for refining the feature set is error analysis.  First,\\nwe select a development set, containing the corpus data for\\ncreating the model.  This development set is then subdivided\\ninto the training set and the dev-test set.\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; train_names = labeled_names[1500:]\\n&gt;&gt;&gt; devtest_names = labeled_names[500:1500]\\n&gt;&gt;&gt; test_names = labeled_names[:500]\\n\\n\\n\\nThe training set is used to train the model, and the dev-test set is\\nused to perform error analysis.  The test set serves in our final\\nevaluation of the system.  For reasons discussed below, it is\\nimportant that we employ a separate dev-test set for error analysis,\\nrather than just using the test set.  The division of the corpus data\\ninto different subsets is shown in 1.3.\\n\\n\\nFigure 1.3: Organization of corpus data for training supervised classifiers.\\nThe corpus data is divided into two sets: the development set,\\nand the test set.  The development set is often further subdivided\\ninto a training set and a dev-test set.\\n\\nHaving divided the corpus into appropriate datasets, we train a model\\nusing the training set , and then run it on the\\ndev-test set .\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; train_set = [(gender_features(n), gender) for (n, gender) in train_names]\\n&gt;&gt;&gt; devtest_set = [(gender_features(n), gender) for (n, gender) in devtest_names]\\n&gt;&gt;&gt; test_set = [(gender_features(n), gender) for (n, gender) in test_names]\\n&gt;&gt;&gt; classifier = nltk.NaiveBayesClassifier.train(train_set) \\n&gt;&gt;&gt; print(nltk.classify.accuracy(classifier, devtest_set)) \\n0.75\\n\\n\\n\\nUsing the dev-test set, we can generate a list of the errors that the\\nclassifier makes when predicting name genders:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; errors = []\\n&gt;&gt;&gt; for (name, tag) in devtest_names:\\n...     guess = classifier.classify(gender_features(name))\\n...     if guess != tag:\\n...         errors.append( (tag, guess, name) )\\n\\n\\n\\nWe can then examine individual error cases where the model predicted\\nthe wrong label, and try to determine what additional pieces of\\ninformation would allow it to make the right decision (or which\\nexisting pieces of information are tricking it into making the wrong\\ndecision).  The feature set can then be adjusted accordingly.  The\\nnames classifier that we have built generates about 100 errors on the\\ndev-test corpus:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; for (tag, guess, name) in sorted(errors):\\n...     print(\\'correct={:&lt;8} guess={:&lt;8s} name={:&lt;30}\\'.format(tag, guess, name))\\ncorrect=female   guess=male     name=Abigail\\n  ...\\ncorrect=female   guess=male     name=Cindelyn\\n  ...\\ncorrect=female   guess=male     name=Katheryn\\ncorrect=female   guess=male     name=Kathryn\\n  ...\\ncorrect=male     guess=female   name=Aldrich\\n  ...\\ncorrect=male     guess=female   name=Mitch\\n  ...\\ncorrect=male     guess=female   name=Rich\\n  ...\\n\\n\\n\\nLooking through this list of errors makes it clear that some suffixes\\nthat are more than one letter can be indicative of name genders.  For\\nexample, names ending in yn appear to be predominantly female,\\ndespite the fact that names ending in n tend to be male; and names\\nending in ch are usually male, even though names that end in h\\ntend to be female.  We therefore\\nadjust our feature extractor to include features for two-letter\\nsuffixes:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; def gender_features(word):\\n...     return {\\'suffix1\\': word[-1:],\\n...             \\'suffix2\\': word[-2:]}\\n\\n\\n\\nRebuilding the classifier with the new feature extractor, we see that\\nthe performance on the dev-test dataset improves by almost 2\\npercentage points (from 76.5% to 78.2%):\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; train_set = [(gender_features(n), gender) for (n, gender) in train_names]\\n&gt;&gt;&gt; devtest_set = [(gender_features(n), gender) for (n, gender) in devtest_names]\\n&gt;&gt;&gt; classifier = nltk.NaiveBayesClassifier.train(train_set)\\n&gt;&gt;&gt; print(nltk.classify.accuracy(classifier, devtest_set))\\n0.782\\n\\n\\n\\nThis error analysis procedure can then be repeated, checking for\\npatterns in the errors that are made by the newly improved classifier.\\nEach time the error analysis procedure is repeated, we should select a\\ndifferent dev-test/training split, to ensure that the classifier\\ndoes not start to reflect idiosyncrasies in the dev-test set.\\nBut once we\\'ve used the dev-test set to help us develop the\\nmodel, we can no longer trust that it will give us an accurate idea of\\nhow well the model would perform on new data.  It is therefore\\nimportant to keep the test set separate, and unused, until our model\\ndevelopment is complete.  At that point, we can use the test set to\\nevaluate how well our model will perform on new input values.\\n\\n\\n1.3&nbsp;&nbsp;&nbsp;Document Classification\\n<!-- Determinize, for the sake of doctest:\\n\\n>>> import random; random.seed(12345) -->\\nIn 1, we saw several examples of\\ncorpora where documents have been labeled with categories.  Using\\nthese corpora, we can build classifiers that will automatically tag\\nnew documents with appropriate category labels.  First, we\\nconstruct a list of documents, labeled with the appropriate\\ncategories.  For this example, we\\'ve chosen the Movie Reviews Corpus,\\nwhich categorizes each review as positive or negative.\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; from nltk.corpus import movie_reviews\\n&gt;&gt;&gt; documents = [(list(movie_reviews.words(fileid)), category)\\n...              for category in movie_reviews.categories()\\n...              for fileid in movie_reviews.fileids(category)]\\n&gt;&gt;&gt; random.shuffle(documents)\\n\\n\\n\\nNext, we define a feature extractor for documents, so the classifier\\nwill know which aspects of the data it should pay attention to\\n(1.4).  For document topic identification, we can\\ndefine a feature for each word, indicating whether the document\\ncontains that word.  To limit the number of features that the\\nclassifier needs to process, we begin by constructing a list of the\\n2000 most frequent words in the overall corpus\\n.  We can then define a feature extractor\\n that simply checks whether each of these\\nwords is present in a given document.\\n\\n\\n\\n\\n&nbsp;\\nall_words = nltk.FreqDist(w.lower() for w in movie_reviews.words())\\nword_features = list(all_words)[:2000] \\n\\ndef document_features(document): \\n    document_words = set(document) \\n    features = {}\\n    for word in word_features:\\n        features[\\'contains({})\\'.format(word)] = (word in document_words)\\n    return features\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; print(document_features(movie_reviews.words(\\'pos/cv957_8737.txt\\'))) \\n{\\'contains(waste)\\': False, \\'contains(lot)\\': False, ...}\\n\\n\\nExample 1.4 (code_document_classify_fd.py): Figure 1.4: A feature extractor for document classification, whose\\nfeatures indicate whether or not individual words are present\\nin a given document.\\n\\n\\nNote\\nThe reason that we compute the set of all words in a document in\\n, rather than just checking if\\nword in document, is\\nthat checking whether a word occurs in a set is much faster than\\nchecking whether it occurs in a list (4.7).\\n\\nNow that we\\'ve defined our feature extractor, we can use it to train a\\nclassifier to label new movie reviews (1.5).  To\\ncheck how reliable the resulting classifier is, we compute its\\naccuracy on the test set .  And once again,\\nwe can use show_most_informative_features() to find out which\\nfeatures the classifier found to be most informative\\n.\\n\\n\\n\\n\\n&nbsp;\\nfeaturesets = [(document_features(d), c) for (d,c) in documents]\\ntrain_set, test_set = featuresets[100:], featuresets[:100]\\nclassifier = nltk.NaiveBayesClassifier.train(train_set)\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; print(nltk.classify.accuracy(classifier, test_set)) \\n0.81\\n&gt;&gt;&gt; classifier.show_most_informative_features(5) \\nMost Informative Features\\n   contains(outstanding) = True              pos : neg    =     11.1 : 1.0\\n        contains(seagal) = True              neg : pos    =      7.7 : 1.0\\n   contains(wonderfully) = True              pos : neg    =      6.8 : 1.0\\n         contains(damon) = True              pos : neg    =      5.9 : 1.0\\n        contains(wasted) = True              neg : pos    =      5.8 : 1.0\\n\\n\\nExample 1.5 (code_document_classify_use.py): Figure 1.5: Training and testing a classifier for document classification.\\n\\nApparently in this corpus, a review that mentions \"Seagal\" is almost 8\\ntimes more likely to be negative than positive, while a review that\\nmentions \"Damon\" is about 6 times more likely to be positive.\\n<!-- I tried adding simple frequencies w/ binning, but it didn\\'t improve\\nperformance.  Using something more advanced like tf-idf probably\\nwould, but I don\\'t want to take up the space here to explain all\\nthat. -->\\n<!-- This classifier gives different results for most_informative_features on\\nsuccessive runs. [EK] -->\\n\\n\\n1.4&nbsp;&nbsp;&nbsp;Part-of-Speech Tagging\\n\\nIn 5. we built a regular expression tagger that chooses a\\npart-of-speech tag for a word by looking at the internal make-up of\\nthe word.  However, this regular expression tagger had to be\\nhand-crafted.  Instead, we can train a classifier to work out which\\nsuffixes are most informative.  Let\\'s begin by finding out what the\\nmost common suffixes are:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; from nltk.corpus import brown\\n&gt;&gt;&gt; suffix_fdist = nltk.FreqDist()\\n&gt;&gt;&gt; for word in brown.words():\\n...     word = word.lower()\\n...     suffix_fdist[word[-1:]] += 1\\n...     suffix_fdist[word[-2:]] += 1\\n...     suffix_fdist[word[-3:]] += 1\\n\\n\\n\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; common_suffixes = [suffix for (suffix, count) in suffix_fdist.most_common(100)]\\n&gt;&gt;&gt; print(common_suffixes)\\n[\\'e\\', \\',\\', \\'.\\', \\'s\\', \\'d\\', \\'t\\', \\'he\\', \\'n\\', \\'a\\', \\'of\\', \\'the\\',\\n \\'y\\', \\'r\\', \\'to\\', \\'in\\', \\'f\\', \\'o\\', \\'ed\\', \\'nd\\', \\'is\\', \\'on\\', \\'l\\',\\n \\'g\\', \\'and\\', \\'ng\\', \\'er\\', \\'as\\', \\'ing\\', \\'h\\', \\'at\\', \\'es\\', \\'or\\',\\n \\'re\\', \\'it\\', \\'``\\', \\'an\\', \"\\'\\'\", \\'m\\', \\';\\', \\'i\\', \\'ly\\', \\'ion\\', ...]\\n\\n\\n\\n\\nNext, we\\'ll define a feature extractor function which checks a given\\nword for these suffixes:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; def pos_features(word):\\n...     features = {}\\n...     for suffix in common_suffixes:\\n...         features[\\'endswith({})\\'.format(suffix)] = word.lower().endswith(suffix)\\n...     return features\\n\\n\\n\\nFeature extraction functions behave like tinted glasses, highlighting\\nsome of the properties (colors) in our data and making it impossible\\nto see other properties.  The classifier will rely exclusively on\\nthese highlighted properties when determining how to label inputs.  In\\nthis case, the classifier will make its decisions based only on\\ninformation about which of the common suffixes (if any) a given word\\nhas.\\n\\n\\nNow that we\\'ve defined our feature extractor, we can use it to\\ntrain a new \"decision tree\" classifier (to be discussed in\\n4):\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; tagged_words = brown.tagged_words(categories=\\'news\\')\\n&gt;&gt;&gt; featuresets = [(pos_features(n), g) for (n,g) in tagged_words]\\n\\n\\n\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; size = int(len(featuresets) * 0.1)\\n&gt;&gt;&gt; train_set, test_set = featuresets[size:], featuresets[:size]\\n\\n\\n\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; classifier = nltk.DecisionTreeClassifier.train(train_set)\\n&gt;&gt;&gt; nltk.classify.accuracy(classifier, test_set)\\n0.62705121829935351\\n\\n\\n\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; classifier.classify(pos_features(\\'cats\\'))\\n\\'NNS\\'\\n\\n\\n\\n<!-- XXX raise the issue of interpretability of models earlier?\\n(E.g. deliberately use the word \"interpret\" in the opening of the chapter?) -->\\nOne nice feature of decision tree models is that they are often fairly\\neasy to interpret — we can even instruct NLTK to print them\\nout as pseudocode:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; print(classifier.pseudocode(depth=4))\\nif endswith(,) == True: return \\',\\'\\nif endswith(,) == False:\\n  if endswith(the) == True: return \\'AT\\'\\n  if endswith(the) == False:\\n    if endswith(s) == True:\\n      if endswith(is) == True: return \\'BEZ\\'\\n      if endswith(is) == False: return \\'VBZ\\'\\n    if endswith(s) == False:\\n      if endswith(.) == True: return \\'.\\'\\n      if endswith(.) == False: return \\'NN\\'\\n\\n\\n\\n<!-- XXX NOTE: Last time I ran the doctests, I got a slightly different\\ndecision tree.  Is this nondeterministic?  It appeared that the\\ndecision tree was making *mostly* the same decisions.  We can\\nupdate this, but it will require updating the following text too.\\nHere\\'s the decision tree I got on the most recent run:\\n\\n if endswith(the) == False:\\n   if endswith(,) == False:\\n     if endswith(s) == False:\\n       if endswith(.) == False: return \\'.\\'\\n       if endswith(.) == True: return \\'.\\'\\n     if endswith(s) == True:\\n       if endswith(is) == False: return \\'PP$\\'\\n       if endswith(is) == True: return \\'BEZ\\'\\n   if endswith(,) == True: return \\',\\'\\n if endswith(the) == True: return \\'AT\\' -->\\nHere, we can see that the classifier begins by checking whether a word\\nends with a comma — if so, then it will receive the special tag\\n\",\".  Next, the classifier checks if the word ends in \"the\",\\nin which case it\\'s almost certainly a determiner.  This \"suffix\" gets\\nused early by the decision tree because the word \"the\" is so common.\\nContinuing on, the classifier checks if the word ends in \"s\".  If so,\\nthen it\\'s most likely to receive the verb tag VBZ (unless it\\'s\\nthe word \"is\", which has a special tag BEZ), and if not,\\nthen it\\'s most likely a noun (unless it\\'s the punctuation mark \".\").\\nThe actual classifier contains further nested if-then statements below\\nthe ones shown here, but the depth=4 argument just displays the\\ntop portion of the decision tree.\\n<!-- XXX How about some features that match word patterns more generally,\\nlinking back to the knowledge represented in a regular expression\\ntagger, such as: (r\\'^-?[0-9]+(.[0-9]+)?$\\', \\'CD\\')\\n-> response: this is a very large search space, though we could\\nmanually throw some of them in. -->\\n\\n\\n1.5&nbsp;&nbsp;&nbsp;Exploiting Context\\n\\nBy augmenting the feature extraction function, we could modify this\\npart-of-speech tagger to leverage a variety of other word-internal\\nfeatures, such as the length of the word, the number of syllables it\\ncontains, or its prefix.  However, as long as the feature extractor\\njust looks at the target word, we have no way to add features that\\ndepend on the context that the word appears in.  But contextual\\nfeatures often provide powerful clues about the correct tag — for\\nexample, when tagging the word \"fly,\" knowing that the previous word\\nis \"a\" will allow us to determine that it is functioning as a noun, not\\na verb.\\nIn order to accommodate features that depend on a word\\'s context, we\\nmust revise the pattern that we used to define our feature extractor.\\nInstead of just passing in the word to be tagged, we will pass in a\\ncomplete (untagged) sentence, along with the index of the target word.\\nThis approach is demonstrated in 1.6, which employs a\\ncontext-dependent feature extractor to define a part of speech tag\\nclassifier.\\n\\n\\n\\n\\n&nbsp;\\ndef pos_features(sentence, i): \\n    features = {\"suffix(1)\": sentence[i][-1:],\\n                \"suffix(2)\": sentence[i][-2:],\\n                \"suffix(3)\": sentence[i][-3:]}\\n    if i == 0:\\n        features[\"prev-word\"] = \"&lt;START&gt;\"\\n    else:\\n        features[\"prev-word\"] = sentence[i-1]\\n    return features\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; pos_features(brown.sents()[0], 8)\\n{\\'suffix(3)\\': \\'ion\\', \\'prev-word\\': \\'an\\', \\'suffix(2)\\': \\'on\\', \\'suffix(1)\\': \\'n\\'}\\n\\n&gt;&gt;&gt; tagged_sents = brown.tagged_sents(categories=\\'news\\')\\n&gt;&gt;&gt; featuresets = []\\n&gt;&gt;&gt; for tagged_sent in tagged_sents:\\n...     untagged_sent = nltk.tag.untag(tagged_sent)\\n...     for i, (word, tag) in enumerate(tagged_sent):\\n...         featuresets.append( (pos_features(untagged_sent, i), tag) )\\n\\n&gt;&gt;&gt; size = int(len(featuresets) * 0.1)\\n&gt;&gt;&gt; train_set, test_set = featuresets[size:], featuresets[:size]\\n&gt;&gt;&gt; classifier = nltk.NaiveBayesClassifier.train(train_set)\\n\\n&gt;&gt;&gt; nltk.classify.accuracy(classifier, test_set)\\n0.78915962207856782\\n\\n\\nExample 1.6 (code_suffix_pos_tag.py): Figure 1.6: A part-of-speech classifier whose feature detector\\nexamines the context in which a word appears in order to\\ndetermine which part of speech tag should be assigned.  In\\nparticular, the identity of the previous word is included as a\\nfeature.\\n\\n\\n<!-- It would be nice to actually show this (using\\nshow_most_informative_features or something): -->\\nIt is clear that exploiting contextual features improves the performance\\nof our part-of-speech tagger.  For example, the classifier learns\\nthat a word is likely to be a noun if it comes immediately after the\\nword \"large\" or the word \"gubernatorial\".  However, it is unable to\\nlearn the generalization that a word is probably a noun if it follows\\nan adjective, because it doesn\\'t have access to the previous word\\'s\\npart-of-speech tag.  In general, simple classifiers always treat each\\ninput as independent from all other inputs.  In many contexts, this\\nmakes perfect sense.  For example, decisions about whether names tend\\nto be male or female can be made on a case-by-case basis.  However,\\nthere are often cases, such as part-of-speech tagging, where we are\\ninterested in solving classification problems that are closely related\\nto one another.\\n\\n\\n1.6&nbsp;&nbsp;&nbsp;Sequence Classification\\nIn order to capture the dependencies between related classification\\ntasks, we can use joint classifier models, which choose an\\nappropriate labeling for a collection of related inputs.  In the case\\nof part-of-speech tagging, a variety of different sequence\\nclassifier models can be used to jointly choose part-of-speech\\ntags for all the words in a given sentence.\\n<!-- note that consecutive classification isn\\'t a widely-used term,\\nbut it seems reasonably good to me. -->\\nOne sequence classification strategy, known as consecutive\\nclassification or greedy sequence classification, is to\\nfind the most likely class label for the first input,\\nthen to use that answer to help find the best label for the next\\ninput.  The process can then be repeated until all of the inputs have\\nbeen labeled.  This is the approach that was taken by the bigram\\ntagger from 5, which began by choosing a\\npart-of-speech tag for the first word in the sentence, and then chose\\nthe tag for each subsequent word based on the word itself and the\\npredicted tag for the previous word.\\nThis strategy is demonstrated in 1.7.\\nFirst, we must\\naugment our feature extractor function to take a history\\nargument, which provides a list of the tags that we\\'ve predicted for\\nthe sentence so far .\\nEach tag in history corresponds with a word in sentence.  But\\nnote that history will only contain tags for words we\\'ve already\\nclassified, that is, words to the left of the target word.  Thus, while it is\\npossible to look at some features of words to the right\\nof the target word, it is not possible to look at the tags for those\\nwords (since we haven\\'t generated them yet).\\nHaving defined a feature extractor, we can proceed to build our\\nsequence classifier .  During training, we use\\nthe annotated tags to\\nprovide the appropriate history to the feature extractor, but when\\ntagging new sentences, we generate the history list based on the\\noutput of the tagger itself.\\n\\n\\n\\n\\n&nbsp;\\n def pos_features(sentence, i, history): \\n     features = {\"suffix(1)\": sentence[i][-1:],\\n                 \"suffix(2)\": sentence[i][-2:],\\n                 \"suffix(3)\": sentence[i][-3:]}\\n     if i == 0:\\n         features[\"prev-word\"] = \"&lt;START&gt;\"\\n         features[\"prev-tag\"] = \"&lt;START&gt;\"\\n     else:\\n         features[\"prev-word\"] = sentence[i-1]\\n         features[\"prev-tag\"] = history[i-1]\\n     return features\\n\\nclass ConsecutivePosTagger(nltk.TaggerI): \\n\\n    def __init__(self, train_sents):\\n        train_set = []\\n        for tagged_sent in train_sents:\\n            untagged_sent = nltk.tag.untag(tagged_sent)\\n            history = []\\n            for i, (word, tag) in enumerate(tagged_sent):\\n                featureset = pos_features(untagged_sent, i, history)\\n                train_set.append( (featureset, tag) )\\n                history.append(tag)\\n        self.classifier = nltk.NaiveBayesClassifier.train(train_set)\\n\\n    def tag(self, sentence):\\n        history = []\\n        for i, word in enumerate(sentence):\\n            featureset = pos_features(sentence, i, history)\\n            tag = self.classifier.classify(featureset)\\n            history.append(tag)\\n        return zip(sentence, history)\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; tagged_sents = brown.tagged_sents(categories=\\'news\\')\\n&gt;&gt;&gt; size = int(len(tagged_sents) * 0.1)\\n&gt;&gt;&gt; train_sents, test_sents = tagged_sents[size:], tagged_sents[:size]\\n&gt;&gt;&gt; tagger = ConsecutivePosTagger(train_sents)\\n&gt;&gt;&gt; print(tagger.evaluate(test_sents))\\n0.79796012981\\n\\n\\nExample 1.7 (code_consecutive_pos_tagger.py): Figure 1.7: Part of Speech Tagging with a Consecutive Classifier\\n\\n\\n\\n1.7&nbsp;&nbsp;&nbsp;Other Methods for Sequence Classification\\nOne shortcoming of this approach is that we commit to every decision\\nthat we make.  For example, if we decide to label a word as a noun,\\nbut later find evidence that it should have been a verb, there\\'s no\\nway to go back and fix our mistake.  One solution to this problem is\\nto adopt a transformational strategy instead.  Transformational joint\\nclassifiers work by creating an initial assignment of labels for the\\ninputs, and then iteratively refining that assignment in an attempt to\\nrepair inconsistencies between related inputs.  The Brill tagger,\\ndescribed in (1), is a good example of this strategy.\\nAnother solution is to assign scores to all of the possible\\nsequences of part-of-speech tags, and to choose the sequence\\nwhose overall score is highest. This is the approach taken by\\nHidden Markov Models.\\nHidden Markov Models are similar to consecutive classifiers in\\nthat they look at both the inputs and the history of predicted\\ntags. However, rather than simply finding the single best tag\\nfor a given word, they generate a probability distribution over\\ntags. These probabilities are then combined to calculate\\nprobability scores for tag sequences, and the tag sequence with\\nthe highest probability is chosen. Unfortunately, the number of\\npossible tag sequences is quite large. Given a tag set with 30\\ntags, there are about 600 trillion (3010) ways to label\\na 10-word sentence.  In order to avoid considering all these possible\\nsequences separately, Hidden Markov Models require that the\\nfeature extractor only look at the most recent tag (or the most\\nrecent n tags, where n is fairly small). Given that\\nrestriction, it is possible to use dynamic programming (4.7)\\nto efficiently find the most likely tag sequence. In particular,\\nfor each consecutive word index i,\\na score is computed for each possible current and previous tag.\\nThis same basic approach is taken by\\ntwo more advanced models, called Maximum Entropy Markov Models and\\nLinear-Chain Conditional Random Field Models;\\nbut different algorithms are used to find scores for tag sequences.\\n\\n\\n\\n2&nbsp;&nbsp;&nbsp;Further Examples of Supervised Classification\\n\\n2.1&nbsp;&nbsp;&nbsp;Sentence Segmentation\\nSentence segmentation can be viewed as a classification task for\\npunctuation: whenever we encounter a symbol that could possibly end a\\nsentence, such as a period or a question mark, we have to decide\\nwhether it terminates the preceding sentence.\\nThe first step is to obtain some data that has already been segmented\\ninto sentences and convert it into a form that is suitable for\\nextracting features:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; sents = nltk.corpus.treebank_raw.sents()\\n&gt;&gt;&gt; tokens = []\\n&gt;&gt;&gt; boundaries = set()\\n&gt;&gt;&gt; offset = 0\\n&gt;&gt;&gt; for sent in sents:\\n...     tokens.extend(sent)\\n...     offset += len(sent)\\n...     boundaries.add(offset-1)\\n\\n\\n\\nHere, tokens is a merged list of tokens from the individual\\nsentences, and boundaries is a set containing the indexes of all\\nsentence-boundary tokens.  Next, we need to specify the features of\\nthe data that will be used in order to decide whether punctuation\\nindicates a sentence-boundary:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; def punct_features(tokens, i):\\n...     return {\\'next-word-capitalized\\': tokens[i+1][0].isupper(),\\n...             \\'prev-word\\': tokens[i-1].lower(),\\n...             \\'punct\\': tokens[i],\\n...             \\'prev-word-is-one-char\\': len(tokens[i-1]) == 1}\\n\\n\\n\\nBased on this feature extractor, we can create a list of labeled\\nfeaturesets by selecting all the punctuation tokens, and tagging\\nwhether they are boundary tokens or not:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; featuresets = [(punct_features(tokens, i), (i in boundaries))\\n...                for i in range(1, len(tokens)-1)\\n...                if tokens[i] in \\'.?!\\']\\n\\n\\n\\nUsing these featuresets, we can train and evaluate a\\npunctuation classifier:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; size = int(len(featuresets) * 0.1)\\n&gt;&gt;&gt; train_set, test_set = featuresets[size:], featuresets[:size]\\n&gt;&gt;&gt; classifier = nltk.NaiveBayesClassifier.train(train_set)\\n&gt;&gt;&gt; nltk.classify.accuracy(classifier, test_set)\\n0.936026936026936\\n\\n\\n\\nTo use this classifier to perform sentence segmentation, we simply\\ncheck each punctuation mark to see whether it\\'s labeled as a boundary;\\nand divide the list of words at the boundary marks.  The listing\\nin 2.1 shows how this can be done.\\n\\n\\n\\n\\n&nbsp;\\ndef segment_sentences(words):\\n    start = 0\\n    sents = []\\n    for i, word in enumerate(words):\\n        if word in \\'.?!\\' and classifier.classify(punct_features(words, i)) == True:\\n            sents.append(words[start:i+1])\\n            start = i+1\\n    if start &lt; len(words):\\n        sents.append(words[start:])\\n    return sents\\n\\n\\nExample 2.1 (code_classification_based_segmenter.py): Figure 2.1: Classification Based Sentence Segmenter\\n\\n\\n\\n2.2&nbsp;&nbsp;&nbsp;Identifying Dialogue Act Types\\nWhen processing dialogue, it can be useful to think of\\nutterances as a type of action performed by the speaker.  This\\ninterpretation is most straightforward for performative statements\\nsuch as \"I forgive you\" or \"I bet you can\\'t climb that hill.\"  But\\ngreetings, questions, answers, assertions, and clarifications can all\\nbe thought of as types of speech-based actions.  Recognizing the\\ndialogue acts underlying the utterances in a dialogue can be an\\nimportant first step in understanding the conversation.\\nThe NPS Chat Corpus, which was demonstrated in\\n1, consists of over 10,000 posts from\\ninstant messaging sessions.  These posts have all been labeled with\\none of 15 dialogue act types, such as \"Statement,\" \"Emotion,\"\\n\"ynQuestion\", and \"Continuer.\"  We can therefore use this data to\\nbuild a classifier that can identify the dialogue act types for new\\ninstant messaging posts.  The first step is to extract the basic\\nmessaging data.  We will call xml_posts() to get a data structure\\nrepresenting the XML annotation for each post:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; posts = nltk.corpus.nps_chat.xml_posts()[:10000]\\n\\n\\n\\nNext, we\\'ll define a simple feature extractor that checks what words\\nthe post contains:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; def dialogue_act_features(post):\\n...     features = {}\\n...     for word in nltk.word_tokenize(post):\\n...         features[\\'contains({})\\'.format(word.lower())] = True\\n...     return features\\n\\n\\n\\nFinally, we construct the training and testing data by applying the\\nfeature extractor to each post (using post.get(\\'class\\') to get\\na post\\'s dialogue act type), and create a new classifier:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; featuresets = [(dialogue_act_features(post.text), post.get(\\'class\\'))\\n...                for post in posts]\\n&gt;&gt;&gt; size = int(len(featuresets) * 0.1)\\n&gt;&gt;&gt; train_set, test_set = featuresets[size:], featuresets[:size]\\n&gt;&gt;&gt; classifier = nltk.NaiveBayesClassifier.train(train_set)\\n&gt;&gt;&gt; print(nltk.classify.accuracy(classifier, test_set))\\n0.67\\n\\n\\n\\n\\n<!-- *We had planned to extend this example by showing how a sequence\\nclassifier can be used to take advantage of the fact that\\nsome sequences (eg ynquestion->yanswer) are more likely than\\nothers.* -->\\n\\n\\n2.3&nbsp;&nbsp;&nbsp;Recognizing Textual Entailment\\nRecognizing textual entailment (RTE) is the task of determining\\nwhether a given piece of text T entails another text called the\\n\"hypothesis\" (as already discussed in 5).\\nTo date, there have been four RTE Challenges, where\\nshared development and test data is made available to competing teams.\\nHere are a couple of examples of text/hypothesis pairs from the\\nChallenge 3 development dataset. The label True indicates that the\\nentailment holds, and False, that it fails to hold.\\n\\nChallenge 3, Pair 34 (True)\\n\\nT: Parviz Davudi was representing Iran at a meeting of the Shanghai\\nCo-operation Organisation (SCO), the fledgling association that\\nbinds Russia, China and four former Soviet republics of central\\nAsia together to fight terrorism.\\nH: China is a member of SCO.\\n\\nChallenge 3, Pair 81 (False)\\n\\nT: According to NC Articles of Organization, the members of LLC\\ncompany are H. Nelson Beavers, III, H. Chester Beavers and Jennie\\nBeavers Stewart.\\nH: Jennie Beavers Stewart is a share-holder of Carolina Analytical\\nLaboratory.\\n\\n\\nIt should be emphasized that the relationship between text and\\nhypothesis is not intended to be logical entailment, but rather\\nwhether a human would conclude that the text provides reasonable\\nevidence for taking the hypothesis to be true.\\nWe can treat RTE as a classification task, in which we try to\\npredict the True/False label for each pair. Although it seems\\nlikely that successful approaches to this task will involve a\\ncombination of parsing, semantics and real world\\nknowledge, many early attempts at RTE achieved reasonably good results\\nwith shallow analysis, based on similarity between the text and\\nhypothesis at the word level. In the ideal case, we would expect that if\\nthere is an entailment, then all the information expressed by the hypothesis\\nshould also be present in the text. Conversely, if there is information\\nfound in the hypothesis that is absent from the text, then there\\nwill be no entailment.\\nIn our RTE feature detector (2.2), we let words\\n(i.e., word types) serve as proxies for information, and\\nour features count the degree of word overlap, and the degree to which\\nthere are words in the hypothesis but not in the text (captured by the\\nmethod hyp_extra()). Not all words are equally important —\\nNamed Entity mentions such as the names of people, organizations and\\nplaces are likely to be more significant, which motivates us to\\nextract distinct information for words  and nes (Named\\nEntities). In addition, some high frequency function words are\\nfiltered out as \"stopwords\".\\n\\n\\n\\n\\n[xx]give some intro to RTEFeatureExtractor??\\n\\n\\n\\n\\n\\n\\n&nbsp;\\ndef rte_features(rtepair):\\n    extractor = nltk.RTEFeatureExtractor(rtepair)\\n    features = {}\\n    features[\\'word_overlap\\'] = len(extractor.overlap(\\'word\\'))\\n    features[\\'word_hyp_extra\\'] = len(extractor.hyp_extra(\\'word\\'))\\n    features[\\'ne_overlap\\'] = len(extractor.overlap(\\'ne\\'))\\n    features[\\'ne_hyp_extra\\'] = len(extractor.hyp_extra(\\'ne\\'))\\n    return features\\n\\n\\nExample 2.2 (code_rte_features.py): Figure 2.2: \"Recognizing Text Entailment\" Feature Extractor.  The\\nRTEFeatureExtractor class builds a bag\\nof words for both the text and the hypothesis after throwing\\naway some stopwords, then calculates overlap and difference.\\n\\nTo illustrate the content of these features, we examine some\\nattributes of the text/hypothesis Pair 34 shown earlier:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; rtepair = nltk.corpus.rte.pairs([\\'rte3_dev.xml\\'])[33]\\n&gt;&gt;&gt; extractor = nltk.RTEFeatureExtractor(rtepair)\\n&gt;&gt;&gt; print(extractor.text_words)\\n{\\'Russia\\', \\'Organisation\\', \\'Shanghai\\', \\'Asia\\', \\'four\\', \\'at\\',\\n\\'operation\\', \\'SCO\\', ...}\\n&gt;&gt;&gt; print(extractor.hyp_words)\\n{\\'member\\', \\'SCO\\', \\'China\\'}\\n&gt;&gt;&gt; print(extractor.overlap(\\'word\\'))\\nset()\\n&gt;&gt;&gt; print(extractor.overlap(\\'ne\\'))\\n{\\'SCO\\', \\'China\\'}\\n&gt;&gt;&gt; print(extractor.hyp_extra(\\'word\\'))\\n{\\'member\\'}\\n\\n\\n\\nThese features indicate that all important words in the hypothesis are\\ncontained in the text, and thus there is some evidence for labeling this\\nas True.\\nThe module nltk.classify.rte_classify reaches just over 58%\\naccuracy on the combined RTE test data using methods like these. Although\\nthis figure is not very impressive, it requires significant effort, and more\\nlinguistic processing, to achieve much better results.\\n\\n\\n2.4&nbsp;&nbsp;&nbsp;Scaling Up to Large Datasets\\nPython provides an excellent environment for performing basic text\\nprocessing and feature extraction.  However, it is not able to perform\\nthe numerically intensive calculations required by machine learning\\nmethods nearly as quickly as lower-level languages such as C.  Thus,\\nif you attempt to use the pure-Python machine learning implementations\\n(such as nltk.NaiveBayesClassifier) on large datasets, you may\\nfind that the learning algorithm takes an unreasonable amount of time\\nand memory to complete.\\nIf you plan to train classifiers with large amounts of training data\\nor a large number of features, we recommend that you explore\\nNLTK\\'s facilities for interfacing with external machine learning\\npackages.  Once these packages have been installed, NLTK can\\ntransparently invoke them (via system calls) to train classifier\\nmodels significantly faster than the pure-Python classifier\\nimplementations.  See the NLTK webpage for a list of recommended\\nmachine learning packages that are supported by NLTK.\\n<!-- SB: I think numpy and other numerical libraries do their work in C, and\\nare probably quite efficient.  Not clear about support for sparse arrays. -->\\n<!-- EL: Yes, but even with numpy etc, python is *slow* for this stuff.\\nI speak from experience. :) -->\\n<!-- XXX Note that the additive nature of a lot of training tasks mean\\nthat parallelization will work fine.  If those can be done using\\nMapReduce we\\'ll be able to use NLTK for some non-toy problems here. -->\\n\\n\\n\\n3&nbsp;&nbsp;&nbsp;Evaluation\\nIn order to decide whether a classification model is accurately\\ncapturing a pattern, we must evaluate that model.  The result of this\\nevaluation is important for deciding how trustworthy the model is, and\\nfor what purposes we can use it.  Evaluation can also be an effective tool\\nfor guiding us in making future improvements to the model.\\n\\n3.1&nbsp;&nbsp;&nbsp;The Test Set\\nMost evaluation techniques calculate a score for a model by comparing\\nthe labels that it generates for the inputs in a test set\\n(or evaluation set)\\nwith the correct labels for those inputs.  This test set\\ntypically has the same format as the training set.  However, it is\\nvery important that the test set be distinct from the training\\ncorpus: if we simply re-used the training set as the test\\nset, then a model that simply memorized its input, without learning\\nhow to generalize to new examples, would receive misleadingly high\\nscores.\\nWhen building the test set, there is often a trade-off between\\nthe amount of data available for testing and the amount available\\nfor training.  For classification tasks that have a small\\nnumber of well-balanced labels and a diverse test set, a\\nmeaningful evaluation can be performed with as few as 100 evaluation\\ninstances.  But if a classification task has a large number of labels,\\nor includes very infrequent labels, then the size of the test\\nset should be chosen to ensure that the least frequent label occurs at\\nleast 50 times.  Additionally, if the test set contains many\\nclosely related instances — such as instances drawn from a single\\ndocument — then the size of the test set should be increased to\\nensure that this lack of diversity does not skew the evaluation\\nresults.  When large amounts of annotated data are available, it is\\ncommon to err on the side of safety by using 10% of the overall data\\nfor evaluation.\\nAnother consideration when choosing the test set is the degree\\nof similarity between instances in the test set and those in the\\ndevelopment set.  The more similar these two datasets are, the less\\nconfident we can be that evaluation results will generalize to other\\ndatasets.  For example, consider the part-of-speech tagging task.  At\\none extreme, we could create the training set and test set by\\nrandomly assigning sentences from a data source that reflects a single\\ngenre (news):\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; import random\\n&gt;&gt;&gt; from nltk.corpus import brown\\n&gt;&gt;&gt; tagged_sents = list(brown.tagged_sents(categories=\\'news\\'))\\n&gt;&gt;&gt; random.shuffle(tagged_sents)\\n&gt;&gt;&gt; size = int(len(tagged_sents) * 0.1)\\n&gt;&gt;&gt; train_set, test_set = tagged_sents[size:], tagged_sents[:size]\\n\\n\\n\\nIn this case, our test set will be very similar to our training\\nset.  The training set and test set are taken from the same\\ngenre, and so we cannot be confident that evaluation results would\\ngeneralize to other genres.  What\\'s worse, because of the call to\\nrandom.shuffle(), the test set contains sentences that are\\ntaken from the same documents that were used for training.  If there\\nis any consistent pattern within a document — say, if a given word\\nappears with a particular part-of-speech tag especially frequently — then\\nthat difference will be reflected in both the development set and the\\ntest set.  A somewhat better approach is to ensure that\\nthe training set and test set are taken from different documents:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; file_ids = brown.fileids(categories=\\'news\\')\\n&gt;&gt;&gt; size = int(len(file_ids) * 0.1)\\n&gt;&gt;&gt; train_set = brown.tagged_sents(file_ids[size:])\\n&gt;&gt;&gt; test_set = brown.tagged_sents(file_ids[:size])\\n\\n\\n\\nIf we want to perform a more stringent evaluation, we can draw the\\ntest set from documents that are less closely related to those\\nin the training set:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; train_set = brown.tagged_sents(categories=\\'news\\')\\n&gt;&gt;&gt; test_set = brown.tagged_sents(categories=\\'fiction\\')\\n\\n\\n\\nIf we build a classifier that performs well on this test set,\\nthen we can be confident that it has the power to generalize well\\nbeyond the data that it was trained on.\\n\\n\\n3.2&nbsp;&nbsp;&nbsp;Accuracy\\nThe simplest metric that can be used to evaluate a classifier,\\naccuracy, measures the percentage of inputs in the test\\nset that the classifier correctly labeled.  For example, a name gender\\nclassifier that predicts the correct name 60 times in a test\\nset containing 80 names would have an accuracy of 60/80 = 75%.  The\\nfunction nltk.classify.accuracy() will calculate the\\naccuracy of a classifier model on a given test set:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; classifier = nltk.NaiveBayesClassifier.train(train_set) \\n&gt;&gt;&gt; print(\\'Accuracy: {:4.2f}\\'.format(nltk.classify.accuracy(classifier, test_set))) \\n0.75\\n\\n\\n\\nWhen interpreting the accuracy score of a classifier, it is important\\nto take into consideration the frequencies of the individual class\\nlabels in the test set.  For example, consider a classifier that\\ndetermines the correct word sense for each occurrence of the word\\nbank.  If we evaluate this classifier on financial newswire text,\\nthen we may find that the financial-institution sense appears 19\\ntimes out of 20.  In that case, an accuracy of 95% would hardly be\\nimpressive, since we could achieve that accuracy with a model that\\nalways returns the financial-institution sense.  However, if we\\ninstead evaluate the classifier on a more balanced corpus, where the\\nmost frequent word sense has a frequency of 40%, then a 95% accuracy\\nscore would be a much more positive result.  (A similar issue arises\\nwhen measuring inter-annotator agreement in\\n2.)\\n\\n\\n3.3&nbsp;&nbsp;&nbsp;Precision and Recall\\nAnother instance where accuracy scores can be misleading is in\\n\"search\" tasks, such as information retrieval, where we are attempting\\nto find documents that are relevant to a particular task.  Since the\\nnumber of irrelevant documents far outweighs the number of relevant\\ndocuments, the accuracy score for a model that labels every document\\nas irrelevant would be very close to 100%.\\n\\n\\nFigure 3.1: True and False Positives and Negatives\\n\\nIt is therefore conventional to employ a different set of measures for\\nsearch tasks, based on the number of items in each of the four\\ncategories shown in 3.1:\\n\\nTrue positives are relevant items that we correctly identified\\nas relevant.\\nTrue negatives are irrelevant items that we correctly identified\\nas irrelevant.\\nFalse positives (or Type I errors) are irrelevant items\\nthat we incorrectly identified as relevant.\\nFalse negatives (or Type II errors) are relevant items\\nthat we incorrectly identified as irrelevant.\\n\\nGiven these four numbers, we can define the following metrics:\\n\\nPrecision, which indicates how many of the items that we\\nidentified were relevant, is TP/(TP+FP).\\nRecall, which indicates how many of the relevant items that we\\nidentified, is TP/(TP+FN).\\nThe F-Measure (or F-Score), which combines the precision\\nand recall to give a single score, is defined to be the harmonic\\nmean of the precision and recall:\\n(2 × Precision × Recall) / (Precision + Recall).\\n\\n\\n\\n\\n3.4&nbsp;&nbsp;&nbsp;Confusion Matrices\\n<!-- Repeat some code from chapter 5, which recreates the tagger t2.\\nThis tagger is used to illustrate the confusion matrix:\\n\\n >>> from nltk.corpus import brown\\n >>> brown_tagged_sents = brown.tagged_sents(categories=\\'news\\')\\n >>> size = int(len(brown_tagged_sents) * 0.9)\\n >>> train_sents = brown_tagged_sents[:size]\\n >>> test_sents = brown_tagged_sents[size:]\\n >>> t0 = nltk.DefaultTagger(\\'NN\\')\\n >>> t1 = nltk.UnigramTagger(train_sents, backoff=t0)\\n >>> t2 = nltk.BigramTagger(train_sents, backoff=t1) -->\\nWhen performing classification tasks with three or more labels, it can\\nbe informative to subdivide the errors made by the model based on\\nwhich types of mistake it made.  A confusion matrix is a table\\nwhere each cell [i,j] indicates how often label j was\\npredicted when the correct label was i.  Thus, the diagonal\\nentries (i.e., cells |ii|) indicate labels that were\\ncorrectly predicted, and the off-diagonal entries indicate errors.  In\\nthe following example, we generate a confusion matrix for the bigram\\ntagger developed in 4:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; def tag_list(tagged_sents):\\n...     return [tag for sent in tagged_sents for (word, tag) in sent]\\n&gt;&gt;&gt; def apply_tagger(tagger, corpus):\\n...     return [tagger.tag(nltk.tag.untag(sent)) for sent in corpus]\\n&gt;&gt;&gt; gold = tag_list(brown.tagged_sents(categories=\\'editorial\\'))\\n&gt;&gt;&gt; test = tag_list(apply_tagger(t2, brown.tagged_sents(categories=\\'editorial\\')))\\n&gt;&gt;&gt; cm = nltk.ConfusionMatrix(gold, test)\\n&gt;&gt;&gt; print(cm.pretty_format(sort_by_count=True, show_percents=True, truncate=9))\\n    |                                         N                      |\\n    |      N      I      A      J             N             V      N |\\n    |      N      N      T      J      .      S      ,      B      P |\\n----+----------------------------------------------------------------+\\n NN | &lt;11.8%&gt;  0.0%      .   0.2%      .   0.0%      .   0.3%   0.0% |\\n IN |   0.0%  &lt;9.0%&gt;     .      .      .   0.0%      .      .      . |\\n AT |      .      .  &lt;8.6%&gt;     .      .      .      .      .      . |\\n JJ |   1.7%      .      .  &lt;3.9%&gt;     .      .      .   0.0%   0.0% |\\n  . |      .      .      .      .  &lt;4.8%&gt;     .      .      .      . |\\nNNS |   1.5%      .      .      .      .  &lt;3.2%&gt;     .      .   0.0% |\\n  , |      .      .      .      .      .      .  &lt;4.4%&gt;     .      . |\\n VB |   0.9%      .      .   0.0%      .      .      .  &lt;2.4%&gt;     . |\\n NP |   1.0%      .      .   0.0%      .      .      .      .  &lt;1.8%&gt;|\\n----+----------------------------------------------------------------+\\n(row = reference; col = test)\\n\\n\\n\\n\\nThe confusion matrix indicates that common errors include a\\nsubstitution of NN for JJ (for 1.6% of words), and of NN for NNS (for\\n1.5% of words).  Note that periods (.) indicate cells\\nwhose value is 0, and that the diagonal entries — which correspond to\\ncorrect classifications — are marked with angle brackets.\\n.. XXX explain use of \"reference\" in the legend above.\\n\\n\\n3.5&nbsp;&nbsp;&nbsp;Cross-Validation\\nIn order to evaluate our models, we must reserve a portion of the\\nannotated data for the test set.  As we already mentioned,\\nif the test set is too small, then\\nour evaluation may not be accurate.  However, making the test set\\nlarger usually means making the training set smaller, which can have a\\nsignificant impact on performance if a limited amount of annotated\\ndata is available.\\nOne solution to this problem is to perform multiple evaluations on\\ndifferent test sets, then to combine the scores from those\\nevaluations, a technique known as cross-validation.  In\\nparticular, we subdivide the original corpus into N\\nsubsets called folds.  For each of these folds, we train a model using all\\nof the data except the data in that fold, and then test that\\nmodel on the fold.  Even though the individual folds might\\nbe too small to give accurate evaluation scores on their own, the\\ncombined evaluation score is based on a large amount of data, and is\\ntherefore quite reliable.\\nA second, and equally important, advantage of using cross-validation\\nis that it allows us to examine how widely the performance varies\\nacross different training sets.  If we get very similar scores for all\\nN training sets, then we can be fairly confident that the\\nscore is accurate.  On the other hand, if scores vary widely across\\nthe N training sets, then we should probably be skeptical\\nabout the accuracy of the evaluation score.\\n\\n\\n\\n\\n\\n4&nbsp;&nbsp;&nbsp;Decision Trees\\nIn the next three sections, we\\'ll take a closer look at three machine\\nlearning methods that can be used to automatically build\\nclassification models: decision trees, naive Bayes classifiers, and\\nMaximum Entropy classifiers.  As we\\'ve seen, it\\'s possible to treat these\\nlearning methods as black boxes, simply training models and using them\\nfor prediction without understanding how they work.  But there\\'s a lot\\nto be learned from taking a closer look at how these learning methods\\nselect models based on the data in a training set.  An\\nunderstanding of these methods can help guide our selection of\\nappropriate features, and especially our decisions about how those\\nfeatures should be encoded.  And an understanding of the generated\\nmodels can allow us to extract information about which features\\nare most informative, and how those features relate to one another.\\n<!-- Note that they haven\\'t necessarily seen syntax trees before this, so\\nit may seem odd to them (or at least not obvious) that these \"trees\"\\nare upside down. -->\\n<!-- XXX some people will know the concept of a \"dichotomous key\".\\n(\"tree-structured flowchart\" assumes domain knowledge so doesn\\'t\\ncommunicate as widely) -->\\nA decision tree is a simple flowchart that selects\\nlabels for input values.  This flowchart consists of decision\\nnodes, which check feature values, and leaf nodes, which\\nassign labels.  To choose the label for an input value, we begin at\\nthe flowchart\\'s initial decision node, known as its root node.\\nThis node contains a condition that checks one of the input value\\'s\\nfeatures, and selects a branch based on that feature\\'s value.\\nFollowing the branch that describes our input value, we arrive at a\\nnew decision node, with a new condition on the input value\\'s features.\\nWe continue following the branch selected by each node\\'s condition,\\nuntil we arrive at a leaf node which provides a label for the input\\nvalue.  4.1 shows an example decision tree model for\\nthe name gender task.\\n\\n\\nFigure 4.1: Decision Tree model for the name gender task.  Note that tree\\ndiagrams are conventionally drawn \"upside down,\" with the root at the\\ntop, and the leaves at the bottom.\\n\\n<!-- XXX show: train decision tree, then print it as pseudocode & as more\\ncompact tree. -->\\n\\nOnce we have a decision tree, it is straightforward to\\nuse it to assign labels to new input values.  What\\'s less straightforward\\nis how we can build a decision tree that models a given\\ntraining set.  But before we look at the learning algorithm for\\nbuilding decision trees, we\\'ll consider a simpler task: picking the\\nbest \"decision stump\" for a corpus.  A decision stump is a\\ndecision tree with a single node that decides how to classify inputs\\nbased on a single feature.  It contains one leaf for each possible\\nfeature value, specifying the class label that should be assigned to\\ninputs whose features have that value.  In order to build a decision\\nstump, we must first decide which feature should be used.  The\\nsimplest method is to just build a decision stump for each possible\\nfeature, and see which one achieves the highest accuracy on the\\ntraining data, although there are other alternatives that we will discuss below.\\nOnce we\\'ve picked a feature, we can build the decision stump by assigning a\\nlabel to each leaf based on the most frequent label for the selected\\nexamples in the training set (i.e., the examples where the selected\\nfeature has that value).\\n\\n\\nGiven the algorithm for choosing decision stumps, the algorithm for\\ngrowing larger decision trees is straightforward.  We\\nbegin by selecting the overall best decision stump for the\\nclassification task.  We\\nthen check the accuracy of each of the leaves on the training set.\\nLeaves that do not achieve sufficient accuracy are then\\nreplaced by new decision stumps, trained on the subset of the training\\ncorpus that is selected by the path to the leaf.  For example, we\\ncould grow the decision tree in 4.1 by replacing the\\nleftmost leaf with a new decision stump, trained on the subset of the\\ntraining set names that do not start with a \"k\" or end with a vowel\\nor an \"l.\"\\n\\n4.1&nbsp;&nbsp;&nbsp;Entropy and Information Gain\\nAs was mentioned before, there are several methods for identifying\\nthe most informative feature for a decision stump.  One\\npopular alternative, called information gain, measures how\\nmuch more organized the input values become when we divide them up\\nusing a given feature.  To measure how disorganized the original set\\nof input values are, we calculate entropy of their labels, which will\\nbe high if the input values have highly varied labels, and low if many\\ninput values all have the same label.  In particular, entropy is\\ndefined as the sum of the probability of each label times the log\\nprobability of that same label:\\n\\n  (1)H = −Σl |in| labelsP(l) × log2P(l).\\n\\n\\nFigure 4.2: The entropy of labels in the name gender prediction task, as a\\nfunction of the percentage of names in a given set that are male.\\n\\nFor example, 4.2 shows how the entropy of labels in\\nthe name gender prediction task depends on the ratio of male to female\\nnames.  Note that if most input values have the same label (e.g., if\\nP(male) is near 0 or near 1), then entropy is low.  In particular,\\nlabels that have low frequency do not contribute much to the entropy\\n(since P(l) is small), and labels with high frequency also do\\nnot contribute much to the entropy (since log2P(l) is small).  On the other hand, if the input values have\\na wide variety of labels, then there are many labels with a \"medium\"\\nfrequency, where neither P(l) nor log2P(l) is small, so the entropy is high.\\n4.3 demonstrates how to calculate the entropy\\nof a list of labels.\\n<!-- (skip doctest on the 0-entropy example below because it prints\\n-0.0, and it\\'s not worth it to explain *why* it does that.) -->\\n\\n\\n\\n\\n&nbsp;\\nimport math\\ndef entropy(labels):\\n    freqdist = nltk.FreqDist(labels)\\n    probs = [freqdist.freq(l) for l in freqdist]\\n    return -sum(p * math.log(p,2) for p in probs)\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; print(entropy([\\'male\\', \\'male\\', \\'male\\', \\'male\\'])) \\n0.0\\n&gt;&gt;&gt; print(entropy([\\'male\\', \\'female\\', \\'male\\', \\'male\\']))\\n0.811...\\n&gt;&gt;&gt; print(entropy([\\'female\\', \\'male\\', \\'female\\', \\'male\\']))\\n1.0\\n&gt;&gt;&gt; print(entropy([\\'female\\', \\'female\\', \\'male\\', \\'female\\']))\\n0.811...\\n&gt;&gt;&gt; print(entropy([\\'female\\', \\'female\\', \\'female\\', \\'female\\'])) \\n0.0\\n\\n\\nExample 4.3 (code_entropy.py): Figure 4.3: Calculating the Entropy of a List of Labels\\n\\n<!-- The doctest: +SKIP in the last example is because doctest prints\\n-0.0 on some systems rather than 0.0, which can be confusing;\\nbut isn\\'t worth explaining here. -->\\nOnce we have calculated the entropy of the original set of input\\nvalues\\' labels, we can determine how much more organized the labels\\nbecome once we apply the decision stump.  To do so, we calculate the\\nentropy for each of the decision stump\\'s leaves, and take the average\\nof those leaf entropy values (weighted by the number of samples in\\neach leaf).  The information gain is then equal to the original\\nentropy minus this new, reduced entropy.  The higher the information\\ngain, the better job the decision stump does of dividing the input\\nvalues into coherent groups, so we can build decision trees by\\nselecting the decision stumps with the highest information gain.\\nAnother consideration for decision trees is efficiency.  The simple\\nalgorithm for selecting decision stumps described above must construct\\na candidate decision stump for every possible feature, and this\\nprocess must be repeated for every node in the constructed decision\\ntree.  A number of algorithms have been developed to cut down on the\\ntraining time by storing and reusing information about previously\\nevaluated examples.\\n>. -->\\n<!-- XXX the point about decision trees being easy to interpret\\nhas already been made, so this sounds repetitive.\\nIt would be fine to delete the earlier mention of this point,\\nsince it was not in a section on decision trees. -->\\nDecision trees have a number of useful qualities.  To begin with,\\nthey\\'re simple to understand, and easy to interpret.  This is\\nespecially true near the top of the decision tree, where it is usually\\npossible for the learning algorithm to find very useful features.\\nDecision trees are especially well suited to cases where many\\nhierarchical categorical distinctions can be made.  For example,\\ndecision trees can be very effective at capturing phylogeny trees.\\nHowever, decision trees also have a few disadvantages.  One problem is\\nthat, since each branch in the decision tree splits the training data,\\nthe amount of training data available to train nodes lower in the tree\\ncan become quite small.  As a result, these lower decision nodes may\\n\\noverfit the training set, learning patterns that reflect\\nidiosyncrasies of the training set rather than linguistically significant\\npatterns in\\nthe underlying problem.  One solution to this problem is to stop\\ndividing nodes once the amount of training data becomes too small.\\nAnother solution is to grow a full decision tree, but then to\\nprune decision nodes that do not improve performance on a\\ndev-test.\\nA second problem with decision trees is that they force features to be\\nchecked in a specific order, even when features may act relatively\\nindependently of one another.  For example, when classifying documents\\ninto topics (such as sports, automotive, or murder mystery), features\\nsuch as hasword(football) are highly indicative of a specific\\nlabel, regardless of what other the feature values are.  Since there\\nis limited space near the top of the decision tree, most of these\\nfeatures will need to be repeated on many different branches in the\\ntree.  And since the number of branches increases exponentially as we\\ngo down the tree, the amount of repetition can be very large.\\nA related problem is that decision trees are not good at making use of\\nfeatures that are weak predictors of the correct label.  Since these\\nfeatures make relatively small incremental improvements, they tend to\\noccur very low in the decision tree.  But by the time the decision\\ntree learner has descended far enough to use these features, there is\\nnot enough training data left to reliably determine what effect they\\nshould have.  If we could instead look at the effect of these features\\nacross the entire training set, then we might be able to make some\\nconclusions about how they should affect the choice of label.\\nThe fact that decision trees require that features be checked in a\\nspecific order limits their ability to exploit features that are\\nrelatively independent of one another.  The naive Bayes classification\\nmethod, which we\\'ll discuss next, overcomes this limitation by\\nallowing all features to act \"in parallel.\"\\n\\n\\n\\n5&nbsp;&nbsp;&nbsp;Naive Bayes Classifiers\\nIn naive Bayes classifiers, every feature gets a say in\\ndetermining which label should be assigned to a given input value.  To\\nchoose a label for an input value, the naive Bayes classifier begins\\nby calculating the prior probability of each label, which is\\ndetermined by checking frequency of each label in the training set.\\nThe contribution from each feature is then combined with this prior\\nprobability, to arrive at a likelihood estimate for each label.  The\\nlabel whose likelihood estimate is the highest is then assigned to the\\ninput value.  5.1 illustrates this process.\\n<!-- I go back and forth on whether we should include a figure like this\\none.  I think it gives a good high-level feeling of what\\'s going\\non, but the details don\\'t really line up with the algorithm\\'s\\nspecifics, and it takes a good amount of work to explain the figure. -->\\n\\n\\nFigure 5.1: An abstract illustration of the procedure used by the naive Bayes\\nclassifier to choose the topic for a document.  In the training\\ncorpus, most documents are automotive, so the classifier starts out\\nat a point closer to the \"automotive\" label.  But it then\\nconsiders the effect of each feature.  In this example, the input\\ndocument contains the word \"dark,\" which is a weak indicator for\\nmurder mysteries, but it also contains the word \"football,\" which\\nis a strong indicator for sports documents.  After every feature\\nhas made its contribution, the classifier checks which label it is\\nclosest to, and assigns that label to the input.\\n\\nIndividual features make their contribution to the overall decision by\\n\"voting against\" labels that don\\'t occur with that feature very often.\\nIn particular, the likelihood score for each label is reduced by\\nmultiplying it by the probability that an input value with that label\\nwould have the feature.  For example, if the word run occurs in 12%\\nof the sports documents, 10% of the murder mystery documents, and 2%\\nof the automotive documents, then the likelihood score for the sports\\nlabel will be multiplied by 0.12; the likelihood score for the murder\\nmystery label will be multiplied by 0.1, and the likelihood score for\\nthe automotive label will be multiplied by 0.02.  The overall effect\\nwill be to reduce the score of the murder mystery label slightly more\\nthan the score of the sports label, and to significantly reduce the\\nautomotive label with respect to the other two labels.  This\\nprocess is illustrated in 5.2 and\\n5.3.\\n\\n\\nFigure 5.2: Calculating label likelihoods with naive Bayes.  Naive Bayes begins\\nby calculating the prior probability of each label, based on how\\nfrequently each label occurs in the training data.  Every feature\\nthen contributes to the likelihood estimate for each label, by\\nmultiplying it by the probability that input values with that label\\nwill have that feature.  The resulting likelihood score can be\\nthought of as an estimate of the probability that a randomly\\nselected value from the training set would have both the given\\nlabel and the set of features, assuming that the feature\\nprobabilities are all independent.\\n\\n\\n5.1&nbsp;&nbsp;&nbsp;Underlying Probabilistic Model\\nAnother way of understanding the naive Bayes classifier is that it\\nchooses the most likely label for an input, under the assumption that\\nevery input value is generated by first choosing a class label for\\nthat input value, and then generating each feature, entirely\\nindependent of every other feature.  Of course, this assumption is\\nunrealistic; features are often highly dependent on one another.  We\\'ll\\nreturn to some of the consequences of this assumption at the end of\\nthis section.  This simplifying assumption, known as the\\nnaive Bayes assumption (or independence assumption)\\nmakes it much\\neasier to combine the contributions of the different features, since\\nwe don\\'t need to worry about how they should interact with one\\nanother.\\n\\n\\nFigure 5.3: A Bayesian Network Graph illustrating the generative process\\nthat is assumed by the naive Bayes classifier.  To generate a\\nlabeled input, the model first chooses a label for the input,\\nthen it generates each of the input\\'s features based on that label.\\nEvery feature is assumed to be entirely independent of every other\\nfeature, given the label.\\n\\nBased on this assumption, we can calculate an expression for\\nP(label|features), the probability that an input will have a\\nparticular label given that it has a particular set of features.  To\\nchoose a label for a new input, we can then simply pick the label\\nl that maximizes P(l|features).\\nTo begin, we note that P(label|features) is equal to the\\nprobability that an input has a particular label and the specified\\nset of features, divided by the probability that it has the specified\\nset of features:\\n\\n  (2)P(label|features) = P(features, label)/P(features)\\nNext, we note that P(features) will be the same for every\\nchoice of label, so if we are simply interested in finding the most\\nlikely label, it suffices to calculate P(features, label),\\nwhich we\\'ll call the label likelihood.\\n\\nNote\\nIf we want to generate a probability estimate for each\\nlabel, rather than just choosing the most likely label, then the\\neasiest way to compute P(features) is to simply calculate the sum\\nover labels of P(features, label):\\n\\n  (3)P(features) =\\nΣl in| labels P(features, label)\\n\\nThe label likelihood can be expanded out as the probability of the\\nlabel times the probability of the features given the label:\\n\\n  (4)P(features, label) = P(label) × P(features|label)\\nFurthermore, since the features are all independent of one another\\n(given the label), we can separate out the probability of each\\nindividual feature:\\n\\n  (5)P(features, label) = P(label) × Prodf in| featuresP(f|label)`\\nThis is exactly the equation we discussed above for calculating the\\nlabel likelihood: P(label) is the prior probability for a\\ngiven label, and each P(f|label) is the contribution of a single\\nfeature to the label likelihood.\\n\\n\\n5.2&nbsp;&nbsp;&nbsp;Zero Counts and Smoothing\\nThe simplest way to calculate P(f|label), the contribution of a\\nfeature f toward the label likelihood for a label label, is to\\ntake the percentage of training instances with the given label that\\nalso have the given feature:\\n\\n  (6)P(f|label) = count(f, label) / count(label)\\nHowever, this simple approach can become problematic when a feature\\nnever occurs with a given label in the training set.  In this\\ncase, our calculated value for P(f|label) will be zero, which will\\ncause the label likelihood for the given label to be zero.  Thus, the\\ninput will never be assigned this label, regardless of how well the\\nother features fit the label.\\nThe basic problem here is with our calculation of P(f|label), the\\nprobability that an input will have a feature, given a label.  In\\nparticular, just because we haven\\'t seen a feature/label combination\\noccur in the training set, doesn\\'t mean it\\'s impossible for that\\ncombination to occur.  For example, we may not have seen any murder\\nmystery documents that contained the word \"football,\" but we wouldn\\'t\\nwant to conclude that it\\'s completely impossible for such documents to\\nexist.\\nThus, although count(f,label)/count(label) is a good estimate for\\nP(f|label) when count(f, label) is relatively high, this\\nestimate becomes less reliable when count(f) becomes smaller.\\nTherefore, when building naive Bayes models, we usually employ\\nmore sophisticated techniques, known as smoothing techniques,\\nfor calculating P(f|label), the probability of a feature given a\\nlabel.  For example, the Expected Likelihood Estimation for the\\nprobability of a feature given a label basically adds 0.5 to each\\ncount(f,label) value, and the Heldout Estimation uses a heldout\\ncorpus to calculate the relationship between feature frequencies and\\nfeature probabilities.  The nltk.probability module provides support\\nfor a wide variety of smoothing techniques.\\n\\n\\n\\n\\n5.3&nbsp;&nbsp;&nbsp;Non-Binary Features\\nWe have assumed here that each feature is binary, i.e.\\nthat each input either has a feature or does not.  Label-valued\\nfeatures (e.g., a color feature which could be red, green, blue,\\nwhite, or orange) can be converted to binary features by replacing\\nthem with binary features such as \"color-is-red\".  Numeric features can be\\nconverted to binary features by binning, which replaces them with\\nfeatures such as \"4&lt;x&lt;6\".\\nAnother alternative is to use regression methods to model the\\nprobabilities of numeric features.  For example, if we assume that the\\nheight feature has a bell curve distribution, then we could estimate\\nP(height|label) by finding the mean and variance of the heights of the\\ninputs with each label.  In this case, P(f=v|label) would not\\nbe a fixed value, but would vary depending on the value of v.\\n\\n\\n5.4&nbsp;&nbsp;&nbsp;The Naivete of Independence\\nThe reason that naive Bayes classifiers are called \"naive\" is that\\nit\\'s unreasonable to assume that all features are independent of one\\nanother (given the label).  In particular, almost all real-world\\nproblems contain features with varying degrees of dependence on one\\nanother.  If we had to avoid any features that were dependent on one\\nanother, it would be very difficult to construct good feature sets\\nthat provide the required information to the machine learning\\nalgorithm.\\nSo what happens when we ignore the independence assumption, and use\\nthe naive Bayes classifier with features that are not independent?\\nOne problem that arises is that the classifier can end up\\n\"double-counting\" the effect of highly correlated features, pushing\\nthe classifier closer to a given label than is justified.\\nTo see how this can occur, consider a name gender classifier that\\ncontains two identical features, f1 and f2.  In other words, f2 is an exact copy of\\nf1, and contains no new information.\\nWhen the classifier is considering an input, it will include the\\ncontribution of both f1 and f2 when\\ndeciding which label to choose.  Thus, the information content of\\nthese two features will be given more weight than it deserves.\\nOf course, we don\\'t usually build naive Bayes classifiers that contain\\ntwo identical features.  However, we do build classifiers that contain\\nfeatures which are dependent on one another.  For example, the\\nfeatures ends-with(a) and ends-with(vowel) are dependent on\\none another, because if an input value has the first feature, then it\\nmust also have the second feature.  For features like these, the\\nduplicated information may be given more weight than is justified by\\nthe training set.\\n\\n\\n5.5&nbsp;&nbsp;&nbsp;The Cause of Double-Counting\\nThe reason for the double-counting problem is that\\nduring training, feature contributions are computed separately;\\nbut when using the classifier to choose labels for new inputs, those\\nfeature contributions are combined.  One solution, therefore, is to\\nconsider the possible interactions between feature contributions\\nduring training.  We could then use those interactions to adjust the\\ncontributions that individual features make.\\nTo make this more precise, we can rewrite the equation used to\\ncalculate the likelihood of a label, separating out the\\ncontribution made by each feature (or label):\\n\\n\\n  (7)P(features, label) = w[label] × Prodf |in| features w[f, label]\\nHere, w[label] is the \"starting score\" for a given label, and\\nw[f, label] is the contribution made by a given feature\\ntowards a label\\'s likelihood.  We call these values w[label]\\nand w[f, label] the parameters or weights for the\\nmodel.  Using the naive Bayes algorithm, we set each of these\\nparameters independently:\\n\\n  (8)w[label] = P(label)\\n\\n  (9)w[f, label] = P(f|label)\\nHowever, in the next section, we\\'ll look at a classifier that\\nconsiders the possible interactions between these parameters when\\nchoosing their values.\\n\\n\\n\\n6&nbsp;&nbsp;&nbsp;Maximum Entropy Classifiers\\nThe Maximum Entropy classifier uses a model that is very\\nsimilar to the model employed by the naive Bayes classifier.  But rather\\nthan using probabilities to set the model\\'s parameters, it uses search\\ntechniques to find a set of parameters that will maximize the\\nperformance of the classifier.  In particular, it looks for the set of\\nparameters that maximizes the total likelihood of the training\\ncorpus, which is defined as:\\n\\n  (10)P(features) =\\nΣx |in| corpus P(label(x)|features(x))\\nWhere P(label|features), the probability that an input whose\\nfeatures are features will have class label label, is defined as:\\n\\n  (11)P(label|features) = P(label, features) /\\nΣlabel P(label, features)\\nBecause of the potentially complex interactions between the effects of\\nrelated features, there is no way to directly calculate the model\\nparameters that maximize the likelihood of the training set.\\nTherefore, Maximum Entropy classifiers choose the model parameters\\nusing iterative optimization techniques, which initialize the\\nmodel\\'s parameters to random values, and then repeatedly refine those\\nparameters to bring them closer to the optimal solution.  These\\niterative optimization techniques guarantee that each refinement of\\nthe parameters will bring them closer to the optimal values, but do\\nnot necessarily provide a means of determining when those optimal\\nvalues have been reached.  Because the parameters for Maximum Entropy\\nclassifiers are selected using iterative optimization techniques, they\\ncan take a long time to learn.  This is especially true when the size\\nof the training set, the number of features, and the number of\\nlabels are all large.\\n\\nNote\\nSome iterative optimization techniques are much faster than\\nothers.  When training Maximum Entropy models, avoid the use of\\nGeneralized Iterative Scaling (GIS) or Improved Iterative Scaling\\n(IIS), which are both considerably slower than the Conjugate\\nGradient (CG) and the BFGS optimization methods.\\n\\n<!-- The above note could go in the further reading section with a couple\\nof literature references -->\\n<!-- For related work section:?\\nThe use of iterative optimization techniques to find the parameters\\nthat maximize the performance of a model is quite common in machine\\nlearning. -->\\n\\n6.1&nbsp;&nbsp;&nbsp;The Maximum Entropy Model\\nThe Maximum Entropy classifier model is a generalization of the model\\nused by the naive Bayes classifier.  Like the naive Bayes model, the\\nMaximum Entropy classifier calculates the likelihood of each label for\\na given input value by multiplying together the parameters that are\\napplicable for the input value and label.  The naive Bayes classifier\\nmodel defines a parameter for each label, specifying its prior\\nprobability, and a parameter for each (feature, label) pair,\\nspecifying the contribution of individual features towards a label\\'s\\nlikelihood.\\nIn contrast, the Maximum Entropy classifier model leaves it up to the\\nuser to decide what combinations of labels and features should receive\\ntheir own parameters.  In particular, it is possible to use a single\\nparameter to associate a feature with more than one label; or to\\nassociate more than one feature with a given label.  This will\\nsometimes allow the model to \"generalize\" over some of the\\ndifferences between related labels or features.\\nEach combination of labels and features that receives its own\\nparameter is called a joint-feature.  Note that joint-features\\nare properties of labeled values, whereas (simple) features are\\nproperties of unlabeled values.\\n\\nNote\\nIn literature that describes and discusses Maximum Entropy\\nmodels, the term \"features\" often refers to\\njoint-features; the term \"contexts\" refers to what\\nwe have been calling (simple) features.\\n\\nTypically, the joint-features that are used to construct Maximum\\nEntropy models exactly mirror those that are used by the naive Bayes\\nmodel.  In particular, a joint-feature is defined for each label,\\ncorresponding to w[label], and for each combination of\\n(simple) feature and label, corresponding to w[f,label].\\nGiven the joint-features for a Maximum Entropy model, the score\\nassigned to a label for a given input is simply the product of the\\nparameters associated with the joint-features that apply to that input\\nand label:\\n\\n\\n  (12)P(input, label) = Prodjoint-features(input,label) w[joint-feature]\\n\\n\\n6.2&nbsp;&nbsp;&nbsp;Maximizing Entropy\\nThe intuition that motivates Maximum Entropy classification is that we\\nshould build a model that captures the frequencies of individual\\njoint-features, without making any unwarranted assumptions.\\nAn example will help to illustrate this principle.\\nSuppose we are assigned the task of picking the correct word sense for\\na given word, from a list of ten possible senses (labeled A-J).  At\\nfirst, we are not told anything more about the word or the senses.\\nThere are many probability distributions that we could choose for the\\nten senses, such as:\\nTable 6.1\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nA\\nB\\nC\\nD\\nE\\nF\\nG\\nH\\nI\\nJ\\n\\n\\n\\n(i)\\n10%\\n10%\\n10%\\n10%\\n10%\\n10%\\n10%\\n10%\\n10%\\n10%\\n\\n(ii)\\n5%\\n15%\\n0%\\n30%\\n0%\\n8%\\n12%\\n0%\\n6%\\n24%\\n\\n(iii)\\n0%\\n100%\\n0%\\n0%\\n0%\\n0%\\n0%\\n0%\\n0%\\n0%\\n\\n\\n\\n\\nAlthough any of these distributions might be correct, we are likely\\nto choose distribution (i), because without any more information,\\nthere is no reason to believe that any word sense is more likely than\\nany other.  On the other hand, distributions (ii) and (iii) reflect\\nassumptions that are not supported by what we know.\\nOne way to capture this intuition that distribution (i) is more \"fair\"\\nthan the other two is to invoke the concept of entropy.  In the\\ndiscussion of decision trees, we described entropy as a measure of how\\n\"disorganized\" a set of labels was.  In particular, if a single label\\ndominates then entropy is low, but if the labels are more evenly\\ndistributed then entropy is high.  In our example, we chose\\ndistribution (i) because its label probabilities are evenly\\ndistributed — in other words, because its entropy is high.  In\\ngeneral, the Maximum Entropy principle states that, among the\\ndistributions that are consistent with what we know, we should choose\\nthe distribution whose entropy is highest.\\nNext, suppose that we are told that sense A appears 55% of the time.\\nOnce again, there are many distributions that are consistent with this\\nnew piece of information, such as:\\nTable 6.2\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nA\\nB\\nC\\nD\\nE\\nF\\nG\\nH\\nI\\nJ\\n\\n\\n\\n(iv)\\n55%\\n45%\\n0%\\n0%\\n0%\\n0%\\n0%\\n0%\\n0%\\n0%\\n\\n(v)\\n55%\\n5%\\n5%\\n5%\\n5%\\n5%\\n5%\\n5%\\n5%\\n5%\\n\\n(vi)\\n55%\\n3%\\n1%\\n2%\\n9%\\n5%\\n0%\\n25%\\n0%\\n0%\\n\\n\\n\\n\\nBut again, we will likely choose the distribution that makes the\\nfewest unwarranted assumptions — in this case, distribution (v).\\nFinally, suppose that we are told that the word \"up\" appears in the\\nnearby context 10% of the time, and that when it does appear in the\\ncontext there\\'s an 80% chance that sense A or C will be used.  In\\nthis case, we will have a harder time coming up with an appropriate\\ndistribution by hand; however, we can verify that the following\\ndistribution looks appropriate:\\nTable 6.3\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n&nbsp;\\nA\\nB\\nC\\nD\\nE\\nF\\nG\\nH\\nI\\nJ\\n\\n\\n\\n(vii)\\n+up\\n5.1%\\n0.25%\\n2.9%\\n0.25%\\n0.25%\\n0.25%\\n0.25%\\n0.25%\\n0.25%\\n0.25%\\n\\n` `\\n-up\\n49.9%\\n4.46%\\n4.46%\\n4.46%\\n4.46%\\n4.46%\\n4.46%\\n4.46%\\n4.46%\\n4.46%\\n\\n\\n\\n\\nIn particular, the distribution is consistent with what we know: if we\\nadd up the probabilities in column A, we get 55%; if we add up the\\nprobabilities of row 1, we get 10%; and if we add up the boxes for\\nsenses A and C in the +up row, we get 8% (or 80% of the +up cases).\\nFurthermore, the remaining probabilities appear to be \"evenly\\ndistributed.\"\\nThroughout this example, we have restricted ourselves to distributions\\nthat are consistent with what we know; among these, we chose the\\ndistribution with the highest entropy.  This is exactly what the\\nMaximum Entropy classifier does as well.  In particular, for each\\njoint-feature, the Maximum Entropy model calculates the \"empirical\\nfrequency\" of that feature — i.e., the frequency with which it occurs\\nin the training set.  It then searches for the distribution which\\nmaximizes entropy, while still predicting the correct frequency for\\neach joint-feature.\\n\\n\\n6.3&nbsp;&nbsp;&nbsp;Generative vs Conditional Classifiers\\n\\nAn important difference between the naive Bayes classifier and the\\nMaximum Entropy classifier concerns the type of questions they can be\\nused to answer.  The naive Bayes classifier is an example of a\\ngenerative classifier, which builds a model that predicts\\nP(input, label), the joint probability of a (input,\\nlabel) pair.  As a result, generative models can be used to\\nanswer the following questions:\\n\\nWhat is the most likely label for a given input?\\nHow likely is a given label for a given input?\\nWhat is the most likely input value?\\nHow likely is a given input value?\\nHow likely is a given input value with a given label?\\nWhat is the most likely label for an input that might have one\\nof two values (but we don\\'t know which)?\\n\\n<!-- XXX possibly add expressions like argmax_label P(label|input) to each\\nof the above questions, to make more explicit connections with the\\nearlier discussion -->\\nThe Maximum Entropy classifier, on the other hand, is an example of a\\nconditional classifier.  Conditional classifiers build models\\nthat predict P(label|input) — the probability of a label\\ngiven the input value.  Thus, conditional models can still be used\\nto answer questions 1 and 2.  However, conditional models can not be\\nused to answer the remaining questions 3-6.\\nIn general, generative models are strictly more powerful than\\nconditional models, since we can calculate the conditional probability\\nP(label|input) from the joint probability P(input,\\nlabel), but not vice versa.\\nHowever, this additional power comes at a price.  Because the model is\\nmore powerful, it has more \"free parameters\" which need to be learned.\\nHowever, the size of the training set is fixed.  Thus, when using a\\nmore powerful model, we end up with less data that can be used to\\ntrain each parameter\\'s value, making it harder to find the best\\nparameter values.  As a result, a generative model may not do as good\\na job at answering questions 1 and 2 as a conditional model, since the\\nconditional model can focus its efforts on those two questions.\\nHowever, if we do need answers to questions like 3-6, then we have no\\nchoice but to use a generative model.\\nThe difference between a generative model and a conditional model is\\nanalogous to the difference between a topographical map and a picture of\\na skyline.  Although the topographical map can be used to answer a wider\\nvariety of questions, it is significantly more difficult to generate\\nan accurate topographical map than it is to generate an accurate skyline.\\n<!-- I want a figure here.  But the images I used for this in the past\\n(on powerpoint) are probably copyrighted, so I\\'ll need to draw/find\\nsome new images.  They should be a side-by-side picture of a\\ntopographical map and a skyline.  The left/right axis of each should\\nbe labeled as (output value), the up/down axis of each should be\\nlabeled as (probability), and the forward/back axis of the topo map\\nshould be (input value). -->\\n\\n\\n\\n7&nbsp;&nbsp;&nbsp;Modeling Linguistic Patterns\\nClassifiers can help us to understand the linguistic patterns that\\noccur in natural language, by allowing us to create explicit\\nmodels that capture those patterns.  Typically, these models are\\nusing supervised classification techniques, but it is also possible to\\nbuild analytically motivated models.  Either way, these explicit\\nmodels serve two important purposes: they help us to understand\\nlinguistic patterns, and they can be used to make predictions about\\nnew language data.\\nThe extent to which explicit models can give us insights into\\nlinguistic patterns depends largely on what kind of model is used.\\nSome models, such as decision trees, are relatively transparent, and\\ngive us direct information about which factors are important in making\\ndecisions and about which factors are related to one another.  Other\\nmodels, such as multi-level neural networks, are much more opaque.\\nAlthough it can be possible to gain insight by studying them, it\\ntypically takes a lot more work.\\nBut all explicit models can make predictions about new \"unseen\"\\nlanguage data that was not included in the corpus used to build the\\nmodel.  These predictions can be evaluated to assess the accuracy of\\nthe model.  Once a model is deemed sufficiently accurate, it can then\\nbe used to automatically predict information about new language\\ndata.  These predictive models can be combined into systems that\\nperform many useful language processing tasks, such as document\\nclassification, automatic translation, and question answering.\\n\\n7.1&nbsp;&nbsp;&nbsp;What do models tell us?\\nIt\\'s important to understand what we can learn about language from an\\nautomatically constructed model.  One important consideration when\\ndealing with models of language is the distinction between descriptive\\nmodels and explanatory models.  Descriptive models capture patterns in\\nthe data but they don\\'t provide any information about why the data\\ncontains those patterns.  For example, as we saw in 3.1,\\nthe synonyms absolutely and definitely are not\\ninterchangeable: we say absolutely adore not definitely\\nadore, and definitely prefer not absolutely prefer.\\nIn contrast, explanatory models attempt to capture properties and\\nrelationships that cause the linguistic patterns.  For example, we\\nmight introduce the abstract concept of \"polar verb\", as one that\\nhas an extreme meaning, and categorize some verb like\\nadore and detest as polar.  Our explanatory model would\\ncontain the constraint that absolutely can only combine with\\npolar verbs, and definitely can only combine with non-polar\\nverbs.  In summary, descriptive models provide information about\\ncorrelations in the data, while explanatory models go further to\\npostulate causal relationships.\\nMost models that are automatically constructed from a corpus are\\ndescriptive models; in other words, they can tell us what features are\\nrelevant to a given pattern or construction, but they can\\'t\\nnecessarily tell us how those features and patterns relate to one\\nanother.  If our goal is to understand the linguistic patterns, then\\nwe can use this information about which features are related as a\\nstarting point for further experiments designed to tease apart the\\nrelationships between features and patterns.  On the other hand, if\\nwe\\'re just interested in using the model to make predictions (e.g., as\\npart of a language processing system), then we can use the model to\\nmake predictions about new data without worrying about the details\\nof underlying causal relationships.\\n\\n\\n\\n8&nbsp;&nbsp;&nbsp;Summary\\n\\nModeling the linguistic data found in corpora can help us to\\nunderstand linguistic patterns, and can be used to make predictions\\nabout new language data.\\nSupervised classifiers use labeled training corpora to build models\\nthat predict the label of an input based on specific features of\\nthat input.\\nSupervised classifiers can perform a wide variety of\\nNLP tasks, including document classification, part-of-speech\\ntagging, sentence segmentation, dialogue act type identification,\\nand determining entailment relations, and many other tasks.\\nWhen training a supervised classifier, you should split your corpus\\ninto three datasets: a training set for building the\\nclassifier model; a dev-test set for helping select\\nand tune the model\\'s features; and a test set for\\nevaluating the final model\\'s performance.\\nWhen evaluating a supervised classifier, it is important that you\\nuse fresh data, that was not included in the training or dev-test\\nset.  Otherwise, your evaluation results may be unrealistically\\noptimistic.\\nDecision trees are automatically constructed tree-structured\\nflowcharts that are used to assign labels to input values based on\\ntheir features.  Although they\\'re easy to interpret, they are not\\nvery good at handling cases where feature values interact in\\ndetermining the proper label.\\nIn naive Bayes classifiers, each feature independently contributes\\nto the decision of which label should be used.  This allows feature\\nvalues to interact, but can be problematic when two or more features\\nare highly correlated with one another.\\nMaximum Entropy classifiers use a basic model that is similar to the\\nmodel used by naive Bayes; however, they employ iterative optimization\\nto find the set of feature weights that maximizes the probability of\\nthe training set.\\nMost of the models that are automatically constructed from a corpus\\nare descriptive — they let us know which features are relevant\\nto a given patterns or construction, but they don\\'t give any\\ninformation about causal relationships between those features and\\npatterns.\\n\\n\\n\\n9&nbsp;&nbsp;&nbsp;Further Reading\\nPlease consult http://nltk.org/ for further materials on this chapter and on how to\\ninstall external machine learning packages, such as Weka, Mallet,\\nTADM, and MEGAM.\\nFor more examples of classification and machine learning with NLTK,\\nplease see the classification HOWTOs at http://nltk.org/howto.\\nFor a general introduction to machine learning, we recommend\\n(Alpaydin, 2004).  For a more mathematically intense introduction to\\nthe theory of machine learning, see (Hastie, Tibshirani, &amp; Friedman, 2009).  Excellent books on\\nusing machine learning techniques for NLP include (Abney, 2008),\\n(Daelemans &amp; Bosch, 2005), (Feldman &amp; Sanger, 2007), (Segaran, 2007), (Weiss et al, 2004).  For\\nmore on smoothing techniques for language problems, see\\n(Manning &amp; Schutze, 1999).  For more on sequence modeling, and especially\\nhidden Markov models, see (Manning &amp; Schutze, 1999) or (Jurafsky &amp; Martin, 2008).\\nChapter 13 of (Manning, Raghavan, &amp; Schutze, 2008) discusses the use of naive Bayes for\\nclassifying texts.\\nMany of the machine learning algorithms discussed in this chapter are\\nnumerically intensive, and as a result, they will run slowly when\\ncoded naively in Python.  For information on increasing the efficiency\\nof numerically intensive algorithms in Python, see (Kiusalaas, 2005).\\nThe classification techniques described in this chapter can be applied\\nto a very wide variety of problems.  For example, (Agirre &amp; Edmonds, 2007) uses\\nclassifiers to perform word-sense disambiguation; and (Melamed, 2001)\\nuses classifiers to create parallel texts.  Recent textbooks that\\ncover text classification include (Manning, Raghavan, &amp; Schutze, 2008) and (Croft, Metzler, &amp; Strohman, 2009).\\nMuch of the current research in the application of machine learning\\ntechniques to NLP problems is driven by government-sponsored\\n\"challenges,\" where a set of research organizations are all provided\\nwith the same development corpus, and asked to build a system; and the\\nresulting systems are compared based on a reserved test set.  Examples\\nof these challenge competitions include CoNLL Shared Tasks, the ACE\\ncompetitions, the Recognizing Textual Entailment competitions,\\nand the AQUAINT competitions.  Consult http://nltk.org/ for a\\nlist of pointers to the webpages for these challenges.\\n<!-- - supported ML packages\\n- http://www.cs.waikato.ac.nz/~ml/weka/book.html\\n- choosing prepositions http://itre.cis.upenn.edu/~myl/languagelog/archives/002003.html\\n- Jurafsky and Martin\\n- publications describing some of the corpora that were created for the\\n  purpose of training and testing classifiers\\n\\nXin Li, Dan Roth, Learning Question Classifiers: The Role of Semantic Information\\nhttp://l2r.cs.uiuc.edu/~danr/Papers/LiRo05a.pdf\\n\\nCurrent challenge: The problem of applying models trained on one genre\\nto another (portability across domains; domain adaptation);\\nUse of unsupervised or weakly supervised approaches to exploit a small\\namount of in-domain training data, or of unlabeled in-domain training data\\n[REFS] -->\\n\\n\\n10&nbsp;&nbsp;&nbsp;Exercises\\n\\n☼ Read up on one of the language technologies mentioned in this section, such as\\nword sense disambiguation, semantic role labeling, question answering, machine translation,\\nnamed entity detection.\\nFind out what type and quantity of annotated data is required for developing such systems.\\nWhy do you think a large amount of data is required?\\n\\n☼ Using any of the three classifiers described in this\\nchapter, and any features you can think of, build the best name\\ngender classifier you can.  Begin by splitting the Names Corpus\\ninto three subsets: 500 words for the test set, 500 words for the\\ndev-test set, and the remaining 6900 words for the training set.\\nThen, starting with the example name gender classifier, make\\nincremental improvements.  Use the dev-test set to check your\\nprogress.  Once you are satisfied with your classifier, check its\\nfinal performance on the test set.  How does the performance on\\nthe test set compare to the performance on the dev-test set?\\nIs this what you\\'d expect?\\n\\n☼ The Senseval 2 Corpus contains data intended to train\\nword-sense disambiguation classifiers.  It contains data for\\nfour words: hard, interest, line, and serve.  Choose one of these\\nfour words, and load the corresponding data:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; from nltk.corpus import senseval\\n&gt;&gt;&gt; instances = senseval.instances(\\'hard.pos\\')\\n&gt;&gt;&gt; size = int(len(instances) * 0.1)\\n&gt;&gt;&gt; train_set, test_set = instances[size:], instances[:size]\\n\\n\\n\\nUsing this dataset, build a classifier that predicts the correct\\nsense tag for a given instance.  See the corpus HOWTO at\\nhttp://nltk.org/howto for information on using the instance objects\\nreturned by the Senseval 2 Corpus.\\n\\n☼\\nUsing the movie review document classifier discussed in this\\nchapter, generate a list of the 30 features that the classifier\\nfinds to be most informative.  Can you explain why these particular\\nfeatures are informative?  Do you find any of them surprising?\\n\\n☼\\nSelect one of the classification tasks described in this chapter,\\nsuch as name gender detection, document classification,\\npart-of-speech tagging, or dialog act classification.  Using the\\nsame training and test data, and the same feature extractor,\\nbuild three classifiers for the task: a decision tree, a naive\\nBayes classifier, and a Maximum Entropy classifier.  Compare\\nthe performance of the three classifiers on your selected task.\\nHow do you think that your results might be different if you used\\na different feature extractor?\\n\\n☼\\nThe synonyms strong and powerful pattern\\ndifferently (try combining them with chip and sales).\\nWhat features are relevant in this distinction?\\nBuild a classifier that predicts when each word should be used.\\n\\n◑\\nThe dialog act classifier assigns labels to individual posts,\\nwithout considering the context in which the post is found.\\nHowever, dialog acts are highly dependent on context, and some\\nsequences of dialog act are much more likely than others.  For\\nexample, a ynQuestion dialog act is much more likely to be\\nanswered by a yanswer than by a greeting.  Make use of this\\nfact to build a consecutive classifier for labeling dialog acts.\\nBe sure to consider what features might be useful.  See the code\\nfor the consecutive classifier for part-of-speech tags in\\n1.7 to get some ideas.\\n\\n◑\\nWord features can be very useful for performing document\\nclassification, since the words that appear in a document give a\\nstrong indication about what its semantic content is.  However,\\nmany words occur very infrequently, and some of the most\\ninformative words in a document may never have occurred in our\\ntraining data.  One solution is to make use of a lexicon,\\nwhich describes how different words relate to one another.  Using\\nWordNet lexicon, augment the movie review document classifier\\npresented in this chapter to use features that generalize the words\\nthat appear in a document, making it more likely that they will\\nmatch words found in the training data.\\n\\n★\\nThe PP Attachment Corpus is a corpus describing\\nprepositional phrase attachment decisions.  Each instance in the\\ncorpus is encoded as a PPAttachment object:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; from nltk.corpus import ppattach\\n&gt;&gt;&gt; ppattach.attachments(\\'training\\')\\n[PPAttachment(sent=\\'0\\', verb=\\'join\\', noun1=\\'board\\',\\n              prep=\\'as\\', noun2=\\'director\\', attachment=\\'V\\'),\\n PPAttachment(sent=\\'1\\', verb=\\'is\\', noun1=\\'chairman\\',\\n              prep=\\'of\\', noun2=\\'N.V.\\', attachment=\\'N\\'),\\n ...]\\n&gt;&gt;&gt; inst = ppattach.attachments(\\'training\\')[1]\\n&gt;&gt;&gt; (inst.noun1, inst.prep, inst.noun2)\\n(\\'chairman\\', \\'of\\', \\'N.V.\\')\\n\\n\\n\\nSelect only the instances where inst.attachment is N:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; nattach = [inst for inst in ppattach.attachments(\\'training\\')\\n...            if inst.attachment == \\'N\\']\\n\\n\\n\\nUsing this sub-corpus, build a classifier that attempts to predict\\nwhich preposition is used to connect a given pair of nouns.  For\\nexample, given the pair of nouns \"team\" and \"researchers,\" the\\nclassifier should predict the preposition \"of\".  See the corpus\\nHOWTO at http://nltk.org/howto for more information on using the PP\\nattachment corpus.\\n\\n★ Suppose you wanted to automatically generate a prose description of a scene,\\nand already had a word to uniquely describe each entity, such as the jar,\\nand simply wanted to decide whether to use in or on in relating\\nvarious items, e.g. the book is in the cupboard vs the book is on the shelf.\\nExplore this issue by looking at corpus data; writing programs as needed.\\n\\n\\n\\n  (13)\\n  a.in the car versus on the train\\n\\n  b.in town versus on campus\\n\\n  c.in the picture versus on the screen\\n\\n  d.in Macbeth versus on Letterman\\n\\n\\n\\nAbout this document...\\nUPDATED FOR NLTK 3.0.\\nThis is a chapter from Natural Language Processing with Python,\\nby Steven Bird, Ewan Klein and Edward Loper,\\nCopyright © 2019 the authors.\\nIt is distributed with the Natural Language Toolkit [http://nltk.org/],\\nVersion 3.0, under the terms of the\\nCreative Commons Attribution-Noncommercial-No Derivative Works 3.0 United States License\\n[http://creativecommons.org/licenses/by-nc-nd/3.0/us/].\\nThis document was built on\\nWed  4 Sep 2019 11:40:48 ACST\\n\\n\\n\\nDocutils System Messages\\n\\nSystem Message: ERROR/3 (ch06.rst2, line 1264); backlink\\nUndefined substitution referenced: \"ii\".\\n\\n\\n\\n\\n',\n",
       " '\\n\\n\\n\\nfunction astext(node)\\n{\\n    return node.innerHTML.replace(/(]+)>)/ig,\"\")\\n                         .replace(/&gt;/ig, \">\")\\n                         .replace(/&lt;/ig, \"<\")\\n                         .replace(/&quot;/ig, \\'\"\\')\\n                         .replace(/&amp;/ig, \"&\");\\n}\\n\\nfunction copy_notify(node, bar_color, data)\\n{\\n    // The outer box: relative + inline positioning.\\n    var box1 = document.createElement(\"div\");\\n    box1.style.position = \"relative\";\\n    box1.style.display = \"inline\";\\n    box1.style.top = \"2em\";\\n    box1.style.left = \"1em\";\\n  \\n    // A shadow for fun\\n    var shadow = document.createElement(\"div\");\\n    shadow.style.position = \"absolute\";\\n    shadow.style.left = \"-1.3em\";\\n    shadow.style.top = \"-1.3em\";\\n    shadow.style.background = \"#404040\";\\n    \\n    // The inner box: absolute positioning.\\n    var box2 = document.createElement(\"div\");\\n    box2.style.position = \"relative\";\\n    box2.style.border = \"1px solid #a0a0a0\";\\n    box2.style.left = \"-.2em\";\\n    box2.style.top = \"-.2em\";\\n    box2.style.background = \"white\";\\n    box2.style.padding = \".3em .4em .3em .4em\";\\n    box2.style.fontStyle = \"normal\";\\n    box2.style.background = \"#f0e0e0\";\\n\\n    node.insertBefore(box1, node.childNodes.item(0));\\n    box1.appendChild(shadow);\\n    shadow.appendChild(box2);\\n    box2.innerHTML=\"Copied&nbsp;to&nbsp;the&nbsp;clipboard: \" +\\n                   \"\"+\\n                   data+\"\";\\n    setTimeout(function() { node.removeChild(box1); }, 1000);\\n\\n    var elt = node.parentNode.firstChild;\\n    elt.style.background = \"#ffc0c0\";\\n    setTimeout(function() { elt.style.background = bar_color; }, 200);\\n}\\n\\nfunction copy_codeblock_to_clipboard(node)\\n{\\n    var data = astext(node)+\"\\\\n\";\\n    if (copy_text_to_clipboard(data)) {\\n        copy_notify(node, \"#40a060\", data);\\n    }\\n}\\n\\nfunction copy_doctest_to_clipboard(node)\\n{\\n    var s = astext(node)+\"\\\\n   \";\\n    var data = \"\";\\n\\n    var start = 0;\\n    var end = s.indexOf(\"\\\\n\");\\n    while (end >= 0) {\\n        if (s.substring(start, start+4) == \">>> \") {\\n            data += s.substring(start+4, end+1);\\n        }\\n        else if (s.substring(start, start+4) == \"... \") {\\n            data += s.substring(start+4, end+1);\\n        }\\n        /*\\n        else if (end-start > 1) {\\n            data += \"# \" + s.substring(start, end+1);\\n        }*/\\n        // Grab the next line.\\n        start = end+1;\\n        end = s.indexOf(\"\\\\n\", start);\\n    }\\n    \\n    if (copy_text_to_clipboard(data)) {\\n        copy_notify(node, \"#4060a0\", data);\\n    }\\n}\\n    \\nfunction copy_text_to_clipboard(data)\\n{\\n    if (window.clipboardData) {\\n        window.clipboardData.setData(\"Text\", data);\\n        return true;\\n     }\\n    else if (window.netscape) {\\n        // w/ default firefox settings, permission will be denied for this:\\n        netscape.security.PrivilegeManager\\n                      .enablePrivilege(\"UniversalXPConnect\");\\n    \\n        var clip = Components.classes[\"@mozilla.org/widget/clipboard;1\"]\\n                      .createInstance(Components.interfaces.nsIClipboard);\\n        if (!clip) return;\\n    \\n        var trans = Components.classes[\"@mozilla.org/widget/transferable;1\"]\\n                       .createInstance(Components.interfaces.nsITransferable);\\n        if (!trans) return;\\n    \\n        trans.addDataFlavor(\"text/unicode\");\\n    \\n        var str = new Object();\\n        var len = new Object();\\n    \\n        var str = Components.classes[\"@mozilla.org/supports-string;1\"]\\n                     .createInstance(Components.interfaces.nsISupportsString);\\n        var datacopy=data;\\n        str.data=datacopy;\\n        trans.setTransferData(\"text/unicode\",str,datacopy.length*2);\\n        var clipid=Components.interfaces.nsIClipboard;\\n    \\n        if (!clip) return false;\\n    \\n        clip.setData(trans,null,clipid.kGlobalClipboard);\\n        return true;\\n    }\\n    return false;\\n}\\n//-->\\n\\n\\n\\n\\n\\n\\n7. Extracting Information from Text\\n\\n\\n/*\\n:Author: Edward Loper, James Curran\\n:Copyright: This stylesheet has been placed in the public domain.\\n\\nStylesheet for use with Docutils.\\n\\nThis stylesheet defines new css classes used by NLTK.\\n\\nIt uses a Python syntax highlighting scheme that matches\\nthe colour scheme used by IDLE, which makes it easier for\\nbeginners to check they are typing things in correctly.\\n*/\\n\\n/* Include the standard docutils stylesheet. */\\n@import url(default.css);\\n\\n/* Custom inline roles */\\nspan.placeholder    { font-style: italic; font-family: monospace; }\\nspan.example        { font-style: italic; }\\nspan.emphasis       { font-style: italic; }\\nspan.termdef        { font-weight: bold; }\\n/*span.term           { font-style: italic; }*/\\nspan.category       { font-variant: small-caps; }\\nspan.feature        { font-variant: small-caps; }\\nspan.fval           { font-style: italic; }\\nspan.math           { font-style: italic; }\\nspan.mathit         { font-style: italic; }\\nspan.lex            { font-variant: small-caps; }\\nspan.guide-linecount{ text-align: right; display: block;}\\n\\n/* Python souce code listings */\\nspan.pysrc-prompt   { color: #9b0000; }\\nspan.pysrc-more     { color: #9b00ff; }\\nspan.pysrc-keyword  { color: #e06000; }\\nspan.pysrc-builtin  { color: #940094; }\\nspan.pysrc-string   { color: #00aa00; }\\nspan.pysrc-comment  { color: #ff0000; }\\nspan.pysrc-output   { color: #0000ff; }\\nspan.pysrc-except   { color: #ff0000; }\\nspan.pysrc-defname  { color: #008080; }\\n\\n\\n/* Doctest blocks */\\npre.doctest         { margin: 0; padding: 0; font-weight: bold; }\\ndiv.doctest         { margin: 0 1em 1em 1em; padding: 0; }\\ntable.doctest       { margin: 0; padding: 0;\\n                      border-top: 1px solid gray;\\n                      border-bottom: 1px solid gray; }\\npre.copy-notify     { margin: 0; padding: 0.2em; font-weight: bold;\\n                      background-color: #ffffff; }\\n\\n/* Python source listings */\\ndiv.pylisting       { margin: 0 1em 1em 1em; padding: 0; }\\ntable.pylisting     { margin: 0; padding: 0;\\n                      border-top: 1px solid gray; }\\ntd.caption { border-top: 1px solid black; margin: 0; padding: 0; }\\n.caption-label { font-weight: bold;  }\\ntd.caption p { margin: 0; padding: 0; font-style: normal;}\\n\\ntable tr td.codeblock { \\n  padding: 0.2em ! important; margin: 0;\\n  border-left: 1px solid gray;\\n  border-right: 2px solid gray;\\n  border-top: 0px solid gray;\\n  border-bottom: 1px solid gray;\\n  font-weight: bold; background-color: #eeffee;\\n}\\ntable pre span {\\n  white-space: pre-wrap;\\n}\\ntable tr td.doctest  { \\n  padding: 0.2em; margin: 0;\\n  border-left: 1px solid gray;\\n  border-right: 2px solid gray;\\n  border-top: 0px solid gray;\\n  border-bottom: 1px solid gray;\\n  font-weight: bold; background-color: #eeeeff;\\n}\\n\\ntd.codeblock table tr td.copybar {\\n    background: #40a060; border: 1px solid gray;\\n    font-family: monospace; padding: 0; margin: 0; }\\ntd.doctest table tr td.copybar {\\n    background: #4060a0; border: 1px solid gray;\\n    font-family: monospace; padding: 0; margin: 0; }\\n\\ntd.pysrc { padding-left: 0.5em; }\\n\\nimg.callout { border-width: 0px; }\\n\\ntable.docutils {\\n    border-style: solid;\\n    border-width: 1px;\\n    margin-top: 6px;\\n    border-color: grey;\\n    border-collapse: collapse; }\\n\\ntable.docutils th {\\n    border-style: none;\\n    border-width: 1px;\\n    border-color: grey;\\n    padding: 0 .5em 0 .5em; }\\n\\ntable.docutils td {\\n    border-style: none;\\n    border-width: 1px;\\n    border-color: grey; \\n    padding: 0 .5em 0 .5em; }\\n\\ntable.footnote td { padding: 0; }\\ntable.footnote { border-width: 0; }\\ntable.footnote td { border-width: 0; }\\ntable.footnote th { border-width: 0; }\\n\\ntable.noborder { border-width: 0; }\\n\\ntable.example pre { margin-top: 4px; margin-bottom: 0; }\\n\\n/* For figures & tables */\\np.caption { margin-bottom: 0; }\\ndiv.figure { text-align: center; }\\n\\n/* The index */\\ndiv.index { border: 1px solid black;\\n            background-color: #eeeeee; }\\ndiv.index h1 { padding-left: 0.5em; margin-top: 0.5ex;\\n               border-bottom: 1px solid black; }\\nul.index { margin-left: 0.5em; padding-left: 0; }\\nli.index { list-style-type: none; }\\np.index-heading { font-size: 120%; font-style: italic; margin: 0; }\\nli.index ul { margin-left: 2em; padding-left: 0; }\\n\\n/* \\'Note\\' callouts */\\ndiv.note\\n{\\n  border-right:   #87ceeb 1px solid;\\n  padding-right: 4px;\\n  border-top: #87ceeb 1px solid;\\n  padding-left: 4px;\\n  padding-bottom: 4px;\\n  margin: 2px 5% 10px;\\n  border-left: #87ceeb 1px solid;\\n  padding-top: 4px;\\n  border-bottom: #87ceeb 1px solid;\\n  font-style: normal;\\n  font-family: verdana, arial;\\n  background-color: #b0c4de;\\n}\\n\\ntable.avm { border: 0px solid black; width: 0; }\\ntable.avm tbody tr {border: 0px solid black; }\\ntable.avm tbody tr td { padding: 2px; }\\ntable.avm tbody tr td.avm-key { padding: 5px; font-variant: small-caps; }\\ntable.avm tbody tr td.avm-eq { padding: 5px; }\\ntable.avm tbody tr td.avm-val { padding: 5px; font-style: italic; }\\np.avm-empty { font-style: normal; }\\ntable.avm colgroup col { border: 0px solid black; }\\ntable.avm tbody tr td.avm-topleft \\n    { border-left: 2px solid #000080; border-top: 2px solid #000080; }\\ntable.avm tbody tr td.avm-botleft \\n    { border-left: 2px solid #000080; border-bottom: 2px solid #000080; }\\ntable.avm tbody tr td.avm-topright\\n    { border-right: 2px solid #000080; border-top: 2px solid #000080; }\\ntable.avm tbody tr td.avm-botright\\n    { border-right: 2px solid #000080; border-bottom: 2px solid #000080; }\\ntable.avm tbody tr td.avm-left\\n    { border-left: 2px solid #000080; }\\ntable.avm tbody tr td.avm-right\\n    { border-right: 2px solid #000080; }\\ntable.avm tbody tr td.avm-topbotleft\\n    { border: 2px solid #000080; border-right: 0px solid black; }\\ntable.avm tbody tr td.avm-topbotright\\n    { border: 2px solid #000080; border-left: 0px solid black; }\\ntable.avm tbody tr td.avm-ident\\n    { font-size: 80%; padding: 0; padding-left: 2px; vertical-align: top; }\\n.avm-pointer\\n{ border: 1px solid #008000; padding: 1px; color: #008000; \\n  background: #c0ffc0; font-style: normal; }\\n\\ntable.gloss { border: 0px solid black; width: 0; }\\ntable.gloss tbody tr { border: 0px solid black; }\\ntable.gloss tbody tr td { border: 0px solid black; }\\ntable.gloss colgroup col { border: 0px solid black; }\\ntable.gloss p { margin: 0; padding: 0; }\\n\\ntable.rst-example { border: 1px solid black; }\\ntable.rst-example tbody tr td { background: #eeeeee; }\\ntable.rst-example thead tr th { background: #c0ffff; }\\ntd.rst-raw { width: 0; }\\n\\n/* Used by nltk.org/doc/test: */\\ndiv.doctest-list { text-align: center; }\\ntable.doctest-list { border: 1px solid black;\\n  margin-left: auto; margin-right: auto;\\n}\\ntable.doctest-list tbody tr td { background: #eeeeee;\\n  border: 1px solid #cccccc; text-align: left; }\\ntable.doctest-list thead tr th { background: #304050; color: #ffffff;\\n  border: 1px solid #000000;}\\ntable.doctest-list thead tr a { color: #ffffff; }\\nspan.doctest-passed { color: #008000; }\\nspan.doctest-failed { color: #800000; }\\n\\n\\n\\n\\n\\n\\n7. Extracting Information from Text\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n<!-- Grammatical Category - e.g. NP and verb as technical terms\\n.. role:: gc\\n   :class: category -->\\n\\n\\n\\n\\n\\n\\n\\n\\n<!-- standard global imports\\n\\n>>> from __future__ import division\\n>>> import nltk, re, pprint -->\\n\\n<!-- XXX mention somewhere that for IE precision is often more important\\nthan recall? -->\\nFor any given question, it\\'s likely that someone has written the\\nanswer down somewhere.  The amount of natural language text that is\\navailable in electronic form is truly staggering, and is increasing\\nevery day.  However, the complexity of natural language can make it\\nvery difficult to access the information in that text.\\nThe state of the art in NLP is still a long way from being\\nable to build general-purpose representations of meaning from unrestricted text.\\nIf we instead focus our efforts on a limited set of questions or\\n\"entity relations,\" such as \"where are different facilities located,\"\\nor \"who is employed by what company,\" we can make significant progress.\\nThe goal of this chapter is to answer the following questions:\\n\\nHow can we build a system that extracts structured data, such as\\ntables, from unstructured text?\\nWhat are some robust methods for identifying the entities and\\nrelationships described in a text?\\nWhich corpora are appropriate for this work, and how do we use\\nthem for training and evaluating our models?\\n\\nAlong the way, we\\'ll apply techniques from the last two chapters to\\nthe problems of chunking and named-entity recognition.\\n\\n1&nbsp;&nbsp;&nbsp;Information Extraction\\nInformation comes in many shapes and sizes. One important form is\\nstructured data, where there is a regular and predictable\\norganization of entities and relationships. For example,\\nwe might be interested in the\\nrelation between companies and locations. Given a particular company,\\nwe would like to be able to identify the locations where it does\\nbusiness; conversely, given a location, we would like to discover\\nwhich companies do business in that location. If our data is in tabular\\nform, such as the example in 1.1, then\\nanswering these queries is straightforward.\\nTable 1.1: Locations data\\n\\n\\n\\n\\n\\nOrgName\\nLocationName\\n\\n\\n\\nOmnicom\\nNew York\\n\\nDDB Needham\\nNew York\\n\\nKaplan Thaler Group\\nNew York\\n\\nBBDO South\\nAtlanta\\n\\nGeorgia-Pacific\\nAtlanta\\n\\n\\n\\n\\n\\nIf this location data was stored in Python as a list of tuples\\n(entity, relation, entity), then the question\\n\"Which organizations operate in Atlanta?\" could be\\ntranslated as follows:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; locs = [(\\'Omnicom\\', \\'IN\\', \\'New York\\'),\\n...         (\\'DDB Needham\\', \\'IN\\', \\'New York\\'),\\n...         (\\'Kaplan Thaler Group\\', \\'IN\\', \\'New York\\'),\\n...         (\\'BBDO South\\', \\'IN\\', \\'Atlanta\\'),\\n...         (\\'Georgia-Pacific\\', \\'IN\\', \\'Atlanta\\')]\\n&gt;&gt;&gt; query = [e1 for (e1, rel, e2) in locs if e2==\\'Atlanta\\']\\n&gt;&gt;&gt; print(query)\\n[\\'BBDO South\\', \\'Georgia-Pacific\\']\\n\\n\\n\\nTable 1.2: Companies that operate in Atlanta\\n\\n\\n\\n\\nOrgName\\n\\n\\n\\nBBDO South\\n\\nGeorgia-Pacific\\n\\n\\n\\n\\n\\nThings are more tricky if we try to get similar information out of\\ntext. For example, consider the\\nfollowing snippet (from nltk.corpus.ieer, for fileid NYT19980315.0085).\\n\\n  (1)The fourth Wells account moving to another agency is the packaged\\npaper-products division of Georgia-Pacific Corp., which arrived at\\nWells only last fall. Like Hertz and the History Channel, it is\\nalso leaving for an Omnicom-owned agency, the BBDO South unit of\\nBBDO Worldwide.  BBDO South in Atlanta, which handles corporate\\nadvertising for Georgia-Pacific, will assume additional duties for\\nbrands like Angel Soft toilet tissue and Sparkle paper towels,\\nsaid Ken Haldin, a spokesman for Georgia-Pacific in Atlanta.\\nIf you read through (1), you will glean the information required\\nto answer the example question.\\nBut how do we get a machine to understand enough\\nabout (1) to return the answers in 1.2?  This is\\nobviously a much harder task. Unlike 1.1, (1)\\ncontains no structure that links organization names with\\nlocation names.\\nOne approach to this problem involves building a very general\\nrepresentation of meaning (10.).\\nIn this chapter we take a different approach,\\ndeciding in advance that we will only look for very specific kinds of\\ninformation in text, such as the relation between organizations and\\nlocations.  Rather than trying to\\nuse text like (1) to answer the question directly,\\nwe first convert the unstructured\\ndata of natural language sentences into the structured data of\\n1.1. Then we reap the benefits of powerful query\\ntools such as SQL. This method of getting meaning from text is\\ncalled Information Extraction.\\nInformation Extraction has many applications, including\\nbusiness intelligence, resume harvesting, media analysis, sentiment detection,\\npatent search, and email scanning. A\\nparticularly important area of current research involves the attempt\\nto extract structured data out of electronically-available scientific\\nliterature, especially in the domain of biology and medicine.\\n\\n1.1&nbsp;&nbsp;&nbsp;Information Extraction Architecture\\n1.1 shows the architecture for a simple information\\nextraction system.  It begins by processing a document using several\\nof the procedures discussed in 3 and 5.: first,\\nthe raw text of the document is split into sentences using a sentence\\nsegmenter, and each sentence is further subdivided into words using a\\ntokenizer.  Next, each sentence is tagged with part-of-speech tags,\\nwhich will prove very helpful in the next step, named entity\\ndetection.  In this step, we search for mentions of potentially\\ninteresting entities in each sentence.  Finally, we use relation\\ndetection to search for likely relations between different\\nentities in the text.\\n\\n\\nFigure 1.1: Simple Pipeline Architecture for an Information Extraction System.\\nThis system takes the raw text of a document as its input, and\\ngenerates a list of (entity, relation, entity) tuples as its\\noutput.  For example, given a document that indicates that the\\ncompany Georgia-Pacific is located in Atlanta, it might generate\\nthe tuple ([ORG: \\'Georgia-Pacific\\'] \\'in\\' [LOC: \\'Atlanta\\']).\\n\\nTo perform the first three tasks, we can define a simple function that\\nsimply connects together NLTK\\'s default sentence segmenter\\n, word tokenizer , and part-of-speech tagger\\n:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; def ie_preprocess(document):\\n...    sentences = nltk.sent_tokenize(document) \\n...    sentences = [nltk.word_tokenize(sent) for sent in sentences] \\n...    sentences = [nltk.pos_tag(sent) for sent in sentences] \\n\\n\\n\\n\\nNote\\nRemember that our program samples assume you\\nbegin your interactive session or your program with: import nltk, re, pprint\\n\\nNext, in named entity detection, we segment and label the\\nentities that might participate in interesting relations with one\\nanother.  Typically, these will be definite noun phrases such as the\\nknights who say \"ni\", or proper names such as Monty Python.\\nIn some tasks it is useful to also consider indefinite nouns or noun\\nchunks, such as every student or cats,\\nand these do not necessarily refer to\\nentities in the same way as definite NPs and proper names.\\nFinally, in relation extraction, we search for specific patterns\\nbetween pairs of entities that occur near one another in the text, and\\nuse those patterns to build tuples recording the relationships\\nbetween the entities.\\n\\n\\n\\n2&nbsp;&nbsp;&nbsp;Chunking\\nThe basic technique we will use for entity detection is\\nchunking, which segments and labels multi-token sequences as\\nillustrated in 2.1.  The smaller boxes show the\\nword-level tokenization and part-of-speech tagging, while the large\\nboxes show higher-level chunking.  Each of these larger boxes is\\ncalled a chunk.\\nLike tokenization, which omits whitespace,\\nchunking usually selects a subset of the tokens.\\nAlso like tokenization, the pieces produced by a chunker do not overlap\\nin the source text.\\n\\n\\nFigure 2.1: Segmentation and Labeling at both the Token and Chunk Levels\\n\\nIn this section, we will explore chunking in some depth, beginning\\nwith the definition and representation of chunks.  We will see regular\\nexpression and n-gram approaches to chunking, and will develop and\\nevaluate chunkers using the CoNLL-2000 chunking corpus. We will then return in\\n(5) and 6\\nto the tasks of named entity recognition and relation extraction.\\n\\n2.1&nbsp;&nbsp;&nbsp;Noun Phrase Chunking\\nWe will begin by considering the task of noun phrase chunking,\\nor NP-chunking, where we search for chunks corresponding to\\nindividual noun phrases.  For example, here is some Wall Street\\nJournal text with NP-chunks marked using brackets:\\n\\n  (2)[ The/DT market/NN ] for/IN [ system-management/NN software/NN ]\\nfor/IN [ Digital/NNP ] [ \\'s/POS hardware/NN ] is/VBZ fragmented/JJ\\nenough/RB that/IN [ a/DT giant/NN ] such/JJ as/IN [ Computer/NNP\\nAssociates/NNPS ] should/MD do/VB well/RB there/RB ./.\\nAs we can see, NP-chunks are often smaller pieces than complete\\nnoun phrases.  For example, the market for system-management software\\nfor Digital\\'s hardware is a single noun phrase (containing two\\nnested noun phrases), but it is captured in NP-chunks by the\\nsimpler chunk the market.  One of the motivations for this\\ndifference is that NP-chunks are defined so as not to contain\\nother NP-chunks.  Consequently, any\\nprepositional phrases or subordinate clauses that modify a nominal\\nwill not be included in the corresponding NP-chunk, since they\\nalmost certainly contain further noun phrases.\\nOne of the most useful sources of information for NP-chunking is\\npart-of-speech tags.  This is one of the motivations for\\nperforming part-of-speech tagging in our information extraction\\nsystem.  We demonstrate this approach using an example sentence that\\nhas been part-of-speech tagged in 2.2.  In order to create an\\nNP-chunker, we will first define a chunk grammar, consisting of rules\\nthat indicate how sentences should be chunked.  In this case, we will\\ndefine a simple grammar with a single regular-expression rule\\n.  This rule says that an NP chunk should be formed\\nwhenever the chunker finds an optional determiner (DT) followed by any\\nnumber of adjectives (JJ) and then a noun (NN).  Using this grammar,\\nwe create a chunk parser , and test it on our example\\nsentence .  The result is a tree, which we can either\\nprint , or display graphically .\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; sentence = [(\"the\", \"DT\"), (\"little\", \"JJ\"), (\"yellow\", \"JJ\"), \\n... (\"dog\", \"NN\"), (\"barked\", \"VBD\"), (\"at\", \"IN\"),  (\"the\", \"DT\"), (\"cat\", \"NN\")]\\n\\n&gt;&gt;&gt; grammar = \"NP: {&lt;DT&gt;?&lt;JJ&gt;*&lt;NN&gt;}\" \\n\\n&gt;&gt;&gt; cp = nltk.RegexpParser(grammar) \\n&gt;&gt;&gt; result = cp.parse(sentence) \\n&gt;&gt;&gt; print(result) \\n(S\\n  (NP the/DT little/JJ yellow/JJ dog/NN)\\n  barked/VBD\\n  at/IN\\n  (NP the/DT cat/NN))\\n&gt;&gt;&gt; result.draw() \\n\\n\\nExample 2.2 (code_chunkex.py): Figure 2.2: Example of a Simple Regular Expression Based NP Chunker.\\n\\n\\n\\n\\n2.2&nbsp;&nbsp;&nbsp;Tag Patterns\\nThe rules that make up a chunk grammar use tag patterns to\\ndescribe sequences of tagged words.\\nA tag pattern is a sequence of part-of-speech tags delimited\\nusing angle brackets, e.g. &lt;DT&gt;?&lt;JJ&gt;*&lt;NN&gt;.  Tag patterns are\\nsimilar to regular expression patterns (3.4).\\nNow, consider the following noun phrases from the Wall Street Journal:\\nanother/DT sharp/JJ dive/NN\\ntrade/NN figures/NNS\\nany/DT new/JJ policy/NN measures/NNS\\nearlier/JJR stages/NNS\\nPanamanian/JJ dictator/NN Manuel/NNP Noriega/NNP\\n\\nWe can match these noun phrases using a slight refinement of the first tag pattern\\nabove, i.e. &lt;DT&gt;?&lt;JJ.*&gt;*&lt;NN.*&gt;+.  This will chunk any sequence\\nof tokens beginning with an optional determiner, followed by\\nzero or more adjectives of any type (including relative\\nadjectives like earlier/JJR), followed by one or more nouns of any\\ntype.  However, it is easy to find many more complicated examples which\\nthis rule will not cover:\\nhis/PRP$ Mansion/NNP House/NNP speech/NN\\nthe/DT price/NN cutting/VBG\\n3/CD %/NN to/TO 4/CD %/NN\\nmore/JJR than/IN 10/CD %/NN\\nthe/DT fastest/JJS developing/VBG trends/NNS\\n\\'s/POS skill/NN\\n\\n\\nNote\\nYour Turn:\\nTry to come up with tag patterns to cover these cases.\\nTest them using the graphical interface\\nnltk.app.chunkparser().  Continue to refine your\\ntag patterns with the help of the feedback given by this tool.\\n\\n\\n\\n2.3&nbsp;&nbsp;&nbsp;Chunking with Regular Expressions\\nTo find the chunk structure for a given sentence, the RegexpParser\\nchunker begins with a flat structure in which no tokens are\\nchunked.  The chunking rules are applied in turn,\\nsuccessively updating the\\nchunk structure.  Once all of the rules have been invoked, the\\nresulting chunk structure is returned.\\n2.3 shows a\\nsimple chunk grammar consisting of two rules.  The first rule\\nmatches an optional determiner or possessive pronoun,\\nzero or more adjectives, then a noun.\\nThe second rule matches one or more proper nouns.\\nWe also define an example sentence to be chunked ,\\nand run the chunker on this input .\\n\\n\\n\\n\\n&nbsp;\\ngrammar = r\"\"\"\\n  NP: {&lt;DT|PP\\\\$&gt;?&lt;JJ&gt;*&lt;NN&gt;}   # chunk determiner/possessive, adjectives and noun\\n      {&lt;NNP&gt;+}                # chunk sequences of proper nouns\\n\"\"\"\\ncp = nltk.RegexpParser(grammar)\\nsentence = [(\"Rapunzel\", \"NNP\"), (\"let\", \"VBD\"), (\"down\", \"RP\"), \\n                 (\"her\", \"PP$\"), (\"long\", \"JJ\"), (\"golden\", \"JJ\"), (\"hair\", \"NN\")]\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; print(cp.parse(sentence)) \\n(S\\n  (NP Rapunzel/NNP)\\n  let/VBD\\n  down/RP\\n  (NP her/PP$ long/JJ golden/JJ hair/NN))\\n\\n\\nExample 2.3 (code_chunker1.py): Figure 2.3: Simple Noun Phrase Chunker\\n\\n\\nNote\\nThe $ symbol is a special character in regular\\nexpressions, and must be backslash escaped\\nin order to match the tag PP$.\\n\\nIf a tag pattern matches at overlapping locations, the leftmost\\nmatch takes precedence.  For example, if we apply a rule that matches\\ntwo consecutive nouns to a text containing three consecutive nouns,\\nthen only the first two nouns will be chunked:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; nouns = [(\"money\", \"NN\"), (\"market\", \"NN\"), (\"fund\", \"NN\")]\\n&gt;&gt;&gt; grammar = \"NP: {&lt;NN&gt;&lt;NN&gt;}  # Chunk two consecutive nouns\"\\n&gt;&gt;&gt; cp = nltk.RegexpParser(grammar)\\n&gt;&gt;&gt; print(cp.parse(nouns))\\n(S (NP money/NN market/NN) fund/NN)\\n\\n\\n\\nOnce we have created the chunk for money market, we have\\nremoved the context that would have permitted fund to be\\nincluded in a chunk.  This issue would have been avoided with\\na more permissive chunk rule, e.g. NP: {&lt;NN&gt;+}.\\n\\nNote\\nWe have added a comment to each of our chunk rules.\\nThese are optional; when they are present, the chunker\\nprints these comments as part of its tracing output.\\n\\n\\n\\n2.4&nbsp;&nbsp;&nbsp;Exploring Text Corpora\\nIn 2 we saw how we could interrogate\\na tagged corpus to extract phrases matching a particular\\nsequence of part-of-speech tags.  We can do the same work\\nmore easily with a chunker, as follows:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; cp = nltk.RegexpParser(\\'CHUNK: {&lt;V.*&gt; &lt;TO&gt; &lt;V.*&gt;}\\')\\n&gt;&gt;&gt; brown = nltk.corpus.brown\\n&gt;&gt;&gt; for sent in brown.tagged_sents():\\n...     tree = cp.parse(sent)\\n...     for subtree in tree.subtrees():\\n...         if subtree.label() == \\'CHUNK\\': print(subtree)\\n...\\n(CHUNK combined/VBN to/TO achieve/VB)\\n(CHUNK continue/VB to/TO place/VB)\\n(CHUNK serve/VB to/TO protect/VB)\\n(CHUNK wanted/VBD to/TO wait/VB)\\n(CHUNK allowed/VBN to/TO place/VB)\\n(CHUNK expected/VBN to/TO become/VB)\\n...\\n(CHUNK seems/VBZ to/TO overtake/VB)\\n(CHUNK want/VB to/TO buy/VB)\\n\\n\\n\\n\\nNote\\nYour Turn:\\nEncapsulate the above example inside a function find_chunks()\\nthat takes a chunk string like \"CHUNK: {&lt;V.*&gt; &lt;TO&gt; &lt;V.*&gt;}\" as an argument.\\nUse it to search the corpus for several other patterns, such as four\\nor more nouns in a row, e.g. \"NOUNS: {&lt;N.*&gt;{4,}}\"\\n\\n\\n\\n2.5&nbsp;&nbsp;&nbsp;Chinking\\nSometimes it is easier to define what we want to exclude from\\na chunk.  We can define a chink to be a sequence\\nof tokens that is not included in a chunk.\\nIn the following example, barked/VBD at/IN is a chink:\\n[ the/DT little/JJ yellow/JJ dog/NN ] barked/VBD at/IN [ the/DT cat/NN ]\\n\\nChinking is the process of removing a sequence of tokens from a\\nchunk.  If the matching sequence of tokens spans an entire chunk, then the\\nwhole chunk is removed; if the sequence of tokens appears in the\\nmiddle of the chunk, these tokens are removed, leaving two chunks\\nwhere there was only one before.  If the sequence is at the periphery\\nof the chunk, these tokens are removed, and a smaller chunk remains.\\nThese three possibilities are illustrated in 2.1.\\nTable 2.1: Three chinking rules applied to the same chunk\\n\\n\\n\\n\\n\\n\\n\\n` `\\nEntire chunk\\nMiddle of a chunk\\nEnd of a chunk\\n\\n\\n\\nInput\\n[a/DT little/JJ\\ndog/NN]\\n[a/DT little/JJ\\ndog/NN]\\n[a/DT little/JJ\\ndog/NN]\\n\\nOperation\\nChink \"DT JJ NN\"\\nChink \"JJ\"\\nChink \"NN\"\\n\\nPattern\\n}DT JJ NN{\\n}JJ{\\n}NN{\\n\\nOutput\\na/DT little/JJ\\ndog/NN\\n[a/DT] little/JJ\\n[dog/NN]\\n[a/DT little/JJ]\\ndog/NN\\n\\n\\n\\n\\n\\nIn 2.4, we put the entire sentence into a single chunk,\\nthen excise the chinks.\\n\\n\\n\\n\\n&nbsp;\\ngrammar = r\"\"\"\\n  NP:\\n    {&lt;.*&gt;+}          # Chunk everything\\n    }&lt;VBD|IN&gt;+{      # Chink sequences of VBD and IN\\n  \"\"\"\\nsentence = [(\"the\", \"DT\"), (\"little\", \"JJ\"), (\"yellow\", \"JJ\"),\\n       (\"dog\", \"NN\"), (\"barked\", \"VBD\"), (\"at\", \"IN\"),  (\"the\", \"DT\"), (\"cat\", \"NN\")]\\ncp = nltk.RegexpParser(grammar)\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; print(cp.parse(sentence))\\n (S\\n   (NP the/DT little/JJ yellow/JJ dog/NN)\\n   barked/VBD\\n   at/IN\\n   (NP the/DT cat/NN))\\n\\n\\nExample 2.4 (code_chinker.py): Figure 2.4: Simple Chinker\\n\\n<!-- We haven\\'t talked about using conll yet; and these results are\\nvery far from impressive anyway: :)\\n\\n>>> from nltk.corpus import conll2000\\n>>> test_sents = conll2000.chunked_sents(\\'test.txt\\', chunk_types=[\\'NP\\'])\\n>>> print(nltk.chunk.accuracy(cp, test_sents))\\n 0.5810414336070245 -->\\n<!-- Section: \"Multiple Chunk Types\" was here.  I moved it to\\nch07-extras because I didn\\'t see that it added much, and it didn\\'t\\nfeel very motivated. -->\\n<!-- Section\" Chunking vs Parsing\" was here.  I moved it to\\nch07-extras for now.  We might fold it back in later in the chapter,\\nor somewhere in the parsing chapter, but here seemed like an odd\\nplace for it. -->\\n\\n\\n2.6&nbsp;&nbsp;&nbsp;Representing Chunks: Tags vs Trees\\nAs befits their intermediate status between tagging and parsing (8.),\\nchunk structures can be represented using either tags or trees.  The most\\nwidespread file representation uses IOB tags.  In this\\nscheme, each token is tagged with one of three special chunk tags,\\nI (inside), O (outside), or B (begin).  A token is tagged\\nas B if it marks the beginning of a chunk.  Subsequent tokens\\nwithin the chunk are tagged I.  All other tokens are tagged O.\\nThe B and I tags are suffixed with the chunk type,\\ne.g. B-NP, I-NP.  Of course, it is not necessary to specify a\\nchunk type for tokens that appear outside a chunk, so these are just\\nlabeled O. An example of this scheme is shown in 2.5.\\n\\n\\nFigure 2.5: Tag Representation of Chunk Structures\\n\\nIOB tags have become the standard way to represent chunk structures in\\nfiles, and we will also be using this format.  Here is\\nhow the information in 2.5 would appear in a file:\\nWe PRP B-NP\\nsaw VBD O\\nthe DT B-NP\\nyellow JJ I-NP\\ndog NN I-NP\\n\\nIn this representation there is one token per line, each with\\nits part-of-speech tag and chunk tag.  This format permits us\\nto represent more than one chunk type, so long as the chunks do not overlap.\\nAs we saw earlier, chunk structures can also be represented using\\ntrees.  These have the benefit that each chunk is a constituent that\\ncan be manipulated directly.  An example is shown in 2.6.\\n\\n\\nFigure 2.6: Tree Representation of Chunk Structures\\n\\n\\nNote\\nNLTK uses trees for its internal representation of chunks, but\\nprovides methods for reading and writing such trees to the IOB format.\\n\\n\\n\\n\\n3&nbsp;&nbsp;&nbsp;Developing and Evaluating Chunkers\\nNow you have a taste of what chunking does, but we haven\\'t\\nexplained how to evaluate chunkers.\\nAs usual, this requires a suitably annotated corpus.\\nWe begin by looking at the mechanics of converting IOB format into an\\nNLTK tree, then at how this is done on a larger scale using a\\nchunked corpus.  We will see how to score\\nthe accuracy of a chunker relative to a corpus,\\nthen look at some more data-driven ways to search for NP chunks.\\nOur focus throughout will be on expanding the coverage of a chunker.\\n<!-- Section: \"Developing chunkers\" was here.  I moved it to\\nch07-extras.  I don\\'t think it added much. -->\\n\\n3.1&nbsp;&nbsp;&nbsp;Reading IOB Format and the CoNLL 2000 Corpus\\nUsing the corpus module we can load Wall Street Journal\\ntext that has been tagged then chunked using the IOB notation.  The\\nchunk categories provided in this corpus are NP, VP and PP.  As we\\nhave seen, each sentence is represented using multiple lines, as shown\\nbelow:\\nhe PRP B-NP\\naccepted VBD B-VP\\nthe DT B-NP\\nposition NN I-NP\\n...\\n\\nA conversion function chunk.conllstr2tree() builds a tree\\nrepresentation from one of these multi-line strings.  Moreover, it\\npermits us to choose any subset of the three chunk types to use,\\nhere just for NP chunks:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; text = \\'\\'\\'\\n... he PRP B-NP\\n... accepted VBD B-VP\\n... the DT B-NP\\n... position NN I-NP\\n... of IN B-PP\\n... vice NN B-NP\\n... chairman NN I-NP\\n... of IN B-PP\\n... Carlyle NNP B-NP\\n... Group NNP I-NP\\n... , , O\\n... a DT B-NP\\n... merchant NN I-NP\\n... banking NN I-NP\\n... concern NN I-NP\\n... . . O\\n... \\'\\'\\'\\n&gt;&gt;&gt; nltk.chunk.conllstr2tree(text, chunk_types=[\\'NP\\']).draw()\\n\\n\\n\\n\\nWe can use the NLTK corpus module to access a larger amount of chunked\\ntext.  The CoNLL 2000 corpus contains 270k words of Wall Street\\nJournal text, divided into \"train\" and \"test\" portions, annotated with\\npart-of-speech tags and chunk tags in the IOB format.  We can access\\nthe data using nltk.corpus.conll2000.  Here is an\\nexample that reads the 100th sentence of the \"train\" portion of the corpus:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; from nltk.corpus import conll2000\\n&gt;&gt;&gt; print(conll2000.chunked_sents(\\'train.txt\\')[99])\\n(S\\n  (PP Over/IN)\\n  (NP a/DT cup/NN)\\n  (PP of/IN)\\n  (NP coffee/NN)\\n  ,/,\\n  (NP Mr./NNP Stone/NNP)\\n  (VP told/VBD)\\n  (NP his/PRP$ story/NN)\\n  ./.)\\n\\n\\n\\nAs you can see, the CoNLL 2000 corpus contains three chunk types:\\nNP chunks, which we have already seen; VP chunks such as\\nhas already delivered; and PP chunks such as because of.\\nSince we are only interested in the NP chunks right now, we can use the\\nchunk_types argument to select them:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; print(conll2000.chunked_sents(\\'train.txt\\', chunk_types=[\\'NP\\'])[99])\\n(S\\n  Over/IN\\n  (NP a/DT cup/NN)\\n  of/IN\\n  (NP coffee/NN)\\n  ,/,\\n  (NP Mr./NNP Stone/NNP)\\n  told/VBD\\n  (NP his/PRP$ story/NN)\\n  ./.)\\n\\n\\n\\n\\n\\n3.2&nbsp;&nbsp;&nbsp;Simple Evaluation and Baselines\\nNow that we can access a chunked corpus, we can evaluate chunkers.\\nWe start off by establishing a baseline for the trivial chunk parser\\ncp that creates no chunks:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; from nltk.corpus import conll2000\\n&gt;&gt;&gt; cp = nltk.RegexpParser(\"\")\\n&gt;&gt;&gt; test_sents = conll2000.chunked_sents(\\'test.txt\\', chunk_types=[\\'NP\\'])\\n&gt;&gt;&gt; print(cp.evaluate(test_sents))\\nChunkParse score:\\n    IOB Accuracy:  43.4%\\n    Precision:      0.0%\\n    Recall:         0.0%\\n    F-Measure:      0.0%\\n\\n\\n\\nThe IOB tag accuracy indicates that more than a third of the words are\\ntagged with O, i.e. not in an NP chunk.  However, since our\\ntagger did not find any chunks, its precision, recall, and f-measure\\nare all zero.  Now let\\'s try a naive regular expression chunker that\\nlooks for tags beginning with letters that are characteristic of noun phrase tags\\n(e.g. CD, DT, and JJ).\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; grammar = r\"NP: {&lt;[CDJNP].*&gt;+}\"\\n&gt;&gt;&gt; cp = nltk.RegexpParser(grammar)\\n&gt;&gt;&gt; print(cp.evaluate(test_sents))\\nChunkParse score:\\n    IOB Accuracy:  87.7%\\n    Precision:     70.6%\\n    Recall:        67.8%\\n    F-Measure:     69.2%\\n\\n\\n\\nAs you can see, this approach achieves decent results.  However, we\\ncan improve on it by adopting a more data-driven approach, where we\\nuse the training corpus to find the chunk tag (I, O, or B)\\nthat is most likely for each part-of-speech tag.  In other words, we\\ncan build a chunker using a unigram tagger (4).\\nBut rather than trying to determine the correct part-of-speech tag for\\neach word, we are trying to determine the correct chunk tag, given\\neach word\\'s part-of-speech tag.\\nIn 3.1, we define the UnigramChunker class, which\\nuses a unigram tagger to label sentences with chunk tags.  Most of the\\ncode in this class is simply used to convert back and forth between\\nthe chunk tree representation used by NLTK\\'s ChunkParserI\\ninterface, and the IOB representation used by the embedded tagger.\\nThe class defines two methods: a constructor\\n which is called when we build a new\\nUnigramChunker; and the parse method \\nwhich is used to chunk new sentences.\\n\\n\\n\\n\\n&nbsp;\\nclass UnigramChunker(nltk.ChunkParserI):\\n    def __init__(self, train_sents): \\n        train_data = [[(t,c) for w,t,c in nltk.chunk.tree2conlltags(sent)]\\n                      for sent in train_sents]\\n        self.tagger = nltk.UnigramTagger(train_data) \\n\\n    def parse(self, sentence): \\n        pos_tags = [pos for (word,pos) in sentence]\\n        tagged_pos_tags = self.tagger.tag(pos_tags)\\n        chunktags = [chunktag for (pos, chunktag) in tagged_pos_tags]\\n        conlltags = [(word, pos, chunktag) for ((word,pos),chunktag)\\n                     in zip(sentence, chunktags)]\\n        return nltk.chunk.conlltags2tree(conlltags)\\n\\n\\nExample 3.1 (code_unigram_chunker.py): Figure 3.1: Noun Phrase Chunking with a Unigram Tagger\\n\\nThe constructor  expects a list of\\ntraining sentences, which will be in the form of chunk trees.  It\\nfirst converts training data to a form that is suitable for training the\\ntagger, using tree2conlltags to map each chunk tree to a list of\\nword,tag,chunk triples.  It then uses that converted training data\\nto train a unigram tagger, and stores it in self.tagger for later\\nuse.\\nThe parse method  takes a tagged sentence\\nas its input, and begins by extracting the part-of-speech tags from\\nthat sentence.  It then tags the part-of-speech tags with IOB chunk\\ntags, using the tagger self.tagger that was trained in the\\nconstructor.  Next, it extracts the chunk tags, and combines them with\\nthe original sentence, to yield conlltags.  Finally, it uses\\nconlltags2tree to convert the result back into a chunk tree.\\nNow that we have UnigramChunker, we can train it using the CoNLL\\n2000 corpus, and test its resulting performance:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; test_sents = conll2000.chunked_sents(\\'test.txt\\', chunk_types=[\\'NP\\'])\\n&gt;&gt;&gt; train_sents = conll2000.chunked_sents(\\'train.txt\\', chunk_types=[\\'NP\\'])\\n&gt;&gt;&gt; unigram_chunker = UnigramChunker(train_sents)\\n&gt;&gt;&gt; print(unigram_chunker.evaluate(test_sents))\\nChunkParse score:\\n    IOB Accuracy:  92.9%\\n    Precision:     79.9%\\n    Recall:        86.8%\\n    F-Measure:     83.2%\\n\\n\\n\\nThis chunker does reasonably well, achieving an overall f-measure\\nscore of 83%.  Let\\'s take a look at what it\\'s learned, by using its\\nunigram tagger to assign a tag to each of the part-of-speech tags that\\nappear in the corpus:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; postags = sorted(set(pos for sent in train_sents\\n...                      for (word,pos) in sent.leaves()))\\n&gt;&gt;&gt; print(unigram_chunker.tagger.tag(postags))\\n[(\\'#\\', \\'B-NP\\'), (\\'$\\', \\'B-NP\\'), (\"\\'\\'\", \\'O\\'), (\\'(\\', \\'O\\'), (\\')\\', \\'O\\'),\\n (\\',\\', \\'O\\'), (\\'.\\', \\'O\\'), (\\':\\', \\'O\\'), (\\'CC\\', \\'O\\'), (\\'CD\\', \\'I-NP\\'),\\n (\\'DT\\', \\'B-NP\\'), (\\'EX\\', \\'B-NP\\'), (\\'FW\\', \\'I-NP\\'), (\\'IN\\', \\'O\\'),\\n (\\'JJ\\', \\'I-NP\\'), (\\'JJR\\', \\'B-NP\\'), (\\'JJS\\', \\'I-NP\\'), (\\'MD\\', \\'O\\'),\\n (\\'NN\\', \\'I-NP\\'), (\\'NNP\\', \\'I-NP\\'), (\\'NNPS\\', \\'I-NP\\'), (\\'NNS\\', \\'I-NP\\'),\\n (\\'PDT\\', \\'B-NP\\'), (\\'POS\\', \\'B-NP\\'), (\\'PRP\\', \\'B-NP\\'), (\\'PRP$\\', \\'B-NP\\'),\\n (\\'RB\\', \\'O\\'), (\\'RBR\\', \\'O\\'), (\\'RBS\\', \\'B-NP\\'), (\\'RP\\', \\'O\\'), (\\'SYM\\', \\'O\\'),\\n (\\'TO\\', \\'O\\'), (\\'UH\\', \\'O\\'), (\\'VB\\', \\'O\\'), (\\'VBD\\', \\'O\\'), (\\'VBG\\', \\'O\\'),\\n (\\'VBN\\', \\'O\\'), (\\'VBP\\', \\'O\\'), (\\'VBZ\\', \\'O\\'), (\\'WDT\\', \\'B-NP\\'),\\n (\\'WP\\', \\'B-NP\\'), (\\'WP$\\', \\'B-NP\\'), (\\'WRB\\', \\'O\\'), (\\'``\\', \\'O\\')]\\n\\n\\n\\nIt has discovered that most punctuation marks occur outside of NP\\nchunks, with the exception of # and $, both of which are used\\nas currency markers.  It has also found that determiners (DT) and\\npossessives (PRP$ and WP$) occur at the beginnings of NP chunks,\\nwhile noun types (NN, NNP, NNPS, NNS) mostly occur\\ninside of NP chunks.\\n<!-- Commented out because we use it but don\\'t bother to define it in\\nthe text of the book, since it\\'s trivial:\\n\\n>>> class BigramChunker(nltk.ChunkParserI):\\n...     def __init__(self, train_sents):\\n...         train_data = [[(t,c) for w,t,c in nltk.chunk.tree2conlltags(sent)]\\n...                       for sent in train_sents]\\n...         self.tagger = nltk.BigramTagger(train_data)\\n...\\n...     def parse(self, sentence):\\n...         pos_tags = [pos for (word,pos) in sentence]\\n...         tagged_pos_tags = self.tagger.tag(pos_tags)\\n...         chunktags = [chunktag for (pos, chunktag) in tagged_pos_tags]\\n...         conlltags = [(word, pos, chunktag) for ((word,pos),chunktag)\\n...                      in zip(sentence, chunktags)]\\n...         return nltk.chunk.conlltags2tree(conlltags) -->\\nHaving built a unigram chunker, it is quite easy to build a bigram\\nchunker: we simply change the class name to BigramChunker, and\\nmodify line  in 3.1\\nto construct a BigramTagger rather than a UnigramTagger.\\nThe resulting chunker has slightly higher performance than the unigram chunker:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; bigram_chunker = BigramChunker(train_sents)\\n&gt;&gt;&gt; print(bigram_chunker.evaluate(test_sents))\\nChunkParse score:\\n    IOB Accuracy:  93.3%\\n    Precision:     82.3%\\n    Recall:        86.8%\\n    F-Measure:     84.5%\\n\\n\\n\\n\\n\\n3.3&nbsp;&nbsp;&nbsp;Training Classifier-Based Chunkers\\nBoth the regular-expression based chunkers and the n-gram chunkers\\ndecide what chunks to create entirely based on part-of-speech tags.\\nHowever, sometimes part-of-speech tags\\nare insufficient to determine how a sentence should be chunked.\\nFor example, consider the following two statements:\\n\\n  (3)\\n  a.Joey/NN sold/VBD the/DT farmer/NN rice/NN ./.\\n\\n  b.Nick/NN broke/VBD my/DT computer/NN monitor/NN ./.\\n\\nThese two sentences have the same part-of-speech tags,\\nyet they are chunked differently.  In the first sentence,\\nthe farmer and rice are separate chunks, while the\\ncorresponding material in the second sentence,\\nthe computer monitor, is a single chunk.  Clearly, we need to make\\nuse of information about the content of the words, in addition to just\\ntheir part-of-speech tags, if we wish to maximize chunking\\nperformance.\\nOne way that we can incorporate information about the content of words\\nis to use a classifier-based tagger to chunk the sentence.  Like the\\nn-gram chunker considered in the previous section, this\\nclassifier-based chunker will work by assigning IOB tags to the words\\nin a sentence, and then converting those tags to chunks.  For the\\nclassifier-based tagger itself, we will use the same approach that we\\nused in 1 to build a part-of-speech tagger.\\nThe basic code for the classifier-based NP chunker is shown in\\n3.2.  It consists of two classes.  The first\\nclass  is almost identical to the\\nConsecutivePosTagger class from 1.5.\\nThe only two differences are that it calls a different feature\\nextractor  and that it uses a MaxentClassifier rather\\nthan a NaiveBayesClassifier .  The second class\\n is basically a wrapper around the tagger class that\\nturns it into a chunker.  During training, this second class maps the\\nchunk trees in the training corpus into tag sequences; in the\\nparse() method, it converts the tag sequence provided by the\\ntagger back into a chunk tree.\\n\\n\\n\\n\\n&nbsp;\\nclass ConsecutiveNPChunkTagger(nltk.TaggerI): \\n\\n    def __init__(self, train_sents):\\n        train_set = []\\n        for tagged_sent in train_sents:\\n            untagged_sent = nltk.tag.untag(tagged_sent)\\n            history = []\\n            for i, (word, tag) in enumerate(tagged_sent):\\n                featureset = npchunk_features(untagged_sent, i, history) \\n                train_set.append( (featureset, tag) )\\n                history.append(tag)\\n        self.classifier = nltk.MaxentClassifier.train( \\n            train_set, algorithm=\\'megam\\', trace=0)\\n\\n    def tag(self, sentence):\\n        history = []\\n        for i, word in enumerate(sentence):\\n            featureset = npchunk_features(sentence, i, history)\\n            tag = self.classifier.classify(featureset)\\n            history.append(tag)\\n        return zip(sentence, history)\\n\\nclass ConsecutiveNPChunker(nltk.ChunkParserI): \\n    def __init__(self, train_sents):\\n        tagged_sents = [[((w,t),c) for (w,t,c) in\\n                         nltk.chunk.tree2conlltags(sent)]\\n                        for sent in train_sents]\\n        self.tagger = ConsecutiveNPChunkTagger(tagged_sents)\\n\\n    def parse(self, sentence):\\n        tagged_sents = self.tagger.tag(sentence)\\n        conlltags = [(w,t,c) for ((w,t),c) in tagged_sents]\\n        return nltk.chunk.conlltags2tree(conlltags)\\n\\n\\nExample 3.2 (code_classifier_chunker.py): Figure 3.2: Noun Phrase Chunking with a Consecutive Classifier\\n\\n<!-- Pre-load megam, so we won\\'t get random trace output when it\\'s loaded:\\n>>> nltk.classify.config_megam()\\n[Found megam: ...] -->\\nThe only piece left to fill in is the feature extractor.  We begin by\\ndefining a simple feature extractor which just provides the\\npart-of-speech tag of the current token.  Using this feature extractor, our\\nclassifier-based chunker is very similar to the unigram chunker, as is\\nreflected in its performance:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; def npchunk_features(sentence, i, history):\\n...     word, pos = sentence[i]\\n...     return {\"pos\": pos}\\n&gt;&gt;&gt; chunker = ConsecutiveNPChunker(train_sents)\\n&gt;&gt;&gt; print(chunker.evaluate(test_sents))\\nChunkParse score:\\n    IOB Accuracy:  92.9%\\n    Precision:     79.9%\\n    Recall:        86.7%\\n    F-Measure:     83.2%\\n\\n\\n\\nWe can also add a feature for the previous part-of-speech tag.  Adding this\\nfeature allows the classifier to model interactions between adjacent\\ntags, and results in a chunker that is closely related to the bigram\\nchunker.\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; def npchunk_features(sentence, i, history):\\n...     word, pos = sentence[i]\\n...     if i == 0:\\n...         prevword, prevpos = \"&lt;START&gt;\", \"&lt;START&gt;\"\\n...     else:\\n...         prevword, prevpos = sentence[i-1]\\n...     return {\"pos\": pos, \"prevpos\": prevpos}\\n&gt;&gt;&gt; chunker = ConsecutiveNPChunker(train_sents)\\n&gt;&gt;&gt; print(chunker.evaluate(test_sents))\\nChunkParse score:\\n    IOB Accuracy:  93.6%\\n    Precision:     81.9%\\n    Recall:        87.2%\\n    F-Measure:     84.5%\\n\\n\\n\\nNext, we\\'ll try adding a feature for the current word, since we\\nhypothesized that word content should be useful for chunking.  We find\\nthat this feature does indeed improve the chunker\\'s performance,\\nby about 1.5 percentage points (which corresponds to about a 10%\\nreduction in the error rate).\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; def npchunk_features(sentence, i, history):\\n...     word, pos = sentence[i]\\n...     if i == 0:\\n...         prevword, prevpos = \"&lt;START&gt;\", \"&lt;START&gt;\"\\n...     else:\\n...         prevword, prevpos = sentence[i-1]\\n...     return {\"pos\": pos, \"word\": word, \"prevpos\": prevpos}\\n&gt;&gt;&gt; chunker = ConsecutiveNPChunker(train_sents)\\n&gt;&gt;&gt; print(chunker.evaluate(test_sents))\\nChunkParse score:\\n    IOB Accuracy:  94.5%\\n    Precision:     84.2%\\n    Recall:        89.4%\\n    F-Measure:     86.7%\\n\\n\\n\\nFinally, we can try extending the feature extractor with a variety of\\nadditional features, such as lookahead features ,\\npaired features , and complex contextual features\\n.  This last feature, called tags-since-dt, creates a\\nstring describing the set of all part-of-speech tags that have been\\nencountered since the most recent determiner, or since the beginning\\nof the sentence if there is no determiner before index i.\\n.\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; def npchunk_features(sentence, i, history):\\n...     word, pos = sentence[i]\\n...     if i == 0:\\n...         prevword, prevpos = \"&lt;START&gt;\", \"&lt;START&gt;\"\\n...     else:\\n...         prevword, prevpos = sentence[i-1]\\n...     if i == len(sentence)-1:\\n...         nextword, nextpos = \"&lt;END&gt;\", \"&lt;END&gt;\"\\n...     else:\\n...         nextword, nextpos = sentence[i+1]\\n...     return {\"pos\": pos,\\n...             \"word\": word,\\n...             \"prevpos\": prevpos,\\n...             \"nextpos\": nextpos, \\n...             \"prevpos+pos\": \"%s+%s\" % (prevpos, pos),  \\n...             \"pos+nextpos\": \"%s+%s\" % (pos, nextpos),\\n...             \"tags-since-dt\": tags_since_dt(sentence, i)}  \\n\\n\\n\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; def tags_since_dt(sentence, i):\\n...     tags = set()\\n...     for word, pos in sentence[:i]:\\n...         if pos == \\'DT\\':\\n...             tags = set()\\n...         else:\\n...             tags.add(pos)\\n...     return \\'+\\'.join(sorted(tags))\\n\\n\\n\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; chunker = ConsecutiveNPChunker(train_sents)\\n&gt;&gt;&gt; print(chunker.evaluate(test_sents))\\nChunkParse score:\\n    IOB Accuracy:  96.0%\\n    Precision:     88.6%\\n    Recall:        91.0%\\n    F-Measure:     89.8%\\n\\n\\n\\n\\nNote\\nYour Turn:\\nTry adding different features to the feature extractor function\\nnpchunk_features, and see if you can further improve the\\nperformance of the NP chunker.\\n\\n\\n\\n\\n4&nbsp;&nbsp;&nbsp;Recursion in Linguistic Structure\\n\\n4.1&nbsp;&nbsp;&nbsp;Building Nested Structure with Cascaded Chunkers\\nSo far, our chunk structures have been relatively flat.  Trees consist\\nof tagged tokens, optionally grouped under a chunk node such as\\nNP.  However, it is possible to build chunk structures of\\narbitrary depth, simply by creating a multi-stage chunk grammar\\ncontaining recursive rules.  4.1 has\\npatterns for noun phrases, prepositional phrases, verb phrases, and\\nsentences.\\nThis is a four-stage chunk grammar, and can be used to create\\nstructures having a depth of at most four.\\n<!-- I changed this example grammar to use \"CLAUSE\" rather than \"S\",\\nsince there\\'s an \"S\" node that\\'s automatically supplied by the\\nchunk parser.  And the fact that we have these two different \"S\"\\nnodes is confusing. -->\\n\\n\\n\\n\\n&nbsp;\\ngrammar = r\"\"\"\\n  NP: {&lt;DT|JJ|NN.*&gt;+}          # Chunk sequences of DT, JJ, NN\\n  PP: {&lt;IN&gt;&lt;NP&gt;}               # Chunk prepositions followed by NP\\n  VP: {&lt;VB.*&gt;&lt;NP|PP|CLAUSE&gt;+$} # Chunk verbs and their arguments\\n  CLAUSE: {&lt;NP&gt;&lt;VP&gt;}           # Chunk NP, VP\\n  \"\"\"\\ncp = nltk.RegexpParser(grammar)\\nsentence = [(\"Mary\", \"NN\"), (\"saw\", \"VBD\"), (\"the\", \"DT\"), (\"cat\", \"NN\"),\\n    (\"sit\", \"VB\"), (\"on\", \"IN\"), (\"the\", \"DT\"), (\"mat\", \"NN\")]\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; print(cp.parse(sentence))\\n(S\\n  (NP Mary/NN)\\n  saw/VBD\\n  (CLAUSE\\n    (NP the/DT cat/NN)\\n    (VP sit/VB (PP on/IN (NP the/DT mat/NN)))))\\n\\n\\nExample 4.1 (code_cascaded_chunker.py): Figure 4.1: A Chunker that Handles NP, PP, VP and S\\n\\nUnfortunately this result misses the VP headed by saw.  It has\\nother shortcomings too.  Let\\'s see what happens when we apply this\\nchunker to a sentence having deeper nesting.  Notice that it fails to\\nidentify the VP chunk starting at .\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; sentence = [(\"John\", \"NNP\"), (\"thinks\", \"VBZ\"), (\"Mary\", \"NN\"),\\n...     (\"saw\", \"VBD\"), (\"the\", \"DT\"), (\"cat\", \"NN\"), (\"sit\", \"VB\"),\\n...     (\"on\", \"IN\"), (\"the\", \"DT\"), (\"mat\", \"NN\")]\\n&gt;&gt;&gt; print(cp.parse(sentence))\\n(S\\n  (NP John/NNP)\\n  thinks/VBZ\\n  (NP Mary/NN)\\n  saw/VBD # [_saw-vbd]\\n  (CLAUSE\\n    (NP the/DT cat/NN)\\n    (VP sit/VB (PP on/IN (NP the/DT mat/NN)))))\\n\\n\\n\\nThe solution to these problems is to get the chunker to loop over its\\npatterns: after trying all of them, it repeats the process.\\nWe add an optional second argument loop to specify the number\\nof times the set of patterns should be run:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; cp = nltk.RegexpParser(grammar, loop=2)\\n&gt;&gt;&gt; print(cp.parse(sentence))\\n(S\\n  (NP John/NNP)\\n  thinks/VBZ\\n  (CLAUSE\\n    (NP Mary/NN)\\n    (VP\\n      saw/VBD\\n      (CLAUSE\\n        (NP the/DT cat/NN)\\n        (VP sit/VB (PP on/IN (NP the/DT mat/NN)))))))\\n\\n\\n\\n\\nNote\\nThis cascading process enables us to create deep structures.  However,\\ncreating and debugging a cascade is difficult, and there comes\\na point where it is more effective to do full parsing (see 8.).\\nAlso, the cascading process can only produce trees of fixed depth\\n(no deeper than the number of stages in the cascade), and this is\\ninsufficient for complete syntactic analysis.\\n\\n\\n\\n4.2&nbsp;&nbsp;&nbsp;Trees\\nA tree is a set of connected labeled nodes, each reachable\\nby a unique path from a distinguished root node.  Here\\'s an\\nexample of a tree (note that they are standardly drawn upside-down):\\n\\n  (4)\\nWe use a \\'family\\' metaphor to talk about the\\nrelationships of nodes in a tree: for example, S is the\\nparent of VP; conversely VP is a child\\nof S.  Also, since NP and VP are both\\nchildren of S, they are also siblings.\\nFor convenience, there is also a text format for specifying\\ntrees:\\n\\n\\n\\n\\n&nbsp;\\n(S\\n   (NP Alice)\\n   (VP\\n      (V chased)\\n      (NP\\n         (Det the)\\n         (N rabbit))))\\n\\n\\n\\nAlthough we will focus on syntactic trees, trees can be used to encode\\nany homogeneous hierarchical structure that spans a sequence\\nof linguistic forms (e.g. morphological structure, discourse structure).\\nIn the general case, leaves and node values do not have to be strings.\\nIn NLTK, we create a tree by giving a node label and a list of children:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; tree1 = nltk.Tree(\\'NP\\', [\\'Alice\\'])\\n&gt;&gt;&gt; print(tree1)\\n(NP Alice)\\n&gt;&gt;&gt; tree2 = nltk.Tree(\\'NP\\', [\\'the\\', \\'rabbit\\'])\\n&gt;&gt;&gt; print(tree2)\\n(NP the rabbit)\\n\\n\\n\\nWe can incorporate these into successively larger trees as follows:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; tree3 = nltk.Tree(\\'VP\\', [\\'chased\\', tree2])\\n&gt;&gt;&gt; tree4 = nltk.Tree(\\'S\\', [tree1, tree3])\\n&gt;&gt;&gt; print(tree4)\\n(S (NP Alice) (VP chased (NP the rabbit)))\\n\\n\\n\\nHere are some of the methods available for tree objects:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; print(tree4[1])\\n(VP chased (NP the rabbit))\\n&gt;&gt;&gt; tree4[1].label()\\n\\'VP\\'\\n&gt;&gt;&gt; tree4.leaves()\\n[\\'Alice\\', \\'chased\\', \\'the\\', \\'rabbit\\']\\n&gt;&gt;&gt; tree4[1][1][1]\\n\\'rabbit\\'\\n\\n\\n\\nThe bracketed representation for complex trees can be difficult to read.\\nIn these cases, the draw method can be very useful.\\nIt opens a new window, containing a graphical representation\\nof the tree.  The tree display window allows you to zoom in and out,\\nto collapse and expand subtrees, and to print the graphical\\nrepresentation to a postscript file (for inclusion in a document).\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; tree3.draw()                           \\n\\n\\n\\n\\n\\n\\n4.3&nbsp;&nbsp;&nbsp;Tree Traversal\\nIt is standard to use a recursive function to traverse a tree.\\nThe listing in 4.2 demonstrates this.\\n\\n\\n\\n\\n&nbsp;\\ndef traverse(t):\\n    try:\\n        t.label()\\n    except AttributeError:\\n        print(t, end=\" \")\\n    else:\\n        # Now we know that t.node is defined\\n        print(\\'(\\', t.label(), end=\" \")\\n        for child in t:\\n            traverse(child)\\n        print(\\')\\', end=\" \")\\n\\n &gt;&gt;&gt; t = nltk.Tree(\\'(S (NP Alice) (VP chased (NP the rabbit)))\\')\\n &gt;&gt;&gt; traverse(t)\\n ( S ( NP Alice ) ( VP chased ( NP the rabbit ) ) )\\n\\n\\nExample 4.2 (code_traverse.py): Figure 4.2: A Recursive Function to Traverse a Tree\\n\\n\\nNote\\nWe have used a technique called duck typing to detect that t\\nis a tree (i.e. t.label() is defined).\\n\\n\\n\\n\\n5&nbsp;&nbsp;&nbsp;Named Entity Recognition\\nAt the start of this chapter, we briefly introduced named entities\\n(NEs). Named entities are definite noun phrases that\\nrefer to specific types of individuals, such as organizations, persons,\\ndates, and so on. 5.1 lists some of the more commonly used\\ntypes of NEs. These should be self-explanatory, except for \"Facility\":\\nhuman-made artifacts in the domains of architecture and civil\\nengineering; and \"GPE\": geo-political entities such as city, state/province, and country.\\nTable 5.1: Commonly Used Types of Named Entity\\n\\n\\n\\n\\n\\nNE Type\\nExamples\\n\\n\\n\\nORGANIZATION\\nGeorgia-Pacific Corp., WHO\\n\\nPERSON\\nEddy Bonte, President Obama\\n\\nLOCATION\\nMurray River, Mount Everest\\n\\nDATE\\nJune, 2008-06-29\\n\\nTIME\\ntwo fifty a m, 1:30 p.m.\\n\\nMONEY\\n175 million Canadian Dollars, GBP 10.40\\n\\nPERCENT\\ntwenty pct, 18.75 %\\n\\nFACILITY\\nWashington Monument, Stonehenge\\n\\nGPE\\nSouth East Asia, Midlothian\\n\\n\\n\\n\\n\\nThe goal of a named entity recognition (NER) system is to identify all\\ntextual mentions of the named entities. This can be broken down into\\ntwo sub-tasks: identifying the boundaries of the NE, and identifying its\\ntype.\\nWhile named entity recognition is frequently a prelude to identifying\\nrelations in Information Extraction, it can also contribute to other\\ntasks.  For example, in Question Answering (QA), we try to improve the\\nprecision of Information Retrieval by recovering not whole pages, but\\njust those parts which contain an answer to the user\\'s question. Most\\nQA systems take the documents returned by standard Information\\nRetrieval, and then attempt to isolate the minimal text snippet in the\\ndocument containing the answer. Now suppose the question was Who was\\nthe first President of the US?, and one of the documents that was\\nretrieved contained the following passage:\\n\\n  (5)The Washington Monument is the most prominent structure in\\nWashington, D.C. and one of the city\\'s early attractions.  It was\\nbuilt in honor of George Washington, who led the country to\\nindependence and then became its first President.\\nAnalysis of the question leads us to expect that an answer should be\\nof the form  X was the first President of the US, where X\\nis not only a noun phrase, but also refers to a named entity of type\\nPERSON. This should allow us to ignore the first sentence in the\\npassage.  While it contains two occurrences of Washington,\\nnamed entity recognition should tell us that neither of them\\nhas the correct type.\\nHow do we go about identifying named entities?  One option would be to\\nlook up each word in an appropriate list of names.\\nFor example, in the case of locations, we could use a gazetteer,\\nor geographical dictionary, such as the Alexandria Gazetteer or the\\nGetty Gazetteer.  However, doing this\\nblindly runs into problems, as shown in 5.1.\\n\\n\\nFigure 5.1: Location Detection by Simple Lookup for a News Story: Looking up every\\nword in a gazetteer is error-prone; case distinctions may help, but\\nthese are not always present.\\n\\nObserve that the gazetteer has good coverage of locations in many countries,\\nand incorrectly finds locations like Sanchez in the Dominican Republic\\nand On in Vietnam.\\nOf course we could omit such locations from the gazetteer, but then we won\\'t\\nbe able to identify them when they do appear in a document.\\nIt gets even harder in the case of names for people or organizations.\\nAny list of such names will probably have poor coverage. New organizations\\ncome into existence every day, so if we are trying to deal\\nwith contemporary newswire or blog entries, it is unlikely that\\nwe will be able to recognize many of the entities using gazetteer lookup.\\nAnother major source of difficulty is caused by the fact that many\\nnamed entity terms are ambiguous. Thus\\nMay and North are likely to be parts of named entities for DATE\\nand LOCATION, respectively, but could both be part of a PERSON;\\nconversely Christian Dior looks like a PERSON but is more\\nlikely to be of type ORGANIZATION. A term like Yankee will be\\nordinary modifier in some contexts, but will be marked as an entity of\\ntype ORGANIZATION in the phrase Yankee infielders.\\nFurther challenges are posed by multi-word names like\\nStanford University, and by names that contain other names\\nsuch as Cecil H. Green Library and Escondido Village Conference\\nService Center. In named entity recognition, therefore, we need\\nto be able to identify the beginning and end of multi-token\\nsequences.\\nNamed entity recognition is a task that is well-suited to the type of\\nclassifier-based approach that we saw for noun phrase chunking.  In\\nparticular, we can build a tagger that labels each word in a sentence\\nusing the IOB format, where chunks are labeled by their appropriate type.\\nHere is part of the CONLL 2002 (conll2002) Dutch training data:\\nEddy N B-PER\\nBonte N I-PER\\nis V O\\nwoordvoerder N O\\nvan Prep O\\ndiezelfde Pron O\\nHogeschool N B-ORG\\n. Punc O\\n\\nIn this representation, there is one token per line, each with its\\npart-of-speech tag and its named entity tag.  Based on this training\\ncorpus, we can construct a tagger that can be used to label new\\nsentences; and use the nltk.chunk.conlltags2tree() function to\\nconvert the tag sequences into a chunk tree.\\nNLTK provides a classifier that has already been trained to recognize named entities,\\naccessed with the function nltk.ne_chunk().  If we set the\\nparameter binary=True , then named entities are just\\ntagged as NE; otherwise, the classifier adds category labels such\\nas PERSON, ORGANIZATION, and GPE.\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; sent = nltk.corpus.treebank.tagged_sents()[22]\\n&gt;&gt;&gt; print(nltk.ne_chunk(sent, binary=True)) \\n(S\\n  The/DT\\n  (NE U.S./NNP)\\n  is/VBZ\\n  one/CD\\n  ...\\n  according/VBG\\n  to/TO\\n  (NE Brooke/NNP T./NNP Mossman/NNP)\\n  ...)\\n\\n\\n\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; print(nltk.ne_chunk(sent)) \\n(S\\n  The/DT\\n  (GPE U.S./NNP)\\n  is/VBZ\\n  one/CD\\n  ...\\n  according/VBG\\n  to/TO\\n  (PERSON Brooke/NNP T./NNP Mossman/NNP)\\n  ...)\\n\\n\\n\\n\\n<!-- Leaving this out: I don\\'t see that we need it really.\\n\\n.. XXX change to \"the NE classifier is?\\n\\n.. XXX the following wants a normal citation.  (Would we ever say that a\\n   book was not freely available, but that you could buy it from XYZ\\n   publisher?)\\n\\n   Both chunkers are trained based on the ACE-2 corpus.  This\\n   corpus is not freely available, but a license to use the corpus can be\\n   purchased from the Linguistic Data Consortium (catalog id LDC2003T11). -->\\n\\n\\n6&nbsp;&nbsp;&nbsp;Relation Extraction\\n\\nOnce named entities have been identified in a text, we then want to extract\\nthe relations that exist between them. As indicated earlier, we will\\ntypically be looking for relations between specified types of\\nnamed entity. One way of approaching this task is to initially look for all\\ntriples of the form (X, α, Y), where X and Y are named entities\\nof the required types, and α is the string of words that\\nintervenes between X and Y. We can then use regular expressions to\\npull out just those instances of α that express the relation\\nthat we are looking for. The following example searches for strings\\nthat contain the word in. The special regular expression\\n(?!\\\\b.+ing\\\\b) is a negative lookahead assertion that allows us to\\ndisregard strings such as success in supervising the transition\\nof, where in is followed by a gerund.\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; IN = re.compile(r\\'.*\\\\bin\\\\b(?!\\\\b.+ing)\\')\\n&gt;&gt;&gt; for doc in nltk.corpus.ieer.parsed_docs(\\'NYT_19980315\\'):\\n...     for rel in nltk.sem.extract_rels(\\'ORG\\', \\'LOC\\', doc,\\n...                                      corpus=\\'ieer\\', pattern = IN):\\n...         print(nltk.sem.rtuple(rel))\\n[ORG: \\'WHYY\\'] \\'in\\' [LOC: \\'Philadelphia\\']\\n[ORG: \\'McGlashan &amp;AMP; Sarrail\\'] \\'firm in\\' [LOC: \\'San Mateo\\']\\n[ORG: \\'Freedom Forum\\'] \\'in\\' [LOC: \\'Arlington\\']\\n[ORG: \\'Brookings Institution\\'] \\', the research group in\\' [LOC: \\'Washington\\']\\n[ORG: \\'Idealab\\'] \\', a self-described business incubator based in\\' [LOC: \\'Los Angeles\\']\\n[ORG: \\'Open Text\\'] \\', based in\\' [LOC: \\'Waterloo\\']\\n[ORG: \\'WGBH\\'] \\'in\\' [LOC: \\'Boston\\']\\n[ORG: \\'Bastille Opera\\'] \\'in\\' [LOC: \\'Paris\\']\\n[ORG: \\'Omnicom\\'] \\'in\\' [LOC: \\'New York\\']\\n[ORG: \\'DDB Needham\\'] \\'in\\' [LOC: \\'New York\\']\\n[ORG: \\'Kaplan Thaler Group\\'] \\'in\\' [LOC: \\'New York\\']\\n[ORG: \\'BBDO South\\'] \\'in\\' [LOC: \\'Atlanta\\']\\n[ORG: \\'Georgia-Pacific\\'] \\'in\\' [LOC: \\'Atlanta\\']\\n\\n\\n\\nSearching for the keyword in works reasonably well,\\nthough it will also retrieve false positives such as [ORG: House\\nTransportation Committee] , secured the most money in the [LOC: New\\nYork]; there is unlikely to be simple string-based method of\\nexcluding filler strings such as this.\\n\\nAs shown above, the conll2002 Dutch corpus contains not just named entity\\nannotation but also part-of-speech tags. This allows us to devise\\npatterns that are sensitive to these tags, as shown in the next\\nexample. The method clause() prints out the relations in a\\nclausal form, where the binary relation symbol is specified as the\\nvalue of parameter relsym .\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; from nltk.corpus import conll2002\\n&gt;&gt;&gt; vnv = \"\"\"\\n... (\\n... is/V|    # 3rd sing present and\\n... was/V|   # past forms of the verb zijn (\\'be\\')\\n... werd/V|  # and also present\\n... wordt/V  # past of worden (\\'become)\\n... )\\n... .*       # followed by anything\\n... van/Prep # followed by van (\\'of\\')\\n... \"\"\"\\n&gt;&gt;&gt; VAN = re.compile(vnv, re.VERBOSE)\\n&gt;&gt;&gt; for doc in conll2002.chunked_sents(\\'ned.train\\'):\\n...     for rel in nltk.sem.extract_rels(\\'PER\\', \\'ORG\\', doc,\\n...                                    corpus=\\'conll2002\\', pattern=VAN):\\n...         print(nltk.sem.clause(rel, relsym=\"VAN\")) \\nVAN(\"cornet_d\\'elzius\", \\'buitenlandse_handel\\')\\nVAN(\\'johan_rottiers\\', \\'kardinaal_van_roey_instituut\\')\\nVAN(\\'annie_lennox\\', \\'eurythmics\\')\\n\\n\\n\\n\\nNote\\nYour Turn: Replace the last line , by\\nprint(nltk.rtuple(rel, lcon=True, rcon=True)). This will show you\\nthe actual words that intervene between the two NEs and\\nalso their left and right context, within a default 10-word\\nwindow. With the help of a Dutch dictionary, you might be able to\\nfigure out why the result VAN(\\'annie_lennox\\', \\'eurythmics\\') is\\na false hit.\\n\\n<!-- This is too weak to include:\\n\\nMessage Understanding\\n- - - - - - - - - - - - - - - - - - - - -\\n\\nA message understanding system\\nwill extract salient chunks of text from a news story and populate a\\ndatabase.\\n\\n.. figure:: ../images/chunk-muc.png\\n   :scale: 60\\n\\n   The Message Understanding Process (from Abney 1996)\\n\\nConsider the units that have been selected in this process:\\na name (``Garcia Alvarado``), a verb cluster (``was killed``),\\na locative prepositional phrase (``on his vehicle``).  These\\nare examples of ``NP``, ``VP`` and ``PP`` chunks. -->\\n\\n\\n\\n7&nbsp;&nbsp;&nbsp;Summary\\n\\nInformation extraction systems search large bodies of unrestricted\\ntext for specific types of entities and relations, and use them to\\npopulate well-organized databases.  These databases can then be used\\nto find answers for specific questions.\\nThe typical architecture for an information extraction system begins\\nby segmenting, tokenizing, and part-of-speech tagging the text.\\nThe resulting data is then searched for specific types of entity.\\nFinally, the information extraction system looks at entities that\\nare mentioned near one another in the text, and tries to determine\\nwhether specific relationships hold between those entities.\\nEntity recognition is often performed using chunkers, which\\nsegment multi-token sequences, and label them with the appropriate\\nentity type.  Common entity types include ORGANIZATION, PERSON,\\nLOCATION, DATE, TIME, MONEY, and GPE (geo-political entity).\\nChunkers can be constructed using rule-based systems, such as the\\nRegexpParser class provided by NLTK; or using machine learning\\ntechniques, such as the ConsecutiveNPChunker presented in this\\nchapter.  In either case, part-of-speech tags are often a very\\nimportant feature when searching for chunks.\\nAlthough chunkers are specialized to create relatively flat\\ndata structures, where no two chunks are allowed to overlap,\\nthey can be cascaded together to build nested structures.\\nRelation extraction can be performed using either rule-based\\nsystems which typically look for specific patterns in the\\ntext that connect entities and the intervening words; or using\\nmachine-learning systems which typically attempt to learn\\nsuch patterns automatically from a training corpus.\\n\\n\\n\\n8&nbsp;&nbsp;&nbsp;Further Reading\\nExtra materials for this chapter are posted at http://nltk.org/,\\nincluding links to freely available resources on the web.\\nFor more examples of chunking with NLTK, please see the\\nChunking HOWTO at http://nltk.org/howto.\\nThe popularity of chunking is due in great part to pioneering work by\\nAbney e.g., (Church, Young, &amp; Bloothooft, 1996). Abney\\'s Cass chunker is described in\\nhttp://www.vinartus.net/spa/97a.pdf.\\nThe word chink initially meant a sequence of stopwords,\\naccording to a 1975 paper by Ross and Tukey (Church, Young, &amp; Bloothooft, 1996).\\nThe IOB format (or sometimes  BIO Format) was developed for\\nNP chunking by (Ramshaw &amp; Marcus, 1995), and was used for the shared NP\\nbracketing task run by the Conference on Natural Language Learning\\n(CoNLL) in 1999.  The same format was\\nadopted by CoNLL 2000 for annotating a section of Wall Street\\nJournal text as part of a shared task on NP chunking.\\nSection 13.5 of (Jurafsky &amp; Martin, 2008) contains a discussion of chunking.\\nChapter 22 covers information extraction, including named entity recognition.\\nFor information about text mining in biology and medicine, see\\n(Ananiadou &amp; McNaught, 2006).\\n\\n\\n\\n\\n\\n\\n9&nbsp;&nbsp;&nbsp;Exercises\\n\\n☼ The IOB format categorizes tagged tokens as I,\\nO and B.  Why are three tags necessary?  What\\nproblem would be caused if we used I and O tags\\nexclusively?\\n☼ Write a tag pattern to match noun phrases containing plural head nouns,\\ne.g. \"many/JJ researchers/NNS\", \"two/CD weeks/NNS\", \"both/DT new/JJ positions/NNS\".\\nTry to do this by generalizing the tag pattern that handled singular\\nnoun phrases.\\n☼\\nPick one of the three chunk types in the CoNLL corpus.\\nInspect the CoNLL corpus and try to observe any patterns in the POS tag sequences\\nthat make up this kind of chunk.  Develop a simple chunker using\\nthe regular expression chunker nltk.RegexpParser.\\nDiscuss any tag sequences that are difficult to chunk reliably.\\n☼\\nAn early definition of chunk was the material that occurs between chinks.\\nDevelop a chunker that starts by putting the whole sentence in a single\\nchunk, and then does the rest of its work solely by chinking.\\nDetermine which tags (or tag sequences) are most likely to make up chinks\\nwith the help of your own utility program.  Compare the performance and\\nsimplicity of this approach relative to a chunker based entirely on\\nchunk rules.\\n◑ Write a tag pattern to cover noun phrases that contain gerunds,\\ne.g. \"the/DT receiving/VBG end/NN\", \"assistant/NN managing/VBG editor/NN\".\\nAdd these patterns to the grammar, one per line.  Test your work using\\nsome tagged sentences of your own devising.\\n◑ Write one or more tag patterns to handle coordinated noun phrases,\\ne.g. \"July/NNP and/CC August/NNP\",\\n\"all/DT your/PRP$ managers/NNS and/CC supervisors/NNS\",\\n\"company/NN courts/NNS and/CC adjudicators/NNS\".\\n◑ Carry out the following evaluation tasks for\\nany of the chunkers you have developed earlier.\\n(Note that most chunking corpora contain some internal\\ninconsistencies, such that any reasonable rule-based approach\\nwill produce errors.)\\nEvaluate your chunker on 100 sentences from a chunked corpus,\\nand report the precision, recall and F-measure.\\nUse the chunkscore.missed() and chunkscore.incorrect()\\nmethods to identify the errors made by your chunker.  Discuss.\\nCompare the performance of your chunker to the baseline chunker\\ndiscussed in the evaluation section of this chapter.\\n\\n\\n◑\\nDevelop a chunker for one of the chunk types in the CoNLL corpus using a\\nregular-expression based chunk grammar RegexpChunk.  Use any\\ncombination of rules for chunking, chinking, merging or splitting.\\n◑ Sometimes a word is incorrectly tagged, e.g. the head noun in\\n\"12/CD or/CC so/RB cases/VBZ\".  Instead of requiring manual correction of\\ntagger output, good chunkers are able to work with the erroneous\\noutput of taggers.  Look for other examples of correctly chunked\\nnoun phrases with incorrect tags.\\n◑\\nThe bigram chunker scores about 90% accuracy.\\nStudy its errors and try to work out why it doesn\\'t get 100% accuracy.\\nExperiment with trigram chunking.  Are you able to improve the performance any more?\\n★\\nApply the n-gram and Brill tagging methods to IOB chunk tagging.\\nInstead of assigning POS tags to words, here we will assign IOB tags\\nto the POS tags.  E.g. if the tag DT (determiner) often occurs\\nat the start of a chunk, it will be tagged B (begin).  Evaluate\\nthe performance of these chunking methods relative to the regular\\nexpression chunking methods covered in this chapter.\\n★\\nWe saw in 5. that it is possible to establish\\nan upper limit to tagging performance by looking for ambiguous n-grams,\\nn-grams that are tagged in more than one possible way in the training data.\\nApply the same method to determine an upper bound on the performance\\nof an n-gram chunker.\\n★\\nPick one of the three chunk types in the CoNLL corpus.  Write functions\\nto do the following tasks for your chosen type:\\nList all the tag sequences that occur with each instance of this chunk type.\\nCount the frequency of each tag sequence, and produce a ranked list in\\norder of decreasing frequency; each line should consist of an integer (the frequency)\\nand the tag sequence.\\nInspect the high-frequency tag sequences.  Use these as the basis for\\ndeveloping a better chunker.\\n\\n\\n★\\nThe baseline chunker presented in the evaluation section tends to\\ncreate larger chunks than it should.  For example, the\\nphrase:\\n[every/DT time/NN] [she/PRP] sees/VBZ [a/DT newspaper/NN]\\ncontains two consecutive chunks, and our baseline chunker will\\nincorrectly combine the first two: [every/DT time/NN she/PRP].\\nWrite a program that finds which of these chunk-internal tags\\ntypically occur at the start of a chunk, then\\ndevise one or more rules that will split up these chunks.\\nCombine these with the existing baseline chunker and\\nre-evaluate it, to see if you have discovered an improved baseline.\\n★\\nDevelop an NP chunker that converts POS-tagged text into a list of\\ntuples, where each tuple consists of a verb followed by a sequence of\\nnoun phrases and prepositions,\\ne.g. the little cat sat on the mat becomes (\\'sat\\', \\'on\\', \\'NP\\')...\\n★\\nThe Penn Treebank contains a section of tagged Wall Street Journal text\\nthat has been chunked into noun phrases.  The format uses square brackets,\\nand we have encountered it several times during this chapter.\\nThe Treebank corpus can be accessed using:\\nfor sent in nltk.corpus.treebank_chunk.chunked_sents(fileid).  These are flat trees,\\njust as we got using nltk.corpus.conll2000.chunked_sents().\\nThe functions nltk.tree.pprint() and nltk.chunk.tree2conllstr()\\ncan be used to create Treebank and IOB strings from a tree.\\nWrite functions chunk2brackets() and chunk2iob() that take a single\\nchunk tree as their sole argument, and return the required multi-line string\\nrepresentation.\\nWrite command-line conversion utilities bracket2iob.py and iob2bracket.py\\nthat take a file in Treebank or CoNLL format (resp) and convert it to the other\\nformat.  (Obtain some raw Treebank or CoNLL data from the NLTK Corpora, save it\\nto a file, and then use for line in open(filename) to access it from Python.)\\n\\n\\n★\\nAn n-gram chunker can use information other than the current\\npart-of-speech tag and the n-1 previous chunk tags.\\nInvestigate other models of the context, such as\\nthe n-1 previous part-of-speech tags, or some combination of\\nprevious chunk tags along with previous and following part-of-speech tags.\\n★\\nConsider the way an n-gram tagger uses recent tags to inform its tagging choice.\\nNow observe how a chunker may re-use this sequence information.  For example,\\nboth tasks will make use of the information that nouns tend to follow adjectives\\n(in English).  It would appear that the same information is being maintained in\\ntwo places.  Is this likely to become a problem as the size of the rule sets grows?\\nIf so, speculate about any ways that this problem might be addressed.\\n\\n\\n\\nAbout this document...\\nUPDATED FOR NLTK 3.0.\\nThis is a chapter from Natural Language Processing with Python,\\nby Steven Bird, Ewan Klein and Edward Loper,\\nCopyright © 2019 the authors.\\nIt is distributed with the Natural Language Toolkit [http://nltk.org/],\\nVersion 3.0, under the terms of the\\nCreative Commons Attribution-Noncommercial-No Derivative Works 3.0 United States License\\n[http://creativecommons.org/licenses/by-nc-nd/3.0/us/].\\nThis document was built on\\nWed  4 Sep 2019 11:40:48 ACST\\n\\n<!-- Not used anymore:\\nIdentifying the boundaries of specific types of word sequences is also\\nrequired when we want to recognize pieces of syntactic\\nstructure. Suppose for example that as a preliminary to named entity\\nrecognition, we have decided that it would be useful to just pick out\\nnoun phrases from a piece of text. To carry this out in a complete\\nway, we would probably want to use a proper syntactic parser. But\\nparsing can be quite challenging and computationally expensive |mdash|\\nis there an easier alternative? The answer is Yes: we can look for\\nsequences of part-of-speech tags in a tagged text, using one or more\\npatterns that capture the typical ingredients of a noun phrase. -->\\n\\n\\n\\n\\n',\n",
       " '\\n\\n\\n\\nfunction astext(node)\\n{\\n    return node.innerHTML.replace(/(]+)>)/ig,\"\")\\n                         .replace(/&gt;/ig, \">\")\\n                         .replace(/&lt;/ig, \"<\")\\n                         .replace(/&quot;/ig, \\'\"\\')\\n                         .replace(/&amp;/ig, \"&\");\\n}\\n\\nfunction copy_notify(node, bar_color, data)\\n{\\n    // The outer box: relative + inline positioning.\\n    var box1 = document.createElement(\"div\");\\n    box1.style.position = \"relative\";\\n    box1.style.display = \"inline\";\\n    box1.style.top = \"2em\";\\n    box1.style.left = \"1em\";\\n  \\n    // A shadow for fun\\n    var shadow = document.createElement(\"div\");\\n    shadow.style.position = \"absolute\";\\n    shadow.style.left = \"-1.3em\";\\n    shadow.style.top = \"-1.3em\";\\n    shadow.style.background = \"#404040\";\\n    \\n    // The inner box: absolute positioning.\\n    var box2 = document.createElement(\"div\");\\n    box2.style.position = \"relative\";\\n    box2.style.border = \"1px solid #a0a0a0\";\\n    box2.style.left = \"-.2em\";\\n    box2.style.top = \"-.2em\";\\n    box2.style.background = \"white\";\\n    box2.style.padding = \".3em .4em .3em .4em\";\\n    box2.style.fontStyle = \"normal\";\\n    box2.style.background = \"#f0e0e0\";\\n\\n    node.insertBefore(box1, node.childNodes.item(0));\\n    box1.appendChild(shadow);\\n    shadow.appendChild(box2);\\n    box2.innerHTML=\"Copied&nbsp;to&nbsp;the&nbsp;clipboard: \" +\\n                   \"\"+\\n                   data+\"\";\\n    setTimeout(function() { node.removeChild(box1); }, 1000);\\n\\n    var elt = node.parentNode.firstChild;\\n    elt.style.background = \"#ffc0c0\";\\n    setTimeout(function() { elt.style.background = bar_color; }, 200);\\n}\\n\\nfunction copy_codeblock_to_clipboard(node)\\n{\\n    var data = astext(node)+\"\\\\n\";\\n    if (copy_text_to_clipboard(data)) {\\n        copy_notify(node, \"#40a060\", data);\\n    }\\n}\\n\\nfunction copy_doctest_to_clipboard(node)\\n{\\n    var s = astext(node)+\"\\\\n   \";\\n    var data = \"\";\\n\\n    var start = 0;\\n    var end = s.indexOf(\"\\\\n\");\\n    while (end >= 0) {\\n        if (s.substring(start, start+4) == \">>> \") {\\n            data += s.substring(start+4, end+1);\\n        }\\n        else if (s.substring(start, start+4) == \"... \") {\\n            data += s.substring(start+4, end+1);\\n        }\\n        /*\\n        else if (end-start > 1) {\\n            data += \"# \" + s.substring(start, end+1);\\n        }*/\\n        // Grab the next line.\\n        start = end+1;\\n        end = s.indexOf(\"\\\\n\", start);\\n    }\\n    \\n    if (copy_text_to_clipboard(data)) {\\n        copy_notify(node, \"#4060a0\", data);\\n    }\\n}\\n    \\nfunction copy_text_to_clipboard(data)\\n{\\n    if (window.clipboardData) {\\n        window.clipboardData.setData(\"Text\", data);\\n        return true;\\n     }\\n    else if (window.netscape) {\\n        // w/ default firefox settings, permission will be denied for this:\\n        netscape.security.PrivilegeManager\\n                      .enablePrivilege(\"UniversalXPConnect\");\\n    \\n        var clip = Components.classes[\"@mozilla.org/widget/clipboard;1\"]\\n                      .createInstance(Components.interfaces.nsIClipboard);\\n        if (!clip) return;\\n    \\n        var trans = Components.classes[\"@mozilla.org/widget/transferable;1\"]\\n                       .createInstance(Components.interfaces.nsITransferable);\\n        if (!trans) return;\\n    \\n        trans.addDataFlavor(\"text/unicode\");\\n    \\n        var str = new Object();\\n        var len = new Object();\\n    \\n        var str = Components.classes[\"@mozilla.org/supports-string;1\"]\\n                     .createInstance(Components.interfaces.nsISupportsString);\\n        var datacopy=data;\\n        str.data=datacopy;\\n        trans.setTransferData(\"text/unicode\",str,datacopy.length*2);\\n        var clipid=Components.interfaces.nsIClipboard;\\n    \\n        if (!clip) return false;\\n    \\n        clip.setData(trans,null,clipid.kGlobalClipboard);\\n        return true;\\n    }\\n    return false;\\n}\\n//-->\\n\\n\\n\\n\\n\\n\\n8. Analyzing Sentence Structure\\n\\n\\n/*\\n:Author: Edward Loper, James Curran\\n:Copyright: This stylesheet has been placed in the public domain.\\n\\nStylesheet for use with Docutils.\\n\\nThis stylesheet defines new css classes used by NLTK.\\n\\nIt uses a Python syntax highlighting scheme that matches\\nthe colour scheme used by IDLE, which makes it easier for\\nbeginners to check they are typing things in correctly.\\n*/\\n\\n/* Include the standard docutils stylesheet. */\\n@import url(default.css);\\n\\n/* Custom inline roles */\\nspan.placeholder    { font-style: italic; font-family: monospace; }\\nspan.example        { font-style: italic; }\\nspan.emphasis       { font-style: italic; }\\nspan.termdef        { font-weight: bold; }\\n/*span.term           { font-style: italic; }*/\\nspan.category       { font-variant: small-caps; }\\nspan.feature        { font-variant: small-caps; }\\nspan.fval           { font-style: italic; }\\nspan.math           { font-style: italic; }\\nspan.mathit         { font-style: italic; }\\nspan.lex            { font-variant: small-caps; }\\nspan.guide-linecount{ text-align: right; display: block;}\\n\\n/* Python souce code listings */\\nspan.pysrc-prompt   { color: #9b0000; }\\nspan.pysrc-more     { color: #9b00ff; }\\nspan.pysrc-keyword  { color: #e06000; }\\nspan.pysrc-builtin  { color: #940094; }\\nspan.pysrc-string   { color: #00aa00; }\\nspan.pysrc-comment  { color: #ff0000; }\\nspan.pysrc-output   { color: #0000ff; }\\nspan.pysrc-except   { color: #ff0000; }\\nspan.pysrc-defname  { color: #008080; }\\n\\n\\n/* Doctest blocks */\\npre.doctest         { margin: 0; padding: 0; font-weight: bold; }\\ndiv.doctest         { margin: 0 1em 1em 1em; padding: 0; }\\ntable.doctest       { margin: 0; padding: 0;\\n                      border-top: 1px solid gray;\\n                      border-bottom: 1px solid gray; }\\npre.copy-notify     { margin: 0; padding: 0.2em; font-weight: bold;\\n                      background-color: #ffffff; }\\n\\n/* Python source listings */\\ndiv.pylisting       { margin: 0 1em 1em 1em; padding: 0; }\\ntable.pylisting     { margin: 0; padding: 0;\\n                      border-top: 1px solid gray; }\\ntd.caption { border-top: 1px solid black; margin: 0; padding: 0; }\\n.caption-label { font-weight: bold;  }\\ntd.caption p { margin: 0; padding: 0; font-style: normal;}\\n\\ntable tr td.codeblock { \\n  padding: 0.2em ! important; margin: 0;\\n  border-left: 1px solid gray;\\n  border-right: 2px solid gray;\\n  border-top: 0px solid gray;\\n  border-bottom: 1px solid gray;\\n  font-weight: bold; background-color: #eeffee;\\n}\\ntable pre span {\\n  white-space: pre-wrap;\\n}\\ntable tr td.doctest  { \\n  padding: 0.2em; margin: 0;\\n  border-left: 1px solid gray;\\n  border-right: 2px solid gray;\\n  border-top: 0px solid gray;\\n  border-bottom: 1px solid gray;\\n  font-weight: bold; background-color: #eeeeff;\\n}\\n\\ntd.codeblock table tr td.copybar {\\n    background: #40a060; border: 1px solid gray;\\n    font-family: monospace; padding: 0; margin: 0; }\\ntd.doctest table tr td.copybar {\\n    background: #4060a0; border: 1px solid gray;\\n    font-family: monospace; padding: 0; margin: 0; }\\n\\ntd.pysrc { padding-left: 0.5em; }\\n\\nimg.callout { border-width: 0px; }\\n\\ntable.docutils {\\n    border-style: solid;\\n    border-width: 1px;\\n    margin-top: 6px;\\n    border-color: grey;\\n    border-collapse: collapse; }\\n\\ntable.docutils th {\\n    border-style: none;\\n    border-width: 1px;\\n    border-color: grey;\\n    padding: 0 .5em 0 .5em; }\\n\\ntable.docutils td {\\n    border-style: none;\\n    border-width: 1px;\\n    border-color: grey; \\n    padding: 0 .5em 0 .5em; }\\n\\ntable.footnote td { padding: 0; }\\ntable.footnote { border-width: 0; }\\ntable.footnote td { border-width: 0; }\\ntable.footnote th { border-width: 0; }\\n\\ntable.noborder { border-width: 0; }\\n\\ntable.example pre { margin-top: 4px; margin-bottom: 0; }\\n\\n/* For figures & tables */\\np.caption { margin-bottom: 0; }\\ndiv.figure { text-align: center; }\\n\\n/* The index */\\ndiv.index { border: 1px solid black;\\n            background-color: #eeeeee; }\\ndiv.index h1 { padding-left: 0.5em; margin-top: 0.5ex;\\n               border-bottom: 1px solid black; }\\nul.index { margin-left: 0.5em; padding-left: 0; }\\nli.index { list-style-type: none; }\\np.index-heading { font-size: 120%; font-style: italic; margin: 0; }\\nli.index ul { margin-left: 2em; padding-left: 0; }\\n\\n/* \\'Note\\' callouts */\\ndiv.note\\n{\\n  border-right:   #87ceeb 1px solid;\\n  padding-right: 4px;\\n  border-top: #87ceeb 1px solid;\\n  padding-left: 4px;\\n  padding-bottom: 4px;\\n  margin: 2px 5% 10px;\\n  border-left: #87ceeb 1px solid;\\n  padding-top: 4px;\\n  border-bottom: #87ceeb 1px solid;\\n  font-style: normal;\\n  font-family: verdana, arial;\\n  background-color: #b0c4de;\\n}\\n\\ntable.avm { border: 0px solid black; width: 0; }\\ntable.avm tbody tr {border: 0px solid black; }\\ntable.avm tbody tr td { padding: 2px; }\\ntable.avm tbody tr td.avm-key { padding: 5px; font-variant: small-caps; }\\ntable.avm tbody tr td.avm-eq { padding: 5px; }\\ntable.avm tbody tr td.avm-val { padding: 5px; font-style: italic; }\\np.avm-empty { font-style: normal; }\\ntable.avm colgroup col { border: 0px solid black; }\\ntable.avm tbody tr td.avm-topleft \\n    { border-left: 2px solid #000080; border-top: 2px solid #000080; }\\ntable.avm tbody tr td.avm-botleft \\n    { border-left: 2px solid #000080; border-bottom: 2px solid #000080; }\\ntable.avm tbody tr td.avm-topright\\n    { border-right: 2px solid #000080; border-top: 2px solid #000080; }\\ntable.avm tbody tr td.avm-botright\\n    { border-right: 2px solid #000080; border-bottom: 2px solid #000080; }\\ntable.avm tbody tr td.avm-left\\n    { border-left: 2px solid #000080; }\\ntable.avm tbody tr td.avm-right\\n    { border-right: 2px solid #000080; }\\ntable.avm tbody tr td.avm-topbotleft\\n    { border: 2px solid #000080; border-right: 0px solid black; }\\ntable.avm tbody tr td.avm-topbotright\\n    { border: 2px solid #000080; border-left: 0px solid black; }\\ntable.avm tbody tr td.avm-ident\\n    { font-size: 80%; padding: 0; padding-left: 2px; vertical-align: top; }\\n.avm-pointer\\n{ border: 1px solid #008000; padding: 1px; color: #008000; \\n  background: #c0ffc0; font-style: normal; }\\n\\ntable.gloss { border: 0px solid black; width: 0; }\\ntable.gloss tbody tr { border: 0px solid black; }\\ntable.gloss tbody tr td { border: 0px solid black; }\\ntable.gloss colgroup col { border: 0px solid black; }\\ntable.gloss p { margin: 0; padding: 0; }\\n\\ntable.rst-example { border: 1px solid black; }\\ntable.rst-example tbody tr td { background: #eeeeee; }\\ntable.rst-example thead tr th { background: #c0ffff; }\\ntd.rst-raw { width: 0; }\\n\\n/* Used by nltk.org/doc/test: */\\ndiv.doctest-list { text-align: center; }\\ntable.doctest-list { border: 1px solid black;\\n  margin-left: auto; margin-right: auto;\\n}\\ntable.doctest-list tbody tr td { background: #eeeeee;\\n  border: 1px solid #cccccc; text-align: left; }\\ntable.doctest-list thead tr th { background: #304050; color: #ffffff;\\n  border: 1px solid #000000;}\\ntable.doctest-list thead tr a { color: #ffffff; }\\nspan.doctest-passed { color: #008000; }\\nspan.doctest-failed { color: #800000; }\\n\\n\\n\\n\\n\\n\\n8. Analyzing Sentence Structure\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n<!-- Grammatical Category - e.g. NP and verb as technical terms\\n.. role:: gc\\n   :class: category -->\\n\\n\\n\\n\\n\\n\\n\\n\\n<!-- standard global imports\\n\\n>>> from __future__ import division\\n>>> import nltk, re, pprint -->\\n\\n\\n\\n<!-- TODO: give an example of a text generated from bigrams, then talk the\\nreader through constructing a simple grammar over this by talking\\nabout local contexts, and contrast this with the grammar -->\\n NP PP replaced\\nby Nom -> Nom PP -->\\n\\n\\nEarlier chapters focused on words: how to identify them,\\nanalyze their structure, assign them to lexical categories,\\nand access their meanings.\\nWe have also seen how to identify patterns in word sequences or n-grams.\\nHowever, these methods only scratch the surface of the complex constraints\\nthat govern sentences.\\nWe need a way to deal with the ambiguity that natural language is famous for.\\nWe also need to be able to cope with the fact that there are an unlimited number\\nof possible sentences, and we can only write finite programs to analyze their\\nstructures and discover their meanings.\\nThe goal of this chapter is to answer the following questions:\\n\\nHow can we use a formal grammar to describe the structure of an unlimited set of sentences?\\nHow do we represent the structure of sentences using syntax trees?\\nHow do parsers analyze a sentence and automatically build a syntax tree?\\n\\nAlong the way, we will cover the fundamentals of English syntax, and\\nsee that there are systematic aspects of meaning that are much easier\\nto capture once we have identified the structure of sentences.\\n\\n1&nbsp;&nbsp;&nbsp;Some Grammatical Dilemmas\\n\\n1.1&nbsp;&nbsp;&nbsp;Linguistic Data and Unlimited Possibilities\\nPrevious chapters have shown you how to process and analyse text\\ncorpora, and we have stressed the challenges for NLP in dealing with\\nthe vast amount of electronic language data that is growing\\ndaily. Let\\'s consider this data more closely, and make the thought\\nexperiment that we have a gigantic corpus consisting of everything\\nthat has been either uttered or written in English over, say, the last\\n50 years. Would we be justified in calling this corpus \"the language\\nof modern English\"? There are a number of reasons why we might answer\\nNo. Recall that in 3, we asked you to search\\nthe web for instances of the pattern the of.  Although it is\\neasy to find examples on the web containing this word sequence, such as\\nNew man at the of IMG\\n(http://www.telegraph.co.uk/sport/2387900/New-man-at-the-of-IMG.html),\\nspeakers of English will say that most such examples are errors, and\\ntherefore not part of English after all.\\nAccordingly, we can argue\\nthat the \"modern English\" is not equivalent to the very big\\nset of word sequences in our imaginary corpus. Speakers\\nof English can make judgements about these sequences, and will reject\\nsome of them as being ungrammatical.\\nEqually, it is easy to compose a new sentence and have speakers agree that it is perfectly\\ngood English.  For example, sentences have an interesting property\\nthat they can be embedded inside larger sentences.  Consider the\\nfollowing sentences:\\n\\n  (1)\\n  a.Usain Bolt broke the 100m record\\n\\n  b.The Jamaica Observer reported that Usain Bolt broke the 100m record\\n\\n  c.Andre said The Jamaica Observer reported that Usain Bolt broke the 100m record\\n\\n  d.I think Andre said the Jamaica Observer reported that Usain Bolt broke the 100m record\\n\\nIf we replaced whole sentences with the symbol S, we would see patterns like\\nAndre said S and I think S.  These are templates for taking a sentence\\nand constructing a bigger sentence.  There are other templates we can use, like\\nS but S, and S when S.  With a bit of ingenuity we can\\nconstruct some really long sentences using these templates.\\nHere\\'s an impressive example from a Winnie the Pooh story by A.A. Milne,\\nIn which Piglet is Entirely Surrounded by Water:\\n\\n[You can imagine Piglet\\'s joy when at last the ship came in sight of\\nhim.] In after-years he liked to think that he had been in Very\\nGreat Danger during the Terrible Flood, but the only danger he had\\nreally been in was the last half-hour of his imprisonment, when\\nOwl, who had just flown up, sat on a branch of his tree to comfort\\nhim, and told him a very long story about an aunt who had once laid\\na seagull\\'s egg by mistake, and the story went on and on, rather\\nlike this sentence, until Piglet who was listening out of his\\nwindow without much hope, went to sleep quietly and naturally,\\nslipping slowly out of the window towards the water until he was\\nonly hanging on by his toes, at which moment, luckily, a sudden\\nloud squawk from Owl, which was really part of the story, being\\nwhat his aunt said, woke the Piglet up and just gave him time to\\njerk himself back into safety and say, \"How interesting, and did\\nshe?\" when — well, you can imagine his joy when at last he saw\\nthe good ship, Brain of Pooh (Captain, C. Robin; 1st Mate, P. Bear)\\ncoming over the sea to rescue him...\\nThis long sentence actually has a simple structure that begins\\nS but S when S.  We can see from this example that language\\nprovides us with constructions which seem to allow us to extend\\nsentences indefinitely.  It is also striking that\\nwe can understand sentences of arbitrary length\\nthat we\\'ve never heard before:  it\\'s not hard to concoct an\\nentirely novel sentence, one that has probably never been used before\\nin the history of the language, yet all speakers of the language\\nwill understand it.\\nThe purpose of a grammar is to give an explicit description of a\\nlanguage. But the way in which we think of a grammar is closely\\nintertwined with what we consider to be a language. Is it a\\nlarge but finite set of observed utterances and written texts? Is it\\nsomething more abstract like the implicit knowledge that competent\\nspeakers have about grammatical sentences? Or is it some combination\\nof the two? We won\\'t take a stand on this issue, but instead will\\nintroduce the main approaches.\\nIn this chapter, we will adopt the formal framework\\nof \"generative grammar\", in which\\na \"language\" is considered to be nothing more than an\\nenormous collection of all grammatical sentences, and a\\ngrammar is a formal notation that can be used for \"generating\" the\\nmembers of this set.  Grammars use recursive productions\\nof the form S → S and S, as we will explore in\\n3.  In 10. we will extend this,\\nto automatically build up the meaning of a sentence out of the meanings\\nof its parts.\\n\\n\\n1.2&nbsp;&nbsp;&nbsp;Ubiquitous Ambiguity\\nA well-known example of ambiguity is shown in (2),\\nfrom the Groucho Marx movie, Animal Crackers (1930):\\n\\n\\n  (2)While hunting in Africa, I shot an elephant in my pajamas.\\nHow he got into my pajamas, I don\\'t know.\\nLet\\'s take a closer look at the ambiguity in the phrase:\\nI shot an elephant in my pajamas.  First we\\nneed to define a simple grammar:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; groucho_grammar = nltk.CFG.fromstring(\"\"\"\\n... S -&gt; NP VP\\n... PP -&gt; P NP\\n... NP -&gt; Det N | Det N PP | \\'I\\'\\n... VP -&gt; V NP | VP PP\\n... Det -&gt; \\'an\\' | \\'my\\'\\n... N -&gt; \\'elephant\\' | \\'pajamas\\'\\n... V -&gt; \\'shot\\'\\n... P -&gt; \\'in\\'\\n... \"\"\")\\n\\n\\n\\nThis grammar permits the sentence to be analyzed in two ways,\\ndepending on whether the prepositional phrase in my pajamas\\ndescribes the elephant or the shooting event.\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; sent = [\\'I\\', \\'shot\\', \\'an\\', \\'elephant\\', \\'in\\', \\'my\\', \\'pajamas\\']\\n&gt;&gt;&gt; parser = nltk.ChartParser(groucho_grammar)\\n&gt;&gt;&gt; for tree in parser.parse(sent):\\n...     print(tree)\\n...\\n(S\\n  (NP I)\\n  (VP\\n    (VP (V shot) (NP (Det an) (N elephant)))\\n    (PP (P in) (NP (Det my) (N pajamas)))))\\n(S\\n  (NP I)\\n  (VP\\n    (V shot)\\n    (NP (Det an) (N elephant) (PP (P in) (NP (Det my) (N pajamas))))))\\n\\n\\n\\nThe program produces two bracketed structures, which we can depict as\\ntrees, as shown in (3b):\\n\\n  (3)\\n  a.\\n\\n  b.\\n\\nNotice that there\\'s no ambiguity concerning the meaning of any of the words;\\ne.g. the word shot doesn\\'t refer to the act of using a gun in the first sentence,\\nand using a camera in the second sentence.\\n\\nNote\\nYour Turn:\\nConsider the following sentences and see if you can think of two quite different\\ninterpretations: Fighting animals could be dangerous.\\nVisiting relatives can be tiresome.  Is ambiguity of the individual\\nwords to blame?  If not, what is the cause of the ambiguity?\\n\\nThis chapter presents grammars and parsing, as the formal and\\ncomputational methods for investigating and modeling the linguistic\\nphenomena we have been discussing.\\nAs we shall see, patterns of well-formedness and ill-formedness in a\\nsequence of words can be understood with respect to the\\nphrase structure and dependencies.  We can develop formal\\nmodels of these structures using grammars and parsers.\\nAs before, a key motivation is natural language understanding.  How\\nmuch more of the meaning of a text can we access when we can reliably\\nrecognize the linguistic structures it contains?  Having read in a\\ntext, can a program \"understand\" it enough to be able to answer simple\\nquestions about \"what happened\" or \"who did what to whom\"?  Also as\\nbefore, we will develop simple programs to process annotated corpora\\nand perform useful tasks.\\n\\n\\n\\n2&nbsp;&nbsp;&nbsp;What\\'s the Use of Syntax?\\n\\n2.1&nbsp;&nbsp;&nbsp;Beyond n-grams\\nWe gave an example in 2. of how to use\\nthe frequency information in bigrams to generate text that seems\\nperfectly acceptable for small sequences of words but rapidly\\ndegenerates into nonsense. Here\\'s another pair of examples that we created by\\ncomputing the bigrams over the text of a childrens\\' story, The\\nAdventures of Buster Brown (http://www.gutenberg.org/files/22816/22816.txt):\\n\\n  (4)\\n  a.He roared with me the pail slip down his back\\n\\n  b.The worst part and clumsy looking for whoever heard light\\n\\nYou intuitively know that these sequences are \"word-salad\", but you\\nprobably find it hard to pin down what\\'s wrong with them. One\\nbenefit of studying grammar is that it provides a conceptual framework\\nand vocabulary for spelling out these intuitions. Let\\'s take a closer look\\nat the sequence the worst part and clumsy looking. This looks like a coordinate\\nstructure, where two phrases are joined by a coordinating\\nconjunction such as and, but or or. Here\\'s an\\ninformal (and simplified) statement of how coordination works\\nsyntactically:\\nCoordinate Structure:\\n\\nIf v1 and v2 are both phrases of grammatical\\ncategory X, then v1 and v2 is also a\\nphrase of category  X.\\nHere are a couple of examples. In the first, two NPs (noun\\nphrases) have been conjoined to make an NP, while in the second,\\ntwo APs (adjective phrases) have been conjoined to make an\\nAP.\\n\\n  (5)\\n  a.The book\\'s ending was (NP the worst part and the best part) for me.\\n\\n  b.On land they are (AP slow and clumsy looking).\\n\\nWhat we can\\'t do is conjoin an NP and an AP, which is\\nwhy the worst part and clumsy looking is ungrammatical.\\nBefore we can formalize these ideas, we need to\\nunderstand the concept of constituent structure.\\nConstituent structure is based on the observation that words combine\\nwith other words to form units. The evidence that a sequence of words\\nforms such a unit is given by substitutability — that is, a\\nsequence of words in a well-formed sentence can be replaced by a\\nshorter sequence without rendering the sentence ill-formed. To clarify\\nthis idea, consider the following sentence:\\n\\n  (6)The little bear saw the fine fat trout in the brook.\\nThe fact that we can substitute He for The little bear\\nindicates that the latter sequence is a unit. By contrast, we cannot\\nreplace  little bear saw in the same way.\\n\\n\\n  (7)\\n  a.He saw the fine fat trout in the brook.\\n\\n  b.*The he the fine fat trout in the brook.\\n\\nIn 2.1, we systematically substitute longer sequences\\nby shorter ones in a way which preserves grammaticality. Each sequence\\nthat forms a unit can in fact be replaced by a single word, and we end\\nup with just two elements.\\n\\n\\nFigure 2.1: Substitution of Word Sequences: working from the top row, we can replace\\nparticular sequences of words (e.g. the brook) with individual\\nwords (e.g. it); repeating this process we arrive at a grammatical\\ntwo-word sentence.\\n\\nIn 2.2, we have added\\ngrammatical category labels to the words we saw in the earlier figure.\\nThe labels NP, VP, and PP stand for noun phrase,\\nverb phrase and prepositional phrase respectively.\\n\\n\\nFigure 2.2: Substitution of Word Sequences Plus Grammatical Categories:\\nThis diagram reproduces 2.1 along with grammatical\\ncategories corresponding to noun phrases (NP), verb phrases (VP),\\nprepositional phrases (PP), and nominals (Nom).\\n\\nIf we now strip out the words apart from the topmost row, add an\\nS node, and flip the figure over, we end up with a standard\\nphrase structure tree, shown in (8).\\nEach node in this tree (including the words) is called\\na constituent.  The immediate constituents of\\nS are NP and VP.\\n\\n  (8)\\nAs we will see in the next section, a grammar specifies how the sentence\\ncan be subdivided into its immediate constituents, and how these can be further\\nsubdivided until we reach the level of individual words.\\n\\nNote\\nAs we saw in 1, sentences can have arbitrary length.\\nConsequently, phrase structure trees can have arbitrary depth.\\nThe cascaded chunk parsers we saw in 4\\ncan only produce structures of bounded depth, so chunking methods\\naren\\'t applicable here.\\n\\n\\n\\n\\n3&nbsp;&nbsp;&nbsp;Context Free Grammar\\n\\n3.1&nbsp;&nbsp;&nbsp;A Simple Grammar\\n\\nLet\\'s start off by looking at a simple context-free grammar.  By\\nconvention, the left-hand-side of the first production is the\\nstart-symbol of the grammar, typically S, and all\\nwell-formed trees must have this symbol as their root label. In\\nNLTK, context-free grammars are defined in the nltk.grammar\\nmodule.  In 3.1 we define a grammar and show how to parse a\\nsimple sentence admitted by the grammar.\\n\\n\\n\\n\\n&nbsp;\\ngrammar1 = nltk.CFG.fromstring(\"\"\"\\n  S -&gt; NP VP\\n  VP -&gt; V NP | V NP PP\\n  PP -&gt; P NP\\n  V -&gt; \"saw\" | \"ate\" | \"walked\"\\n  NP -&gt; \"John\" | \"Mary\" | \"Bob\" | Det N | Det N PP\\n  Det -&gt; \"a\" | \"an\" | \"the\" | \"my\"\\n  N -&gt; \"man\" | \"dog\" | \"cat\" | \"telescope\" | \"park\"\\n  P -&gt; \"in\" | \"on\" | \"by\" | \"with\"\\n  \"\"\")\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; sent = \"Mary saw Bob\".split()\\n&gt;&gt;&gt; rd_parser = nltk.RecursiveDescentParser(grammar1)\\n&gt;&gt;&gt; for tree in rd_parser.parse(sent):\\n...      print(tree)\\n(S (NP Mary) (VP (V saw) (NP Bob)))\\n\\n\\nExample 3.1 (code_cfg1.py): Figure 3.1: A Simple Context-Free Grammar\\n\\nThe grammar in 3.1 contains productions involving various syntactic categories,\\nas laid out in 3.1.\\nTable 3.1: Syntactic Categories\\n\\n\\n\\n\\n\\n\\nSymbol\\nMeaning\\nExample\\n\\n\\n\\nS\\nsentence\\nthe man walked\\n\\nNP\\nnoun phrase\\na dog\\n\\nVP\\nverb phrase\\nsaw a park\\n\\nPP\\nprepositional phrase\\nwith a telescope\\n\\nDet\\ndeterminer\\nthe\\n\\nN\\nnoun\\ndog\\n\\nV\\nverb\\nwalked\\n\\nP\\npreposition\\nin\\n\\n\\n\\n\\n\\nA production like VP -&gt; V NP | V NP PP has a disjunction on the\\nrighthand side, shown by the | and is an abbreviation for the two productions\\nVP -&gt; V NP and VP -&gt; V NP PP.\\n\\n\\nFigure 3.2: Recursive Descent Parser Demo: This tool allows you to watch the operation of\\na recursive descent parser as it grows the parse tree and matches it against\\nthe input words.\\n\\n\\nNote\\nYour Turn:\\nTry developing a simple grammar of your own, using the\\nrecursive descent parser application, nltk.app.rdparser(),\\nshown in 3.2.\\nIt comes already loaded with a sample grammar, but you can\\nedit this as you please (using the Edit menu).\\nChange the grammar, and the sentence to be parsed, and\\nrun the parser using the autostep button.\\n\\nIf we parse the sentence The dog saw a man in the park using\\nthe grammar shown in 3.1, we end up with two trees, similar to\\nthose we saw for (3b):\\n\\n  (9)\\n  a.\\n\\n  b.\\n\\nSince our grammar licenses two trees for this sentence, the sentence is\\nsaid to be structurally ambiguous.  The ambiguity in question is called\\na prepositional phrase attachment ambiguity, as we saw earlier in this chapter.\\nAs you may recall, it is an ambiguity about attachment since the\\nPP in the park needs to be attached to one of two places\\nin the tree: either as a child of VP or else as a child of NP.\\nWhen the PP is attached to VP, the intended interpretation\\nis that the seeing event happened\\nin the park.  However, if the PP is attached to NP,\\nthen it was the man who was in the park, and the agent of the seeing (the dog)\\nmight have been sitting on the balcony of an apartment overlooking the\\npark.\\n\\n\\n3.2&nbsp;&nbsp;&nbsp;Writing Your Own Grammars\\nIf you are interested in experimenting with writing CFGs, you will\\nfind it helpful to create and edit your grammar in a text\\nfile, say mygrammar.cfg. You can then load it into NLTK and\\nparse with it as follows:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; grammar1 = nltk.data.load(\\'file:mygrammar.cfg\\')\\n&gt;&gt;&gt; sent = \"Mary saw Bob\".split()\\n&gt;&gt;&gt; rd_parser = nltk.RecursiveDescentParser(grammar1)\\n&gt;&gt;&gt; for tree in rd_parser.parse(sent):\\n...      print(tree)\\n\\n\\n\\nMake sure that you put a .cfg suffix on the filename, and that\\nthere are no spaces in the string \\'file:mygrammar.cfg\\'. If the\\ncommand print(tree) produces no output, this is probably because\\nyour sentence sent is not admitted by your grammar. In this case,\\ncall the parser with tracing set to be on: rd_parser =\\nnltk.RecursiveDescentParser(grammar1, trace=2). You can also check\\nwhat productions are currently in the grammar with the command for p\\nin grammar1.productions(): print(p).\\nWhen you write CFGs for parsing in NLTK, you cannot combine\\ngrammatical categories with lexical items on the righthand side of the\\nsame production. Thus, a production such as PP -&gt; \\'of\\' NP is disallowed. In\\naddition, you are not permitted to place multi-word lexical items on the\\nrighthand side of a production. So rather than writing NP -&gt; \\'New\\nYork\\', you have to resort to something like NP -&gt; \\'New_York\\'\\ninstead.\\n\\n\\n3.3&nbsp;&nbsp;&nbsp;Recursion in Syntactic Structure\\nA grammar is said to be recursive if a category occurring on the left hand\\nside of a production also appears on\\nthe righthand side of a production, as illustrated in 3.3.\\nThe production Nom -&gt; Adj Nom (where Nom is the\\ncategory of nominals) involves direct recursion on the category\\nNom, whereas indirect recursion on S arises from the\\ncombination of two productions, namely S -&gt; NP VP and VP -&gt; V S.\\n\\n\\n\\n\\n&nbsp;\\ngrammar2 = nltk.CFG.fromstring(\"\"\"\\n  S  -&gt; NP VP\\n  NP -&gt; Det Nom | PropN\\n  Nom -&gt; Adj Nom | N\\n  VP -&gt; V Adj | V NP | V S | V NP PP\\n  PP -&gt; P NP\\n  PropN -&gt; \\'Buster\\' | \\'Chatterer\\' | \\'Joe\\'\\n  Det -&gt; \\'the\\' | \\'a\\'\\n  N -&gt; \\'bear\\' | \\'squirrel\\' | \\'tree\\' | \\'fish\\' | \\'log\\'\\n  Adj  -&gt; \\'angry\\' | \\'frightened\\' |  \\'little\\' | \\'tall\\'\\n  V -&gt;  \\'chased\\'  | \\'saw\\' | \\'said\\' | \\'thought\\' | \\'was\\' | \\'put\\'\\n  P -&gt; \\'on\\'\\n  \"\"\")\\n\\n\\nExample 3.3 (code_cfg2.py): Figure 3.3: A Recursive Context-Free Grammar\\n\\nTo see how recursion arises from this grammar, consider the following\\ntrees.  (10a) involves nested nominal phrases,\\nwhile (10b) contains nested sentences.\\n\\n  (10)\\n  a.\\n\\n  b.\\n\\nWe\\'ve only illustrated two levels of recursion here, but there\\'s\\nno upper limit on the depth.  You can experiment with parsing\\nsentences that involve more deeply nested structures.\\nBeware that the RecursiveDescentParser is unable to handle\\nleft-recursive productions of the form X -&gt; X Y; we will\\nreturn to this in 4.\\n\\n\\n\\n4&nbsp;&nbsp;&nbsp;Parsing With Context Free Grammar\\n>> grammar1 = nltk.CFG.fromstring(\"\"\"\\n...     S -> NP VP\\n...     VP -> V NP | V NP PP\\n...     PP -> P NP\\n...     V -> \"saw\" | \"ate\" | \"walked\"\\n...     NP -> \"John\" | \"Mary\" | \"Bob\" | Det N | Det N PP\\n...     Det -> \"a\" | \"an\" | \"the\" | \"my\"\\n...     N -> \"man\" | \"dog\" | \"cat\" | \"telescope\" | \"park\"\\n...     P -> \"in\" | \"on\" | \"by\" | \"with\"\\n...     \"\"\")\\n>>> groucho_grammar = nltk.CFG.fromstring(\"\"\"\\n... S -> NP VP\\n... PP -> P NP\\n... NP -> Det N | Det N PP | \\'I\\'\\n... VP -> V NP | VP PP\\n... Det -> \\'an\\' | \\'my\\'\\n... N -> \\'elephant\\' | \\'pajamas\\'\\n... V -> \\'shot\\'\\n... P -> \\'in\\'\\n... \"\"\") -->\\nA parser processes input sentences according to the\\nproductions of a grammar, and builds one or more\\nconstituent structures that conform to the grammar.\\nA grammar is a declarative specification of well-formedness —\\nit is actually just a string, not a program.\\nA parser is a procedural interpretation of the grammar.\\nIt searches through the space of trees licensed by a grammar\\nto find one that has the required sentence along its fringe.\\n<!-- XXX does the following read as though our audience does not consist\\nof people who want to do the tasks described in this chapter? -->\\nA parser permits a grammar to be evaluated against\\na collection of test sentences, helping linguists\\nto discover mistakes in their grammatical analysis.\\nA parser can serve as a model of psycholinguistic processing,\\nhelping to explain the difficulties that humans have with processing\\ncertain syntactic constructions.\\nMany natural language applications involve parsing at some point;\\nfor example, we would expect the natural language questions\\nsubmitted to a question-answering system to undergo parsing as an initial step.\\nIn this section we see two simple parsing algorithms,\\na top-down method called recursive descent parsing,\\nand a bottom-up method called shift-reduce parsing.\\nWe also see some more sophisticated algorithms,\\na top-down method with bottom-up filtering called\\nleft-corner parsing, and a dynamic programming\\ntechnique called chart parsing.\\n\\n4.1&nbsp;&nbsp;&nbsp;Recursive Descent Parsing\\nThe simplest kind of parser interprets a grammar as a specification\\nof how to break a high-level goal into several lower-level subgoals.\\nThe top-level goal is to find an S.  The S → NP VP\\nproduction permits the parser to replace this goal with two subgoals:\\nfind an NP, then find a VP.  Each of these subgoals can be\\nreplaced in turn by sub-sub-goals, using productions that have NP\\nand VP on their left-hand side.  Eventually, this expansion\\nprocess leads to subgoals such as: find the word telescope.  Such\\nsubgoals can be directly compared against the input sequence, and\\nsucceed if the next word is matched.  If there is no match the parser\\nmust back up and try a different alternative.\\nThe recursive descent parser builds a parse tree during the above\\nprocess.  With the initial goal (find an S), the S root node\\nis created.  As the above process recursively expands its goals using\\nthe productions of the grammar, the parse tree is extended downwards\\n(hence the name recursive descent).  We can see this in action using\\nthe graphical demonstration nltk.app.rdparser().\\nSix stages of the execution of this parser are shown in 4.1.\\n\\n\\nFigure 4.1: Six Stages of a Recursive Descent Parser: the parser begins with a\\ntree consisting of the node S; at each stage it consults the grammar\\nto find a production that can be used to enlarge the tree; when\\na lexical production is encountered, its word is compared against the input;\\nafter a complete parse has been found, the parser backtracks to look for\\nmore parses.\\n\\nDuring this process, the parser is often forced to choose between several\\npossible productions.  For example, in going from step 3 to step 4, it\\ntries to find productions with N on the left-hand side.  The\\nfirst of these is N → man.  When this does not work\\nit backtracks, and tries other N productions in order, until it\\ngets to N → dog, which matches the next word in the\\ninput sentence.  Much later, as shown in step 5, it finds a complete\\nparse.  This is a tree that covers the entire sentence, without any\\ndangling edges.  Once a parse has been found, we can get the parser to\\nlook for additional parses.  Again it will backtrack and explore other\\nchoices of production in case any of them result in a parse.\\nNLTK provides a recursive descent parser:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; rd_parser = nltk.RecursiveDescentParser(grammar1)\\n&gt;&gt;&gt; sent = \\'Mary saw a dog\\'.split()\\n&gt;&gt;&gt; for tree in rd_parser.parse(sent):\\n...     print(tree)\\n(S (NP Mary) (VP (V saw) (NP (Det a) (N dog))))\\n\\n\\n\\n\\nNote\\nRecursiveDescentParser() takes an optional parameter trace.\\nIf trace is greater than zero, then the parser will report the steps\\nthat it takes as it parses a text.\\n\\nRecursive descent parsing has three key shortcomings.  First,\\nleft-recursive productions like NP -&gt; NP PP send it\\ninto an infinite loop.  Second, the parser wastes a lot of time\\nconsidering words and structures that do not correspond to the input\\nsentence.  Third, the backtracking process may discard parsed\\nconstituents that will need to be rebuilt again later.  For example,\\nbacktracking over VP -&gt; V NP will discard the subtree\\ncreated for the NP.  If the parser then proceeds with\\nVP -&gt; V NP PP, then the NP subtree must be created all\\nover again.\\nRecursive descent parsing is a kind of top-down parsing.\\nTop-down parsers use a grammar to predict what the input will be,\\nbefore inspecting the input!  However, since the input is available to\\nthe parser all along, it would be more sensible to consider the input\\nsentence from the very beginning.  This approach is called\\nbottom-up parsing, and we will see an example in the next section.\\n\\n\\n4.2&nbsp;&nbsp;&nbsp;Shift-Reduce Parsing\\nA simple kind of bottom-up parser is the shift-reduce parser.\\nIn common with all bottom-up parsers, a shift-reduce\\nparser tries to find sequences of words and phrases that correspond\\nto the right hand side of a grammar production, and replace them\\nwith the left-hand side, until the whole sentence is reduced to\\nan S.\\n<!-- XXX earlier section no longer talks about stacks.  Concepts of\\npushing and popping will need to be explained somewhere. -->\\nThe shift-reduce parser repeatedly pushes the next input word onto a\\nstack (4.1); this is the shift operation.\\nIf the top n items on the stack match\\nthe n items on the right hand side of some production,\\nthen they are all popped off the stack, and the item on the left-hand\\nside of the production is pushed on the stack.  This replacement of\\nthe top n items with a single item is the reduce operation.\\nThis operation may only be applied to the top of the stack;\\nreducing items lower in the stack must be done before later items are\\npushed onto the stack.  The parser finishes when all the input is\\nconsumed and there is only one item remaining on the stack, a parse\\ntree with an S node as its root.\\nThe shift-reduce parser builds a parse tree during the above process.\\nEach time it pops n items off the stack it combines them into\\na partial parse tree, and pushes this back on the stack.\\nWe can see the shift-reduce parsing algorithm in action using the\\ngraphical demonstration nltk.app.srparser().\\nSix stages of the execution of this parser are shown in 4.2.\\n\\n\\nFigure 4.2: Six Stages of a Shift-Reduce Parser: the parser begins by shifting the\\nfirst input word onto its stack; once the top items on the stack match\\nthe right hand side of a grammar production, they can be replaced with\\nthe left hand side of that production; the parser succeeds once all input\\nis consumed and one S item remains on the stack.\\n\\nNLTK provides ShiftReduceParser(), a simple\\nimplementation of a shift-reduce parser.  This parser does not\\nimplement any backtracking, so it is not guaranteed to find a parse\\nfor a text, even if one exists.  Furthermore, it will only find at\\nmost one parse, even if more parses exist.  We can provide an\\noptional trace parameter that controls how verbosely the\\nparser reports the steps that it takes as it parses a text:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; sr_parser = nltk.ShiftReduceParser(grammar1)\\n&gt;&gt;&gt; sent = \\'Mary saw a dog\\'.split()\\n&gt;&gt;&gt; for tree in sr_parser.parse(sent):\\n...     print(tree)\\n  (S (NP Mary) (VP (V saw) (NP (Det a) (N dog))))\\n\\n\\n\\n\\nNote\\nYour Turn:\\nRun the above parser in tracing mode to see the sequence of shift and reduce\\noperations, using sr_parse = nltk.ShiftReduceParser(grammar1, trace=2)\\n\\nA shift-reduce parser can reach a dead end and fail to find any parse,\\neven if the input sentence is well-formed according to the grammar.\\nWhen this happens, no input remains, and the stack contains items\\nwhich cannot be reduced to an S.  The problem arises because\\nthere are choices made earlier that cannot be undone by the parser\\n(although users of the graphical demonstration can undo their choices).\\nThere are two kinds of choices to be made by the parser:\\n(a) which reduction to do when more than one is possible\\n(b) whether to shift or reduce when either action is possible.\\nA shift-reduce parser may be extended to implement policies for resolving such\\nconflicts.  For example, it may address shift-reduce conflicts by\\nshifting only when no reductions are possible, and it may address\\nreduce-reduce conflicts by favoring the reduction operation that removes\\nthe most items from the stack.  (A generalization of shift-reduce\\nparser, a \"lookahead LR parser\", is commonly used in programming\\nlanguage compilers.)\\nThe advantage of shift-reduce parsers over recursive descent parsers\\nis that they only build structure that corresponds to the words in the\\ninput.  Furthermore, they only build each sub-structure once,\\ne.g. NP(Det(the), N(man)) is only built and pushed onto the stack\\na single time, regardless of whether it will later be used by the\\nVP -&gt; V NP PP reduction or the NP -&gt; NP PP reduction.\\n\\n\\n4.3&nbsp;&nbsp;&nbsp;The Left-Corner Parser\\nOne of the problems with the recursive descent parser is that it\\ngoes into an infinite loop when it encounters a left-recursive production.\\nThis is because it applies the grammar\\nproductions blindly, without considering the actual input sentence.\\nA left-corner parser is a hybrid between the bottom-up and top-down\\napproaches we have seen.\\nGrammar grammar1 allows us to produce the following parse of John saw\\nMary:\\n\\n  (11)\\nRecall that the grammar (defined in 3.3) has the following productions for expanding NP:\\n\\n  (12)\\n  a.NP -&gt; Det N\\n\\n  b.NP -&gt; Det N PP\\n\\n  c.NP -&gt; \"John\" | \"Mary\" | \"Bob\"\\n\\n<!-- XXX Following notation with DoubleRightArrow wrongly assumes\\nthis relation has been defined. -->\\nSuppose we ask you to first look at tree (11), and then decide\\nwhich of the NP productions you\\'d want a recursive descent parser to\\napply first — obviously, (12c) is the right choice! How do you\\nknow that it would be pointless to apply (12a) or (12b) instead? Because\\nneither of these productions will derive a sequence whose first word is\\nJohn.  That is, we can easily tell that in a successful\\nparse of John saw Mary, the parser has to expand NP in\\nsuch a way that NP derives the sequence John α. More\\ngenerally, we say that a category B is a left-corner of\\na tree rooted in A if  A ⇒*\\nB α.\\n\\n  (13)\\nA left-corner parser is a top-down parser with bottom-up filtering.\\nUnlike an ordinary recursive descent parser, it does not get trapped\\nin left recursive productions.\\nBefore starting its work, a left-corner parser preprocesses the\\ncontext-free grammar to build a table where each row contains two\\ncells, the first holding a non-terminal, and the second holding the\\ncollection of possible left corners of that non-terminal. 4.1\\nillustrates this for the grammar from grammar2.\\nTable 4.1: Left-Corners in grammar2\\n\\n\\n\\n\\n\\nCategory\\nLeft-Corners (pre-terminals)\\n\\n\\n\\nS\\nNP\\n\\nNP\\nDet, PropN\\n\\nVP\\nV\\n\\nPP\\nP\\n\\n\\n\\n\\n\\nEach time a production is considered by the parser, it checks that the\\nnext input word is compatible with at least one of the pre-terminal\\ncategories in the left-corner table.\\n\\n\\n\\n4.4&nbsp;&nbsp;&nbsp;Well-Formed Substring Tables\\nThe simple parsers discussed above suffer from limitations in\\nboth completeness and efficiency. In order to remedy these, we will\\napply the algorithm design technique of dynamic programming to\\nthe parsing problem.  As we saw in 4.7,\\ndynamic programming stores intermediate results and re-uses them when\\nappropriate, achieving significant efficiency gains. This technique\\ncan be applied to syntactic parsing, allowing us to store\\npartial solutions to the parsing task and then look them up as\\nnecessary in order to efficiently arrive at a complete solution.\\nThis approach to parsing is known as chart parsing.  We introduce\\nthe main idea in this section; see the online materials available for\\nthis chapter for more implementation details.\\nDynamic programming allows us to build the PP in my pajamas\\njust once.  The first time we build it we save it in a table, then we look it\\nup when we need to use it as a subconstituent of either the object NP or\\nthe higher VP. This table is known as a\\nwell-formed substring table, or WFST for short.\\n(The term \"substring\" refers to a contiguous sequence of words within a sentence.)\\nWe will show how to construct the WFST bottom-up so as to systematically record\\nwhat syntactic constituents have been found.\\nLet\\'s set our input to be the sentence in (2).  The numerically specified\\nspans of the WFST are reminiscent of Python\\'s slice notation (3.2).  Another\\nway to think about the data structure is shown in 4.3, a data\\nstructure known as a chart.\\n\\n\\nFigure 4.3: The Chart Data Structure: words are the edge labels of a linear graph structure.\\n\\nIn a WFST, we record the position of the words\\nby filling in cells in a triangular matrix:\\nthe vertical axis will denote the start position of a substring,\\nwhile the horizontal axis will denote the end position\\n(thus shot will appear in the cell with coordinates (1, 2)).\\nTo simplify this presentation, we will assume each word has a unique\\nlexical category, and we will store this (not the word) in the matrix.\\nSo cell (1, 2) will contain the entry V.\\nMore generally, if our input string is\\na0a1 ... an, and our grammar\\ncontains a production of the form A → ai, then we add A to\\nthe cell (i, `i`+1).\\n\\nSystem Message: WARNING/2 (ch08.rst2, line 900); backlink\\nInline interpreted text or phrase reference start-string without end-string.\\nSo, for every word in text, we can look up in our grammar what\\ncategory it belongs to.\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; text = [\\'I\\', \\'shot\\', \\'an\\', \\'elephant\\', \\'in\\', \\'my\\', \\'pajamas\\']\\n&gt;&gt;&gt; groucho_grammar.productions(rhs=text[1])\\n[V -&gt; \\'shot\\']\\n\\n\\n\\nFor our WFST, we create an (n-1) × (n-1) matrix\\nas a list of lists in Python, and initialize it\\nwith the lexical categories of each token, in the init_wfst()\\nfunction in 4.4.  We also define a utility function display()\\nto pretty-print the WFST for us.\\nAs expected, there is a V in cell (1, 2).\\n\\n\\n\\n\\n&nbsp;\\ndef init_wfst(tokens, grammar):\\n    numtokens = len(tokens)\\n    wfst = [[None for i in range(numtokens+1)] for j in range(numtokens+1)]\\n    for i in range(numtokens):\\n        productions = grammar.productions(rhs=tokens[i])\\n        wfst[i][i+1] = productions[0].lhs()\\n    return wfst\\n\\ndef complete_wfst(wfst, tokens, grammar, trace=False):\\n    index = dict((p.rhs(), p.lhs()) for p in grammar.productions())\\n    numtokens = len(tokens)\\n    for span in range(2, numtokens+1):\\n        for start in range(numtokens+1-span):\\n            end = start + span\\n            for mid in range(start+1, end):\\n                nt1, nt2 = wfst[start][mid], wfst[mid][end]\\n                if nt1 and nt2 and (nt1,nt2) in index:\\n                    wfst[start][end] = index[(nt1,nt2)]\\n                    if trace:\\n                        print(\"[%s] %3s [%s] %3s [%s] ==&gt; [%s] %3s [%s]\" % \\\\\\n                        (start, nt1, mid, nt2, end, start, index[(nt1,nt2)], end))\\n    return wfst\\n\\ndef display(wfst, tokens):\\n    print(\\'\\\\nWFST \\' + \\' \\'.join((\"%-4d\" % i) for i in range(1, len(wfst))))\\n    for i in range(len(wfst)-1):\\n        print(\"%d   \" % i, end=\" \")\\n        for j in range(1, len(wfst)):\\n            print(\"%-4s\" % (wfst[i][j] or \\'.\\'), end=\" \")\\n        print()\\n&gt;&gt;&gt; tokens = \"I shot an elephant in my pajamas\".split()\\n&gt;&gt;&gt; wfst0 = init_wfst(tokens, groucho_grammar)\\n&gt;&gt;&gt; display(wfst0, tokens)\\nWFST 1    2    3    4    5    6    7\\n0    NP   .    .    .    .    .    .\\n1    .    V    .    .    .    .    .\\n2    .    .    Det  .    .    .    .\\n3    .    .    .    N    .    .    .\\n4    .    .    .    .    P    .    .\\n5    .    .    .    .    .    Det  .\\n6    .    .    .    .    .    .    N\\n&gt;&gt;&gt; wfst1 = complete_wfst(wfst0, tokens, groucho_grammar)\\n&gt;&gt;&gt; display(wfst1, tokens)\\nWFST 1    2    3    4    5    6    7\\n0    NP   .    .    S    .    .    S\\n1    .    V    .    VP   .    .    VP\\n2    .    .    Det  NP   .    .    .\\n3    .    .    .    N    .    .    .\\n4    .    .    .    .    P    .    PP\\n5    .    .    .    .    .    Det  NP\\n6    .    .    .    .    .    .    N\\n\\n\\nExample 4.4 (code_wfst.py): Figure 4.4: Acceptor Using Well-Formed Substring Table\\n\\nReturning to our tabular representation, given that we have Det\\nin cell (2, 3) for the word an, and N in cell (3, 4) for the\\nword elephant, what should we put into cell (2, 4) for an elephant?\\nWe need to find a production of the form A → Det N.\\nConsulting the grammar, we know that we can enter NP in cell (2, 4).\\n\\nMore generally, we can enter A in (i, j) if there\\nis a production A → B C, and we find\\nnonterminal B in (i, k) and C in (k, j).\\nThe program in 4.4 uses this rule to complete the WFST.\\nBy setting trace to True when calling the function complete_wfst(),\\nwe see tracing output that shows the WFST being constructed:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; wfst1 = complete_wfst(wfst0, tokens, groucho_grammar, trace=True)\\n[2] Det [3]   N [4] ==&gt; [2]  NP [4]\\n[5] Det [6]   N [7] ==&gt; [5]  NP [7]\\n[1]   V [2]  NP [4] ==&gt; [1]  VP [4]\\n[4]   P [5]  NP [7] ==&gt; [4]  PP [7]\\n[0]  NP [1]  VP [4] ==&gt; [0]   S [4]\\n[1]  VP [4]  PP [7] ==&gt; [1]  VP [7]\\n[0]  NP [1]  VP [7] ==&gt; [0]   S [7]\\n\\n\\n\\nFor example, this says that since we found Det at\\nwfst[2][3] and N at wfst[3][4], we can add NP to\\nwfst[2][4].\\n\\nNote\\nTo help us easily retrieve productions by their right hand\\nsides, we create an index for the grammar.\\nThis is an example of a space-time trade-off: we do a reverse lookup\\non the grammar, instead of having to check through the entire list of\\nproductions each time we want to look up via the right hand side.\\n\\n\\n\\nFigure 4.5: The Chart Data Structure: non-terminals are represented as extra edges in the chart.\\n\\nWe conclude that there is a parse for the whole input string once\\nwe have constructed an S node in cell (0, 7), showing that we\\nhave found a sentence that covers the whole input.  The final state of\\nthe WFST is depicted in 4.5.\\nNotice that we have not used any built-in parsing functions here.\\nWe\\'ve implemented a complete, primitive chart parser from the ground up!\\n<!-- XXX distinction between recognition and parsing should be explained\\nmore carefully or else dropped. -->\\nWFST\\'s have several shortcomings.\\nFirst, as you can see, the WFST is not itself a parse tree, so the technique is\\nstrictly speaking recognizing that a sentence is admitted by a\\ngrammar, rather than parsing it.\\nSecond, it requires every non-lexical grammar production to be binary.\\nAlthough it is possible to convert an arbitrary CFG into this form,\\nwe would prefer to use an approach without such a requirement.\\nThird, as a bottom-up approach it is potentially wasteful, being\\nable to propose constituents in locations that would not be licensed by\\nthe grammar.\\n NP PP edge in the table -->\\nFinally, the WFST did not represent the structural ambiguity in\\nthe sentence (i.e. the two verb phrase readings).  The VP\\nin cell (1, 7) was actually entered twice, once for a V NP\\nreading, and once for a VP PP reading.  These are different\\nhypotheses, and the second overwrote the first (as it happens this didn\\'t\\nmatter since the left hand side was the same.)\\nChart parsers use a slighly richer data structure and some interesting\\nalgorithms to solve these problems (see the Further Reading section at\\nthe end of this chapter for details).\\n\\nNote\\nYour Turn:\\nTry out the interactive chart parser application nltk.app.chartparser().\\n\\n\\n\\n\\n5&nbsp;&nbsp;&nbsp;Dependencies and Dependency Grammar\\nPhrase structure grammar is concerned with how words and sequences of\\nwords combine to form constituents. A distinct and complementary\\napproach, dependency grammar, focusses instead on how words\\nrelate to other words. Dependency is a binary asymmetric relation that\\nholds between a head and its dependents.\\nThe head of a sentence is usually taken to be the tensed verb, and every other word is\\neither dependent on the sentence head, or connects to it through a path of\\ndependencies.\\n\\nA dependency representation is a labeled directed graph, where the\\nnodes are the lexical items and the labeled arcs represent dependency\\nrelations from heads to dependents.  5.1 illustrates a\\ndependency graph, where arrows point from heads to their dependents.\\n\\n\\nFigure 5.1: Dependency Structure: arrows point from heads to their dependents;\\nlabels indicate the grammatical function of the dependent as\\nsubject, object or modifier.\\n\\nThe arcs in 5.1 are labeled with the grammatical\\nfunction that holds between a dependent and its head. For example,\\nI is the SBJ (subject) of shot (which is the head of\\nthe whole sentence), and in is an NMOD (noun modifier of\\nelephant). In contrast to phrase structure grammar, therefore,\\ndependency grammars can be used to directly express grammatical\\nfunctions as a type of dependency.\\nHere\\'s one way of encoding a\\ndependency grammar in NLTK — note that it only captures bare\\ndependency information without specifying the type of dependency:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; groucho_dep_grammar = nltk.DependencyGrammar.fromstring(\"\"\"\\n... \\'shot\\' -&gt; \\'I\\' | \\'elephant\\' | \\'in\\'\\n... \\'elephant\\' -&gt; \\'an\\' | \\'in\\'\\n... \\'in\\' -&gt; \\'pajamas\\'\\n... \\'pajamas\\' -&gt; \\'my\\'\\n... \"\"\")\\n&gt;&gt;&gt; print(groucho_dep_grammar)\\nDependency grammar with 7 productions\\n  \\'shot\\' -&gt; \\'I\\'\\n  \\'shot\\' -&gt; \\'elephant\\'\\n  \\'shot\\' -&gt; \\'in\\'\\n  \\'elephant\\' -&gt; \\'an\\'\\n  \\'elephant\\' -&gt; \\'in\\'\\n  \\'in\\' -&gt; \\'pajamas\\'\\n  \\'pajamas\\' -&gt; \\'my\\'\\n\\n\\n\\nA dependency graph is projective if, when all the words are\\nwritten in linear order, the edges can be drawn above the words\\nwithout crossing. This is equivalent to saying that a word and all its\\ndescendents (dependents and dependents of its dependents, etc.) form a\\ncontiguous sequence of words within the sentence. 5.1 is\\nprojective, and we can parse many sentences in English using a\\nprojective dependency parser. The next example shows how\\ngroucho_dep_grammar provides an alternative approach to capturing\\nthe attachment ambiguity that we examined earlier with phrase\\nstructure grammar.\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; pdp = nltk.ProjectiveDependencyParser(groucho_dep_grammar)\\n&gt;&gt;&gt; sent = \\'I shot an elephant in my pajamas\\'.split()\\n&gt;&gt;&gt; trees = pdp.parse(sent)\\n&gt;&gt;&gt; for tree in trees:\\n...     print(tree)\\n(shot I (elephant an (in (pajamas my))))\\n(shot I (elephant an) (in (pajamas my)))\\n\\n\\n\\nThese bracketed dependency structures can also be displayed as trees,\\nwhere dependents are shown as children of their heads.\\n\\n  (14)\\nIn languages with more flexible word order than English,\\nnon-projective dependencies are more frequent.\\n\\n\\nVarious criteria have been proposed for deciding what is the head H and\\nwhat is the dependent D in a construction C. Some of the most important\\nare the following:\\n\\nH determines the distribution class of C; or alternatively, the\\nexternal syntactic properties of C are due to H.\\nH determines the semantic type of C.\\nH is obligatory while D may be optional.\\nH selects D and determines whether it is obligatory or\\noptional.\\nThe morphological form of D is determined by H (e.g. agreement\\nor case government).\\n\\nWhen we say in a phrase structure grammar that the immediate\\nconstituents of a PP are P and NP, we are implicitly\\nappealing to the head / dependent distinction. A prepositional phrase\\nis a phrase whose head is a preposition; moreover, the NP is a\\ndependent of P.  The same distinction carries over to the other\\ntypes of phrase that we have discussed. The key point to note here is\\nthat although phrase structure grammars seem very different from\\ndependency grammars, they implicitly embody a recognition of\\ndependency relations. While CFGs are not intended to directly capture\\ndependencies, more recent linguistic frameworks have increasingly\\nadopted formalisms which combine aspects of both approaches.\\n\\n5.1&nbsp;&nbsp;&nbsp;Valency and the Lexicon\\nLet us take a closer look at verbs and their dependents.\\nThe grammar in 3.3 correctly generates examples like\\n(15d).\\n\\n  (15)\\n  a.The squirrel was frightened.\\n\\n  b.Chatterer saw the bear.\\n\\n  c.Chatterer thought Buster was angry.\\n\\n  d.Joe put the fish on the log.\\n\\n\\nThese possibilities correspond to the following productions:\\nTable 5.1: VP productions and their lexical heads\\n\\n\\n\\n\\n\\nVP -&gt; V Adj\\nwas\\n\\nVP -&gt; V NP\\nsaw\\n\\nVP -&gt; V S\\nthought\\n\\nVP -&gt; V NP PP\\nput\\n\\n\\n\\n\\n\\n\\nThat is, was can occur with a following Adj, saw can occur with a\\nfollowing NP, thought can occur with a following S and put can\\noccur with a following NP and PP. The dependents Adj, NP, PP and\\nS are often called complements of the respective verbs and there are strong\\nconstraints on what verbs can occur with what complements. By contrast with\\n(15d), the word sequences in (16d) are ill-formed:\\n\\n  (16)\\n  a.*The squirrel was Buster was angry.\\n\\n  b.*Chatterer saw frightened.\\n\\n  c.*Chatterer thought the bear.\\n\\n  d.*Joe put on the log.\\n\\n\\nNote\\nWith a little imagination, it is possible to\\ninvent contexts in which unusual combinations of verbs and\\ncomplements are interpretable. However, we assume that the above\\nexamples are to be interpreted in neutral contexts.\\n\\n<!-- XXX does the historical note matter here?  Can we just talk about\\nvalency without linking it to a tradition? -->\\nIn the tradition of dependency grammar, the verbs in 5.1 are said\\nto have different valencies. Valency restrictions are not just\\napplicable to verbs, but also to the other classes of heads.\\n<!-- XXX \"techniques within frameworks based on PSG\" overly complicated.\\nThis paragraph probably works just fine without this sentence. -->\\nWithin frameworks based on phrase structure grammar, various\\ntechniques have been proposed for excluding the\\nungrammatical examples in (16d). In a CFG, we need some way of constraining\\ngrammar productions which expand VP so that verbs only co-occur\\nwith their correct complements. We can do this by dividing the class of\\nverbs into \"subcategories\", each of which is associated with a\\ndifferent set of complements. For example, transitive verbs such\\nas chased and saw require a following NP\\nobject complement; that is, they are subcategorized for NP\\ndirect objects. If we introduce a new category label for transitive verbs, namely\\nTV (for Transitive Verb), then we can use it in the following productions:\\nVP -&gt; TV NP\\nTV -&gt; \\'chased\\' | \\'saw\\'\\n\\nNow *Joe thought the bear is excluded since we haven\\'t listed\\nthought as a TV, but Chatterer saw the bear is still allowed.\\n5.2 provides more examples of labels for verb subcategories.\\nTable 5.2: Verb Subcategories\\n\\n\\n\\n\\n\\n\\nSymbol\\nMeaning\\nExample\\n\\n\\n\\nIV\\nintransitive verb\\nbarked\\n\\nTV\\ntransitive verb\\nsaw a man\\n\\nDatV\\ndative verb\\ngave a dog to a man\\n\\nSV\\nsentential verb\\nsaid that a dog barked\\n\\n\\n\\n\\n\\nValency is a property of lexical items, and we will discuss it further\\nin 9..\\nComplements are often contrasted with modifiers (or adjuncts),\\nalthough both are kinds of dependent. Prepositional phrases,\\nadjectives and adverbs typically function as modifiers. Unlike\\ncomplements, modifiers are optional, can often be iterated, and are\\nnot selected for by heads in the same way as complements. For\\nexample, the adverb really can be added as a modifer to all the\\nsentence in (17d):\\n\\n  (17)\\n  a.The squirrel really was frightened.\\n\\n  b.Chatterer really saw the bear.\\n\\n  c.Chatterer really thought Buster was angry.\\n\\n  d.Joe really put the fish on the log.\\n\\nThe structural ambiguity of PP attachment, which we have\\nillustrated in both phrase structure and dependency grammars,\\ncorresponds semantically to an ambiguity in the scope of the modifier.\\n\\n\\n5.2&nbsp;&nbsp;&nbsp;Scaling Up\\nSo far, we have only considered \"toy grammars,\" small grammars that\\nillustrate the key aspects of parsing.  But there is an obvious\\nquestion as to whether the approach can be scaled up to cover\\nlarge corpora of natural languages. How hard would it be to construct\\nsuch a set of productions by hand? In general, the answer is: very\\nhard. Even if we allow ourselves to use various formal devices that\\ngive much more succinct representations of grammar productions, it is still extremely\\ndifficult to keep control of the complex interactions between the many\\nproductions required to cover the major constructions of a\\nlanguage. In other words, it is hard to modularize grammars so that\\none portion can be developed independently of the other parts. This in\\nturn means that it is difficult to distribute the task of grammar\\nwriting across a team of linguists. Another difficulty is that as the\\ngrammar expands to cover a wider and wider range of constructions,\\nthere is a corresponding increase in the number of analyses which are\\nadmitted for any one sentence. In other words, ambiguity increases\\nwith coverage.\\nDespite these problems, some large collaborative\\nprojects have achieved interesting and impressive results in\\ndeveloping rule-based grammars for several languages. Examples are the\\nLexical Functional Grammar (LFG) Pargram project,\\nthe Head-Driven Phrase Structure Grammar (HPSG) LinGO Matrix framework,\\nand the Lexicalized Tree Adjoining Grammar XTAG Project.\\n\\n\\n\\n6&nbsp;&nbsp;&nbsp;Grammar Development\\nParsing builds trees over sentences, according to a phrase\\nstructure grammar.  Now, all the examples we gave above\\nonly involved toy grammars containing a handful of productions.\\nWhat happens if we try to scale up this approach to deal\\nwith realistic corpora of language?  In this section we will\\nsee how to access treebanks, and look at the challenge of developing\\nbroad-coverage grammars.\\n\\n6.1&nbsp;&nbsp;&nbsp;Treebanks and Grammars\\nThe corpus module defines the treebank corpus reader,\\nwhich contains a 10% sample of the Penn Treebank corpus.\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; from nltk.corpus import treebank\\n&gt;&gt;&gt; t = treebank.parsed_sents(\\'wsj_0001.mrg\\')[0]\\n&gt;&gt;&gt; print(t)\\n(S\\n  (NP-SBJ\\n    (NP (NNP Pierre) (NNP Vinken))\\n    (, ,)\\n    (ADJP (NP (CD 61) (NNS years)) (JJ old))\\n    (, ,))\\n  (VP\\n    (MD will)\\n    (VP\\n      (VB join)\\n      (NP (DT the) (NN board))\\n      (PP-CLR\\n        (IN as)\\n        (NP (DT a) (JJ nonexecutive) (NN director)))\\n      (NP-TMP (NNP Nov.) (CD 29))))\\n  (. .))\\n\\n\\n\\nWe can use this data to help develop a grammar.\\nFor example, the program in 6.1\\nuses a simple filter to find verbs that take sentential complements.\\nAssuming we already have a production of the form VP -&gt; Vs S,\\nthis information enables us to identify particular verbs\\nthat would be included in the expansion of Vs.\\n\\n\\n\\n\\n&nbsp;\\ndef filter(tree):\\n    child_nodes = [child.label() for child in tree\\n                   if isinstance(child, nltk.Tree)]\\n    return  (tree.label() == \\'VP\\') and (\\'S\\' in child_nodes)\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; from nltk.corpus import treebank\\n&gt;&gt;&gt; [subtree for tree in treebank.parsed_sents()\\n...          for subtree in tree.subtrees(filter)]\\n [Tree(\\'VP\\', [Tree(\\'VBN\\', [\\'named\\']), Tree(\\'S\\', [Tree(\\'NP-SBJ\\', ...]), ...]), ...]\\n\\n\\nExample 6.1 (code_sentential_complement.py): Figure 6.1: Searching a Treebank to find Sentential Complements\\n\\nThe Prepositional Phrase Attachment Corpus, nltk.corpus.ppattach\\nis another source of information about the valency of particular verbs.\\nHere we illustrate a technique for mining this corpus.\\nIt finds pairs of prepositional phrases where the preposition\\nand noun are fixed, but where the choice of verb determines whether\\nthe prepositional phrase is attached to the VP or to the NP.\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; from collections import defaultdict\\n&gt;&gt;&gt; entries = nltk.corpus.ppattach.attachments(\\'training\\')\\n&gt;&gt;&gt; table = defaultdict(lambda: defaultdict(set))\\n&gt;&gt;&gt; for entry in entries:\\n...     key = entry.noun1 + \\'-\\' + entry.prep + \\'-\\' + entry.noun2\\n...     table[key][entry.attachment].add(entry.verb)\\n...\\n&gt;&gt;&gt; for key in sorted(table):\\n...     if len(table[key]) &gt; 1:\\n...         print(key, \\'N:\\', sorted(table[key][\\'N\\']), \\'V:\\', sorted(table[key][\\'V\\']))\\n\\n\\n\\nAmongst the output lines of this program we find\\noffer-from-group N: [\\'rejected\\'] V: [\\'received\\'],\\nwhich indicates that received expects a separate\\nPP complement attached to the VP, while rejected does not.\\nAs before, we can use this information to help construct the grammar.\\nThe NLTK corpus collection includes data from the PE08\\nCross-Framework and Cross Domain Parser Evaluation Shared Task.\\nA collection of larger grammars has been prepared for the purpose of\\ncomparing different parsers, which can be obtained by downloading\\nthe large_grammars package\\n(e.g. python -m nltk.downloader large_grammars).\\nThe NLTK corpus collection also includes a sample from the Sinica Treebank Corpus,\\nconsisting of 10,000 parsed sentences drawn from the\\nAcademia Sinica Balanced Corpus of Modern Chinese.\\nLet\\'s load and display one of the trees in this corpus.\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; nltk.corpus.sinica_treebank.parsed_sents()[3450].draw()               \\n\\n\\n\\n\\n\\n\\n6.2&nbsp;&nbsp;&nbsp;Pernicious Ambiguity\\nUnfortunately, as the coverage of\\nthe grammar increases and the length of the input sentences grows, the\\nnumber of parse trees grows rapidly.  In fact, it grows at an\\nastronomical rate.\\nLet\\'s explore this issue with the help of a simple example.\\nThe word\\nfish is both a noun and a verb.  We can make up the sentence\\nfish fish fish, meaning fish like to fish for other fish.\\n(Try this with police if you prefer something more sensible.)\\nHere is a toy grammar for the \"fish\" sentences.\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; grammar = nltk.CFG.fromstring(\"\"\"\\n... S -&gt; NP V NP\\n... NP -&gt; NP Sbar\\n... Sbar -&gt; NP V\\n... NP -&gt; \\'fish\\'\\n... V -&gt; \\'fish\\'\\n... \"\"\")\\n\\n\\n\\nNow we can try parsing a longer sentence, fish fish fish fish\\nfish, which amongst other things, means \\'fish that other fish\\nfish are in the habit of fishing fish themselves\\'. We use the NLTK\\nchart parser, which was mentioned earlier in this chapter.  This\\nsentence has two readings.\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; tokens = [\"fish\"] * 5\\n&gt;&gt;&gt; cp = nltk.ChartParser(grammar)\\n&gt;&gt;&gt; for tree in cp.parse(tokens):\\n...     print(tree)\\n(S (NP fish) (V fish) (NP (NP fish) (Sbar (NP fish) (V fish))))\\n(S (NP (NP fish) (Sbar (NP fish) (V fish))) (V fish) (NP fish))\\n\\n\\n\\nAs the length of this sentence goes up (3, 5, 7, ...) we get the\\nfollowing numbers of parse trees:\\n1; 2; 5; 14; 42; 132; 429; 1,430; 4,862; 16,796; 58,786; 208,012; ...\\n(These are the Catalan numbers, which we saw in an exercise\\nin 4).\\nThe last of these is for a sentence of length 23, the average length\\nof sentences in the  WSJ section of Penn Treebank.  For a sentence\\nof length 50 there would be over 1012 parses, and this\\nis only half the length of the Piglet sentence\\n(1),\\nwhich young children process effortlessly.\\nNo practical NLP system could construct millions of trees for a\\nsentence and choose the appropriate one in the context.\\nIt\\'s clear that humans don\\'t do this either!\\nNote that the problem is not with our choice of example.\\n(Church &amp; Patil, 1982) point out that the syntactic ambiguity of PP\\nattachment in sentences like (18) also grows in proportion to the Catalan\\nnumbers.\\n\\n  (18)Put the block in the box on the table.\\nSo much for structural ambiguity; what about lexical ambiguity?\\nAs soon as we try to construct a broad-coverage grammar, we\\nare forced to make lexical entries highly ambiguous for their part of\\nspeech.  In a toy grammar, a is only a determiner, dog is\\nonly a noun, and runs is only a verb.  However, in a\\nbroad-coverage grammar, a is also a noun (e.g. part a),\\ndog is also a verb (meaning to follow closely), and runs\\nis also a noun (e.g. ski runs).  In fact, all words can be\\nreferred to by name: e.g. the verb \\'ate\\' is spelled with three\\nletters; in speech we do not need to supply quotation marks.\\nFurthermore, it is possible to verb most nouns.  Thus a parser for a\\nbroad-coverage grammar will be overwhelmed with ambiguity.  Even\\ncomplete gibberish will often have a reading, e.g. the a are of\\nI.  As (Klavans &amp; Resnik, 1996) has pointed out, this is not word salad but a\\ngrammatical noun phrase, in which are is a noun meaning a\\nhundredth of a hectare (or 100 sq m), and a and I are\\nnouns designating coordinates, as shown in 6.2.\\n\\n\\nFigure 6.2: \"The a are of I\": a schematic drawing of 27 paddocks, each being\\none \"are\" in size, and each identified using coordinates;\\nthe top left cell is the a \"are\" of column I (after Abney).\\n\\n\\nEven though this phrase is unlikely, it is still grammatical and\\na broad-coverage parser should be able to construct a parse tree\\nfor it.  Similarly, sentences that seem to be\\nunambiguous, such as John saw Mary, turn out to have other\\nreadings we would not have anticipated  (as Abney explains).  This\\nambiguity is unavoidable, and leads to horrendous inefficiency in\\nparsing seemingly innocuous sentences.\\nThe solution to these problems is provided by\\nprobabilistic parsing, which allows us to rank\\nthe parses of an ambiguous sentence on the basis of evidence from corpora.\\n\\n\\n6.3&nbsp;&nbsp;&nbsp;Weighted Grammar\\n<!-- TODO: mention interest in having weights is because they can be learned.\\nWithout this it is mysterious why we would want to bother.\\nTechnical aspects follow, but this is important motivation (Steven) -->\\nAs we have just seen, dealing with ambiguity\\nis a key challenge in developing broad coverage parsers.\\nChart parsers improve the efficiency of computing multiple\\nparses of the same sentences, but they are still overwhelmed by\\nthe sheer number of possible parses.  Weighted grammars and\\nprobabilistic parsing algorithms have provided an effective\\nsolution to these problems.\\nBefore looking at these, we need to understand why the notion of\\ngrammaticality could be gradient.  Considering the verb give.\\nThis verb requires both a direct object (the thing being given)\\nand an indirect object (the recipient).\\nThese complements can be given in either order, as\\nillustrated in (19).  In the \"prepositional dative\" form in\\n(19a), the direct object appears first, followed\\nby a prepositional phrase containing the indirect object.\\n\\n  (19)\\n  a.Kim gave a bone to the dog\\n\\n  b.Kim gave the dog a bone\\n\\nIn the \"double object\" form in (19b),\\nthe indirect object appears first, followed by the direct object.\\nIn the above case, either order is acceptable.  However, if\\nthe indirect object is a pronoun, there is a strong preference for\\nthe double object construction:\\n\\n  (20)\\n  a.Kim gives the heebie-jeebies to me (*prepositional dative)\\n\\n  b.Kim gives me the heebie-jeebies (double object)\\n\\nUsing the Penn Treebank sample, we can examine all instances of\\nprepositional dative and double object constructions involving\\ngive, as shown in 6.3.\\n\\n\\n\\n\\n&nbsp;\\ndef give(t):\\n    return t.label() == \\'VP\\' and len(t) &gt; 2 and t[1].label() == \\'NP\\'\\\\\\n           and (t[2].label() == \\'PP-DTV\\' or t[2].label() == \\'NP\\')\\\\\\n           and (\\'give\\' in t[0].leaves() or \\'gave\\' in t[0].leaves())\\ndef sent(t):\\n    return \\' \\'.join(token for token in t.leaves() if token[0] not in \\'*-0\\')\\ndef print_node(t, width):\\n        output = \"%s %s: %s / %s: %s\" %\\\\\\n            (sent(t[0]), t[1].label(), sent(t[1]), t[2].label(), sent(t[2]))\\n        if len(output) &gt; width:\\n            output = output[:width] + \"...\"\\n        print(output)\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; for tree in nltk.corpus.treebank.parsed_sents():\\n...     for t in tree.subtrees(give):\\n...         print_node(t, 72)\\ngave NP: the chefs / NP: a standing ovation\\ngive NP: advertisers / NP: discounts for maintaining or increasing ad sp...\\ngive NP: it / PP-DTV: to the politicians\\ngave NP: them / NP: similar help\\ngive NP: them / NP:\\ngive NP: only French history questions / PP-DTV: to students in a Europe...\\ngive NP: federal judges / NP: a raise\\ngive NP: consumers / NP: the straight scoop on the U.S. waste crisis\\ngave NP: Mitsui / NP: access to a high-tech medical product\\ngive NP: Mitsubishi / NP: a window on the U.S. glass industry\\ngive NP: much thought / PP-DTV: to the rates she was receiving , nor to ...\\ngive NP: your Foster Savings Institution / NP: the gift of hope and free...\\ngive NP: market operators / NP: the authority to suspend trading in futu...\\ngave NP: quick approval / PP-DTV: to $ 3.18 billion in supplemental appr...\\ngive NP: the Transportation Department / NP: up to 50 days to review any...\\ngive NP: the president / NP: such power\\ngive NP: me / NP: the heebie-jeebies\\ngive NP: holders / NP: the right , but not the obligation , to buy a cal...\\ngave NP: Mr. Thomas / NP: only a `` qualified \\'\\' rating , rather than ``...\\ngive NP: the president / NP: line-item veto power\\n\\n\\nExample 6.3 (code_give.py): Figure 6.3: Usage of Give and Gave in the Penn Treebank sample\\n\\nWe can observe a strong tendency for the shortest complement to appear\\nfirst.  However, this does not account for a form like\\ngive NP: federal judges / NP: a raise, where animacy may\\nplay a role.  In fact there turn out to be a large number of contributing\\nfactors, as surveyed by (Bresnan &amp; Hay, 2006).\\nSuch preferences can be represented in a weighted grammar.\\nA probabilistic context free grammar (or PCFG) is a context free\\ngrammar that associates a probability with each of its productions.\\nIt generates the same set of parses for a text that the corresponding\\ncontext free grammar does, and assigns a probability to each parse.\\nThe probability of a parse generated by a PCFG is simply the product\\nof the probabilities of the productions used to generate it.\\nThe simplest way to define a PCFG is to load it from a specially\\nformatted string consisting of a sequence of weighted productions,\\nwhere weights appear in brackets, as shown in 6.4.\\n\\n\\n\\n\\n&nbsp;\\ngrammar = nltk.PCFG.fromstring(\"\"\"\\n    S    -&gt; NP VP              [1.0]\\n    VP   -&gt; TV NP              [0.4]\\n    VP   -&gt; IV                 [0.3]\\n    VP   -&gt; DatV NP NP         [0.3]\\n    TV   -&gt; \\'saw\\'              [1.0]\\n    IV   -&gt; \\'ate\\'              [1.0]\\n    DatV -&gt; \\'gave\\'             [1.0]\\n    NP   -&gt; \\'telescopes\\'       [0.8]\\n    NP   -&gt; \\'Jack\\'             [0.2]\\n    \"\"\")\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; print(grammar)\\nGrammar with 9 productions (start state = S)\\n    S -&gt; NP VP [1.0]\\n    VP -&gt; TV NP [0.4]\\n    VP -&gt; IV [0.3]\\n    VP -&gt; DatV NP NP [0.3]\\n    TV -&gt; \\'saw\\' [1.0]\\n    IV -&gt; \\'ate\\' [1.0]\\n    DatV -&gt; \\'gave\\' [1.0]\\n    NP -&gt; \\'telescopes\\' [0.8]\\n    NP -&gt; \\'Jack\\' [0.2]\\n\\n\\nExample 6.4 (code_pcfg1.py): Figure 6.4: Defining a Probabilistic Context Free Grammar (PCFG)\\n\\nIt is sometimes convenient to combine multiple productions into a single line,\\ne.g. VP -&gt; TV NP [0.4] | IV [0.3] | DatV NP NP [0.3].\\nIn order to ensure that the trees generated by the grammar form a\\nprobability distribution, PCFG grammars impose the constraint\\nthat all productions with a given left-hand side must have\\nprobabilities that sum to one.\\nThe grammar in 6.4 obeys this constraint: for S,\\nthere is only one production, with a probability of 1.0; for VP,\\n0.4+0.3+0.3=1.0; and for NP, 0.8+0.2=1.0.\\nThe parse tree returned by parse() includes probabilities:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; viterbi_parser = nltk.ViterbiParser(grammar)\\n&gt;&gt;&gt; for tree in viterbi_parser.parse([\\'Jack\\', \\'saw\\', \\'telescopes\\']):\\n...     print(tree)\\n(S (NP Jack) (VP (TV saw) (NP telescopes))) (p=0.064)\\n\\n\\n\\nNow that parse trees are assigned probabilities, it no longer matters\\nthat there may be a huge number of possible parses for a given sentence.\\nA parser will be responsible for finding the most likely parses.\\n\\n\\n\\n7&nbsp;&nbsp;&nbsp;Summary\\n\\nSentences have internal organization\\nthat can be represented using a tree. Notable features of constituent\\nstructure are: recursion, heads, complements and modifiers.\\nA grammar is a compact characterization of a potentially infinite set of sentences;\\nwe say that a tree is well-formed according to a grammar, or that a grammar licenses a tree.\\nA grammar is a formal model for describing whether a given phrase can be\\nassigned a particular constituent or dependency structure.\\nGiven a set of syntactic categories, a context-free grammar\\nuses a set of productions to say how a phrase of some category A can\\nbe analyzed into a sequence of smaller parts α1\\n... αn.\\nA dependency grammar uses productions to specify what the dependents\\nare of a given lexical head.\\nSyntactic ambiguity arises when one sentence has more than one syntactic analysis\\n(e.g. prepositional phrase attachment ambiguity).\\nA parser is a procedure for finding one or more trees corresponding to a grammatically\\nwell-formed sentence.\\nA simple top-down parser is the recursive descent parser, which recursively\\nexpands the start symbol (usually S) with the help of the grammar\\nproductions, and tries to match the input sentence.  This parser cannot\\nhandle left-recursive productions (e.g., productions such as NP -&gt; NP PP).\\nIt is inefficient in the way it blindly expands\\ncategories without checking whether they are compatible with the input string, and\\nin repeatedly expanding the same non-terminals and discarding the results.\\nA simple bottom-up parser is the shift-reduce parser, which shifts input onto\\na stack and tries to match the items at the top of the stack with the right\\nhand side of grammar productions.  This parser is not guaranteed to find\\na valid parse for the input even if one exists, and builds substructure without\\nchecking whether it is globally consistent with the grammar.\\n\\n\\n\\n\\n8&nbsp;&nbsp;&nbsp;Further Reading\\nExtra materials for this chapter are posted at http://nltk.org/, including links to freely\\navailable resources on the web.\\nFor more examples of parsing with NLTK, please see the\\nParsing HOWTO at http://nltk.org/howto.\\nThere are many introductory books on syntax. (O\\'Grady et al, 2004) is a\\ngeneral introduction to linguistics, while (Radford, 1988) provides a\\ngentle introduction to transformational grammar, and can be\\nrecommended for its coverage of transformational approaches to\\nunbounded dependency constructions.  The most widely used\\nterm in linguistics for formal grammar is generative grammar,\\nthough it has nothing to do with generation (Chomsky, 1965).\\nThe framework of X-bar Syntax is\\ndue to (Jacobs &amp; Rosenbaum, 1970), and is explored at greater length in (Jackendoff, 1977)\\n(The primes we use replace Chomsky\\'s typographically more demanding horizontal bars.)\\n(Burton-Roberts, 1997) is a practically oriented textbook on how to\\nanalyze constituency in English, with extensive exemplification and\\nexercises. (Huddleston &amp; Pullum, 2002) provides an up-to-date and comprehensive analysis of\\nsyntactic phenomena in English.\\nChapter 12 of (Jurafsky &amp; Martin, 2008) covers formal grammars of English;\\nSections 13.1-3 cover simple parsing algorithms and techniques\\nfor dealing with ambiguity;\\nChapter 14 covers statistical parsing;\\nChapter 16 covers the Chomsky hierarchy and the formal complexity\\nof natural language.\\n(Levin, 1993) has categorized English verbs into fine-grained classes,\\naccording to their syntactic properties.\\nThere are several ongoing efforts to build large-scale rule-based grammars,\\ne.g. the LFG Pargram project http://www2.parc.com/istl/groups/nltt/pargram/,\\nthe HPSG LinGO Matrix framework http://www.delph-in.net/matrix/\\nand the XTAG Project http://www.cis.upenn.edu/~xtag/.\\n\\n\\n9&nbsp;&nbsp;&nbsp;Exercises\\n\\n☼ Can you come up with grammatical sentences that have probably never\\nbeen uttered before?  (Take turns with a partner.)  What does this tell you\\nabout human language?\\n\\n☼ Recall Strunk and White\\'s prohibition against sentence-initial\\nhowever used to mean \"although\".\\nDo a web search for however used at the start of the sentence.\\nHow widely used is this construction?\\n\\n☼ Consider the sentence Kim arrived or Dana left and everyone cheered.\\nWrite down the parenthesized forms to show the relative scope of and\\nand or.  Generate tree structures corresponding to both of these interpretations.\\n\\n☼ The Tree class implements a variety of other useful methods.\\nSee the Tree help documentation for more details, i.e. import\\nthe Tree class and then type help(Tree).\\n\\n☼ In this exercise you will manually construct some parse trees.\\n\\nWrite code to produce two trees, one for each reading of the phrase\\nold men and women\\nEncode any of the trees presented in this chapter as a labeled\\nbracketing and use nltk.Tree() to check that it is well-formed.\\nNow use draw() to display the tree.\\nAs in (a) above, draw a tree for The woman saw a man last Thursday.\\n\\n\\n☼ Write a recursive function to traverse a tree and return the\\ndepth of the tree, such that a tree with a single node would have\\ndepth zero.  (Hint: the depth of a subtree is the maximum depth\\nof its children, plus one.)\\n\\n☼ Analyze the A.A. Milne sentence about Piglet, by underlining all\\nof the sentences it contains then replacing these with S\\n(e.g. the first sentence becomes S when:lx` S).\\nDraw a tree structure for this \"compressed\" sentence.  What are\\nthe main syntactic constructions used for building such a long\\nsentence?\\n\\n☼ In the recursive descent parser demo, experiment with changing the\\nsentence to be parsed by selecting Edit Text in the Edit menu.\\n\\n☼ Can the grammar in grammar1 be used to describe sentences that are\\nmore than 20 words in length?\\n\\n☼ Use the graphical chart-parser interface to experiment with\\ndifferent rule invocation strategies. Come up with your own strategy\\nthat you can execute manually using the graphical interface. Describe\\nthe steps, and report any efficiency improvements it has (e.g. in terms\\nof the size of the resulting chart). Do these improvements depend on\\nthe structure of the grammar? What do you think of the prospects for\\nsignificant performance boosts from cleverer rule invocation\\nstrategies?\\n\\n☼ With pen and paper, manually trace the execution of a recursive descent\\nparser and a shift-reduce parser, for a CFG you have already seen, or one\\nof your own devising.\\n\\n☼ We have seen that a chart parser adds but never removes edges\\nfrom a chart.  Why?\\n\\n☼ Consider the sequence of words:\\nBuffalo buffalo Buffalo buffalo buffalo buffalo Buffalo buffalo.\\nThis is a grammatically correct sentence, as explained at\\nhttp://en.wikipedia.org/wiki/Buffalo_buffalo_Buffalo_buffalo_buffalo_buffalo_Buffalo_buffalo.\\nConsider the tree diagram presented on this Wikipedia page, and write down a suitable\\ngrammar.  Normalize case to lowercase, to simulate the problem that a listener has when hearing\\nthis sentence.  Can you find other parses for this sentence?\\nHow does the number of parse trees grow as the sentence gets longer?\\n(More examples of these sentences can be found at http://en.wikipedia.org/wiki/List_of_homophonous_phrases).\\n\\n◑ You can modify the grammar in the recursive descent parser demo\\nby selecting Edit Grammar  in the Edit menu. Change\\nthe second expansion production, namely NP -&gt; Det N PP, to NP -&gt; NP\\nPP. Using the Step button, try to build a parse tree. What happens?\\n\\n◑ Extend the grammar in grammar2 with productions that expand prepositions as\\nintransitive, transitive and requiring a PP\\ncomplement. Based on these productions, use the method of the\\npreceding exercise to draw a tree for the sentence Lee ran away home.\\n\\n◑ Pick some common verbs and complete the following tasks:\\n\\nWrite a program to find those verbs in the Prepositional Phrase Attachment Corpus\\nnltk.corpus.ppattach.  Find any cases where the same verb\\nexhibits two different attachments, but where the first noun,\\nor second noun, or preposition, stay unchanged (as we saw in\\nour discussion of syntactic ambiguity in 2).\\nDevise CFG grammar productions to cover some of these cases.\\n\\n\\n◑ Write a program to compare the efficiency of a top-down chart parser\\ncompared with a recursive descent parser (4).\\nUse the same grammar and input sentences for both.  Compare their performance\\nusing the timeit module (see 4.7 for an example of\\nhow to do this).\\n\\n◑ Compare the performance of the top-down, bottom-up, and left-corner\\nparsers using the same grammar and three grammatical test\\nsentences. Use timeit to log the amount of time each\\nparser takes on the same sentence.  Write a function that runs all\\nthree parsers on all three sentences, and prints a 3-by-3 grid of\\ntimes, as well as row and column totals. Discuss your findings.\\n\\n◑ Read up on \"garden path\" sentences.  How might the computational\\nwork of a parser relate to the difficulty humans have with\\nprocessing these sentences?\\nhttp://en.wikipedia.org/wiki/Garden_path_sentence\\n\\n◑ To compare multiple trees in a single window, we can use the\\ndraw_trees() method.  Define some trees and try it out:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; from nltk.draw.tree import draw_trees\\n&gt;&gt;&gt; draw_trees(tree1, tree2, tree3)                    \\n\\n\\n\\n\\n◑ Using tree positions, list the subjects of the first 100\\nsentences in the Penn treebank; to make the results easier to view,\\nlimit the extracted subjects to subtrees whose height is 2.\\n\\n◑ Inspect the Prepositional Phrase Attachment Corpus\\nand try to suggest some factors that influence PP attachment.\\n\\n◑ In this section we claimed that there are linguistic regularities\\nthat cannot be described simply in terms of n-grams.\\nConsider the following sentence, particularly the position of the phrase\\nin his turn.  Does this illustrate a problem for an approach based\\non n-grams?\\n\\nWhat was more, the in his turn somewhat youngish Nikolay Parfenovich\\nalso turned out to be the only person in the entire world to acquire a\\nsincere liking to our \"discriminated-against\" public procurator.\\n(Dostoevsky: The Brothers Karamazov)\\n\\n\\n◑ Write a recursive function that produces a nested bracketing for\\na tree, leaving out the leaf nodes, and displaying the non-terminal\\nlabels after their subtrees.  So the above example about Pierre\\nVinken would produce:\\n[[[NNP NNP]NP , [ADJP [CD NNS]NP JJ]ADJP ,]NP-SBJ MD [VB [DT NN]NP [IN [DT JJ NN]NP]PP-CLR [NNP CD]NP-TMP]VP .]S\\nConsecutive categories should be separated by space.\\n\\n◑ Download several electronic books from Project Gutenberg.\\nWrite a program to scan these texts for any extremely long sentences.\\nWhat is the longest sentence you can find?  What syntactic construction(s)\\nare responsible for such long sentences?\\n\\n◑ Modify the functions init_wfst() and complete_wfst() so\\nthat the contents of each cell in the WFST is a set of\\nnon-terminal symbols rather than a single non-terminal.\\n\\n◑ Consider the algorithm in 4.4.  Can you explain why\\nparsing context-free grammar is proportional to n3, where n\\nis the length of the input sentence.\\n\\n◑ Process each tree of the Treebank corpus sample nltk.corpus.treebank\\nand extract the productions with the help of Tree.productions().  Discard\\nthe productions that occur only once.  Productions with the same left hand side,\\nand similar right hand sides can be collapsed, resulting in an equivalent but\\nmore compact set of rules.  Write code to output a compact grammar.\\n\\n★ One common way of defining the subject of a sentence S in\\nEnglish is as the noun phrase that is the child of S and\\nthe sibling of VP.   Write a function that takes the tree for\\na sentence and returns the subtree corresponding to the subject of the\\nsentence.  What should it do if the root node of the tree passed to\\nthis function is not S, or it lacks a subject?\\n\\n★ Write a function that takes a grammar (such as the one defined in\\n3.1) and returns a random sentence generated by the grammar.\\n(Use grammar.start() to find the start symbol of the grammar;\\ngrammar.productions(lhs) to get the list of productions from the grammar\\nthat have the specified left-hand side; and production.rhs() to get\\nthe right-hand side of a production.)\\n\\n★ Implement a version of the shift-reduce parser using backtracking,\\nso that it finds all possible parses for a sentence, what might be called\\na \"recursive ascent parser.\"  Consult the Wikipedia entry for backtracking\\nat http://en.wikipedia.org/wiki/Backtracking\\n\\n★\\nAs we saw in 7., it is possible\\nto collapse chunks down to their chunk label.  When we do this\\nfor sentences involving the word gave, we find patterns\\nsuch as the following:\\ngave NP\\ngave up NP in NP\\ngave NP up\\ngave NP NP\\ngave NP to NP\\n\\n\\nUse this method to study the complementation patterns of a verb\\nof interest, and write suitable grammar productions.  (This task\\nis sometimes called lexical acquisition.)\\nIdentify some English verbs that are near-synonyms, such as the\\ndumped/filled/loaded example from earlier in this chapter.\\nUse the chunking method to study the complementation patterns of\\nthese verbs.  Create a grammar to cover these cases.  Can the verbs\\nbe freely substituted for each other, or are their constraints?\\nDiscuss your findings.\\n\\n\\n★ Develop a left-corner parser based on the\\nrecursive descent parser, and inheriting from ParseI.\\n\\n★ Extend NLTK\\'s shift-reduce parser to incorporate backtracking, so\\nthat it is guaranteed to find all parses that exist (i.e. it is complete).\\n\\n★ Modify the functions init_wfst() and complete_wfst() so\\nthat when a non-terminal symbol is added to a cell in the WFST, it includes\\na record of the cells from which it was derived. Implement a\\nfunction that will convert a WFST in this form to a parse tree.\\n\\n\\n<!-- recurse over tree to look for coordinate constructions (cf 4th\\nexample in chapter 1.1); (possible extension: callback function for Tree.subtrees()) -->\\n\\n\\nAbout this document...\\nUPDATED FOR NLTK 3.0.\\nThis is a chapter from Natural Language Processing with Python,\\nby Steven Bird, Ewan Klein and Edward Loper,\\nCopyright © 2019 the authors.\\nIt is distributed with the Natural Language Toolkit [http://nltk.org/],\\nVersion 3.0, under the terms of the\\nCreative Commons Attribution-Noncommercial-No Derivative Works 3.0 United States License\\n[http://creativecommons.org/licenses/by-nc-nd/3.0/us/].\\nThis document was built on\\nWed  4 Sep 2019 11:40:48 ACST\\n\\n\\n\\n\\n\\n',\n",
       " '\\n\\n\\n\\nfunction astext(node)\\n{\\n    return node.innerHTML.replace(/(]+)>)/ig,\"\")\\n                         .replace(/&gt;/ig, \">\")\\n                         .replace(/&lt;/ig, \"<\")\\n                         .replace(/&quot;/ig, \\'\"\\')\\n                         .replace(/&amp;/ig, \"&\");\\n}\\n\\nfunction copy_notify(node, bar_color, data)\\n{\\n    // The outer box: relative + inline positioning.\\n    var box1 = document.createElement(\"div\");\\n    box1.style.position = \"relative\";\\n    box1.style.display = \"inline\";\\n    box1.style.top = \"2em\";\\n    box1.style.left = \"1em\";\\n  \\n    // A shadow for fun\\n    var shadow = document.createElement(\"div\");\\n    shadow.style.position = \"absolute\";\\n    shadow.style.left = \"-1.3em\";\\n    shadow.style.top = \"-1.3em\";\\n    shadow.style.background = \"#404040\";\\n    \\n    // The inner box: absolute positioning.\\n    var box2 = document.createElement(\"div\");\\n    box2.style.position = \"relative\";\\n    box2.style.border = \"1px solid #a0a0a0\";\\n    box2.style.left = \"-.2em\";\\n    box2.style.top = \"-.2em\";\\n    box2.style.background = \"white\";\\n    box2.style.padding = \".3em .4em .3em .4em\";\\n    box2.style.fontStyle = \"normal\";\\n    box2.style.background = \"#f0e0e0\";\\n\\n    node.insertBefore(box1, node.childNodes.item(0));\\n    box1.appendChild(shadow);\\n    shadow.appendChild(box2);\\n    box2.innerHTML=\"Copied&nbsp;to&nbsp;the&nbsp;clipboard: \" +\\n                   \"\"+\\n                   data+\"\";\\n    setTimeout(function() { node.removeChild(box1); }, 1000);\\n\\n    var elt = node.parentNode.firstChild;\\n    elt.style.background = \"#ffc0c0\";\\n    setTimeout(function() { elt.style.background = bar_color; }, 200);\\n}\\n\\nfunction copy_codeblock_to_clipboard(node)\\n{\\n    var data = astext(node)+\"\\\\n\";\\n    if (copy_text_to_clipboard(data)) {\\n        copy_notify(node, \"#40a060\", data);\\n    }\\n}\\n\\nfunction copy_doctest_to_clipboard(node)\\n{\\n    var s = astext(node)+\"\\\\n   \";\\n    var data = \"\";\\n\\n    var start = 0;\\n    var end = s.indexOf(\"\\\\n\");\\n    while (end >= 0) {\\n        if (s.substring(start, start+4) == \">>> \") {\\n            data += s.substring(start+4, end+1);\\n        }\\n        else if (s.substring(start, start+4) == \"... \") {\\n            data += s.substring(start+4, end+1);\\n        }\\n        /*\\n        else if (end-start > 1) {\\n            data += \"# \" + s.substring(start, end+1);\\n        }*/\\n        // Grab the next line.\\n        start = end+1;\\n        end = s.indexOf(\"\\\\n\", start);\\n    }\\n    \\n    if (copy_text_to_clipboard(data)) {\\n        copy_notify(node, \"#4060a0\", data);\\n    }\\n}\\n    \\nfunction copy_text_to_clipboard(data)\\n{\\n    if (window.clipboardData) {\\n        window.clipboardData.setData(\"Text\", data);\\n        return true;\\n     }\\n    else if (window.netscape) {\\n        // w/ default firefox settings, permission will be denied for this:\\n        netscape.security.PrivilegeManager\\n                      .enablePrivilege(\"UniversalXPConnect\");\\n    \\n        var clip = Components.classes[\"@mozilla.org/widget/clipboard;1\"]\\n                      .createInstance(Components.interfaces.nsIClipboard);\\n        if (!clip) return;\\n    \\n        var trans = Components.classes[\"@mozilla.org/widget/transferable;1\"]\\n                       .createInstance(Components.interfaces.nsITransferable);\\n        if (!trans) return;\\n    \\n        trans.addDataFlavor(\"text/unicode\");\\n    \\n        var str = new Object();\\n        var len = new Object();\\n    \\n        var str = Components.classes[\"@mozilla.org/supports-string;1\"]\\n                     .createInstance(Components.interfaces.nsISupportsString);\\n        var datacopy=data;\\n        str.data=datacopy;\\n        trans.setTransferData(\"text/unicode\",str,datacopy.length*2);\\n        var clipid=Components.interfaces.nsIClipboard;\\n    \\n        if (!clip) return false;\\n    \\n        clip.setData(trans,null,clipid.kGlobalClipboard);\\n        return true;\\n    }\\n    return false;\\n}\\n//-->\\n\\n\\n\\n\\n\\n\\n9. Building Feature Based Grammars\\n\\n\\n/*\\n:Author: Edward Loper, James Curran\\n:Copyright: This stylesheet has been placed in the public domain.\\n\\nStylesheet for use with Docutils.\\n\\nThis stylesheet defines new css classes used by NLTK.\\n\\nIt uses a Python syntax highlighting scheme that matches\\nthe colour scheme used by IDLE, which makes it easier for\\nbeginners to check they are typing things in correctly.\\n*/\\n\\n/* Include the standard docutils stylesheet. */\\n@import url(default.css);\\n\\n/* Custom inline roles */\\nspan.placeholder    { font-style: italic; font-family: monospace; }\\nspan.example        { font-style: italic; }\\nspan.emphasis       { font-style: italic; }\\nspan.termdef        { font-weight: bold; }\\n/*span.term           { font-style: italic; }*/\\nspan.category       { font-variant: small-caps; }\\nspan.feature        { font-variant: small-caps; }\\nspan.fval           { font-style: italic; }\\nspan.math           { font-style: italic; }\\nspan.mathit         { font-style: italic; }\\nspan.lex            { font-variant: small-caps; }\\nspan.guide-linecount{ text-align: right; display: block;}\\n\\n/* Python souce code listings */\\nspan.pysrc-prompt   { color: #9b0000; }\\nspan.pysrc-more     { color: #9b00ff; }\\nspan.pysrc-keyword  { color: #e06000; }\\nspan.pysrc-builtin  { color: #940094; }\\nspan.pysrc-string   { color: #00aa00; }\\nspan.pysrc-comment  { color: #ff0000; }\\nspan.pysrc-output   { color: #0000ff; }\\nspan.pysrc-except   { color: #ff0000; }\\nspan.pysrc-defname  { color: #008080; }\\n\\n\\n/* Doctest blocks */\\npre.doctest         { margin: 0; padding: 0; font-weight: bold; }\\ndiv.doctest         { margin: 0 1em 1em 1em; padding: 0; }\\ntable.doctest       { margin: 0; padding: 0;\\n                      border-top: 1px solid gray;\\n                      border-bottom: 1px solid gray; }\\npre.copy-notify     { margin: 0; padding: 0.2em; font-weight: bold;\\n                      background-color: #ffffff; }\\n\\n/* Python source listings */\\ndiv.pylisting       { margin: 0 1em 1em 1em; padding: 0; }\\ntable.pylisting     { margin: 0; padding: 0;\\n                      border-top: 1px solid gray; }\\ntd.caption { border-top: 1px solid black; margin: 0; padding: 0; }\\n.caption-label { font-weight: bold;  }\\ntd.caption p { margin: 0; padding: 0; font-style: normal;}\\n\\ntable tr td.codeblock { \\n  padding: 0.2em ! important; margin: 0;\\n  border-left: 1px solid gray;\\n  border-right: 2px solid gray;\\n  border-top: 0px solid gray;\\n  border-bottom: 1px solid gray;\\n  font-weight: bold; background-color: #eeffee;\\n}\\ntable pre span {\\n  white-space: pre-wrap;\\n}\\ntable tr td.doctest  { \\n  padding: 0.2em; margin: 0;\\n  border-left: 1px solid gray;\\n  border-right: 2px solid gray;\\n  border-top: 0px solid gray;\\n  border-bottom: 1px solid gray;\\n  font-weight: bold; background-color: #eeeeff;\\n}\\n\\ntd.codeblock table tr td.copybar {\\n    background: #40a060; border: 1px solid gray;\\n    font-family: monospace; padding: 0; margin: 0; }\\ntd.doctest table tr td.copybar {\\n    background: #4060a0; border: 1px solid gray;\\n    font-family: monospace; padding: 0; margin: 0; }\\n\\ntd.pysrc { padding-left: 0.5em; }\\n\\nimg.callout { border-width: 0px; }\\n\\ntable.docutils {\\n    border-style: solid;\\n    border-width: 1px;\\n    margin-top: 6px;\\n    border-color: grey;\\n    border-collapse: collapse; }\\n\\ntable.docutils th {\\n    border-style: none;\\n    border-width: 1px;\\n    border-color: grey;\\n    padding: 0 .5em 0 .5em; }\\n\\ntable.docutils td {\\n    border-style: none;\\n    border-width: 1px;\\n    border-color: grey; \\n    padding: 0 .5em 0 .5em; }\\n\\ntable.footnote td { padding: 0; }\\ntable.footnote { border-width: 0; }\\ntable.footnote td { border-width: 0; }\\ntable.footnote th { border-width: 0; }\\n\\ntable.noborder { border-width: 0; }\\n\\ntable.example pre { margin-top: 4px; margin-bottom: 0; }\\n\\n/* For figures & tables */\\np.caption { margin-bottom: 0; }\\ndiv.figure { text-align: center; }\\n\\n/* The index */\\ndiv.index { border: 1px solid black;\\n            background-color: #eeeeee; }\\ndiv.index h1 { padding-left: 0.5em; margin-top: 0.5ex;\\n               border-bottom: 1px solid black; }\\nul.index { margin-left: 0.5em; padding-left: 0; }\\nli.index { list-style-type: none; }\\np.index-heading { font-size: 120%; font-style: italic; margin: 0; }\\nli.index ul { margin-left: 2em; padding-left: 0; }\\n\\n/* \\'Note\\' callouts */\\ndiv.note\\n{\\n  border-right:   #87ceeb 1px solid;\\n  padding-right: 4px;\\n  border-top: #87ceeb 1px solid;\\n  padding-left: 4px;\\n  padding-bottom: 4px;\\n  margin: 2px 5% 10px;\\n  border-left: #87ceeb 1px solid;\\n  padding-top: 4px;\\n  border-bottom: #87ceeb 1px solid;\\n  font-style: normal;\\n  font-family: verdana, arial;\\n  background-color: #b0c4de;\\n}\\n\\ntable.avm { border: 0px solid black; width: 0; }\\ntable.avm tbody tr {border: 0px solid black; }\\ntable.avm tbody tr td { padding: 2px; }\\ntable.avm tbody tr td.avm-key { padding: 5px; font-variant: small-caps; }\\ntable.avm tbody tr td.avm-eq { padding: 5px; }\\ntable.avm tbody tr td.avm-val { padding: 5px; font-style: italic; }\\np.avm-empty { font-style: normal; }\\ntable.avm colgroup col { border: 0px solid black; }\\ntable.avm tbody tr td.avm-topleft \\n    { border-left: 2px solid #000080; border-top: 2px solid #000080; }\\ntable.avm tbody tr td.avm-botleft \\n    { border-left: 2px solid #000080; border-bottom: 2px solid #000080; }\\ntable.avm tbody tr td.avm-topright\\n    { border-right: 2px solid #000080; border-top: 2px solid #000080; }\\ntable.avm tbody tr td.avm-botright\\n    { border-right: 2px solid #000080; border-bottom: 2px solid #000080; }\\ntable.avm tbody tr td.avm-left\\n    { border-left: 2px solid #000080; }\\ntable.avm tbody tr td.avm-right\\n    { border-right: 2px solid #000080; }\\ntable.avm tbody tr td.avm-topbotleft\\n    { border: 2px solid #000080; border-right: 0px solid black; }\\ntable.avm tbody tr td.avm-topbotright\\n    { border: 2px solid #000080; border-left: 0px solid black; }\\ntable.avm tbody tr td.avm-ident\\n    { font-size: 80%; padding: 0; padding-left: 2px; vertical-align: top; }\\n.avm-pointer\\n{ border: 1px solid #008000; padding: 1px; color: #008000; \\n  background: #c0ffc0; font-style: normal; }\\n\\ntable.gloss { border: 0px solid black; width: 0; }\\ntable.gloss tbody tr { border: 0px solid black; }\\ntable.gloss tbody tr td { border: 0px solid black; }\\ntable.gloss colgroup col { border: 0px solid black; }\\ntable.gloss p { margin: 0; padding: 0; }\\n\\ntable.rst-example { border: 1px solid black; }\\ntable.rst-example tbody tr td { background: #eeeeee; }\\ntable.rst-example thead tr th { background: #c0ffff; }\\ntd.rst-raw { width: 0; }\\n\\n/* Used by nltk.org/doc/test: */\\ndiv.doctest-list { text-align: center; }\\ntable.doctest-list { border: 1px solid black;\\n  margin-left: auto; margin-right: auto;\\n}\\ntable.doctest-list tbody tr td { background: #eeeeee;\\n  border: 1px solid #cccccc; text-align: left; }\\ntable.doctest-list thead tr th { background: #304050; color: #ffffff;\\n  border: 1px solid #000000;}\\ntable.doctest-list thead tr a { color: #ffffff; }\\nspan.doctest-passed { color: #008000; }\\nspan.doctest-failed { color: #800000; }\\n\\n\\n\\n\\n\\n\\n9. Building Feature Based Grammars\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n<!-- Grammatical Category - e.g. NP and verb as technical terms\\n.. role:: gc\\n   :class: category -->\\n\\n\\n\\n\\n\\n\\n\\n\\n<!-- standard global imports\\n\\n>>> from __future__ import division\\n>>> import nltk, re, pprint -->\\n<!-- TODO:\\ndiscuss applications of unify() to dicts, lists, and mixtures of dicts and lists\\nFeatureValueTuple: \\'[x=(1,2,3)]\\'\\nFeatureValueSet: \\'[x={1,2,3}]\\'\\nNB: Unification does *not* descend into tuples or sets; but variable\\nsubstitution from bindings does.  Generally speaking, tuples and set\\nfeature values should never contain feature structures.\\nFeatureValueUnion: \\'{?a+?b}\\', which will automatically collapse to a\\nFeatureValueSet as soon as all top-level variables are replaced with set values.\\nAs with FeatureValueSet, unification does not descend into FeatureValueUnion,\\nbut variable binding does.\\nMore examples are in Edward\\'s email of 24 August 2007\\n\\n* AP: Give example of grammar with HPSG style subcat? Probably not\\n  doable :-(\\n\\n* AP: The \"Heads Revisited\" subsection: there is some interaction\\n  of the material of this subsection with the material of the\\n  previous subsection, on \"Subcategorization\".  In fact, something\\n  like X-bar theory is implicitly introduced in that previous\\n  section, with V\\'\\' = V[SUBCAT ], and V\\' = V[SUBCAT ].\\n  Shouldn\\'t these two subsections be merged (or otherwise\\n  reorganised)?\\n\\n* AP (70) and Figure 9.3: maybe I am thinking in terms of HPSG again,\\n  but would it be possible to generalise these grammar fragments to\\n  gaps of any type, not just NP?  For example, instead of (70), I\\'d\\n  like something like:\\n\\n  (70\\') ?x/?x - ->\\n\\n  and in the grammar in Figure 9.3, the first rule could (?) be\\n  generalised to something like:\\n\\n  S[-INV] - -> ?x S/?x\\n\\n  etc. If this is possible in this formalism, maybe it would make sense\\n  to mention it?\\n\\n* AP (72) and the sentence above - - there are also German verbs taking\\n  *genitive* complements.\\n\\n  Figure 9.4: it\\'s a pity the grammar uses IV/TV instead of illustrating\\n  the list-valued SUBCAT...\\n\\n  \"Further Reading\": reference to GPSG garbled; some interesting\\n  given ereferences not in the bibliography chapter (e.g., Grosz and\\n  Stickel 1983, Dahl and Saint-Dizier 1985, etc.). -->\\nNatural languages have an extensive range of grammatical constructions which\\nare hard to handle with the simple methods described in 8.. In order to gain\\nmore flexibility, we change our treatment of grammatical categories like S,\\nNP and V. In place of atomic labels, we decompose them into structures like\\ndictionaries, where features can take on a range of values.\\nThe goal of this chapter is to answer the following questions:\\n\\nHow can we extend the framework of context free grammars with features so as to\\ngain more fine-grained control over grammatical categories and productions?\\nWhat are the main formal properties of feature structures and how do we use them\\ncomputationally?\\nWhat kinds of linguistic patterns and grammatical constructions can we now capture\\nwith feature based grammars?\\n\\nAlong the way, we will cover more topics in English syntax, including phenomena such as\\nagreement, subcategorization, and unbounded dependency constructions.\\n\\n1&nbsp;&nbsp;&nbsp;Grammatical Features\\nIn chap-data-intensive, we described how to build classifiers that rely on detecting features of\\ntext.  Such features may be quite simple, such as extracting the last letter of a\\nword, or more complex, such as a part-of-speech tag which has itself been predicted\\nby the classifier.  In this chapter, we will investigate the role of features in\\nbuilding rule-based grammars.  In contrast to feature extractors, which record\\nfeatures that have been automatically detected, we are now going to\\ndeclare the features of words and phrases. We start off with a\\nvery simple example, using dictionaries to store features and their values.\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; kim = {\\'CAT\\': \\'NP\\', \\'ORTH\\': \\'Kim\\', \\'REF\\': \\'k\\'}\\n&gt;&gt;&gt; chase = {\\'CAT\\': \\'V\\', \\'ORTH\\': \\'chased\\', \\'REL\\': \\'chase\\'}\\n\\n\\n\\nThe objects kim and chase both have a couple of shared features, CAT\\n(grammatical category) and ORTH (orthography, i.e., spelling). In addition, each\\nhas a more semantically-oriented feature: kim[\\'REF\\'] is intended to give the\\nreferent of kim, while chase[\\'REL\\'] gives the relation expressed by\\nchase.  In the context of rule-based grammars, such pairings of features and\\nvalues are known as feature structures, and we will shortly see alternative\\nnotations for them.\\nFeature structures contain various kinds of information about grammatical\\nentities. The information need not be exhaustive, and we might want to add further\\nproperties. For example, in the case of a verb, it is often useful to know what\\n\"semantic role\" is played by the arguments of the verb. In the case of chase,\\nthe subject plays the role of \"agent\", while the object has the role of\\n\"patient\". Let\\'s add this information, using \\'sbj\\' and \\'obj\\' as placeholders\\nwhich will get filled once the verb combines with its grammatical arguments:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; chase[\\'AGT\\'] = \\'sbj\\'\\n&gt;&gt;&gt; chase[\\'PAT\\'] = \\'obj\\'\\n\\n\\n\\nIf we now process a sentence Kim chased Lee, we want to \"bind\" the verb\\'s agent role to\\nthe subject and the patient role to the object. We do this by linking to the\\nREF feature of the relevant NP. In the following example, we make the\\nsimple-minded assumption that the NPs immediately to the left and right of the\\nverb are the subject and object respectively. We also add a feature\\nstructure for Lee to complete the example.\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; sent = \"Kim chased Lee\"\\n&gt;&gt;&gt; tokens = sent.split()\\n&gt;&gt;&gt; lee = {\\'CAT\\': \\'NP\\', \\'ORTH\\': \\'Lee\\', \\'REF\\': \\'l\\'}\\n&gt;&gt;&gt; def lex2fs(word):\\n...     for fs in [kim, lee, chase]:\\n...         if fs[\\'ORTH\\'] == word:\\n...             return fs\\n&gt;&gt;&gt; subj, verb, obj = lex2fs(tokens[0]), lex2fs(tokens[1]), lex2fs(tokens[2])\\n&gt;&gt;&gt; verb[\\'AGT\\'] = subj[\\'REF\\']\\n&gt;&gt;&gt; verb[\\'PAT\\'] = obj[\\'REF\\']\\n&gt;&gt;&gt; for k in [\\'ORTH\\', \\'REL\\', \\'AGT\\', \\'PAT\\']:\\n...     print(\"%-5s =&gt; %s\" % (k, verb[k]))\\nORTH  =&gt; chased\\nREL   =&gt; chase\\nAGT   =&gt; k\\nPAT   =&gt; l\\n\\n\\n\\nThe same approach could be adopted for a different verb, say surprise, though in\\nthis case, the subject would play the role of \"source\" (SRC) and the object,\\nthe role of \"experiencer\" (EXP):\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; surprise = {\\'CAT\\': \\'V\\', \\'ORTH\\': \\'surprised\\', \\'REL\\': \\'surprise\\',\\n...             \\'SRC\\': \\'sbj\\', \\'EXP\\': \\'obj\\'}\\n\\n\\n\\nFeature structures are pretty powerful, but the way\\nin which we have manipulated them is extremely ad hoc. Our next task in this\\nchapter is to show how the framework of context free grammar and parsing can be\\nexpanded to accommodate feature structures, so that we can build analyses like this\\nin a more generic and principled way.\\nWe will start off by looking  at the\\nphenomenon of syntactic agreement; we will show how agreement\\nconstraints can be expressed elegantly using features, and illustrate\\ntheir use in a simple grammar.\\nSince feature structures are a general data\\nstructure for representing information of any kind, we will briefly\\nlook at them from a more formal point of view, and illustrate the support for feature\\nstructures offered by NLTK. In the final part of the chapter,\\nwe demonstrate that the additional expressiveness of features opens\\nup a wide spectrum of possibilities for describing sophisticated\\naspects of linguistic structure.\\n\\n1.1&nbsp;&nbsp;&nbsp;Syntactic Agreement\\nThe following examples show pairs of word sequences, the first of which is\\ngrammatical and the second not. (We use an asterisk at the start of a\\nword sequence to signal that it is ungrammatical.)\\n\\n  (1)\\n  a.this dog\\n\\n  b.*these dog\\n\\n\\n  (2)\\n  a.these dogs\\n\\n  b.*this dogs\\n\\nIn English, nouns are usually marked as being singular\\nor plural. The form of the demonstrative also varies:\\nthis (singular) and these (plural).\\nExamples (1b) and (2b) show that there are constraints on\\nthe use of demonstratives and nouns within a noun phrase:\\neither both are singular or both are plural. A similar\\nconstraint holds between subjects and predicates:\\n\\n  (3)\\n  a.the dog runs\\n\\n  b.*the dog run\\n\\n\\n  (4)\\n  a.the dogs run\\n\\n  b.*the dogs runs\\n\\n<!-- Proposed for deletion: The element which determines the\\nagreement, here the subject noun phrase, is called the agreement\\n`controller`:dt:, while the element whose form is determined by\\nagreement, here the verb, is called the `target`:dt:. -->\\nHere we can see that morphological properties of the verb co-vary\\nwith syntactic properties of the subject noun phrase.  This co-variance is\\ncalled agreement.\\nIf we look further at verb agreement in English, we will see that\\npresent tense verbs typically have two inflected forms: one for third person\\nsingular, and another for every other combination of person and number,\\nas shown in 1.1.\\nTable 1.1: Agreement Paradigm for English Regular Verbs\\n\\n\\n\\n\\n\\n\\n&nbsp;\\nsingular\\nplural\\n\\n1st per\\nI run\\nwe run\\n\\n2nd per\\nyou run\\nyou run\\n\\n3rd per\\nhe/she/it\\nruns\\nthey run\\n\\n\\n\\n\\n\\nWe can make the role of morphological properties a bit more explicit\\nas illustrated in ex-runs and ex-run. These representations indicate that\\nthe verb agrees with its subject in person and number. (We use \"3\" as\\nan abbreviation for 3rd person, \"SG\" for singular and \"PL\" for plural.)\\n\\nSystem Message: ERROR/3 (ch09.rst2, line 252)\\nError in \"gloss\" directive: may contain a single table only.\\n\\nSystem Message: ERROR/3 (ch09.rst2, line 257)\\nError in \"gloss\" directive: may contain a single table only.\\nLet\\'s see what happens when we encode these agreement constraints in a\\ncontext-free grammar.  We will begin with the simple CFG in (5).\\n\\n  (5)S   -&gt;   NP VP\\nNP  -&gt;   Det N\\nVP  -&gt;   V\\n\\nDet  -&gt;  \\'this\\'\\nN    -&gt;  \\'dog\\'\\nV    -&gt;  \\'runs\\'\\n\\n\\nGrammar (5) allows us to generate the sentence this dog runs;\\nhowever, what we really want to do is also generate these dogs\\nrun while blocking unwanted sequences like *this dogs run\\nand *these dog runs. The most straightforward approach is to\\nadd new non-terminals and productions to the grammar:\\n\\n  (6)S -&gt; NP_SG VP_SG\\nS -&gt; NP_PL VP_PL\\nNP_SG -&gt; Det_SG N_SG\\nNP_PL -&gt; Det_PL N_PL\\nVP_SG -&gt; V_SG\\nVP_PL -&gt; V_PL\\n\\nDet_SG -&gt; \\'this\\'\\nDet_PL -&gt; \\'these\\'\\nN_SG -&gt; \\'dog\\'\\nN_PL -&gt; \\'dogs\\'\\nV_SG -&gt; \\'runs\\'\\nV_PL -&gt; \\'run\\'\\n\\n\\nIn place of a single production expanding S, we now have two\\nproductions, one covering the sentences involving singular subject\\nNPs and VPs, the other covering sentences with plural\\nsubject NPs and VPs. In fact, every production in\\n(5) has two counterparts in (6). With a small grammar,\\nthis is not really such a problem, although it is aesthetically\\nunappealing. However, with a larger grammar that covers a reasonable\\nsubset of English constructions, the prospect of doubling the grammar\\nsize is very unattractive. Let\\'s suppose now that we used the same\\napproach to deal with first, second and third person agreement, for\\nboth singular and plural. This would lead to the original grammar\\nbeing multiplied by a factor of 6, which we definitely want to\\navoid. Can we do better than this? In the next section we will show\\nthat capturing number and person agreement need not come at the cost\\nof \"blowing up\" the number of productions.\\n<!-- Rule multiplication is of course more severe if we add in\\nperson agreement constraints.\\n\"rule multiplication\" will be meaningless to some readers.\\nWe need to be consistent in referring to these as productions. -->\\n\\n\\n1.2&nbsp;&nbsp;&nbsp;Using Attributes and Constraints\\nWe spoke informally of linguistic categories having properties; for\\nexample, that a noun has the property of being plural. Let\\'s\\nmake this explicit:\\n\\n  (7)N[NUM=pl]\\n\\n\\nIn (7), we have introduced some new notation which says that the\\ncategory N has a (grammatical) feature called NUM (short for\\n\\'number\\') and that the value of this feature is pl (short for\\n\\'plural\\'). We can add similar annotations to other categories, and use\\nthem in lexical entries:\\n\\n  (8)Det[NUM=sg] -&gt; \\'this\\'\\nDet[NUM=pl] -&gt; \\'these\\'\\n\\nN[NUM=sg] -&gt; \\'dog\\'\\nN[NUM=pl] -&gt; \\'dogs\\'\\nV[NUM=sg] -&gt; \\'runs\\'\\nV[NUM=pl] -&gt; \\'run\\'\\n\\n\\nDoes this help at all? So far, it looks just like a slightly more\\nverbose alternative to what was specified in (6). Things become\\nmore interesting when we allow variables over feature values, and use\\nthese to state constraints:\\n\\n  (9)S -&gt; NP[NUM=?n] VP[NUM=?n]\\nNP[NUM=?n] -&gt; Det[NUM=?n] N[NUM=?n]\\nVP[NUM=?n] -&gt; V[NUM=?n]\\n\\n\\nWe are using ?n as a variable over values of NUM; it can\\nbe instantiated either to sg or pl, within a given production.\\nWe can read the first production as saying that whatever\\nvalue NP takes for the feature NUM,\\nVP must take the same value.\\nIn order to understand how these feature constraints work, it\\'s\\nhelpful to think about how one would go about building a tree. Lexical\\nproductions will admit the following local trees (trees of\\ndepth one):\\n\\n  (10)\\n  a.\\n\\n  b.\\n\\n\\n  (11)\\n  a.\\n\\n  b.\\n\\nNow S -&gt; NP[NUM=?n] VP[NUM=?n] says that whatever the NUM\\nvalues of N and Det are, they have to be the\\nsame. Consequently, NP[NUM=?n] -&gt; Det[NUM=?n] N[NUM=?n] will\\npermit (10a) and (11a) to be combined into an NP as shown\\nin (12a) and it will also allow (10b) and (11b) to be\\ncombined, as in (12b). By contrast, (13a) and (13b) are\\nprohibited because the roots of their subtrees differ\\nin their values for the NUM feature; this incompatibility of values is\\nindicated informally with a FAIL value at the top node.\\n\\n  (12)\\n  a.\\n\\n  b.\\n\\n\\n  (13)\\n  a.\\n\\n  b.\\n\\nProduction VP[NUM=?n] -&gt; V[NUM=?n] says\\nthat the NUM value of the head verb has to be the same as the\\nNUM value of the VP parent. Combined with the production for\\nexpanding S, we\\nderive the consequence that if the NUM value of the subject head\\nnoun is pl, then so is the NUM value of the VP\\'s\\nhead verb.\\n\\n  (14)\\nGrammar (8) illustrated lexical productions for determiners like this\\nand these which require a singular or plural head noun\\nrespectively. However, other determiners in English are not choosy\\nabout the grammatical number of the noun they combine with.\\nOne way of describing this would be to add\\ntwo lexical entries to the grammar, one each for the singular and\\nplural versions of determiner such as the\\nDet[NUM=sg] -&gt; \\'the\\' | \\'some\\' | \\'any\\'\\nDet[NUM=pl] -&gt; \\'the\\' | \\'some\\' | \\'any\\'\\n\\nHowever, a more elegant solution is to\\nleave the NUM value underspecified and letting it agree\\nin number with whatever noun it combines with. Assigning a variable\\nvalue to NUM is one way of achieving this result:\\nDet[NUM=?n] -&gt; \\'the\\' | \\'some\\' | \\'any\\'\\n\\nBut in fact we can be even more economical, and just omit any\\nspecification for NUM in such productions. We only need\\nto explicitly enter a variable value when this constrains another\\nvalue elsewhere in the same production.\\nThe grammar in 1.1 illustrates most of the ideas we have introduced so\\nfar in this chapter, plus a couple of new ones.\\n\\n<!-- XXX The contents of feat0.fcfg seems to have changed in the file.\\nI won\\'t pull in the updated version in case the discussion also needs to be updated. -->\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; nltk.data.show_cfg(\\'grammars/book_grammars/feat0.fcfg\\')\\n% start S\\n# ###################\\n# Grammar Productions\\n# ###################\\n# S expansion productions\\nS -&gt; NP[NUM=?n] VP[NUM=?n]\\n# NP expansion productions\\nNP[NUM=?n] -&gt; N[NUM=?n]\\nNP[NUM=?n] -&gt; PropN[NUM=?n]\\nNP[NUM=?n] -&gt; Det[NUM=?n] N[NUM=?n]\\nNP[NUM=pl] -&gt; N[NUM=pl]\\n# VP expansion productions\\nVP[TENSE=?t, NUM=?n] -&gt; IV[TENSE=?t, NUM=?n]\\nVP[TENSE=?t, NUM=?n] -&gt; TV[TENSE=?t, NUM=?n] NP\\n# ###################\\n# Lexical Productions\\n# ###################\\nDet[NUM=sg] -&gt; \\'this\\' | \\'every\\'\\nDet[NUM=pl] -&gt; \\'these\\' | \\'all\\'\\nDet -&gt; \\'the\\' | \\'some\\' | \\'several\\'\\nPropN[NUM=sg]-&gt; \\'Kim\\' | \\'Jody\\'\\nN[NUM=sg] -&gt; \\'dog\\' | \\'girl\\' | \\'car\\' | \\'child\\'\\nN[NUM=pl] -&gt; \\'dogs\\' | \\'girls\\' | \\'cars\\' | \\'children\\'\\nIV[TENSE=pres,  NUM=sg] -&gt; \\'disappears\\' | \\'walks\\'\\nTV[TENSE=pres, NUM=sg] -&gt; \\'sees\\' | \\'likes\\'\\nIV[TENSE=pres,  NUM=pl] -&gt; \\'disappear\\' | \\'walk\\'\\nTV[TENSE=pres, NUM=pl] -&gt; \\'see\\' | \\'like\\'\\nIV[TENSE=past] -&gt; \\'disappeared\\' | \\'walked\\'\\nTV[TENSE=past] -&gt; \\'saw\\' | \\'liked\\'\\n\\n\\nExample 1.1 (code_feat0cfg.py): Figure 1.1: Example Feature based Grammar\\n\\nNotice that a syntactic category can have more than one feature; for\\nexample,\\nV[TENSE=pres, NUM=pl].\\nIn general, we can add as many features as we like.\\nA final detail about 1.1 is the statement %start S.\\nThis \"directive\" tells the parser to take S as the\\nstart symbol for the grammar.\\nIn general, when we are trying to develop even a very small grammar,\\nit is convenient to put the productions in a file where they can be edited,\\ntested and revised.  We have saved 1.1 as a file named\\n\\'feat0.fcfg\\' in the NLTK data distribution. You can make your own\\ncopy of this for further experimentation using nltk.data.load().\\n1.2 illustrates the operation of a chart\\nparser with a feature-based grammar.\\nAfter tokenizing the input, we import the load_parser function\\n which takes a grammar filename as input and returns a\\nchart parser cp .  Calling the parser\\'s\\nparse() method will iterate over the resulting parse trees;\\ntrees will be empty if the grammar fails to parse the input and\\nwill contain one or more parse trees, depending on whether the input\\nis syntactically ambiguous or not.\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; tokens = \\'Kim likes children\\'.split()\\n&gt;&gt;&gt; from nltk import load_parser \\n&gt;&gt;&gt; cp = load_parser(\\'grammars/book_grammars/feat0.fcfg\\', trace=2)  \\n&gt;&gt;&gt; for tree in cp.parse(tokens):\\n...     print(tree)\\n...\\n|.Kim .like.chil.|\\nLeaf Init Rule:\\n|[----]    .    .| [0:1] \\'Kim\\'\\n|.    [----]    .| [1:2] \\'likes\\'\\n|.    .    [----]| [2:3] \\'children\\'\\nFeature Bottom Up Predict Combine Rule:\\n|[----]    .    .| [0:1] PropN[NUM=\\'sg\\'] -&gt; \\'Kim\\' *\\nFeature Bottom Up Predict Combine Rule:\\n|[----]    .    .| [0:1] NP[NUM=\\'sg\\'] -&gt; PropN[NUM=\\'sg\\'] *\\nFeature Bottom Up Predict Combine Rule:\\n|[----&gt;    .    .| [0:1] S[] -&gt; NP[NUM=?n] * VP[NUM=?n] {?n: \\'sg\\'}\\nFeature Bottom Up Predict Combine Rule:\\n|.    [----]    .| [1:2] TV[NUM=\\'sg\\', TENSE=\\'pres\\'] -&gt; \\'likes\\' *\\nFeature Bottom Up Predict Combine Rule:\\n|.    [----&gt;    .| [1:2] VP[NUM=?n, TENSE=?t] -&gt; TV[NUM=?n, TENSE=?t] * NP[] {?n: \\'sg\\', ?t: \\'pres\\'}\\nFeature Bottom Up Predict Combine Rule:\\n|.    .    [----]| [2:3] N[NUM=\\'pl\\'] -&gt; \\'children\\' *\\nFeature Bottom Up Predict Combine Rule:\\n|.    .    [----]| [2:3] NP[NUM=\\'pl\\'] -&gt; N[NUM=\\'pl\\'] *\\nFeature Bottom Up Predict Combine Rule:\\n|.    .    [----&gt;| [2:3] S[] -&gt; NP[NUM=?n] * VP[NUM=?n] {?n: \\'pl\\'}\\nFeature Single Edge Fundamental Rule:\\n|.    [---------]| [1:3] VP[NUM=\\'sg\\', TENSE=\\'pres\\'] -&gt; TV[NUM=\\'sg\\', TENSE=\\'pres\\'] NP[] *\\nFeature Single Edge Fundamental Rule:\\n|[==============]| [0:3] S[] -&gt; NP[NUM=\\'sg\\'] VP[NUM=\\'sg\\'] *\\n(S[]\\n  (NP[NUM=\\'sg\\'] (PropN[NUM=\\'sg\\'] Kim))\\n  (VP[NUM=\\'sg\\', TENSE=\\'pres\\']\\n    (TV[NUM=\\'sg\\', TENSE=\\'pres\\'] likes)\\n    (NP[NUM=\\'pl\\'] (N[NUM=\\'pl\\'] children))))\\n\\n\\nExample 1.2 (code_featurecharttrace.py): Figure 1.2: Trace of Feature based Chart Parser\\n\\nThe details of the parsing procedure are not that important for\\npresent purposes. However, there is an implementation issue which\\nbears on our earlier discussion of grammar size. One possible approach\\nto parsing productions containing feature constraints is to compile\\nout all admissible values of the features in question so that we end\\nup with a large, fully specified CFG along the lines of (6). By\\ncontrast, the parser process illustrated above works directly with the\\nunderspecified productions given by the grammar. Feature values \"flow\\nupwards\" from lexical entries, and variable values are then associated\\nwith those values, via bindings (i.e., dictionaries) such as {?n:\\n\\'sg\\', ?t: \\'pres\\'}.  As the parser assembles information about the\\nnodes of the tree it is building, these variable bindings are used to\\ninstantiate values in these nodes; thus the underspecified\\nVP[NUM=?n, TENSE=?t] -&gt; TV[NUM=?n, TENSE=?t] NP[] becomes\\ninstantiated as VP[NUM=\\'sg\\', TENSE=\\'pres\\'] -&gt; TV[NUM=\\'sg\\',\\nTENSE=\\'pres\\'] NP[] by looking up the values of ?n and ?t in\\nthe bindings.\\nFinally, we can inspect the resulting parse trees (in this case, a\\nsingle one).\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; for tree in trees: print(tree)\\n(S[]\\n  (NP[NUM=\\'sg\\'] (PropN[NUM=\\'sg\\'] Kim))\\n  (VP[NUM=\\'sg\\', TENSE=\\'pres\\']\\n    (TV[NUM=\\'sg\\', TENSE=\\'pres\\'] likes)\\n    (NP[NUM=\\'pl\\'] (N[NUM=\\'pl\\'] children))))\\n\\n\\n\\n\\n\\n1.3&nbsp;&nbsp;&nbsp;Terminology\\nSo far, we have only seen feature values like sg and\\npl. These simple values are usually called atomic\\n— that is, they can\\'t be decomposed into subparts. A special\\ncase of atomic values are boolean values, that is, values that\\njust specify whether a property is true or false. For\\nexample, we might want to distinguish auxiliary verbs such as\\ncan, may, will and do with the boolean feature\\nAUX. For example, the production V[TENSE=pres, AUX=+] -&gt; \\'can\\'\\nmeans that can receives the value pres for TENSE and\\n+ or true for AUX. There is a widely adopted\\nconvention which abbreviates the representation of boolean\\nfeatures f; instead of AUX=+ or AUX=-, we use +AUX and\\n-AUX respectively. These are just abbreviations, however, and the\\nparser interprets them as though + and - are like any\\nother atomic value. (15) shows some representative productions:\\n\\n  (15)V[TENSE=pres, +AUX] -&gt; \\'can\\'\\nV[TENSE=pres, +AUX] -&gt; \\'may\\'\\n\\nV[TENSE=pres, -AUX] -&gt; \\'walks\\'\\nV[TENSE=pres, -AUX] -&gt; \\'likes\\'\\n\\n\\nWe have spoken of attaching \"feature annotations\" to\\nsyntactic categories. A more radical approach represents the whole category\\n— that is, the non-terminal symbol plus the annotation —\\nas a bundle of features.  For example, N[NUM=sg] contains part of speech\\ninformation which can be represented as\\nPOS=N.  An alternative notation for this category therefore\\nis [POS=N, NUM=sg].\\nIn addition to atomic-valued features,  features may take values that\\nare themselves feature structures. For example, we can group\\ntogether agreement features (e.g., person, number and gender) as a\\ndistinguished part of a category, grouped together as the value of AGR. In this case,\\nwe say that AGR has a complex value.  (16) depicts the structure, in a format\\nknown as an attribute value matrix (AVM).\\n\\n  (16)[POS = N           ]\\n[                  ]\\n[AGR = [PER = 3   ]]\\n[      [NUM = pl  ]]\\n[      [GND = fem ]]\\n\\n\\n\\n\\nFigure 1.3: Rendering a Feature Structure as an Attribute Value Matrix\\n\\nIn passing, we should point out that there are alternative approaches\\nfor displaying AVMs; 1.3 shows an example.\\nAthough feature structures rendered in the style of (16) are less\\nvisually pleasing, we will stick with this format, since it\\ncorresponds to the output we will be getting from NLTK.\\n\\nOn the topic of representation, we also note that feature structures, like\\ndictionaries, assign no\\nparticular significance to the order of features. So (16) is equivalent to:\\n\\n  (17)[AGR = [NUM = pl  ]]\\n[      [PER = 3   ]]\\n[      [GND = fem ]]\\n[                  ]\\n[POS = N           ]\\n\\n\\nOnce we have the possibility of using features like AGR, we\\ncan refactor a grammar like 1.1 so that agreement features are\\nbundled together. A tiny grammar illustrating this idea is shown in (18).\\n\\n  (18)S                    -&gt; NP[AGR=?n] VP[AGR=?n]\\nNP[AGR=?n]           -&gt; PropN[AGR=?n]\\nVP[TENSE=?t, AGR=?n] -&gt; Cop[TENSE=?t, AGR=?n] Adj\\n\\nCop[TENSE=pres,  AGR=[NUM=sg, PER=3]] -&gt; \\'is\\'\\nPropN[AGR=[NUM=sg, PER=3]]            -&gt; \\'Kim\\'\\nAdj                                   -&gt; \\'happy\\'\\n\\n\\n\\n\\n\\n2&nbsp;&nbsp;&nbsp;Processing  Feature Structures\\nIn this section, we will show how feature structures can be\\nconstructed and manipulated in NLTK. We will also discuss the\\nfundamental operation of unification, which allows us to combine the\\ninformation contained in two different feature structures.\\nFeature structures in NLTK are declared with the\\nFeatStruct() constructor. Atomic feature values can be strings or\\nintegers.\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; fs1 = nltk.FeatStruct(TENSE=\\'past\\', NUM=\\'sg\\')\\n&gt;&gt;&gt; print(fs1)\\n[ NUM   = \\'sg\\'   ]\\n[ TENSE = \\'past\\' ]\\n\\n\\n\\nA feature structure is actually just a kind of dictionary,\\nand so we access its values by indexing in the usual way.\\nWe can use our familiar syntax to assign values to features:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; fs1 = nltk.FeatStruct(PER=3, NUM=\\'pl\\', GND=\\'fem\\')\\n&gt;&gt;&gt; print(fs1[\\'GND\\'])\\nfem\\n&gt;&gt;&gt; fs1[\\'CASE\\'] = \\'acc\\'\\n\\n\\n\\nWe can also define feature structures that have complex values, as\\ndiscussed earlier.\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; fs2 = nltk.FeatStruct(POS=\\'N\\', AGR=fs1)\\n&gt;&gt;&gt; print(fs2)\\n[       [ CASE = \\'acc\\' ] ]\\n[ AGR = [ GND  = \\'fem\\' ] ]\\n[       [ NUM  = \\'pl\\'  ] ]\\n[       [ PER  = 3     ] ]\\n[                        ]\\n[ POS = \\'N\\'              ]\\n&gt;&gt;&gt; print(fs2[\\'AGR\\'])\\n[ CASE = \\'acc\\' ]\\n[ GND  = \\'fem\\' ]\\n[ NUM  = \\'pl\\'  ]\\n[ PER  = 3     ]\\n&gt;&gt;&gt; print(fs2[\\'AGR\\'][\\'PER\\'])\\n3\\n\\n\\n\\nAn alternative method of specifying feature structures is to\\nuse a bracketed string consisting of feature-value pairs in the format\\nfeature=value, where values may themselves be feature structures:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; print(nltk.FeatStruct(\"[POS=\\'N\\', AGR=[PER=3, NUM=\\'pl\\', GND=\\'fem\\']]\"))\\n[       [ GND = \\'fem\\' ] ]\\n[ AGR = [ NUM = \\'pl\\'  ] ]\\n[       [ PER = 3     ] ]\\n[                       ]\\n[ POS = \\'N\\'             ]\\n\\n\\n\\nFeature structures are not inherently tied to linguistic objects; they are\\ngeneral purpose structures for representing knowledge. For example, we\\ncould encode information about a person in a feature structure:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; print(nltk.FeatStruct(NAME=\\'Lee\\', TELNO=\\'01 27 86 42 96\\', AGE=33))\\n[ AGE   = 33               ]\\n[ NAME  = \\'Lee\\'            ]\\n[ TELNO = \\'01 27 86 42 96\\' ]\\n\\n\\n\\nIn the next couple of pages, we are going to use examples like this\\nto explore standard operations over feature structures.\\nThis will briefly divert us from processing natural language,\\nbut we need to lay the groundwork before we can\\nget back to talking about grammars. Hang on tight!\\nIt is often helpful to view feature structures as graphs; more\\nspecifically, directed acyclic graphs (DAGs).\\n(19) is equivalent to the above AVM.\\n\\n  (19)\\nThe feature names appear as labels on the directed arcs, and feature\\nvalues appear as labels on the nodes that are pointed to by the arcs.\\nJust as before, feature values can be complex:\\n\\n  (20)\\nWhen we look at such graphs, it is natural to think in terms of\\npaths through the graph. A feature path is a sequence of arcs\\nthat can be followed from the root node. We will represent paths as\\ntuples. Thus, (\\'ADDRESS\\', \\'STREET\\') is a feature path whose value\\nin (20) is the node labeled \\'rue Pascal\\'.\\nNow let\\'s consider a situation where Lee has a spouse named Kim, and\\nKim\\'s address is the same as Lee\\'s.\\nWe might represent this as (21).\\n\\n  (21)\\nHowever, rather than repeating the address\\ninformation in the feature structure, we can \"share\" the same\\nsub-graph between different arcs:\\n\\n  (22)\\nIn other words, the value of the path (\\'ADDRESS\\') in (22) is\\nidentical to the value of the path (\\'SPOUSE\\', \\'ADDRESS\\').  DAGs\\nsuch as (22) are said to involve structure sharing or\\nreentrancy. When two paths have the same value, they are said to\\nbe equivalent.\\nIn order to indicate reentrancy in our matrix-style representations, we will\\nprefix the first occurrence of a shared feature structure\\nwith an integer in parentheses, such as (1).\\nAny later reference to that structure will use the notation\\n-&gt;(1), as shown below.\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; print(nltk.FeatStruct(\"\"\"[NAME=\\'Lee\\', ADDRESS=(1)[NUMBER=74, STREET=\\'rue Pascal\\'],\\n...                          SPOUSE=[NAME=\\'Kim\\', ADDRESS-&gt;(1)]]\"\"\"))\\n[ ADDRESS = (1) [ NUMBER = 74           ] ]\\n[               [ STREET = \\'rue Pascal\\' ] ]\\n[                                         ]\\n[ NAME    = \\'Lee\\'                         ]\\n[                                         ]\\n[ SPOUSE  = [ ADDRESS -&gt; (1)  ]           ]\\n[           [ NAME    = \\'Kim\\' ]           ]\\n\\n\\n\\nThe bracketed integer is sometimes called a tag or a\\ncoindex. The choice of integer is not significant.\\nThere can be any number of tags within a single feature structure.\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; print(nltk.FeatStruct(\"[A=\\'a\\', B=(1)[C=\\'c\\'], D-&gt;(1), E-&gt;(1)]\"))\\n[ A = \\'a\\'             ]\\n[                     ]\\n[ B = (1) [ C = \\'c\\' ] ]\\n[                     ]\\n[ D -&gt; (1)            ]\\n[ E -&gt; (1)            ]\\n\\n\\n\\n\\n<!-- We can also share empty structures:\\n\\n    >>> fs2 = nltk.FeatStruct(\"[A=(1)[], B=(2)[], C->(1), D->(2)]\")\\n\\n.. _ex-reentrant03:\\n.. ex::\\n      ::\\n\\n         [ A = (1) [ ] ]\\n         [ B = (2) [ ] ]\\n         [ C -> (1)    ]\\n         [ D -> (2)    ] -->\\n\\n2.1&nbsp;&nbsp;&nbsp;Subsumption and Unification\\nIt is standard to think of feature structures as providing partial\\ninformation about some object, in the sense that we can order\\nfeature structures according to how much information they contain. For example,\\n(23a) has less information than (23b), which in turn has less information than (23c).\\n\\n  (23)\\n  a.[NUMBER = 74]\\n\\n\\n\\n  b.[NUMBER = 74          ]\\n[STREET = \\'rue Pascal\\']\\n\\n\\n\\n  c.[NUMBER = 74          ]\\n[STREET = \\'rue Pascal\\']\\n[CITY = \\'Paris\\'       ]\\n\\n\\n\\nThis ordering is called subsumption; FS0 subsumes FS1 if all the\\ninformation contained in FS0 is also contained in FS1.\\nWe use the symbol ⊑ to represent subsumption.\\nWhen we add the possibility of reentrancy, we need to be more careful\\nabout how we describe subsumption: if\\nFS0 ⊑ FS1, then FS1 must have all the\\npaths and reentrancies of FS0. Thus, (20) subsumes\\n(22), since the latter has additional reentrancies. It should\\nbe obvious that subsumption only provides a partial ordering on\\nfeature structures, since some feature structures are\\nincommensurable. For example, (24) neither subsumes nor is subsumed\\nby (23a).\\n\\n  (24)[TELNO = 01 27 86 42 96]\\n\\n\\nSo we have seen that some feature structures carry more information than\\nothers. How do we go about adding more information to a given feature structure?\\nFor example, we might decide that addresses should\\nconsist of not just a street number and a street name, but also a\\ncity. That is, we might want to merge  graph (25b) with (25a) to\\nyield (25c).\\n\\n  (25)\\n  a.\\n\\n  b.\\n\\n  c.\\n\\nMerging information from two feature structures is called\\nunification and is supported by the unify() method.\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; fs1 = nltk.FeatStruct(NUMBER=74, STREET=\\'rue Pascal\\')\\n&gt;&gt;&gt; fs2 = nltk.FeatStruct(CITY=\\'Paris\\')\\n&gt;&gt;&gt; print(fs1.unify(fs2))\\n[ CITY   = \\'Paris\\'      ]\\n[ NUMBER = 74           ]\\n[ STREET = \\'rue Pascal\\' ]\\n\\n\\n\\nUnification is formally defined as a (partial) binary operation:\\nFS0 ⊔\\nFS1.\\nUnification is symmetric, so\\nFS0 ⊔\\nFS1 = FS1 ⊔\\nFS0.\\nThe same is true in Python:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; print(fs2.unify(fs1))\\n[ CITY   = \\'Paris\\'      ]\\n[ NUMBER = 74           ]\\n[ STREET = \\'rue Pascal\\' ]\\n\\n\\n\\n\\n>> fs1.unify(fs2) is fs2.unify(fs1)\\n    False\\nonly works with repr() -->\\nIf we unify two feature structures which stand in the subsumption\\nrelationship, then the result of unification is the most informative of\\nthe two:\\n\\n  (26)If FS0 ⊑ FS1,  then FS0\\n⊔ FS1 = FS1\\nFor example, the result of unifying (23b) with (23c) is (23c).\\nUnification between FS0 and FS1 will fail if the two feature structures share a path π,\\nbut the value of π in FS0 is a distinct\\natom from the value of π in FS1.\\nThis is implemented by setting the result of unification to be None.\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; fs0 = nltk.FeatStruct(A=\\'a\\')\\n&gt;&gt;&gt; fs1 = nltk.FeatStruct(A=\\'b\\')\\n&gt;&gt;&gt; fs2 = fs0.unify(fs1)\\n&gt;&gt;&gt; print(fs2)\\nNone\\n\\n\\n\\nNow, if we look at how unification interacts with structure-sharing,\\nthings become really interesting. First, let\\'s define (21) in Python:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; fs0 = nltk.FeatStruct(\"\"\"[NAME=Lee,\\n...                           ADDRESS=[NUMBER=74,\\n...                                    STREET=\\'rue Pascal\\'],\\n...                           SPOUSE= [NAME=Kim,\\n...                                    ADDRESS=[NUMBER=74,\\n...                                             STREET=\\'rue Pascal\\']]]\"\"\")\\n&gt;&gt;&gt; print(fs0)\\n[ ADDRESS = [ NUMBER = 74           ]               ]\\n[           [ STREET = \\'rue Pascal\\' ]               ]\\n[                                                   ]\\n[ NAME    = \\'Lee\\'                                   ]\\n[                                                   ]\\n[           [ ADDRESS = [ NUMBER = 74           ] ] ]\\n[ SPOUSE  = [           [ STREET = \\'rue Pascal\\' ] ] ]\\n[           [                                     ] ]\\n[           [ NAME    = \\'Kim\\'                     ] ]\\n\\n\\n\\nWhat happens when we augment Kim\\'s address with a specification\\nfor CITY?  Notice that fs1 needs to include the\\nwhole path from the root of the feature structure down to CITY.\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; fs1 = nltk.FeatStruct(\"[SPOUSE = [ADDRESS = [CITY = Paris]]]\")\\n&gt;&gt;&gt; print(fs1.unify(fs0))\\n[ ADDRESS = [ NUMBER = 74           ]               ]\\n[           [ STREET = \\'rue Pascal\\' ]               ]\\n[                                                   ]\\n[ NAME    = \\'Lee\\'                                   ]\\n[                                                   ]\\n[           [           [ CITY   = \\'Paris\\'      ] ] ]\\n[           [ ADDRESS = [ NUMBER = 74           ] ] ]\\n[ SPOUSE  = [           [ STREET = \\'rue Pascal\\' ] ] ]\\n[           [                                     ] ]\\n[           [ NAME    = \\'Kim\\'                     ] ]\\n\\n\\n\\nBy contrast, the result is very different if fs1 is unified with\\nthe structure-sharing version fs2 (also shown earlier as the graph\\n(22)):\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; fs2 = nltk.FeatStruct(\"\"\"[NAME=Lee, ADDRESS=(1)[NUMBER=74, STREET=\\'rue Pascal\\'],\\n...                           SPOUSE=[NAME=Kim, ADDRESS-&gt;(1)]]\"\"\")\\n&gt;&gt;&gt; print(fs1.unify(fs2))\\n[               [ CITY   = \\'Paris\\'      ] ]\\n[ ADDRESS = (1) [ NUMBER = 74           ] ]\\n[               [ STREET = \\'rue Pascal\\' ] ]\\n[                                         ]\\n[ NAME    = \\'Lee\\'                         ]\\n[                                         ]\\n[ SPOUSE  = [ ADDRESS -&gt; (1)  ]           ]\\n[           [ NAME    = \\'Kim\\' ]           ]\\n\\n\\n\\nRather than just updating what was in effect Kim\\'s \"copy\" of Lee\\'s address,\\nwe have now updated both their addresses at the same time. More\\ngenerally, if a unification adds information to the value of some\\npath π, then that unification simultaneously updates the value\\nof any path that is equivalent to π.\\n\\nAs we have already seen, structure sharing can also be stated\\nusing variables such as ?x.\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; fs1 = nltk.FeatStruct(\"[ADDRESS1=[NUMBER=74, STREET=\\'rue Pascal\\']]\")\\n&gt;&gt;&gt; fs2 = nltk.FeatStruct(\"[ADDRESS1=?x, ADDRESS2=?x]\")\\n&gt;&gt;&gt; print(fs2)\\n[ ADDRESS1 = ?x ]\\n[ ADDRESS2 = ?x ]\\n&gt;&gt;&gt; print(fs2.unify(fs1))\\n[ ADDRESS1 = (1) [ NUMBER = 74           ] ]\\n[                [ STREET = \\'rue Pascal\\' ] ]\\n[                                          ]\\n[ ADDRESS2 -&gt; (1)                          ]\\n\\n\\n\\n\\n\\n\\n3&nbsp;&nbsp;&nbsp;Extending a Feature based Grammar\\nIn this section, we return to feature based grammar and explore\\na variety of linguistic issues, and demonstrate the benefits\\nof incorporating features into the grammar.\\n\\n3.1&nbsp;&nbsp;&nbsp;Subcategorization\\nIn 8., we augmented our category labels to\\nrepresent different kinds of verb, and used the labels\\nIV and TV for intransitive and transitive verbs\\nrespectively.  This allowed us to write productions like the\\nfollowing:\\n\\n  (27)VP -&gt; IV\\nVP -&gt; TV NP\\n\\n\\nAlthough we know that IV and TV are two kinds of V,\\nthey are just atomic nonterminal symbols from a CFG, as distinct\\nfrom each other as any other pair of symbols.  This notation doesn\\'t\\nlet us say anything about verbs in general, e.g. we cannot say\\n\"All lexical items of category V can be marked for tense\",\\nsince walk, say, is an item of category IV, not V.\\nSo, can we replace category labels such as TV and IV\\nby V along with a feature that tells us whether\\nthe verb combines with a following NP object\\nor whether it can occur without any complement?\\nA simple approach, originally developed for a grammar framework\\ncalled Generalized Phrase Structure Grammar (GPSG), tries to solve\\nthis problem by allowing lexical\\ncategories to bear a SUBCAT which tells us what subcategorization\\nclass the item belongs to. While GPSG used integer values for\\nSUBCAT, the example below adopts more mnemonic values, namely\\nintrans, trans and clause:\\n\\n  (28)VP[TENSE=?t, NUM=?n] -&gt; V[SUBCAT=intrans, TENSE=?t, NUM=?n]\\nVP[TENSE=?t, NUM=?n] -&gt; V[SUBCAT=trans, TENSE=?t, NUM=?n] NP\\nVP[TENSE=?t, NUM=?n] -&gt; V[SUBCAT=clause, TENSE=?t, NUM=?n] SBar\\n\\nV[SUBCAT=intrans, TENSE=pres, NUM=sg] -&gt; \\'disappears\\' | \\'walks\\'\\nV[SUBCAT=trans, TENSE=pres, NUM=sg] -&gt; \\'sees\\' | \\'likes\\'\\nV[SUBCAT=clause, TENSE=pres, NUM=sg] -&gt; \\'says\\' | \\'claims\\'\\n\\nV[SUBCAT=intrans, TENSE=pres, NUM=pl] -&gt; \\'disappear\\' | \\'walk\\'\\nV[SUBCAT=trans, TENSE=pres, NUM=pl] -&gt; \\'see\\' | \\'like\\'\\nV[SUBCAT=clause, TENSE=pres, NUM=pl] -&gt; \\'say\\' | \\'claim\\'\\n\\nV[SUBCAT=intrans, TENSE=past, NUM=?n] -&gt; \\'disappeared\\' | \\'walked\\'\\nV[SUBCAT=trans, TENSE=past, NUM=?n] -&gt; \\'saw\\' | \\'liked\\'\\nV[SUBCAT=clause, TENSE=past, NUM=?n] -&gt; \\'said\\' | \\'claimed\\'\\n\\n\\nWhen we see a lexical category like V[SUBCAT=trans], we can\\ninterpret the SUBCAT specification as a pointer to a production in\\nwhich V[SUBCAT=trans] is introduced as the head child in a\\nVP production.  By convention, there is a correspondence between\\nthe values of SUBCAT and the productions that introduce lexical\\nheads.  On this approach, SUBCAT can only appear on lexical\\ncategories; it makes no sense, for example, to specify a SUBCAT\\nvalue on VP. As required, walk and like both belong to\\nthe category V. Nevertheless, walk will only occur in\\nVPs expanded by a production with the feature SUBCAT=intrans\\non the right hand side, as opposed to like, which requires a\\nSUBCAT=trans.\\nIn our third class of verbs above, we have specified a category\\nSBar. This is a label for subordinate clauses such as the\\ncomplement of claim in the example You claim that you like\\nchildren. We require two further productions to analyze such sentences:\\n\\n  (29)SBar -&gt; Comp S\\nComp -&gt; \\'that\\'\\n\\n\\nThe resulting structure is the following.\\n\\n  (30)\\nAn alternative treatment of subcategorization, due originally to a framework\\nknown as categorial grammar, is represented in feature based frameworks such as PATR\\nand Head-driven Phrase Structure Grammar. Rather than using\\nSUBCAT values as a way of indexing productions, the SUBCAT\\nvalue directly encodes the valency of a head (the list of\\narguments that it can combine with). For example, a verb like\\nput that takes NP and PP complements (put the\\nbook on the table) might be represented as (31):\\n\\n\\n  (31)V[SUBCAT=&lt;NP, NP, PP&gt;]\\n\\n\\nThis says that the verb can combine with three arguments. The\\nleftmost element in the list is the subject NP, while everything\\nelse — an NP followed by a PP in this case — comprises the\\nsubcategorized-for complements. When a verb like put is combined\\nwith appropriate complements, the requirements which are specified in\\nthe  SUBCAT are discharged, and only a subject NP is\\nneeded. This category, which corresponds to what is traditionally\\nthought of as VP, might be represented as follows.\\n\\n  (32)V[SUBCAT=&lt;NP&gt;]\\n\\n\\nFinally, a sentence is a kind of verbal category that has no\\nrequirements for further arguments, and hence has a SUBCAT\\nwhose value is the empty list. The tree (33) shows how these\\ncategory assignments combine in a parse of Kim put the book on the table.\\n\\n  (33)\\n\\n\\n3.2&nbsp;&nbsp;&nbsp;Heads Revisited\\n\\nWe noted in the previous section that by factoring subcategorization\\ninformation out of the main category label, we could express more\\ngeneralizations about properties of verbs. Another property of this\\nkind is the following: expressions of category V are heads of\\nphrases of category VP. Similarly,\\nNs are heads of NPs,\\nAs (i.e., adjectives) are heads of APs,  and\\nPs (i.e., prepositions) are heads of PPs.\\nNot all phrases have heads — for example, it is standard to say that coordinate\\nphrases (e.g., the book and the bell) lack heads —\\nnevertheless, we would like our grammar formalism to express the\\nparent / head-child relation where it holds.\\nAt present, V and VP are just atomic symbols, and\\nwe need to find a way to relate them using features\\n(as we did earlier to relate IV and TV).\\nX-bar Syntax addresses\\nthis issue by abstracting out the notion of phrasal level. It is\\nusual to recognize three such levels. If N represents the\\nlexical level, then N\\' represents the next level up,\\ncorresponding to the more traditional category Nom, while\\nN\\'\\' represents the phrasal level, corresponding to the\\ncategory NP.   (34a) illustrates a\\nrepresentative structure while  (34b) is the more conventional counterpart.\\n\\n  (34)\\n  a.\\n\\n  b.\\n\\n<!-- XXX The second half of the next paragraph is heavy going, for\\na relatively simple idea; it would be easier to follow if\\nthere was a diagram to demonstrate the contrast, giving\\na pair of structures that are minimally different, e.g.\\n\"put the chair on the stage\" vs \"saw the chair on the stage\".\\nAfter this, prose could formalize the concepts. -->\\nThe head of the structure (34a) is N while N\\'\\nand N\\'\\' are called (phrasal) projections of N. N\\'\\'\\nis the maximal projection, and N is sometimes called the\\nzero projection. One of the central claims of X-bar syntax is\\nthat all constituents share a structural similarity. Using X as\\na variable over N, V, A and P, we say that\\ndirectly subcategorized complements of a lexical head  X are always\\nplaced as siblings of the head, whereas adjuncts are\\nplaced as siblings of the intermediate category, X\\'. Thus, the\\nconfiguration of the two P\\'\\' adjuncts in (35) contrasts with that\\nof the complement P\\'\\' in (34a).\\n\\n  (35)\\nThe productions in (36) illustrate how bar levels can be encoded\\nusing feature structures. The nested structure in (35) is\\nachieved by two applications of the recursive rule expanding N[BAR=1].\\n\\n  (36)S -&gt; N[BAR=2] V[BAR=2]\\nN[BAR=2] -&gt; Det N[BAR=1]\\nN[BAR=1] -&gt; N[BAR=1] P[BAR=2]\\nN[BAR=1] -&gt; N[BAR=0] P[BAR=2]\\nN[BAR=1] -&gt; N[BAR=0]XS\\n\\n\\n\\n\\n3.3&nbsp;&nbsp;&nbsp;Auxiliary Verbs and Inversion\\nInverted clauses — where the order of subject and verb is\\nswitched — occur in English interrogatives and also after\\n\\'negative\\' adverbs:\\n\\n  (37)\\n  a.Do you like children?\\n\\n  b.Can Jody walk?\\n\\n\\n  (38)\\n  a.Rarely do you see Kim.\\n\\n  b.Never have I seen this dog.\\n\\nHowever, we cannot place just any verb in pre-subject position:\\n\\n  (39)\\n  a.*Like you children?\\n\\n  b.*Walks Jody?\\n\\n\\n  (40)\\n  a.*Rarely see you Kim.\\n\\n  b.*Never saw I this dog.\\n\\nVerbs that can be positioned initially in inverted clauses belong to\\nthe class known as auxiliaries, and as well as  do,\\ncan and have  include be, will  and\\nshall. One way of capturing such structures is with the\\nfollowing production:\\n\\n  (41)S[+INV] -&gt; V[+AUX] NP VP\\n\\n\\nThat is, a clause marked as [+INV] consists of an auxiliary\\nverb followed by a VP. (In a more detailed grammar, we would\\nneed to place some constraints on the form of the VP, depending\\non the choice of auxiliary.) (42) illustrates the structure of an\\ninverted clause.\\n\\n  (42)\\n\\n\\n3.4&nbsp;&nbsp;&nbsp;Unbounded Dependency Constructions\\nConsider the following contrasts:\\n\\n  (43)\\n  a.You like Jody.\\n\\n  b.*You like.\\n\\n\\n  (44)\\n  a.You put the card into the slot.\\n\\n  b.*You put into the slot.\\n\\n  c.*You put the card.\\n\\n  d.*You put.\\n\\nThe verb like requires an NP complement, while\\nput requires both a following NP and PP.\\n(43) and (44) show that these complements are obligatory:\\nomitting them leads to ungrammaticality. Yet there are contexts in\\nwhich obligatory complements can be omitted, as (45) and (46)\\nillustrate.\\n\\n  (45)\\n  a.Kim knows who you like.\\n\\n  b.This music, you really like.\\n\\n\\n  (46)\\n  a.Which card do you put into the slot?\\n\\n  b.Which slot do you put the card into?\\n\\nThat is, an obligatory complement can be omitted if there is an\\nappropriate filler in the sentence, such as the question word\\nwho in (45a), the preposed topic this music in (45b), or\\nthe wh phrases which card/slot in (46). It is common to\\nsay that sentences like (45) – (46) contain gaps where\\nthe obligatory complements have been omitted, and these gaps are\\nsometimes made explicit using an underscore:\\n\\n  (47)\\n  a.Which card do you put __ into the slot?\\n\\n  b.Which slot do you put the card into __?\\n\\nSo, a gap can occur if it is licensed by a filler. Conversely,\\nfillers can only occur if there is an appropriate gap elsewhere  in\\nthe sentence, as shown by the following examples.\\n\\n  (48)\\n  a.*Kim knows who you like Jody.\\n\\n  b.*This music, you really like hip-hop.\\n\\n\\n  (49)\\n  a.*Which card do you put this into the slot?\\n\\n  b.*Which slot do you put the card into this one?\\n\\nThe mutual co-occurence between filler and gap is sometimes termed a\\n\"dependency\". One issue of considerable importance in theoretical\\nlinguistics has been the nature of the material that can intervene\\nbetween a filler and the gap that it licenses; in particular, can we\\nsimply list a finite set of sequences that separate the two? The answer\\nis No: there is no upper bound on the distance between filler and\\ngap. This fact can be easily illustrated with constructions involving\\nsentential complements, as shown in (50).\\n\\n  (50)\\n  a.Who do you like __?\\n\\n  b.Who do you claim that you like __?\\n\\n  c.Who do you claim that Jody says that you like __?\\n\\nSince we can have indefinitely deep recursion of sentential\\ncomplements, the gap can be embedded indefinitely far inside the whole\\nsentence. This constellation of properties leads to the notion of an\\nunbounded dependency construction; that is, a filler-gap\\ndependency where there is no upper bound on the distance between\\nfiller and gap.\\nA variety of mechanisms have been suggested for handling unbounded\\ndependencies in formal grammars; here we illustrate the approach due to\\nGeneralized Phrase Structure Grammar that involves\\nslash categories. A slash category has the form Y/XP;\\nwe interpret this as a phrase of category Y that\\nis missing a sub-constituent of category XP. For example,\\nS/NP is an S that is missing an NP. The use of\\nslash categories is illustrated in (51).\\n\\n  (51)\\nThe top part of the tree introduces the filler who (treated as\\nan expression of category NP[+wh]) together with a\\ncorresponding gap-containing constituent S/NP. The gap information is\\nthen \"percolated\" down the tree via the VP/NP category, until it\\nreaches the category NP/NP. At this point, the dependency\\nis discharged by realizing the gap information as the empty string,\\nimmediately dominated by NP/NP.\\nDo we need to think of slash categories as a completely new kind of\\nobject?  Fortunately, we\\ncan accommodate them within our existing feature based framework,\\nby treating slash as a feature, and the category to its right\\nas a value; that is,  S/NP is reducible to S[SLASH=NP]. In practice,\\nthis is also how the parser interprets slash categories.\\nThe grammar shown in 3.1 illustrates\\nthe main principles of slash categories, and also includes productions for\\ninverted clauses. To simplify presentation, we have omitted any\\nspecification of tense on the verbs.\\n<!-- XXX The contents of feat1.fcfg seems to have changed in the file.\\nI won\\'t pull in the updated version in case the discussion also needs to be updated. -->\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; nltk.data.show_cfg(\\'grammars/book_grammars/feat1.fcfg\\')\\n% start S\\n# ###################\\n# Grammar Productions\\n# ###################\\nS[-INV] -&gt; NP VP\\nS[-INV]/?x -&gt; NP VP/?x\\nS[-INV] -&gt; NP S/NP\\nS[-INV] -&gt; Adv[+NEG] S[+INV]\\nS[+INV] -&gt; V[+AUX] NP VP\\nS[+INV]/?x -&gt; V[+AUX] NP VP/?x\\nSBar -&gt; Comp S[-INV]\\nSBar/?x -&gt; Comp S[-INV]/?x\\nVP -&gt; V[SUBCAT=intrans, -AUX]\\nVP -&gt; V[SUBCAT=trans, -AUX] NP\\nVP/?x -&gt; V[SUBCAT=trans, -AUX] NP/?x\\nVP -&gt; V[SUBCAT=clause, -AUX] SBar\\nVP/?x -&gt; V[SUBCAT=clause, -AUX] SBar/?x\\nVP -&gt; V[+AUX] VP\\nVP/?x -&gt; V[+AUX] VP/?x\\n# ###################\\n# Lexical Productions\\n# ###################\\nV[SUBCAT=intrans, -AUX] -&gt; \\'walk\\' | \\'sing\\'\\nV[SUBCAT=trans, -AUX] -&gt; \\'see\\' | \\'like\\'\\nV[SUBCAT=clause, -AUX] -&gt; \\'say\\' | \\'claim\\'\\nV[+AUX] -&gt; \\'do\\' | \\'can\\'\\nNP[-WH] -&gt; \\'you\\' | \\'cats\\'\\nNP[+WH] -&gt; \\'who\\'\\nAdv[+NEG] -&gt; \\'rarely\\' | \\'never\\'\\nNP/NP -&gt;\\nComp -&gt; \\'that\\'\\n\\n\\nExample 3.1 (code_slashcfg.py): Figure 3.1: Grammar with productions for inverted clauses and\\nlong-distance dependencies, making use of slash categories\\n\\nThe grammar in 3.1 contains one \"gap-introduction\"\\nproduction, namely S[-INV] -&gt; NP S/NP.\\nIn order to percolate the slash feature correctly, we need to add\\nslashes with variable values to both sides of the arrow in productions\\nthat expand S, VP and NP. For example, VP/?x -&gt; V SBar/?x is\\nthe slashed version of VP -&gt; V SBar and\\nsays that a slash value can be specified on the VP parent of a\\nconstituent if the same value is also specified on the SBar\\nchild. Finally, NP/NP -&gt;  allows the slash information on NP to\\nbe discharged as the empty string.\\nUsing 3.1, we can parse the sequence who do you claim that you\\nlike\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; tokens = \\'who do you claim that you like\\'.split()\\n&gt;&gt;&gt; from nltk import load_parser\\n&gt;&gt;&gt; cp = load_parser(\\'grammars/book_grammars/feat1.fcfg\\')\\n&gt;&gt;&gt; for tree in cp.parse(tokens):\\n...     print(tree)\\n(S[-INV]\\n  (NP[+WH] who)\\n  (S[+INV]/NP[]\\n    (V[+AUX] do)\\n    (NP[-WH] you)\\n    (VP[]/NP[]\\n      (V[-AUX, SUBCAT=\\'clause\\'] claim)\\n      (SBar[]/NP[]\\n        (Comp[] that)\\n        (S[-INV]/NP[]\\n          (NP[-WH] you)\\n          (VP[]/NP[] (V[-AUX, SUBCAT=\\'trans\\'] like) (NP[]/NP[] )))))))\\n\\n\\n\\n\\nA more readable version of this tree is shown in (52).\\n\\n  (52)\\nThe grammar in 3.1 will also allow us to parse sentences\\nwithout gaps:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; tokens = \\'you claim that you like cats\\'.split()\\n&gt;&gt;&gt; for tree in cp.parse(tokens):\\n...     print(tree)\\n(S[-INV]\\n  (NP[-WH] you)\\n  (VP[]\\n    (V[-AUX, SUBCAT=\\'clause\\'] claim)\\n    (SBar[]\\n      (Comp[] that)\\n      (S[-INV]\\n        (NP[-WH] you)\\n        (VP[] (V[-AUX, SUBCAT=\\'trans\\'] like) (NP[-WH] cats))))))\\n\\n\\n\\nIn addition, it admits inverted sentences which do not involve\\nwh constructions:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; tokens = \\'rarely do you sing\\'.split()\\n&gt;&gt;&gt; for tree in cp.parse(tokens):\\n...     print(tree)\\n(S[-INV]\\n  (Adv[+NEG] rarely)\\n  (S[+INV]\\n    (V[+AUX] do)\\n    (NP[-WH] you)\\n    (VP[] (V[-AUX, SUBCAT=\\'intrans\\'] sing))))\\n\\n\\n\\n\\n\\n3.5&nbsp;&nbsp;&nbsp;Case and Gender in German\\nCompared with English, German has a relatively rich morphology for\\nagreement. For example, the definite article in German varies with\\ncase, gender and number, as shown in 3.1.\\nTable 3.1: Morphological Paradigm for the German definite Article\\n\\n\\n\\n\\n\\n\\n\\n\\nCase\\nMasc\\nFem\\nNeut\\nPlural\\n\\nNom\\nder\\ndie\\ndas\\ndie\\n\\nGen\\ndes\\nder\\ndes\\nder\\n\\nDat\\ndem\\nder\\ndem\\nden\\n\\nAcc\\nden\\ndie\\ndas\\ndie\\n\\n\\n\\n\\n\\nSubjects in German take the nominative case, and most verbs\\ngovern their objects in the accusative case. However, there are\\nexceptions like helfen that govern the dative case:\\n\\n  (53)\\nSystem Message: ERROR/3 (ch09.rst2, line 1693)\\nError in \"gloss\" directive: may contain a single table only.\\n\\nSystem Message: ERROR/3 (ch09.rst2, line 1698)\\nError in \"gloss\" directive: may contain a single table only.\\n\\nSystem Message: ERROR/3 (ch09.rst2, line 1702)\\nError in \"gloss\" directive: may contain a single table only.\\n\\nSystem Message: ERROR/3 (ch09.rst2, line 1707)\\nError in \"gloss\" directive: may contain a single table only.\\n\\nThe grammar in 3.2 illustrates the interaction of agreement\\n(comprising person, number and gender) with case.\\n<!-- XXX The contents of german.fcfg seems to have changed in the file.\\nI won\\'t pull in the updated version in case the discussion also needs to be updated. -->\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; nltk.data.show_cfg(\\'grammars/book_grammars/german.fcfg\\')\\n% start S\\n # Grammar Productions\\n S -&gt; NP[CASE=nom, AGR=?a] VP[AGR=?a]\\n NP[CASE=?c, AGR=?a] -&gt; PRO[CASE=?c, AGR=?a]\\n NP[CASE=?c, AGR=?a] -&gt; Det[CASE=?c, AGR=?a] N[CASE=?c, AGR=?a]\\n VP[AGR=?a] -&gt; IV[AGR=?a]\\n VP[AGR=?a] -&gt; TV[OBJCASE=?c, AGR=?a] NP[CASE=?c]\\n # Lexical Productions\\n # Singular determiners\\n # masc\\n Det[CASE=nom, AGR=[GND=masc,PER=3,NUM=sg]] -&gt; \\'der\\'\\n Det[CASE=dat, AGR=[GND=masc,PER=3,NUM=sg]] -&gt; \\'dem\\'\\n Det[CASE=acc, AGR=[GND=masc,PER=3,NUM=sg]] -&gt; \\'den\\'\\n # fem\\n Det[CASE=nom, AGR=[GND=fem,PER=3,NUM=sg]] -&gt; \\'die\\'\\n Det[CASE=dat, AGR=[GND=fem,PER=3,NUM=sg]] -&gt; \\'der\\'\\n Det[CASE=acc, AGR=[GND=fem,PER=3,NUM=sg]] -&gt; \\'die\\'\\n # Plural determiners\\n Det[CASE=nom, AGR=[PER=3,NUM=pl]] -&gt; \\'die\\'\\n Det[CASE=dat, AGR=[PER=3,NUM=pl]] -&gt; \\'den\\'\\n Det[CASE=acc, AGR=[PER=3,NUM=pl]] -&gt; \\'die\\'\\n # Nouns\\n N[AGR=[GND=masc,PER=3,NUM=sg]] -&gt; \\'Hund\\'\\n N[CASE=nom, AGR=[GND=masc,PER=3,NUM=pl]] -&gt; \\'Hunde\\'\\n N[CASE=dat, AGR=[GND=masc,PER=3,NUM=pl]] -&gt; \\'Hunden\\'\\n N[CASE=acc, AGR=[GND=masc,PER=3,NUM=pl]] -&gt; \\'Hunde\\'\\n N[AGR=[GND=fem,PER=3,NUM=sg]] -&gt; \\'Katze\\'\\n N[AGR=[GND=fem,PER=3,NUM=pl]] -&gt; \\'Katzen\\'\\n # Pronouns\\n PRO[CASE=nom, AGR=[PER=1,NUM=sg]] -&gt; \\'ich\\'\\n PRO[CASE=acc, AGR=[PER=1,NUM=sg]] -&gt; \\'mich\\'\\n PRO[CASE=dat, AGR=[PER=1,NUM=sg]] -&gt; \\'mir\\'\\n PRO[CASE=nom, AGR=[PER=2,NUM=sg]] -&gt; \\'du\\'\\n PRO[CASE=nom, AGR=[PER=3,NUM=sg]] -&gt; \\'er\\' | \\'sie\\' | \\'es\\'\\n PRO[CASE=nom, AGR=[PER=1,NUM=pl]] -&gt; \\'wir\\'\\n PRO[CASE=acc, AGR=[PER=1,NUM=pl]] -&gt; \\'uns\\'\\n PRO[CASE=dat, AGR=[PER=1,NUM=pl]] -&gt; \\'uns\\'\\n PRO[CASE=nom, AGR=[PER=2,NUM=pl]] -&gt; \\'ihr\\'\\n PRO[CASE=nom, AGR=[PER=3,NUM=pl]] -&gt; \\'sie\\'\\n # Verbs\\n IV[AGR=[NUM=sg,PER=1]] -&gt; \\'komme\\'\\n IV[AGR=[NUM=sg,PER=2]] -&gt; \\'kommst\\'\\n IV[AGR=[NUM=sg,PER=3]] -&gt; \\'kommt\\'\\n IV[AGR=[NUM=pl, PER=1]] -&gt; \\'kommen\\'\\n IV[AGR=[NUM=pl, PER=2]] -&gt; \\'kommt\\'\\n IV[AGR=[NUM=pl, PER=3]] -&gt; \\'kommen\\'\\n TV[OBJCASE=acc, AGR=[NUM=sg,PER=1]] -&gt; \\'sehe\\' | \\'mag\\'\\n TV[OBJCASE=acc, AGR=[NUM=sg,PER=2]] -&gt; \\'siehst\\' | \\'magst\\'\\n TV[OBJCASE=acc, AGR=[NUM=sg,PER=3]] -&gt; \\'sieht\\' | \\'mag\\'\\n TV[OBJCASE=dat, AGR=[NUM=sg,PER=1]] -&gt; \\'folge\\' | \\'helfe\\'\\n TV[OBJCASE=dat, AGR=[NUM=sg,PER=2]] -&gt; \\'folgst\\' | \\'hilfst\\'\\n TV[OBJCASE=dat, AGR=[NUM=sg,PER=3]] -&gt; \\'folgt\\' | \\'hilft\\'\\n TV[OBJCASE=acc, AGR=[NUM=pl,PER=1]] -&gt; \\'sehen\\' | \\'moegen\\'\\n TV[OBJCASE=acc, AGR=[NUM=pl,PER=2]] -&gt; \\'sieht\\' | \\'moegt\\'\\n TV[OBJCASE=acc, AGR=[NUM=pl,PER=3]] -&gt; \\'sehen\\' | \\'moegen\\'\\n TV[OBJCASE=dat, AGR=[NUM=pl,PER=1]] -&gt; \\'folgen\\' | \\'helfen\\'\\n TV[OBJCASE=dat, AGR=[NUM=pl,PER=2]] -&gt; \\'folgt\\' | \\'helft\\'\\n TV[OBJCASE=dat, AGR=[NUM=pl,PER=3]] -&gt; \\'folgen\\' | \\'helfen\\'\\n\\n\\nExample 3.2 (code_germancfg.py): Figure 3.2: Example Feature based Grammar\\n\\nAs you can see, the feature objcase is used to specify the case that\\na verb governs on its object. The next example illustrates the parse\\ntree for a sentence containing a verb which governs dative case.\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; tokens = \\'ich folge den Katzen\\'.split()\\n&gt;&gt;&gt; cp = load_parser(\\'grammars/book_grammars/german.fcfg\\')\\n&gt;&gt;&gt; for tree in cp.parse(tokens):\\n...     print(tree)\\n(S[]\\n  (NP[AGR=[NUM=\\'sg\\', PER=1], CASE=\\'nom\\']\\n    (PRO[AGR=[NUM=\\'sg\\', PER=1], CASE=\\'nom\\'] ich))\\n  (VP[AGR=[NUM=\\'sg\\', PER=1]]\\n    (TV[AGR=[NUM=\\'sg\\', PER=1], OBJCASE=\\'dat\\'] folge)\\n    (NP[AGR=[GND=\\'fem\\', NUM=\\'pl\\', PER=3], CASE=\\'dat\\']\\n      (Det[AGR=[NUM=\\'pl\\', PER=3], CASE=\\'dat\\'] den)\\n      (N[AGR=[GND=\\'fem\\', NUM=\\'pl\\', PER=3]] Katzen))))\\n\\n\\n\\nIn developing grammars, excluding ungrammatical word sequences is often as\\nchallenging as parsing grammatical ones. In order to get an idea\\nwhere and why a sequence fails to parse, setting the trace\\nparameter of the load_parser() method can be crucial. Consider the\\nfollowing parse failure:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; tokens = \\'ich folge den Katze\\'.split()\\n&gt;&gt;&gt; cp = load_parser(\\'grammars/book_grammars/german.fcfg\\', trace=2)\\n&gt;&gt;&gt; for tree in cp.parse(tokens):\\n...     print(tree)\\n|.ich.fol.den.Kat.|\\nLeaf Init Rule:\\n|[---]   .   .   .| [0:1] \\'ich\\'\\n|.   [---]   .   .| [1:2] \\'folge\\'\\n|.   .   [---]   .| [2:3] \\'den\\'\\n|.   .   .   [---]| [3:4] \\'Katze\\'\\nFeature Bottom Up Predict Combine Rule:\\n|[---]   .   .   .| [0:1] PRO[AGR=[NUM=\\'sg\\', PER=1], CASE=\\'nom\\']\\n                          -&gt; \\'ich\\' *\\nFeature Bottom Up Predict Combine Rule:\\n|[---]   .   .   .| [0:1] NP[AGR=[NUM=\\'sg\\', PER=1], CASE=\\'nom\\'] -&gt; PRO[AGR=[NUM=\\'sg\\', PER=1], CASE=\\'nom\\'] *\\nFeature Bottom Up Predict Combine Rule:\\n|[---&gt;   .   .   .| [0:1] S[] -&gt; NP[AGR=?a, CASE=\\'nom\\'] * VP[AGR=?a] {?a: [NUM=\\'sg\\', PER=1]}\\nFeature Bottom Up Predict Combine Rule:\\n|.   [---]   .   .| [1:2] TV[AGR=[NUM=\\'sg\\', PER=1], OBJCASE=\\'dat\\'] -&gt; \\'folge\\' *\\nFeature Bottom Up Predict Combine Rule:\\n|.   [---&gt;   .   .| [1:2] VP[AGR=?a] -&gt; TV[AGR=?a, OBJCASE=?c] * NP[CASE=?c] {?a: [NUM=\\'sg\\', PER=1], ?c: \\'dat\\'}\\nFeature Bottom Up Predict Combine Rule:\\n|.   .   [---]   .| [2:3] Det[AGR=[GND=\\'masc\\', NUM=\\'sg\\', PER=3], CASE=\\'acc\\'] -&gt; \\'den\\' *\\n|.   .   [---]   .| [2:3] Det[AGR=[NUM=\\'pl\\', PER=3], CASE=\\'dat\\'] -&gt; \\'den\\' *\\nFeature Bottom Up Predict Combine Rule:\\n|.   .   [---&gt;   .| [2:3] NP[AGR=?a, CASE=?c] -&gt; Det[AGR=?a, CASE=?c] * N[AGR=?a, CASE=?c] {?a: [NUM=\\'pl\\', PER=3], ?c: \\'dat\\'}\\nFeature Bottom Up Predict Combine Rule:\\n|.   .   [---&gt;   .| [2:3] NP[AGR=?a, CASE=?c] -&gt; Det[AGR=?a, CASE=?c] * N[AGR=?a, CASE=?c] {?a: [GND=\\'masc\\', NUM=\\'sg\\', PER=3], ?c: \\'acc\\'}\\nFeature Bottom Up Predict Combine Rule:\\n|.   .   .   [---]| [3:4] N[AGR=[GND=\\'fem\\', NUM=\\'sg\\', PER=3]] -&gt; \\'Katze\\' *\\n\\n\\n\\nThe last two Scanner lines in the trace show that den is recognized as\\nadmitting two possible categories: Det[AGR=[GND=\\'masc\\', NUM=\\'sg\\',\\nPER=3], CASE=\\'acc\\'] and Det[AGR=[NUM=\\'pl\\', PER=3], CASE=\\'dat\\'].\\nWe know from the grammar in 3.2 that Katze has category\\nN[AGR=[GND=fem, NUM=sg, PER=3]]. Thus there is no binding for the\\nvariable ?a in production NP[CASE=?c, AGR=?a] -&gt; Det[CASE=?c,\\nAGR=?a] N[CASE=?c, AGR=?a] which will satisfy these constraints, since the\\nAGR value of Katze will not unify with either of the AGR\\nvalues of  den, that is, with either [GND=\\'masc\\', NUM=\\'sg\\',\\nPER=3] or [NUM=\\'pl\\', PER=3].\\n\\n\\n\\n4&nbsp;&nbsp;&nbsp;Summary\\n\\nThe traditional categories of context-free grammar are atomic\\nsymbols. An important motivation for feature structures is to capture\\nfine-grained distinctions that would otherwise require a massive\\nmultiplication of atomic categories.\\nBy using variables over feature values, we can express constraints\\nin grammar productions that allow the realization of different feature\\nspecifications to be inter-dependent.\\nTypically we specify fixed values of features at the lexical level\\nand constrain the values of features in phrases to unify with the\\ncorresponding values in their children.\\nFeature values are either atomic or complex. A particular sub-case of\\natomic value is the Boolean value, represented by convention as\\n[+/- f].\\nTwo features can share a value (either atomic or\\ncomplex). Structures with shared values are said to be\\nre-entrant. Shared values are represented by numerical indexes (or\\ntags) in AVMs.\\nA path in a feature structure is a tuple of features\\ncorresponding to the labels on  a sequence of arcs from the root of the graph\\nrepresentation.\\nTwo paths are equivalent if they share a value.\\nFeature structures are partially ordered by subsumption.\\nFS0 subsumes FS1 when\\nall the information contained in\\nFS0 is also present in\\nFS1.\\nThe unification of two structures FS0 and\\nFS1, if successful, is the feature\\nstructure FS2 that contains the combined\\ninformation of both FS0 and FS1.\\nIf unification adds information to a path π in FS, then it also\\nadds information to every path π\\' equivalent to π.\\nWe can use feature structures to build succinct analyses of a wide\\nvariety of linguistic phenomena, including verb subcategorization,\\ninversion constructions, unbounded dependency constructions and case government.\\n\\n\\n\\n5&nbsp;&nbsp;&nbsp;Further Reading\\nPlease consult http://nltk.org/ for further materials on this chapter, including\\nfeature structures, feature grammars, and grammar test suites.\\nX-bar Syntax: (Jacobs &amp; Rosenbaum, 1970), (Jackendoff, 1977)\\n(The primes we use replace Chomsky\\'s typographically more demanding horizontal bars.)\\nFor an excellent introduction to the phenomenon of agreement, see\\n(Corbett, 2006).\\nThe earliest use of features in theoretical linguistics was designed\\nto capture phonological properties of phonemes. For example, a sound\\nlike /b/ might be decomposed into the structure [+labial, +voice]. An important motivation was to capture\\ngeneralizations across classes of segments; for example, that /n/ gets\\nrealized as /m/ preceding any +labial consonant.\\nWithin Chomskyan grammar, it was standard to use atomic features for\\nphenomena like agreement, and also to capture generalizations across\\nsyntactic categories, by analogy with phonology.\\nA radical expansion of the use of features in theoretical syntax was\\nadvocated by Generalized Phrase Structure Grammar (GPSG;\\n(Gazdar, Klein, &amp; and, 1985)), particularly in the use of features with complex values.\\nComing more from the perspective of computational linguistics,\\n(Dahl &amp; Saint-Dizier, 1985) proposed that functional aspects of language could be\\ncaptured by unification of attribute-value structures, and a similar\\napproach was elaborated by (Grosz &amp; Stickel, 1983) within the PATR-II\\nformalism. Early work in Lexical-Functional grammar (LFG;\\n(Bresnan, 1982)) introduced the notion of an f-structure that\\nwas primarily intended to represent the grammatical relations and\\npredicate-argument structure associated with a constituent structure\\nparse.  (Shieber, 1986) provides an excellent introduction to this\\nphase of research into feature based grammars.\\nOne conceptual difficulty with algebraic approaches to feature\\nstructures arose when researchers attempted to model negation. An\\nalternative perspective, pioneered by (Kasper &amp; Rounds, 1986) and\\n(Johnson, 1988), argues that grammars involve descriptions of\\nfeature structures rather than the structures themselves. These\\ndescriptions are combined using logical operations such as\\nconjunction, and negation is just the usual logical operation over\\nfeature descriptions. This description-oriented perspective was\\nintegral to LFG from the outset (cf. (Huang &amp; Chen, 1989), and was also adopted by later\\nversions of Head-Driven Phrase Structure Grammar (HPSG;\\n(Sag &amp; Wasow, 1999)). A comprehensive bibliography of HPSG literature can be\\nfound at http://www.cl.uni-bremen.de/HPSG-Bib/.\\nFeature structures, as presented in this chapter, are unable to\\ncapture important constraints on linguistic information. For example,\\nthere is no way of saying that the only permissible values for\\nNUM are sg and pl, while a specification such\\nas [NUM=masc] is anomalous. Similarly, we cannot say\\nthat the complex value of AGR must contain\\nspecifications for the features PER, NUM and\\ngnd, but cannot contain a specification such as\\n[SUBCAT=trans].  Typed feature structures were developed to\\nremedy this deficiency. To begin with, we stipulate that feature\\nvalues are always typed. In the case of atomic values, the values just\\nare types. For example, we would say that the value of NUM is\\nthe type num. Moreover, num is the most general type of value for\\nNUM. Since types are organized hierarchically, we can be more\\ninformative by specifying the value of NUM is a subtype\\nof num, namely either sg or pl.\\nIn the case of complex values, we say that feature structures are\\nthemselves typed. So for example the value of AGR will be a\\nfeature structure of type AGR. We also stipulate that all and only\\nPER, NUM and GND are appropriate features for\\na structure of type AGR.  A good early review of work on typed\\nfeature structures is (Emele &amp; Zajac, 1990). A more comprehensive examination of\\nthe formal foundations can be found in (Carpenter, 1992), while\\n(Copestake, 2002) focuses on implementing an HPSG-oriented approach\\nto typed feature structures.\\nThere is a copious literature on the analysis of German within\\nfeature based grammar frameworks. (Nerbonne, Netter, &amp; Pollard, 1994) is a good\\nstarting point for the HPSG literature on this topic, while\\n(M{\\\\\"u}ller, 2002) gives a very extensive and detailed analysis of\\nGerman syntax in HPSG.\\nChapter 15 of (Jurafsky &amp; Martin, 2008) discusses feature structures,\\nthe unification algorithm, and the integration of unification into\\nparsing algorithms.\\n\\n\\n6&nbsp;&nbsp;&nbsp;Exercises\\n\\n☼ What constraints are required to correctly parse word sequences like I am\\nhappy and she is happy but not *you is happy or\\n*they am happy? Implement two solutions for the present tense\\nparadigm of the verb be in English, first taking Grammar\\n(6) as your starting point, and then taking Grammar (18)\\nas the starting point.\\n\\n☼ Develop a variant of grammar in 1.1 that uses a\\nfeature count to make the distinctions shown below:\\n\\n  (54)\\n  a.The boy sings.\\n\\n\\n  b.*Boy sings.\\n\\n\\n\\n  (55)\\n  a.The boys sing.\\n\\n\\n  b.Boys sing.\\n\\n\\n\\n  (56)\\n  a.The boys sing.\\n\\n\\n  b.Boys sing.\\n\\n\\n\\n  (57)\\n  a.The water is precious.\\n\\n\\n  b.Water is precious.\\n\\n\\n\\n☼ Write a function subsumes() which holds of two feature\\nstructures fs1 and fs2 just in case fs1 subsumes fs2.\\n\\n☼ Modify the grammar illustrated in (28) to\\nincorporate a bar feature for dealing with phrasal projections.\\n\\n☼ Modify the German grammar in 3.2 to incorporate the\\ntreatment of subcategorization presented in 3.\\n\\n◑ Develop a feature based grammar that will correctly describe the following\\nSpanish noun phrases:\\n\\nSystem Message: ERROR/3 (ch09.rst2, line 2028)\\nError in \"gloss\" directive: may contain a single table only.\\n\\n\\nSystem Message: ERROR/3 (ch09.rst2, line 2033)\\nError in \"gloss\" directive: may contain a single table only.\\n\\n\\nSystem Message: ERROR/3 (ch09.rst2, line 2038)\\nError in \"gloss\" directive: may contain a single table only.\\n\\n\\nSystem Message: ERROR/3 (ch09.rst2, line 2043)\\nError in \"gloss\" directive: may contain a single table only.\\n\\n\\n◑ Develop your own version of the EarleyChartParser which only\\nprints a trace if the input sequence fails to parse.\\n\\n◑ Consider the feature structures shown in 6.1.\\n<!-- XX NOTE: This example is somewhat broken - - nltk doesn\\'t support\\nreentrance for base feature values.  (See email ~7/23/08 to the\\nnltk-users mailing list for details.)\\nNow updated to avoid this problem. EK -->\\n\\n\\n\\n\\n&nbsp;\\nfs1 = nltk.FeatStruct(\"[A = ?x, B= [C = ?x]]\")\\nfs2 = nltk.FeatStruct(\"[B = [D = d]]\")\\nfs3 = nltk.FeatStruct(\"[B = [C = d]]\")\\nfs4 = nltk.FeatStruct(\"[A = (1)[B = b], C-&gt;(1)]\")\\nfs5 = nltk.FeatStruct(\"[A = (1)[D = ?x], C = [E -&gt; (1), F = ?x] ]\")\\nfs6 = nltk.FeatStruct(\"[A = [D = d]]\")\\nfs7 = nltk.FeatStruct(\"[A = [D = d], C = [F = [D = d]]]\")\\nfs8 = nltk.FeatStruct(\"[A = (1)[D = ?x, G = ?x], C = [B = ?x, E -&gt; (1)] ]\")\\nfs9 = nltk.FeatStruct(\"[A = [B = b], C = [E = [G = e]]]\")\\nfs10 = nltk.FeatStruct(\"[A = (1)[B = b], C -&gt; (1)]\")\\n\\n\\nExample 6.1 (code_featstructures.py): Figure 6.1: Exploring Feature Structures\\n\\nWork out on paper what the result is of the following\\nunifications. (Hint: you might find it useful to draw the graph structures.)\\n\\nfs1 and fs2\\nfs1 and fs3\\nfs4 and fs5\\nfs5 and fs6\\nfs5 and fs7\\nfs8 and fs9\\nfs8 and fs10\\n\\nCheck your answers using Python.\\n\\n◑ List two feature structures that subsume [A=?x, B=?x].\\n\\n◑ Ignoring structure sharing, give an informal algorithm for unifying\\ntwo feature structures.\\n\\n◑ Extend the German grammar in 3.2 so that it can\\nhandle so-called verb-second structures like the following:\\n\\n  (58)Heute sieht der Hund die Katze.\\n\\n\\n◑ Seemingly synonymous verbs have slightly different\\nsyntactic properties (Levin, 1993).  Consider the patterns\\nof grammaticality for the verbs loaded, filled, and dumped\\nbelow.  Can you write grammar productions to handle such data?\\n\\n  (59)\\n  a.The farmer loaded the cart with sand\\n\\n\\n  b.The farmer loaded sand into the cart\\n\\n\\n  c.The farmer filled the cart with sand\\n\\n\\n  d.*The farmer filled sand into the cart\\n\\n\\n  e.*The farmer dumped the cart with sand\\n\\n\\n  f.The farmer dumped sand into the cart\\n\\n\\n\\n★ Morphological paradigms are rarely completely regular, in\\nthe sense of every cell in the matrix having a different\\nrealization. For example, the present tense conjugation of the\\nlexeme walk only has two distinct forms: walks for the\\n3rd person singular, and walk for all other combinations of\\nperson and number. A successful analysis should not require\\nredundantly specifying that 5 out of the 6 possible morphological\\ncombinations have the same realization.  Propose and implement a\\nmethod for dealing with this.\\n\\n★ So-called head features are shared between the parent\\nnode and head child. For example, TENSE is a head feature\\nthat is shared between a VP and its head V\\nchild. See (Gazdar, Klein, &amp; and, 1985) for more details. Most of the\\nfeatures we have looked at are head features — exceptions are\\nSUBCAT and SLASH. Since the sharing of head\\nfeatures is predictable, it should not need to be stated explicitly\\nin the grammar productions. Develop an approach that automatically\\naccounts for this regular behavior of head features.\\n\\n★ Extend NLTK\\'s treatment of feature structures to allow unification into\\nlist-valued features, and use this to implement an HPSG-style analysis of\\nsubcategorization, whereby the SUBCAT of a head category is the\\nconcatenation its complements\\' categories with the SUBCAT value of its\\nimmediate parent.\\n\\n★ Extend NLTK\\'s treatment of feature structures to allow productions with\\nunderspecified categories, such as S[-INV] --&gt; ?x S/?x.\\n\\n★ Extend NLTK\\'s treatment of feature structures to allow typed feature\\nstructures.\\n\\n★ Pick some grammatical constructions described in (Huddleston &amp; Pullum, 2002),\\nand develop a feature based grammar to account for them.\\n\\n\\n\\n\\nAbout this document...\\nUPDATED FOR NLTK 3.0.\\nThis is a chapter from Natural Language Processing with Python,\\nby Steven Bird, Ewan Klein and Edward Loper,\\nCopyright © 2019 the authors.\\nIt is distributed with the Natural Language Toolkit [http://nltk.org/],\\nVersion 3.0, under the terms of the\\nCreative Commons Attribution-Noncommercial-No Derivative Works 3.0 United States License\\n[http://creativecommons.org/licenses/by-nc-nd/3.0/us/].\\nThis document was built on\\nWed  4 Sep 2019 11:40:48 ACST\\n\\n\\n\\n\\n\\n',\n",
       " \"\\n\\n\\n  NLTK Book\\n  \\n\\n\\n\\nNatural Language Processing with Python\\n– Analyzing Text with the Natural Language Toolkit\\n\\nSteven Bird, Ewan Klein, and Edward Loper\\n\\n\\nThis version of the NLTK book is updated for Python 3 and NLTK 3.\\n  The first edition of the book, published by O'Reilly, is available at\\n  http://nltk.org/book_1ed/.\\n  (There are currently no plans for a second edition of the book.)\\n\\n\\n\\n\\n0. Preface\\n1. Language Processing and Python\\n2. Accessing Text Corpora and Lexical Resources\\n3. Processing Raw Text\\n4. Writing Structured Programs\\n5. Categorizing and Tagging Words (minor fixes still required)\\n6. Learning to Classify Text\\n7. Extracting Information from Text\\n8. Analyzing Sentence Structure\\n9. Building Feature Based Grammars\\n10. Analyzing the Meaning of Sentences (minor fixes still required)\\n11. Managing Linguistic Data (minor fixes still required)\\n12. Afterword: Facing the Language Challenge\\nBibliography  Term Index\\n\\n\\n\\nThis book is made available under the terms of the Creative Commons Attribution Noncommercial No-Derivative-Works 3.0 US License.\\n\\nPlease post any questions about the materials to the nltk-users mailing list.\\nPlease report any errors on the issue tracker.\\n\\n\\n\\n\",\n",
       " '\\n\\n\\n\\nfunction astext(node)\\n{\\n    return node.innerHTML.replace(/(]+)>)/ig,\"\")\\n                         .replace(/&gt;/ig, \">\")\\n                         .replace(/&lt;/ig, \"<\")\\n                         .replace(/&quot;/ig, \\'\"\\')\\n                         .replace(/&amp;/ig, \"&\");\\n}\\n\\nfunction copy_notify(node, bar_color, data)\\n{\\n    // The outer box: relative + inline positioning.\\n    var box1 = document.createElement(\"div\");\\n    box1.style.position = \"relative\";\\n    box1.style.display = \"inline\";\\n    box1.style.top = \"2em\";\\n    box1.style.left = \"1em\";\\n  \\n    // A shadow for fun\\n    var shadow = document.createElement(\"div\");\\n    shadow.style.position = \"absolute\";\\n    shadow.style.left = \"-1.3em\";\\n    shadow.style.top = \"-1.3em\";\\n    shadow.style.background = \"#404040\";\\n    \\n    // The inner box: absolute positioning.\\n    var box2 = document.createElement(\"div\");\\n    box2.style.position = \"relative\";\\n    box2.style.border = \"1px solid #a0a0a0\";\\n    box2.style.left = \"-.2em\";\\n    box2.style.top = \"-.2em\";\\n    box2.style.background = \"white\";\\n    box2.style.padding = \".3em .4em .3em .4em\";\\n    box2.style.fontStyle = \"normal\";\\n    box2.style.background = \"#f0e0e0\";\\n\\n    node.insertBefore(box1, node.childNodes.item(0));\\n    box1.appendChild(shadow);\\n    shadow.appendChild(box2);\\n    box2.innerHTML=\"Copied&nbsp;to&nbsp;the&nbsp;clipboard: \" +\\n                   \"\"+\\n                   data+\"\";\\n    setTimeout(function() { node.removeChild(box1); }, 1000);\\n\\n    var elt = node.parentNode.firstChild;\\n    elt.style.background = \"#ffc0c0\";\\n    setTimeout(function() { elt.style.background = bar_color; }, 200);\\n}\\n\\nfunction copy_codeblock_to_clipboard(node)\\n{\\n    var data = astext(node)+\"\\\\n\";\\n    if (copy_text_to_clipboard(data)) {\\n        copy_notify(node, \"#40a060\", data);\\n    }\\n}\\n\\nfunction copy_doctest_to_clipboard(node)\\n{\\n    var s = astext(node)+\"\\\\n   \";\\n    var data = \"\";\\n\\n    var start = 0;\\n    var end = s.indexOf(\"\\\\n\");\\n    while (end >= 0) {\\n        if (s.substring(start, start+4) == \">>> \") {\\n            data += s.substring(start+4, end+1);\\n        }\\n        else if (s.substring(start, start+4) == \"... \") {\\n            data += s.substring(start+4, end+1);\\n        }\\n        /*\\n        else if (end-start > 1) {\\n            data += \"# \" + s.substring(start, end+1);\\n        }*/\\n        // Grab the next line.\\n        start = end+1;\\n        end = s.indexOf(\"\\\\n\", start);\\n    }\\n    \\n    if (copy_text_to_clipboard(data)) {\\n        copy_notify(node, \"#4060a0\", data);\\n    }\\n}\\n    \\nfunction copy_text_to_clipboard(data)\\n{\\n    if (window.clipboardData) {\\n        window.clipboardData.setData(\"Text\", data);\\n        return true;\\n     }\\n    else if (window.netscape) {\\n        // w/ default firefox settings, permission will be denied for this:\\n        netscape.security.PrivilegeManager\\n                      .enablePrivilege(\"UniversalXPConnect\");\\n    \\n        var clip = Components.classes[\"@mozilla.org/widget/clipboard;1\"]\\n                      .createInstance(Components.interfaces.nsIClipboard);\\n        if (!clip) return;\\n    \\n        var trans = Components.classes[\"@mozilla.org/widget/transferable;1\"]\\n                       .createInstance(Components.interfaces.nsITransferable);\\n        if (!trans) return;\\n    \\n        trans.addDataFlavor(\"text/unicode\");\\n    \\n        var str = new Object();\\n        var len = new Object();\\n    \\n        var str = Components.classes[\"@mozilla.org/supports-string;1\"]\\n                     .createInstance(Components.interfaces.nsISupportsString);\\n        var datacopy=data;\\n        str.data=datacopy;\\n        trans.setTransferData(\"text/unicode\",str,datacopy.length*2);\\n        var clipid=Components.interfaces.nsIClipboard;\\n    \\n        if (!clip) return false;\\n    \\n        clip.setData(trans,null,clipid.kGlobalClipboard);\\n        return true;\\n    }\\n    return false;\\n}\\n//-->\\n\\n\\n\\n\\n\\n\\n11. Managing Linguistic Data\\n\\n\\n/*\\n:Author: Edward Loper, James Curran\\n:Copyright: This stylesheet has been placed in the public domain.\\n\\nStylesheet for use with Docutils.\\n\\nThis stylesheet defines new css classes used by NLTK.\\n\\nIt uses a Python syntax highlighting scheme that matches\\nthe colour scheme used by IDLE, which makes it easier for\\nbeginners to check they are typing things in correctly.\\n*/\\n\\n/* Include the standard docutils stylesheet. */\\n@import url(default.css);\\n\\n/* Custom inline roles */\\nspan.placeholder    { font-style: italic; font-family: monospace; }\\nspan.example        { font-style: italic; }\\nspan.emphasis       { font-style: italic; }\\nspan.termdef        { font-weight: bold; }\\n/*span.term           { font-style: italic; }*/\\nspan.category       { font-variant: small-caps; }\\nspan.feature        { font-variant: small-caps; }\\nspan.fval           { font-style: italic; }\\nspan.math           { font-style: italic; }\\nspan.mathit         { font-style: italic; }\\nspan.lex            { font-variant: small-caps; }\\nspan.guide-linecount{ text-align: right; display: block;}\\n\\n/* Python souce code listings */\\nspan.pysrc-prompt   { color: #9b0000; }\\nspan.pysrc-more     { color: #9b00ff; }\\nspan.pysrc-keyword  { color: #e06000; }\\nspan.pysrc-builtin  { color: #940094; }\\nspan.pysrc-string   { color: #00aa00; }\\nspan.pysrc-comment  { color: #ff0000; }\\nspan.pysrc-output   { color: #0000ff; }\\nspan.pysrc-except   { color: #ff0000; }\\nspan.pysrc-defname  { color: #008080; }\\n\\n\\n/* Doctest blocks */\\npre.doctest         { margin: 0; padding: 0; font-weight: bold; }\\ndiv.doctest         { margin: 0 1em 1em 1em; padding: 0; }\\ntable.doctest       { margin: 0; padding: 0;\\n                      border-top: 1px solid gray;\\n                      border-bottom: 1px solid gray; }\\npre.copy-notify     { margin: 0; padding: 0.2em; font-weight: bold;\\n                      background-color: #ffffff; }\\n\\n/* Python source listings */\\ndiv.pylisting       { margin: 0 1em 1em 1em; padding: 0; }\\ntable.pylisting     { margin: 0; padding: 0;\\n                      border-top: 1px solid gray; }\\ntd.caption { border-top: 1px solid black; margin: 0; padding: 0; }\\n.caption-label { font-weight: bold;  }\\ntd.caption p { margin: 0; padding: 0; font-style: normal;}\\n\\ntable tr td.codeblock { \\n  padding: 0.2em ! important; margin: 0;\\n  border-left: 1px solid gray;\\n  border-right: 2px solid gray;\\n  border-top: 0px solid gray;\\n  border-bottom: 1px solid gray;\\n  font-weight: bold; background-color: #eeffee;\\n}\\ntable pre span {\\n  white-space: pre-wrap;\\n}\\ntable tr td.doctest  { \\n  padding: 0.2em; margin: 0;\\n  border-left: 1px solid gray;\\n  border-right: 2px solid gray;\\n  border-top: 0px solid gray;\\n  border-bottom: 1px solid gray;\\n  font-weight: bold; background-color: #eeeeff;\\n}\\n\\ntd.codeblock table tr td.copybar {\\n    background: #40a060; border: 1px solid gray;\\n    font-family: monospace; padding: 0; margin: 0; }\\ntd.doctest table tr td.copybar {\\n    background: #4060a0; border: 1px solid gray;\\n    font-family: monospace; padding: 0; margin: 0; }\\n\\ntd.pysrc { padding-left: 0.5em; }\\n\\nimg.callout { border-width: 0px; }\\n\\ntable.docutils {\\n    border-style: solid;\\n    border-width: 1px;\\n    margin-top: 6px;\\n    border-color: grey;\\n    border-collapse: collapse; }\\n\\ntable.docutils th {\\n    border-style: none;\\n    border-width: 1px;\\n    border-color: grey;\\n    padding: 0 .5em 0 .5em; }\\n\\ntable.docutils td {\\n    border-style: none;\\n    border-width: 1px;\\n    border-color: grey; \\n    padding: 0 .5em 0 .5em; }\\n\\ntable.footnote td { padding: 0; }\\ntable.footnote { border-width: 0; }\\ntable.footnote td { border-width: 0; }\\ntable.footnote th { border-width: 0; }\\n\\ntable.noborder { border-width: 0; }\\n\\ntable.example pre { margin-top: 4px; margin-bottom: 0; }\\n\\n/* For figures & tables */\\np.caption { margin-bottom: 0; }\\ndiv.figure { text-align: center; }\\n\\n/* The index */\\ndiv.index { border: 1px solid black;\\n            background-color: #eeeeee; }\\ndiv.index h1 { padding-left: 0.5em; margin-top: 0.5ex;\\n               border-bottom: 1px solid black; }\\nul.index { margin-left: 0.5em; padding-left: 0; }\\nli.index { list-style-type: none; }\\np.index-heading { font-size: 120%; font-style: italic; margin: 0; }\\nli.index ul { margin-left: 2em; padding-left: 0; }\\n\\n/* \\'Note\\' callouts */\\ndiv.note\\n{\\n  border-right:   #87ceeb 1px solid;\\n  padding-right: 4px;\\n  border-top: #87ceeb 1px solid;\\n  padding-left: 4px;\\n  padding-bottom: 4px;\\n  margin: 2px 5% 10px;\\n  border-left: #87ceeb 1px solid;\\n  padding-top: 4px;\\n  border-bottom: #87ceeb 1px solid;\\n  font-style: normal;\\n  font-family: verdana, arial;\\n  background-color: #b0c4de;\\n}\\n\\ntable.avm { border: 0px solid black; width: 0; }\\ntable.avm tbody tr {border: 0px solid black; }\\ntable.avm tbody tr td { padding: 2px; }\\ntable.avm tbody tr td.avm-key { padding: 5px; font-variant: small-caps; }\\ntable.avm tbody tr td.avm-eq { padding: 5px; }\\ntable.avm tbody tr td.avm-val { padding: 5px; font-style: italic; }\\np.avm-empty { font-style: normal; }\\ntable.avm colgroup col { border: 0px solid black; }\\ntable.avm tbody tr td.avm-topleft \\n    { border-left: 2px solid #000080; border-top: 2px solid #000080; }\\ntable.avm tbody tr td.avm-botleft \\n    { border-left: 2px solid #000080; border-bottom: 2px solid #000080; }\\ntable.avm tbody tr td.avm-topright\\n    { border-right: 2px solid #000080; border-top: 2px solid #000080; }\\ntable.avm tbody tr td.avm-botright\\n    { border-right: 2px solid #000080; border-bottom: 2px solid #000080; }\\ntable.avm tbody tr td.avm-left\\n    { border-left: 2px solid #000080; }\\ntable.avm tbody tr td.avm-right\\n    { border-right: 2px solid #000080; }\\ntable.avm tbody tr td.avm-topbotleft\\n    { border: 2px solid #000080; border-right: 0px solid black; }\\ntable.avm tbody tr td.avm-topbotright\\n    { border: 2px solid #000080; border-left: 0px solid black; }\\ntable.avm tbody tr td.avm-ident\\n    { font-size: 80%; padding: 0; padding-left: 2px; vertical-align: top; }\\n.avm-pointer\\n{ border: 1px solid #008000; padding: 1px; color: #008000; \\n  background: #c0ffc0; font-style: normal; }\\n\\ntable.gloss { border: 0px solid black; width: 0; }\\ntable.gloss tbody tr { border: 0px solid black; }\\ntable.gloss tbody tr td { border: 0px solid black; }\\ntable.gloss colgroup col { border: 0px solid black; }\\ntable.gloss p { margin: 0; padding: 0; }\\n\\ntable.rst-example { border: 1px solid black; }\\ntable.rst-example tbody tr td { background: #eeeeee; }\\ntable.rst-example thead tr th { background: #c0ffff; }\\ntd.rst-raw { width: 0; }\\n\\n/* Used by nltk.org/doc/test: */\\ndiv.doctest-list { text-align: center; }\\ntable.doctest-list { border: 1px solid black;\\n  margin-left: auto; margin-right: auto;\\n}\\ntable.doctest-list tbody tr td { background: #eeeeee;\\n  border: 1px solid #cccccc; text-align: left; }\\ntable.doctest-list thead tr th { background: #304050; color: #ffffff;\\n  border: 1px solid #000000;}\\ntable.doctest-list thead tr a { color: #ffffff; }\\nspan.doctest-passed { color: #008000; }\\nspan.doctest-failed { color: #800000; }\\n\\n\\n\\n\\n\\n\\n11. Managing Linguistic Data\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n<!-- Grammatical Category - e.g. NP and verb as technical terms\\n.. role:: gc\\n   :class: category -->\\n\\n\\n\\n\\n\\n\\n\\n\\n<!-- standard global imports\\n\\n>>> from __future__ import division\\n>>> import nltk, re, pprint -->\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nStructured collections of annotated linguistic data are essential in most areas of NLP,\\nhowever, we still face many obstacles in using them.\\nThe goal of this chapter is to answer the following questions:\\n\\nHow do we design a new language resource and ensure that its\\ncoverage, balance, and documentation support a wide range of uses?\\nWhen existing data is in the wrong format for some analysis tool,\\nhow can we convert it to a suitable format?\\nWhat is a good way to document the existence of a resource we\\nhave created so that others can easily find it?\\n\\nAlong the way, we will study the design of existing corpora, the\\ntypical workflow for creating a corpus, and the lifecycle of corpus.\\nAs in other chapters, there will be many examples drawn from\\npractical experience managing linguistic data, including\\ndata that has been collected in the course of linguistic fieldwork,\\nlaboratory work, and web crawling.\\n\\n1&nbsp;&nbsp;&nbsp;Corpus Structure: a Case Study\\nThe TIMIT corpus of read speech was the first annotated speech database to be\\nwidely distributed, and it has an especially clear organization.\\nTIMIT was developed by a consortium including Texas Instruments and MIT, from which\\nit derives its name.\\nIt was designed to provide data for the acquisition of acoustic-phonetic knowledge and to\\nsupport the development and evaluation of automatic speech recognition systems.\\n\\n1.1&nbsp;&nbsp;&nbsp;The Structure of TIMIT\\nLike the Brown Corpus, which displays a balanced selection of text genres and sources,\\nTIMIT includes a balanced selection of dialects, speakers, and materials.  For each of\\neight dialect regions, 50 male and female speakers having a range of ages and educational\\nbackgrounds each read ten carefully chosen sentences.  Two sentences, read by all\\nspeakers, were designed to bring out dialect variation:\\n\\n  (1)\\n  a.she had your dark suit in greasy wash water all year\\n\\n  b.don\\'t ask me to carry an oily rag like that\\n\\nThe remaining sentences were chosen to be phonetically rich, involving all phones (sounds) and\\na comprehensive range of diphones (phone bigrams).  Additionally, the design strikes a balance\\nbetween multiple speakers saying the same sentence in order to permit comparison across\\nspeakers, and having a large range of sentences covered by the corpus to get maximal\\ncoverage of diphones.  Five of the sentences read by each speaker are also read by six other\\nspeakers (for comparability).  The remaining three sentences read by each speaker were unique\\nto that speaker (for coverage).\\nNLTK includes a sample from the TIMIT corpus.  You can access its documentation in the usual\\nway, using help(nltk.corpus.timit).  Print nltk.corpus.timit.fileids() to see a list of the\\n160 recorded utterances in the corpus sample.\\nEach file name has internal structure as shown in 1.1.\\n\\n\\nFigure 1.1: Structure of a TIMIT Identifier: Each recording is labeled using a string made\\nup of the speaker\\'s dialect region, gender, speaker identifier, sentence type,\\nand sentence identifier.\\n\\nEach item has a phonetic transcription which can be accessed using the phones()\\nmethod.  We can access the corresponding word tokens in the customary way.  Both access\\nmethods permit an optional argument offset=True which includes the start and end offsets\\nof the corresponding span in the audio file.\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; phonetic = nltk.corpus.timit.phones(\\'dr1-fvmh0/sa1\\')\\n&gt;&gt;&gt; phonetic\\n[\\'h#\\', \\'sh\\', \\'iy\\', \\'hv\\', \\'ae\\', \\'dcl\\', \\'y\\', \\'ix\\', \\'dcl\\', \\'d\\', \\'aa\\', \\'kcl\\',\\n\\'s\\', \\'ux\\', \\'tcl\\', \\'en\\', \\'gcl\\', \\'g\\', \\'r\\', \\'iy\\', \\'s\\', \\'iy\\', \\'w\\', \\'aa\\',\\n\\'sh\\', \\'epi\\', \\'w\\', \\'aa\\', \\'dx\\', \\'ax\\', \\'q\\', \\'ao\\', \\'l\\', \\'y\\', \\'ih\\', \\'ax\\', \\'h#\\']\\n&gt;&gt;&gt; nltk.corpus.timit.word_times(\\'dr1-fvmh0/sa1\\')\\n[(\\'she\\', 7812, 10610), (\\'had\\', 10610, 14496), (\\'your\\', 14496, 15791),\\n(\\'dark\\', 15791, 20720), (\\'suit\\', 20720, 25647), (\\'in\\', 25647, 26906),\\n(\\'greasy\\', 26906, 32668), (\\'wash\\', 32668, 37890), (\\'water\\', 38531, 42417),\\n(\\'all\\', 43091, 46052), (\\'year\\', 46052, 50522)]\\n\\n\\n\\nIn addition to this text data, TIMIT includes a lexicon that provides the canonical\\npronunciation of every word, which can be compared with a particular utterance:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; timitdict = nltk.corpus.timit.transcription_dict()\\n&gt;&gt;&gt; timitdict[\\'greasy\\'] + timitdict[\\'wash\\'] + timitdict[\\'water\\']\\n[\\'g\\', \\'r\\', \\'iy1\\', \\'s\\', \\'iy\\', \\'w\\', \\'ao1\\', \\'sh\\', \\'w\\', \\'ao1\\', \\'t\\', \\'axr\\']\\n&gt;&gt;&gt; phonetic[17:30]\\n[\\'g\\', \\'r\\', \\'iy\\', \\'s\\', \\'iy\\', \\'w\\', \\'aa\\', \\'sh\\', \\'epi\\', \\'w\\', \\'aa\\', \\'dx\\', \\'ax\\']\\n\\n\\n\\nThis gives us a sense of what a speech processing system\\nwould have to do in producing or recognizing speech in this particular dialect\\n(New England).  Finally, TIMIT includes demographic data about the speakers,\\npermitting fine-grained study of vocal, social, and gender characteristics.\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; nltk.corpus.timit.spkrinfo(\\'dr1-fvmh0\\')\\nSpeakerInfo(id=\\'VMH0\\', sex=\\'F\\', dr=\\'1\\', use=\\'TRN\\', recdate=\\'03/11/86\\',\\nbirthdate=\\'01/08/60\\', ht=\\'5\\\\\\'05\"\\', race=\\'WHT\\', edu=\\'BS\\',\\ncomments=\\'BEST NEW ENGLAND ACCENT SO FAR\\')\\n\\n\\n\\n\\n\\n1.2&nbsp;&nbsp;&nbsp;Notable Design Features\\nTIMIT illustrates several key features of corpus design.\\nFirst, the corpus contains two layers of annotation, at the phonetic and orthographic\\nlevels.  In general, a text or speech corpus may be annotated at many different linguistic levels,\\nincluding morphological, syntactic, and discourse levels.  Moreover, even at a given\\nlevel there may be different labeling schemes or even disagreement amongst annotators,\\nsuch that we want to represent multiple versions.\\nA second property of TIMIT is its balance across multiple dimensions of variation,\\nfor coverage of dialect regions and diphones.  The inclusion of speaker\\ndemographics brings in many more independent variables, that may help to\\naccount for variation in the data, and which facilitate later uses of the\\ncorpus for purposes that were not envisaged when the corpus was created,\\nsuch as sociolinguistics.\\nA third property is that there is a sharp division between the original\\nlinguistic event captured as an audio recording, and the annotations of that event.\\nThe same holds true of text corpora, in the sense that the original text usually\\nhas an external source, and is considered to be an immutable artifact.  Any transformations\\nof that artifact which involve human judgment — even something as\\nsimple as tokenization — are subject to later revision, thus it is important to\\nretain the source material in a form that is as close to the original as possible.\\n\\n\\nFigure 1.2: Structure of the Published TIMIT Corpus: The CD-ROM contains doc, train, and test directories\\nat the top level; the train and test directories both have 8 sub-directories, one per\\ndialect region; each of these contains further subdirectories, one per speaker;\\nthe contents of the directory for female speaker aks0 are listed, showing\\n10 wav files accompanied by a text transcription, a word-aligned transcription,\\nand a phonetic transcription.\\n\\nA fourth feature of TIMIT is the hierarchical structure of the corpus.\\nWith 4 files per sentence, and 10 sentences for each of 500 speakers, there are 20,000 files.  These are\\norganized into a tree structure, shown schematically in 1.2.\\nAt the top level there is a split between training and testing sets, which gives\\naway its intended use for developing and evaluating statistical models.\\nFinally, notice that even though TIMIT is a speech corpus, its transcriptions and associated\\ndata are just text, and can be processed using programs just like any other text corpus.\\nTherefore, many of the computational methods described in this book are applicable.\\nMoreover, notice that all of the data types included in the TIMIT corpus fall into\\nthe two basic categories of lexicon and text, which we will discuss below.\\nEven the speaker demographics data is just another instance of the lexicon data type.\\nThis last observation is less surprising when we consider that text and record structures are the\\nprimary domains for the two subfields of computer science that focus on data management,\\nnamely text retrieval and databases.  A notable feature of linguistic data management is\\nthat usually brings both data types together, and that it can draw on results and techniques\\nfrom both fields.\\n\\n\\n1.3&nbsp;&nbsp;&nbsp;Fundamental Data Types\\n\\n\\nFigure 1.3: Basic Linguistic Data Types — Lexicons and Texts: amid their diversity,\\nlexicons have a record structure, while annotated texts have a temporal organization.\\n\\nDespite its complexity, the TIMIT corpus only contains two fundamental data types,\\nnamely lexicons and texts.\\nAs we saw in 2., most lexical resources can be represented using\\na record structure, i.e. a key plus one or more fields, as\\nshown in 1.3.  A lexical resource could be a conventional\\ndictionary or comparative wordlist, as illustrated.  It could also\\nbe a phrasal lexicon, where the key field is a phrase rather than a single word.\\nA thesaurus also consists of record-structured data, where we look up entries\\nvia non-key fields that correspond to topics.\\nWe can also construct special tabulations (known as paradigms)\\nto illustrate contrasts and systematic variation, as shown in\\n1.3 for three verbs.  TIMIT\\'s speaker table is also a kind\\nof lexicon.\\nAt the most abstract level, a text is a representation of a real or fictional speech event,\\nand the time-course of that event carries over into the text itself.  A text could be a small\\nunit, such as a word or sentence, or a complete narrative or dialogue.  It may come with\\nannotations such as part-of-speech tags, morphological analysis, discourse structure, and so forth.\\nAs we saw in the IOB tagging technique (7.), it is possible to represent higher-level\\nconstituents using tags on individual words.  Thus the abstraction of text shown in\\n1.3 is sufficient.\\nDespite the complexities and idiosyncrasies of individual corpora, at base they are\\ncollections of texts together with record-structured data.  The contents of a corpus\\nare often biased towards one or other of these types.\\nFor example, the Brown Corpus contains 500 text files, but we still use a table to relate\\nthe files to 15 different genres.  At the other end of the spectrum, WordNet\\ncontains 117,659 synset records, yet it incorporates many example sentences (mini-texts)\\nto illustrate word usages.  TIMIT is an interesting mid-point on this spectrum, containing substantial\\nfree-standing material of both the text and lexicon types.\\n\\n\\n\\n2&nbsp;&nbsp;&nbsp;The Life-Cycle of a Corpus\\nCorpora are not born fully-formed, but involve careful preparation\\nand input from many people over an extended period.  Raw data needs\\nto be collected, cleaned up, documented, and stored in a systematic\\nstructure.  Various layers of annotation might be applied, some requiring\\nspecialized knowledge of the morphology or syntax of the language.\\nSuccess at this stage depends on creating an efficient workflow\\ninvolving appropriate tools and format converters.\\nQuality control procedures can be put in place to find inconsistencies\\nin the annotations, and to ensure the highest\\npossible level of inter-annotator agreement.  Because of the\\nscale and complexity of the task, large corpora may take years to\\nprepare, and involve tens or hundreds of person-years of effort.\\nIn this section we briefly review the various stages in the\\nlife-cycle of a corpus.\\n\\n2.1&nbsp;&nbsp;&nbsp;Three Corpus Creation Scenarios\\nIn one type of corpus, the design unfolds over\\nin the course of the creator\\'s explorations. This is the pattern\\ntypical of traditional \"field linguistics,\" in which material from\\nelicitation sessions is analyzed as it is gathered, with tomorrow\\'s elicitation often based\\non questions that arise in analyzing today\\'s. The resulting corpus\\nis then used during subsequent years of research, and may serve\\nas an archival resource indefinitely.  Computerization is an\\nobvious boon to work of this type, as exemplified by the popular\\nprogram Shoebox, now over two decades old and re-released as Toolbox\\n(see 4).\\nOther software tools, even simple word processors and spreadsheets, are routinely\\nused to acquire the data.  In the next section we will look at how to extract\\ndata from these sources.\\nAnother corpus creation scenario is typical of experimental research\\nwhere a body of carefully-designed material is collected from a range of human subjects,\\nthen analyzed to evaluate a hypothesis or develop a technology.\\nIt has become common for such databases to be shared and re-used\\nwithin a laboratory or company, and often to be published more\\nwidely. Corpora of this type are the basis of the\\n\"common task\" method of research management, which over the\\npast two decades has become the norm in government-funded research\\nprograms in language technology.\\nWe have already encountered many such corpora in the earlier chapters;\\nwe will see how to write Python programs to implement the kinds of\\ncuration tasks that are necessary before such corpora are published.\\nFinally, there are efforts to gather a \"reference corpus\" for a\\nparticular language, such as the American National Corpus (ANC)\\nand the British National Corpus (BNC). Here the goal\\nhas been to produce a comprehensive record of the\\nmany forms, styles and uses of a language.\\nApart from the sheer challenge of scale, there is a heavy reliance\\non automatic annotation tools together with post-editing to\\nfix any errors.  However, we can write programs to locate\\nand repair the errors, and also to analyze the corpus for balance.\\n\\n\\n2.2&nbsp;&nbsp;&nbsp;Quality Control\\nGood tools for automatic and manual preparation of data\\nare essential.  However the creation of a high-quality corpus depends just\\nas much on such mundane things as documentation, training, and workflow.\\nAnnotation guidelines define the task and document the markup\\nconventions.  They may be regularly updated to cover difficult\\ncases, along with new rules that are devised to achieve more\\nconsistent annotations.  Annotators need to be trained in the\\nprocedures, including methods for resolving cases not covered\\nin the guidelines.  A workflow needs to be established, possibly\\nwith supporting software, to keep track of which files have been initialized,\\nannotated, validated, manually checked, and so on.  There may be multiple layers of\\nannotation, provided by different specialists.  Cases of uncertainty\\nor disagreement may require adjudication.\\nLarge annotation tasks require multiple annotators, which\\nraises the problem of achieving consistency.\\nHow consistently can a group of annotators perform?\\nWe can easily measure consistency by having a portion of\\nthe source material independently annotated by two people.\\nThis may reveal shortcomings in the guidelines or\\ndiffering abilities with the annotation task.\\nIn cases where quality is paramount, the entire corpus can\\nbe annotated twice, and any inconsistencies adjudicated\\nby an expert.\\nIt is considered best practice to report the inter-annotator\\nagreement that was achieved for a corpus (e.g. by double-annotating\\n10% of the corpus).  This score serves as a helpful upper bound on\\nthe expected performance of any automatic system that is trained\\non this corpus.\\n\\nCaution!\\nCare should be exercised when interpreting an inter-annotator\\nagreement score, since annotation tasks vary greatly in their\\ndifficulty.  For example, 90% agreement would be a terrible\\nscore for part-of-speech tagging, but an exceptional score\\nfor semantic role labeling.\\n\\nThe Kappa coefficient K measures agreement between\\ntwo people making category judgments, correcting for expected\\nchance agreement.  For example, suppose an item is to be annotated,\\nand four coding options are equally likely.\\nThen two people coding randomly would be expected to agree 25% of the time.\\nThus, an agreement of 25% will be assigned K = 0, and better\\nlevels of agreement will be scaled accordingly.\\nFor an agreement of 50%, we would get\\nK = 0.333, as 50 is a third of the way from 25 to 100.\\nMany other agreement measures exist; see help(nltk.metrics.agreement) for details.\\n\\n\\nFigure 2.1: Three Segmentations of a Sequence: The small rectangles represent characters,\\nwords, sentences, in short, any sequence which might be divided into\\nlinguistic units; S1 and S2 are in close\\nagreement, but both differ significantly from S3.\\n\\nWe can also measure the agreement between two independent segmentations\\nof language input, e.g. for tokenization, sentence segmentation,\\nnamed-entity detection.  In 2.1 we see three\\npossible segmentations of a sequence of items\\nwhich might have been produced by annotators (or programs).\\nAlthough none of them agree exactly, S1 and S2\\nare in close agreement, and we would like a suitable measure.\\nWindowdiff is a simple algorithm for evaluating the agreement of\\ntwo segmentations by running a sliding window over the\\ndata and awarding partial credit for near misses.\\nIf we preprocess our tokens into a sequence of zeros and ones, to\\nrecord when a token is followed by a boundary, we can represent the\\nsegmentations as strings, and apply the windowdiff scorer.\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; s1 = \"00000010000000001000000\"\\n&gt;&gt;&gt; s2 = \"00000001000000010000000\"\\n&gt;&gt;&gt; s3 = \"00010000000000000001000\"\\n&gt;&gt;&gt; nltk.windowdiff(s1, s1, 3)\\n0.0\\n&gt;&gt;&gt; nltk.windowdiff(s1, s2, 3)\\n0.190...\\n&gt;&gt;&gt; nltk.windowdiff(s2, s3, 3)\\n0.571...\\n\\n\\n\\nIn the above example, the window had a size of 3.\\nThe windowdiff computation slides this window across a pair of strings.\\nAt each position it totals up the number of boundaries found inside this\\nwindow, for both strings, then computes the difference.  These differences\\nare then summed.\\nWe can increase or shrink the window size to control the sensitivity of the\\nmeasure.\\n\\n\\n2.3&nbsp;&nbsp;&nbsp;Curation vs Evolution\\nAs large corpora are published, researchers are increasingly\\nlikely to base their investigations on balanced, focused\\nsubsets that were derived from corpora produced for entirely\\ndifferent reasons.  For instance, the Switchboard database,\\noriginally collected for speaker identification research,\\nhas since been used as the basis for published studies in speech recognition,\\nword pronunciation, disfluency, syntax, intonation and discourse structure.\\nThe motivations for recycling linguistic corpora include the\\ndesire to save time and effort, the desire to work on material\\navailable to others for replication, and sometimes a desire to study\\nmore naturalistic forms of linguistic behavior than would be possible\\notherwise. The process of choosing a subset for such a study may\\ncount as a non-trivial contribution in itself.\\nIn addition to selecting an appropriate subset of a corpus, this\\nnew work could involve reformatting a text file\\n(e.g. converting to XML), renaming files, retokenizing the text,\\nselecting a subset of the data to enrich, and so forth.\\nMultiple research groups might do this work independently, as illustrated\\nin 2.2.  At a later date, should someone want to combine sources\\nof information from different versions, the task will probably be extremely onerous.\\n\\n\\nFigure 2.2: Evolution of a Corpus over Time: After a corpus is published, research\\ngroups will use it independently, selecting and enriching different pieces;\\nlater research that seeks to integrate separate annotations confronts\\nthe difficult challenge of aligning the annotations.\\n\\nThe task of using derived corpora is made even more difficult by the lack of\\nany record about how the derived version was created, and which version is the most\\nup-to-date.\\nAn alternative to this chaotic situation is for a corpus to be centrally curated,\\nand for committees of experts to revise and extend it at periodic\\nintervals, considering submissions from third-parties, and publishing new releases\\nfrom time to time.  Print dictionaries and national corpora may be centrally curated in\\nthis way.  However, for most corpora this model is simply impractical.\\nA middle course is for the original corpus publication to have a scheme for identifying\\nany sub-part.  Each sentence, tree, or lexical entry, could have a globally unique\\nidentifier, and each token, node or field (respectively) could have a relative offset.\\nAnnotations, including segmentations, could reference the source using\\nthis identifier scheme (a method which is known as standoff annotation).\\nThis way, new annotations could be distributed independently of the source, and\\nmultiple independent annotations of the same source could be\\ncompared and updated without touching the source.\\nIf the corpus publication is provided in multiple versions, the version\\nnumber or date could be part of the identification scheme.\\nA table of correspondences between identifiers across editions of the corpus\\nwould permit any standoff annotations to be updated easily.\\n\\nCaution!\\nSometimes an updated corpus contains revisions of base material that\\nhas been externally annotated.  Tokens might be split or merged, and constituents\\nmay have been rearranged.  There may not be a one-to-one correspondence between\\nold and new identifiers.  It is better to cause standoff annotations to break\\non such components of the new version than to silently allow their identifiers\\nto refer to incorrect locations.\\n\\n\\n\\n\\n3&nbsp;&nbsp;&nbsp;Acquiring Data\\n\\n3.1&nbsp;&nbsp;&nbsp;Obtaining Data from the Web\\nThe Web is a rich source of data for language analysis purposes.  We have already\\ndiscussed methods for accessing individual files, RSS feeds, and search engine\\nresults (see 3.1).  However, in some cases we want to\\nobtain large quantities of web text.\\nThe simplest approach is to obtain a published corpus of web text.  The ACL\\nSpecial Interest Group on Web as Corpus (SIGWAC) maintains a list of resources\\nat http://www.sigwac.org.uk/.\\nThe advantage of using a well-defined web corpus is that they are documented,\\nstable, and permit reproducible experimentation.\\nIf the desired content is localized to a particular website, there are many\\nutilities for capturing all the accessible contents of a site, such as\\nGNU Wget http://www.gnu.org/software/wget/.\\nFor maximal flexibility and control, a web crawler can be used,\\nsuch as Heritrix http://crawler.archive.org/.\\nCrawlers permit fine-grained control over where to look, which links to\\nfollow, and how to organize the results (Croft, Metzler, &amp; Strohman, 2009).\\nFor example, if we want to compile a\\nbilingual text collection having corresponding pairs of documents in each language,\\nthe crawler needs to detect the structure of the site in order to extract the\\ncorrespondence between the documents, and it needs to organize the downloaded\\npages in such a way that the correspondence is captured.  It might be tempting\\nto write your own web-crawler, but there are dozens of pitfalls to do with\\ndetecting MIME types, converting relative to absolute URLs, avoiding getting\\ntrapped in cyclic link structures, dealing with network latencies, avoiding\\noverloading the site or being banned from accessing the site, and so on.\\n\\n\\n3.2&nbsp;&nbsp;&nbsp;Obtaining Data from Word Processor Files\\nWord processing software is often used in the manual preparation\\nof texts and lexicons in projects that have limited computational\\ninfrastructure.  Such projects often provide templates for data\\nentry, though the word processing software does not ensure that\\nthe data is correctly structured.  For example, each text may\\nbe required to have a title and date.  Similarly, each lexical\\nentry may have certain obligatory fields.\\nAs the data grows in size and complexity, a larger\\nproportion of time may be spent maintaining its consistency.\\nHow can we extract the content of such files so\\nthat we can manipulate it in external programs?\\nMoreover, how can we validate the content of these files to\\nhelp authors create well-structured data, so that the\\nquality of the data can be maximized in the context of\\nthe original authoring process?\\nConsider a dictionary in\\nwhich each entry has a part-of-speech field, drawn from a set of 20\\npossibilities, displayed after the pronunciation field, and rendered\\nin 11-point bold.  No conventional word processor has search or macro\\nfunctions capable of verifying that all part-of-speech fields have\\nbeen correctly entered and displayed.  This task requires exhaustive\\nmanual checking.  If the word processor permits the document to be\\nsaved in a non-proprietary format, such as text, HTML, or XML, we can\\nsometimes write programs to do this checking automatically.\\nConsider the following fragment of a lexical entry:\\n\"sleep [sli:p] v.i. condition of body and mind...\".\\nWe can enter this in MSWord, then \"Save as Web Page\",\\nthen inspect the resulting HTML file:\\n&lt;p class=MsoNormal&gt;sleep\\n  &lt;span style=\\'mso-spacerun:yes\\'&gt; &lt;/span&gt;\\n  [&lt;span class=SpellE&gt;sli:p&lt;/span&gt;]\\n  &lt;span style=\\'mso-spacerun:yes\\'&gt; &lt;/span&gt;\\n  &lt;b&gt;&lt;span style=\\'font-size:11.0pt\\'&gt;v.i.&lt;/span&gt;&lt;/b&gt;\\n  &lt;span style=\\'mso-spacerun:yes\\'&gt; &lt;/span&gt;\\n  &lt;i&gt;a condition of body and mind ...&lt;o:p&gt;&lt;/o:p&gt;&lt;/i&gt;\\n&lt;/p&gt;\\n\\nObserve that the entry is represented as an HTML paragraph, using the\\n&lt;p&gt; element, and that the part of speech appears inside a &lt;span\\nstyle=\\'font-size:11.0pt\\'&gt; element.  The following program defines\\nthe set of legal parts-of-speech, legal_pos.  Then it extracts all\\n11-point content from the dict.htm file and stores it in the set\\nused_pos.  Observe that the search pattern contains a\\nparenthesized sub-expression; only the material that matches this\\nsub-expression is returned by re.findall.  Finally, the program\\nconstructs the set of illegal parts-of-speech as used_pos -\\nlegal_pos:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; legal_pos = set([\\'n\\', \\'v.t.\\', \\'v.i.\\', \\'adj\\', \\'det\\'])\\n&gt;&gt;&gt; pattern = re.compile(r\"\\'font-size:11.0pt\\'&gt;([a-z.]+)&lt;\")\\n&gt;&gt;&gt; document = open(\"dict.htm\", encoding=\"windows-1252\").read()\\n&gt;&gt;&gt; used_pos = set(re.findall(pattern, document))\\n&gt;&gt;&gt; illegal_pos = used_pos.difference(legal_pos)\\n&gt;&gt;&gt; print(list(illegal_pos))\\n[\\'v.i\\', \\'intrans\\']\\n\\n\\n\\nThis simple program represents the tip of the iceberg.  We can develop\\nsophisticated tools to check the consistency of word processor files,\\nand report errors so that the maintainer of the dictionary can correct\\nthe original file using the original word processor.\\nOnce we know the data is correctly formatted, we\\ncan write other programs to convert the data into a different format.\\nThe program in 3.1 strips out the HTML markup using the BeautifulSoup library,\\nextracts the words and their pronunciations, and generates output\\nin \"comma-separated value\" (CSV) format.\\n\\n\\n\\n\\n&nbsp;\\nfrom bs4 import BeautifulSoup\\n\\ndef lexical_data(html_file, encoding=\"utf-8\"):\\n    SEP = \\'_ENTRY\\'\\n    html = open(html_file, encoding=encoding).read()\\n    html = re.sub(r\\'&lt;p\\', SEP + \\'&lt;p\\', html)\\n    text = BeautifulSoup(html, \\'html.parser\\').get_text()\\n    text = \\' \\'.join(text.split())\\n    for entry in text.split(SEP):\\n        if entry.count(\\' \\') &gt; 2:\\n            yield entry.split(\\' \\', 3)\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; import csv\\n&gt;&gt;&gt; writer = csv.writer(open(\"dict1.csv\", \"w\", encoding=\"utf-8\"))\\n&gt;&gt;&gt; writer.writerows(lexical_data(\"dict.htm\", encoding=\"windows-1252\"))\\n\\n\\nExample 3.1 (code_html2csv.py): Figure 3.1: Converting HTML Created by Microsoft Word into Comma-Separated Values\\n\\n\\nwith gzip.open(fn+\".gz\",\"wb\") as f_out:\\nf_out.write(bytes(s, \\'UTF-8\\'))\\n\\n\\n\\n3.3&nbsp;&nbsp;&nbsp;Obtaining Data from Spreadsheets and Databases\\nSpreadsheets are often used for acquiring wordlists or paradigms.\\nFor example, a comparative wordlist may be created using a spreadsheet,\\nwith a row for each cognate set, and a column for each language\\n(cf. nltk.corpus.swadesh, and www.rosettaproject.org).\\nMost spreadsheet software can export their data in CSV\\n\"comma-separated value\" format.  As we see below, it is easy for\\nPython programs to access these using the csv module.\\nSometimes lexicons are stored in a full-fledged relational database.\\nWhen properly normalized, these databases can ensure the validity\\nof the data.  For example, we can\\nrequire that all parts-of-speech come from a specified vocabulary by\\ndeclaring that the part-of-speech field is an enumerated type\\nor a foreign key that references a separate part-of-speech table.\\nHowever, the relational model requires the structure of the data\\n(the schema) be declared in advance, and this runs counter to\\nthe dominant approach to structuring linguistic data, which is\\nhighly exploratory.  Fields which were assumed to be obligatory\\nand unique often turn out to be optional and repeatable.  A relational\\ndatabase can accommodate this when it is fully known in advance, however\\nif it is not, or if just about every property turns out to be optional\\nor repeatable, the relational approach is unworkable.\\nNevertheless, when our goal is simply to extract the contents from a database,\\nit is enough to dump out the tables (or SQL query results)\\nin CSV format and load them into our program.  Our program might perform\\na linguistically motivated query which cannot be expressed in SQL, e.g.\\nselect all words that appear\\nin example sentences for which no dictionary entry is provided.\\nFor this task, we would need to extract enough information from a record\\nfor it to be uniquely identified, along with the headwords and example\\nsentences.  Let\\'s suppose this information was now available in a CSV file\\ndict.csv:\\n\"sleep\",\"sli:p\",\"v.i\",\"a condition of body and mind ...\"\\n\"walk\",\"wo:k\",\"v.intr\",\"progress by lifting and setting down each foot ...\"\\n\"wake\",\"weik\",\"intrans\",\"cease to sleep\"\\n\\nNow we can express this query as shown below:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; import csv\\n&gt;&gt;&gt; lexicon = csv.reader(open(\\'dict.csv\\'))\\n&gt;&gt;&gt; pairs = [(lexeme, defn) for (lexeme, _, _, defn) in lexicon]\\n&gt;&gt;&gt; lexemes, defns = zip(*pairs)\\n&gt;&gt;&gt; defn_words = set(w for defn in defns for w in defn.split())\\n&gt;&gt;&gt; sorted(defn_words.difference(lexemes))\\n[\\'...\\', \\'a\\', \\'and\\', \\'body\\', \\'by\\', \\'cease\\', \\'condition\\', \\'down\\', \\'each\\',\\n\\'foot\\', \\'lifting\\', \\'mind\\', \\'of\\', \\'progress\\', \\'setting\\', \\'to\\']\\n\\n\\n\\nThis information would then guide the ongoing work to enrich the lexicon,\\nwork that updates the content of the relational database.\\n\\n\\n3.4&nbsp;&nbsp;&nbsp;Converting Data Formats\\nAnnotated linguistic data rarely arrives in the most convenient format,\\nand it is often necessary to perform various kinds of format conversion.\\nConverting between character encodings has already been discussed\\n(see 3.3).  Here we focus on the structure of the data.\\nIn the simplest case, the input and output formats are isomorphic.\\nFor instance, we might be converting lexical data from Toolbox\\nformat to XML, and it is straightforward to transliterate the\\nentries one at a time (4).  The structure\\nof the data is reflected in the structure of the required\\nprogram: a for loop whose body takes care\\nof a single entry.\\nIn another common case, the output is a digested form of the\\ninput, such as an inverted file index.  Here it is necessary\\nto build an index structure in memory (see 4.8),\\nthen write it to a file in the desired format.\\nThe following example constructs an index that maps\\nthe words of a dictionary definition to the corresponding\\nlexeme  for each lexical entry ,\\nhaving tokenized the definition text ,\\nand discarded short words .  Once the index has\\nbeen constructed we open a file and then iterate over\\nthe index entries, to write out the lines in the required format .\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; idx = nltk.Index((defn_word, lexeme) \\n...                  for (lexeme, defn) in pairs \\n...                  for defn_word in nltk.word_tokenize(defn) \\n...                  if len(defn_word) &gt; 3) \\n&gt;&gt;&gt; with open(\"dict.idx\", \"w\") as idx_file:\\n...     for word in sorted(idx):\\n...         idx_words = \\', \\'.join(idx[word])\\n...         idx_line = \"{}: {}\".format(word, idx_words) \\n...         print(idx_line, file=idx_file)\\n\\n\\n\\nThe resulting file dict.idx contains the following lines.  (With a larger\\ndictionary we would expect to find multiple lexemes listed for each index entry.)\\nbody: sleep\\ncease: wake\\ncondition: sleep\\ndown: walk\\neach: walk\\nfoot: walk\\nlifting: walk\\nmind: sleep\\nprogress: walk\\nsetting: walk\\nsleep: wake\\n\\nIn some cases, the input and output data both consist of two or more dimensions.\\nFor instance, the input might be a set of files, each containing a single\\ncolumn of word frequency data.  The required output might be a two-dimensional\\ntable in which the original columns appear as rows.  In such cases we populate\\nan internal data structure by filling up one column at a time, then read off\\nthe data one row at a time as we write data to the output file.\\nIn the most vexing cases, the source and target formats have slightly different\\ncoverage of the domain, and information is unavoidably lost when translating between them.\\nFor example, we could combine multiple Toolbox files to create a single CSV file\\ncontaining a comparative wordlist, loosing all but the \\\\lx field of the input files.\\nIf the CSV file was later modified, it would be a labor-intensive process to inject\\nthe changes into the original Toolbox files.  A partial solution to this \"round-tripping\"\\nproblem is to associate explicit identifiers each linguistic object, and to propagate\\nthe identifiers with the objects.\\n\\n\\n3.5&nbsp;&nbsp;&nbsp;Deciding Which Layers of Annotation to Include\\nPublished corpora vary greatly in the richness of the information they contain.\\nAt a minimum, a corpus will typically\\ncontain at least a sequence of sound or orthographic symbols.  At the\\nother end of the spectrum, a corpus could contain a large amount of\\ninformation about the syntactic structure, morphology, prosody, and\\nsemantic content of every sentence, plus annotation of discourse relations\\nor dialogue acts.  These extra layers of annotation\\nmay be just what someone needs for performing a particular data analysis task.\\nFor example, it may be much easier to find a given\\nlinguistic pattern if we can search for specific syntactic structures;\\nand it may be easier to categorize a linguistic pattern if every word\\nhas been tagged with its sense.  Here are some commonly provided\\nannotation layers:\\n\\nWord Tokenization: The orthographic form of text does not unambiguously\\nidentify its tokens.  A tokenized and normalized version,\\nin addition to the conventional orthographic version,\\nmay be a very convenient resource.\\nSentence Segmentation: As we saw in 3, sentence\\nsegmentation can be more difficult than it seems.  Some corpora\\ntherefore use explicit annotations to mark sentence segmentation.\\nParagraph Segmentation: Paragraphs and other structural elements\\n(headings, chapters, etc.) may be explicitly annotated.\\nPart of Speech: The syntactic category of each word in a document.\\nSyntactic Structure: A tree structure showing the constituent\\nstructure of a sentence.\\nShallow Semantics: Named entity and coreference annotations, semantic role labels.\\nDialogue and Discourse: dialogue act tags, rhetorical structure\\n\\nUnfortunately, there is not much consistency between existing corpora\\nin how they represent their annotations.  However, two general classes\\nof annotation representation should be distinguished.  Inline\\nannotation modifies the original document by inserting special\\nsymbols or control sequences that carry the annotated information.\\nFor example, when part-of-speech tagging a document, the string\\n\"fly\" might be replaced with the string \"fly/NN\", to indicate\\nthat the word fly is a noun in this context.  In contrast, standoff\\nannotation does not modify the original document, but instead\\ncreates a new file that adds annotation information using pointers\\nthat reference the original document.  For example, this new document might\\ncontain the string \"&lt;token id=8 pos=\\'NN\\'/&gt;\", to indicate\\nthat token 8 is a noun.  (We would want to be sure that the tokenization\\nitself was not subject to change, since it would cause such references\\nto break silently.)\\n\\n\\n3.6&nbsp;&nbsp;&nbsp;Standards and Tools\\nFor a corpus to be widely useful, it needs to be available in a widely\\nsupported format.  However, the cutting edge of NLP research depends\\non new kinds of annotations, which by definition are not widely supported.\\nIn general, adequate tools for creation, publication and use of\\nlinguistic data are not widely available.  Most projects must\\ndevelop their own set of tools for internal use, which is no help\\nto others who lack the necessary resources.\\nFurthermore, we do not have adequate, generally-accepted standards for\\nexpressing the structure and content of corpora. Without\\nsuch standards, general-purpose tools are impossible — though at the\\nsame time, without available tools, adequate standards are unlikely to\\nbe developed, used and accepted.\\nOne response to this situation has been to forge ahead with developing\\na generic format which is sufficiently expressive to capture a wide variety of\\nannotation types (see 8 for examples).\\nThe challenge for NLP is to write programs that cope with the generality\\nof such formats.\\nFor example, if the programming task involves tree data, and the\\nfile format permits arbitrary directed graphs, then input data must\\nbe validated to check for tree properties such as rootedness, connectedness,\\nand acyclicity.\\nIf the input files contain other layers of annotation, the program\\nwould need to know how to ignore them when the data was loaded,\\nbut not invalidate or obliterate those layers when the tree data\\nwas saved back to the file.\\nAnother response has been to write one-off scripts\\nto manipulate corpus formats; such scripts litter the filespaces of many\\nNLP researchers.\\nNLTK\\'s corpus readers are a more systematic\\napproach, founded on the premise that the work of parsing a corpus format\\nshould only be done once (per programming language).\\n\\n\\nFigure 3.2: A Common Format vs A Common Interface\\n\\nInstead of focussing on a common format, we believe it is more promising to\\ndevelop a common interface (cf. nltk.corpus).  Consider the\\ncase of treebanks, an important corpus type for work in NLP.\\nThere are many ways to store a phrase structure tree in a file.\\nWe can use nested parentheses, or nested XML elements,\\nor a dependency notation with a (child-id, parent-id) pair on each line,\\nor an XML version of the dependency notation, etc.\\nHowever, in each case the logical structure is almost the same.\\nIt is much easier to devise a common interface that allows\\napplication programmers to write code to access tree data\\nusing methods such as children(), leaves(), depth(), and\\nso forth.\\nNote that this approach follows accepted practice within\\ncomputer science, viz. abstract data types, object oriented design,\\nand the three layer architecture (3.2).\\nThe last of these — from the world of relational databases —\\nallows end-user applications to use a common model (the \"relational model\")\\nand a common language (SQL), to abstract away from the idiosyncrasies\\nof file storage, and allowing innovations in filesystem technologies\\nto occur without disturbing end-user applications.\\nIn the same way, a common corpus interface\\ninsulates application programs from data formats.\\nIn this context, when creating a new corpus for dissemination, it is\\nexpedient to use an existing widely-used format wherever possible.\\nWhen this is not possible, the corpus could be accompanied with software\\n— such as an nltk.corpus module — that supports\\nexisting interface methods.\\n\\n\\n3.7&nbsp;&nbsp;&nbsp;Special Considerations when Working with Endangered Languages\\nThe importance of language to science and the arts is matched in\\nsignificance by the cultural treasure embodied in language.\\nEach of the world\\'s ~7,000 human languages is rich in unique respects,\\nin its oral histories and creation legends, down to its grammatical\\nconstructions and its very words and their nuances of meaning.\\nThreatened remnant cultures have words to distinguish plant subspecies\\naccording to therapeutic uses that are unknown to science.  Languages\\nevolve over time as they come into contact with each other, and each\\none provides a unique window onto human pre-history.\\nIn many parts of the world, small linguistic variations\\nfrom one town to the next add up to a completely different language in\\nthe space of a half-hour drive.  For its breathtaking complexity and\\ndiversity, human language is as a colorful tapestry stretching\\nthrough time and space.\\nHowever, most of the world\\'s languages face extinction.\\nIn response to this, many linguists are hard at work documenting the languages, constructing\\nrich records of this important facet of the world\\'s linguistic heritage.\\nWhat can the field of NLP offer to help with this effort?  Developing\\ntaggers, parsers, named-entity recognizers, etc,\\nis not an early priority, and there is usually insufficient data for\\ndeveloping such tools in any case.  Instead, the most frequently voiced need is\\nto have better tools for collecting and curating data, with a focus\\non texts and lexicons.\\nOn the face of things, it should be a straightforward matter to start collecting texts\\nin an endangered language.  Even if we ignore vexed issues such as who\\nowns the texts, and sensitivities surrounding cultural knowledge contained in the texts,\\nthere is the obvious practical issue of transcription.\\nMost languages lack a standard orthography.  When a language\\nhas no literary tradition, the conventions of spelling and punctuation\\nare not well-established.  Therefore it is common practice\\nto create a lexicon in tandem with a text collection, continually updating\\nthe lexicon as new words appear in the texts.  This work could be done\\nusing a text processor (for the texts) and a spreadsheet (for the lexicon).\\nBetter still, SIL\\'s free linguistic software Toolbox and Fieldworks\\nprovide sophisticated support for integrated creation of texts and lexicons.\\nWhen speakers of the language in question are trained to enter texts themselves,\\na common obstacle is an overriding concern for correct spelling.  Having\\na lexicon greatly helps this process, but we need to have lookup methods\\nthat do not assume someone can determine the citation form of an arbitrary\\nword.  The problem may be acute for languages having a complex morphology that\\nincludes prefixes.  In such cases it helps to tag lexical items with\\nsemantic domains, and to permit lookup by semantic domain or by gloss.\\nPermitting lookup by pronunciation similarity is also a big help.\\nHere\\'s a simple demonstration of how to do this.\\nThe first step is to identify confusible letter sequences,\\nand map complex versions to simpler versions.  We might also notice\\nthat the relative order of letters within a cluster of consonants\\nis a source of spelling errors, and so we normalize the\\norder of consonants.\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; mappings = [(\\'ph\\', \\'f\\'), (\\'ght\\', \\'t\\'), (\\'^kn\\', \\'n\\'), (\\'qu\\', \\'kw\\'),\\n...             (\\'[aeiou]+\\', \\'a\\'), (r\\'(.)\\\\1\\', r\\'\\\\1\\')]\\n&gt;&gt;&gt; def signature(word):\\n...     for patt, repl in mappings:\\n...         word = re.sub(patt, repl, word)\\n...     pieces = re.findall(\\'[^aeiou]+\\', word)\\n...     return \\'\\'.join(char for piece in pieces for char in sorted(piece))[:8]\\n&gt;&gt;&gt; signature(\\'illefent\\')\\n\\'lfnt\\'\\n&gt;&gt;&gt; signature(\\'ebsekwieous\\')\\n\\'bskws\\'\\n&gt;&gt;&gt; signature(\\'nuculerr\\')\\n\\'nclr\\'\\n\\n\\n\\nNext, we create a mapping from signatures to words, for all the words\\nin our lexicon.  We can use this to get candidate corrections for a\\ngiven input word (but we must first compute that word\\'s signature).\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; signatures = nltk.Index((signature(w), w) for w in nltk.corpus.words.words())\\n&gt;&gt;&gt; signatures[signature(\\'nuculerr\\')]\\n[\\'anicular\\', \\'inocular\\', \\'nucellar\\', \\'nuclear\\', \\'unicolor\\', \\'uniocular\\', \\'unocular\\']\\n\\n\\n\\nFinally, we should rank the results in terms of similarity with the original word.\\nThis is done by the function rank().  The only remaining function provides a\\nsimple interface to the user:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; def rank(word, wordlist):\\n...     ranked = sorted((nltk.edit_distance(word, w), w) for w in wordlist)\\n...     return [word for (_, word) in ranked]\\n&gt;&gt;&gt; def fuzzy_spell(word):\\n...     sig = signature(word)\\n...     if sig in signatures:\\n...         return rank(word, signatures[sig])\\n...     else:\\n...         return []\\n&gt;&gt;&gt; fuzzy_spell(\\'illefent\\')\\n[\\'olefiant\\', \\'elephant\\', \\'oliphant\\', \\'elephanta\\']\\n&gt;&gt;&gt; fuzzy_spell(\\'ebsekwieous\\')\\n[\\'obsequious\\']\\n&gt;&gt;&gt; fuzzy_spell(\\'nucular\\')\\n[\\'anicular\\', \\'inocular\\', \\'nucellar\\', \\'nuclear\\', \\'unocular\\', \\'uniocular\\', \\'unicolor\\']\\n\\n\\n\\nThis is just one illustration where a simple program can facilitate access to lexical\\ndata in a context where the writing system of a language may not be standardized, or\\nwhere users of the language may not have a good command of spellings.  Other simple\\napplications of NLP in this area include: building indexes to facilitate access to\\ndata, gleaning wordlists from texts,\\nlocating examples of word usage in constructing a lexicon, detecting prevalent or\\nexceptional patterns in poorly understood data, and performing specialized validation\\non data created using various linguistic software tools.  We will return to the last\\nof these in 5.\\n\\n\\n\\n4&nbsp;&nbsp;&nbsp;Working with XML\\nThe Extensible Markup Language (XML) provides a framework for designing\\ndomain-specific markup languages.  It is sometimes used for representing\\nannotated text and for lexical resources.  Unlike HTML with its predefined\\ntags, XML permits us to make up our own tags.  Unlike a database, XML\\npermits us to create data without first specifying its structure, and it\\npermits us to have optional and repeatable elements.  In this section\\nwe briefly review some features of XML that are relevant for representing\\nlinguistic data, and show how to access data stored in XML files using\\nPython programs.\\n\\n4.1&nbsp;&nbsp;&nbsp;Using XML for Linguistic Structures\\nThanks to its flexibility and extensibility, XML is a natural\\nchoice for representing linguistic structures.  Here\\'s an example\\nof a simple lexical entry.\\n\\n  (2)&lt;entry&gt;\\n  &lt;headword&gt;whale&lt;/headword&gt;\\n  &lt;pos&gt;noun&lt;/pos&gt;\\n  &lt;gloss&gt;any of the larger cetacean mammals having a streamlined\\n    body and breathing through a blowhole on the head&lt;/gloss&gt;\\n&lt;/entry&gt;\\n\\n\\nIt consists of a series of XML tags enclosed in angle brackets.\\nEach opening tag, like &lt;gloss&gt; is matched with a closing tag, like &lt;/gloss&gt;;\\ntogether they constitute an XML element.\\nThe above example has been laid out nicely using whitespace, but it could\\nequally have been put on a single, long line.  Our approach to processing\\nXML will usually not be sensitive to whitespace.  In order for XML to be\\nwell formed, all opening tags must have corresponding closing tags, at the\\nsame level of nesting (i.e. the XML document must be a well-formed tree).\\nXML permits us to repeat elements, e.g. to add another gloss field as we see below.\\nWe will use different whitespace to underscore the point that layout\\ndoes not matter.\\n\\n  (3)&lt;entry&gt;&lt;headword&gt;whale&lt;/headword&gt;&lt;pos&gt;noun&lt;/pos&gt;&lt;gloss&gt;any of the\\nlarger cetacean mammals having a streamlined body and breathing\\nthrough a blowhole on the head&lt;/gloss&gt;&lt;gloss&gt;a very large person;\\nimpressive in size or qualities&lt;/gloss&gt;&lt;/entry&gt;\\n\\n\\nA further step might be to link our lexicon to some external resource, such as WordNet,\\nusing external identifiers.  In (4) we group the gloss and a synset identifier\\ninside a new element which we have called \"sense\".\\n\\n  (4)&lt;entry&gt;\\n  &lt;headword&gt;whale&lt;/headword&gt;\\n  &lt;pos&gt;noun&lt;/pos&gt;\\n  &lt;sense&gt;\\n    &lt;gloss&gt;any of the larger cetacean mammals having a streamlined\\n      body and breathing through a blowhole on the head&lt;/gloss&gt;\\n    &lt;synset&gt;whale.n.02&lt;/synset&gt;\\n  &lt;/sense&gt;\\n  &lt;sense&gt;\\n    &lt;gloss&gt;a very large person; impressive in size or qualities&lt;/gloss&gt;\\n    &lt;synset&gt;giant.n.04&lt;/synset&gt;\\n  &lt;/sense&gt;\\n&lt;/entry&gt;\\n\\n\\nAlternatively, we could have represented the synset identifier using an XML attribute,\\nwithout the need for any nested structure, as in (5).\\n\\n  (5)&lt;entry&gt;\\n  &lt;headword&gt;whale&lt;/headword&gt;\\n  &lt;pos&gt;noun&lt;/pos&gt;\\n  &lt;gloss synset=\"whale.n.02\"&gt;any of the larger cetacean mammals having\\n      a streamlined body and breathing through a blowhole on the head&lt;/gloss&gt;\\n  &lt;gloss synset=\"giant.n.04\"&gt;a very large person; impressive in size or\\n      qualities&lt;/gloss&gt;\\n&lt;/entry&gt;\\n\\n\\nThis illustrates some of the flexibility of XML.  If it seems somewhat arbitrary\\nthat\\'s because it is!  Following the rules of XML we can invent new attribute\\nnames, and nest them as deeply as we like.  We can repeat elements, leave them\\nout, and put them in a different order each time.  We can have fields whose\\npresence depends on the value of some other field, e.g. if the part of speech\\nis \"verb\", then the entry can have a past_tense element to hold the\\npast tense of the verb, but if the part of speech is \"noun\" no past_tense\\nelement is permitted.  To impose some order over all\\nthis freedom, we can constrain the structure of an XML file using a \"schema,\"\\nwhich is a declaration akin to a context free grammar.  Tools exist for\\ntesting the validity of an XML file with respect to a schema.\\n\\n\\n4.2&nbsp;&nbsp;&nbsp;The Role of XML\\nWe can use XML to represent many kinds of linguistic information.\\nHowever, the flexibility comes at a price.  Each time we introduce a complication, such as by permitting\\nan element to be optional or repeated, we make more work for any program\\nthat accesses the data.  We also make it more difficult to check the validity of the\\ndata, or to interrogate the data using one of the XML query languages.\\nThus, using XML to represent linguistic structures does not magically solve the data\\nmodeling problem.  We still have to work out how to structure the data,\\nthen define that structure with a schema, and then\\nwrite programs to read and write the format and convert it to other formats.\\nSimilarly, we still need to follow some standard principles concerning\\ndata normalization.  It is wise to avoid making duplicate copies of the\\nsame information, so that we don\\'t end up with inconsistent data when\\nonly one copy is changed.  For example, a cross-reference that was\\nrepresented as &lt;xref&gt;headword&lt;/xref&gt; would duplicate the storage\\nof the headword of some other lexical entry, and the link would break\\nif the copy of the string at the other location was modified.\\nExistential dependencies between information types need to be\\nmodeled, so that we can\\'t create elements without a home.\\nFor example, if sense definitions cannot exist independently\\nof a lexical entry, the sense element can be nested inside the entry\\nelement.  Many-to-many relations need to be abstracted out of\\nhierarchical structures.  For example, if a word can have many corresponding\\nsenses, and a sense can have several corresponding words, then\\nboth words and senses must be enumerated separately, as must the\\nlist of (word, sense) pairings.  This complex structure might even be split\\nacross three separate XML files.\\nAs we can see, although XML provides us with a convenient format\\naccompanied by an extensive collection of tools, it offers no panacea.\\n\\n\\n4.3&nbsp;&nbsp;&nbsp;The ElementTree Interface\\nPython\\'s ElementTree module provides a convenient way to access\\ndata stored in XML files.  ElementTree is part of Python\\'s\\nstandard library (since Python 2.5), and is also provided as\\npart of NLTK in case you are using Python 2.4.\\nWe will illustrate the use of ElementTree using a collection\\nof Shakespeare plays that have been formatted using XML.\\nLet\\'s load the XML file and inspect the raw data, first\\nat the top of the file , where we see some\\nXML headers and the name of a schema called play.dtd,\\nfollowed by the root element PLAY.\\nWe pick it up again at the start of Act 1 .\\n(Some blank lines have been omitted from the output.)\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; merchant_file = nltk.data.find(\\'corpora/shakespeare/merchant.xml\\')\\n&gt;&gt;&gt; raw = open(merchant_file).read()\\n&gt;&gt;&gt; print(raw[:163]) \\n&lt;?xml version=\"1.0\"?&gt;\\n&lt;?xml-stylesheet type=\"text/css\" href=\"shakes.css\"?&gt;\\n&lt;!-- &lt;!DOCTYPE PLAY SYSTEM \"play.dtd\"&gt; --&gt;\\n&lt;PLAY&gt;\\n&lt;TITLE&gt;The Merchant of Venice&lt;/TITLE&gt;\\n&gt;&gt;&gt; print(raw[1789:2006]) \\n&lt;TITLE&gt;ACT I&lt;/TITLE&gt;\\n&lt;SCENE&gt;&lt;TITLE&gt;SCENE I.  Venice. A street.&lt;/TITLE&gt;\\n&lt;STAGEDIR&gt;Enter ANTONIO, SALARINO, and SALANIO&lt;/STAGEDIR&gt;\\n&lt;SPEECH&gt;\\n&lt;SPEAKER&gt;ANTONIO&lt;/SPEAKER&gt;\\n&lt;LINE&gt;In sooth, I know not why I am so sad:&lt;/LINE&gt;\\n\\n\\n\\nWe have just accessed the XML data as a string.  As we can see,\\nthe string at the start of Act 1 contains XML tags for title, scene, stage directions, and so forth.\\nThe next step is to process the file contents as structured XML data,\\nusing ElementTree.  We are processing a file (a multi-line string)\\nand building a tree, so its not surprising that the method name is parse .\\nThe variable merchant contains an XML element PLAY .\\nThis element has internal structure; we can use an index\\nto get its first child, a TITLE element .\\nWe can also see the text content of this element, the title of the play .\\nTo get a list of all the child elements, we use the\\ngetchildren() method .\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; from xml.etree.ElementTree import ElementTree\\n&gt;&gt;&gt; merchant = ElementTree().parse(merchant_file) \\n&gt;&gt;&gt; merchant\\n&lt;Element \\'PLAY\\' at 0x10ac43d18&gt; # [_element-play]\\n&gt;&gt;&gt; merchant[0]\\n&lt;Element \\'TITLE\\' at 0x10ac43c28&gt; # [_element-title]\\n&gt;&gt;&gt; merchant[0].text\\n\\'The Merchant of Venice\\' # [_element-text]\\n&gt;&gt;&gt; merchant.getchildren() \\n[&lt;Element \\'TITLE\\' at 0x10ac43c28&gt;, &lt;Element \\'PERSONAE\\' at 0x10ac43bd8&gt;,\\n&lt;Element \\'SCNDESCR\\' at 0x10b067f98&gt;, &lt;Element \\'PLAYSUBT\\' at 0x10af37048&gt;,\\n&lt;Element \\'ACT\\' at 0x10af37098&gt;, &lt;Element \\'ACT\\' at 0x10b936368&gt;,\\n&lt;Element \\'ACT\\' at 0x10b934b88&gt;, &lt;Element \\'ACT\\' at 0x10cfd8188&gt;,\\n&lt;Element \\'ACT\\' at 0x10cfadb38&gt;]\\n\\n\\n\\nThe play consists of a title, the personae, a scene description, a subtitle, and five acts.\\nEach act has a title and some scenes, and each scene consists of speeches which are made\\nup of lines, a structure with four levels of nesting.  Let\\'s dig down into Act IV:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; merchant[-2][0].text\\n\\'ACT IV\\'\\n&gt;&gt;&gt; merchant[-2][1]\\n&lt;Element \\'SCENE\\' at 0x10cfd8228&gt;\\n&gt;&gt;&gt; merchant[-2][1][0].text\\n\\'SCENE I.  Venice. A court of justice.\\'\\n&gt;&gt;&gt; merchant[-2][1][54]\\n&lt;Element \\'SPEECH\\' at 0x10cfb02c8&gt;\\n&gt;&gt;&gt; merchant[-2][1][54][0]\\n&lt;Element \\'SPEAKER\\' at 0x10cfb0318&gt;\\n&gt;&gt;&gt; merchant[-2][1][54][0].text\\n\\'PORTIA\\'\\n&gt;&gt;&gt; merchant[-2][1][54][1]\\n&lt;Element \\'LINE\\' at 0x10cfb0368&gt;\\n&gt;&gt;&gt; merchant[-2][1][54][1].text\\n\"The quality of mercy is not strain\\'d,\"\\n\\n\\n\\n\\nNote\\nYour Turn:\\nRepeat some of the above methods, for one of the other Shakespeare plays\\nincluded in the corpus, such as Romeo and Juliet or Macbeth;\\nfor a list, see nltk.corpus.shakespeare.fileids().\\n\\nAlthough we can access the entire tree this way, it is more convenient to search for\\nsub-elements with particular names.  Recall that the elements at the top level have\\nseveral types.  We can iterate over just the types we are interested in (such as\\nthe acts), using merchant.findall(\\'ACT\\').  Here\\'s an example of doing such\\ntag-specific searches at every level of nesting:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; for i, act in enumerate(merchant.findall(\\'ACT\\')):\\n...     for j, scene in enumerate(act.findall(\\'SCENE\\')):\\n...         for k, speech in enumerate(scene.findall(\\'SPEECH\\')):\\n...             for line in speech.findall(\\'LINE\\'):\\n...                 if \\'music\\' in str(line.text):\\n...                     print(\"Act %d Scene %d Speech %d: %s\" % (i+1, j+1, k+1, line.text))\\nAct 3 Scene 2 Speech 9: Let music sound while he doth make his choice;\\nAct 3 Scene 2 Speech 9: Fading in music: that the comparison\\nAct 3 Scene 2 Speech 9: And what is music then? Then music is\\nAct 5 Scene 1 Speech 23: And bring your music forth into the air.\\nAct 5 Scene 1 Speech 23: Here will we sit and let the sounds of music\\nAct 5 Scene 1 Speech 23: And draw her home with music.\\nAct 5 Scene 1 Speech 24: I am never merry when I hear sweet music.\\nAct 5 Scene 1 Speech 25: Or any air of music touch their ears,\\nAct 5 Scene 1 Speech 25: By the sweet power of music: therefore the poet\\nAct 5 Scene 1 Speech 25: But music for the time doth change his nature.\\nAct 5 Scene 1 Speech 25: The man that hath no music in himself,\\nAct 5 Scene 1 Speech 25: Let no such man be trusted. Mark the music.\\nAct 5 Scene 1 Speech 29: It is your music, madam, of the house.\\nAct 5 Scene 1 Speech 32: No better a musician than the wren.\\n\\n\\n\\nInstead of navigating each step of the way down the hierarchy, we can search for\\nparticular embedded elements.  For example, let\\'s examine the sequence of speakers.\\nWe can use a frequency distribution to see who has the most to say:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; from collections import Counter\\n&gt;&gt;&gt; speaker_seq = [s.text for s in merchant.findall(\\'ACT/SCENE/SPEECH/SPEAKER\\')]\\n&gt;&gt;&gt; speaker_freq = Counter(speaker_seq)\\n&gt;&gt;&gt; top5 = speaker_freq.most_common(5)\\n&gt;&gt;&gt; top5\\n[(\\'PORTIA\\', 117), (\\'SHYLOCK\\', 79), (\\'BASSANIO\\', 73),\\n(\\'GRATIANO\\', 48), (\\'LORENZO\\', 47)]\\n\\n\\n\\nWe can also look for patterns in who follows who in the dialogues.\\nSince there\\'s 23 speakers, we need to reduce the \"vocabulary\"\\nto a manageable size first, using the method described in\\n3.\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; from collections import defaultdict\\n&gt;&gt;&gt; abbreviate = defaultdict(lambda: \\'OTH\\')\\n&gt;&gt;&gt; for speaker, _ in top5:\\n...     abbreviate[speaker] = speaker[:4]\\n...\\n&gt;&gt;&gt; speaker_seq2 = [abbreviate[speaker] for speaker in speaker_seq]\\n&gt;&gt;&gt; cfd = nltk.ConditionalFreqDist(nltk.bigrams(speaker_seq2))\\n&gt;&gt;&gt; cfd.tabulate()\\n     ANTO BASS GRAT  OTH PORT SHYL\\nANTO    0   11    4   11    9   12\\nBASS   10    0   11   10   26   16\\nGRAT    6    8    0   19    9    5\\n OTH    8   16   18  153   52   25\\nPORT    7   23   13   53    0   21\\nSHYL   15   15    2   26   21    0\\n\\n\\n\\nIgnoring the entries for exchanges between people\\nother than the top 5 (labeled OTH), the largest value suggests\\nthat Portia and Bassanio have the most frequent interactions.\\n\\n\\n4.4&nbsp;&nbsp;&nbsp;Using ElementTree for Accessing Toolbox Data\\nIn 4 we saw a simple interface\\nfor accessing Toolbox data, a popular and well-established format\\nused by linguists for managing data.\\nIn this section we discuss a variety of\\ntechniques for manipulating Toolbox data in ways that are not supported\\nby the Toolbox software.  The methods we discuss could be\\napplied to other record-structured data, regardless of the actual file format.\\nWe can use the toolbox.xml() method to access a Toolbox\\nfile and load it into an elementtree object.  This file\\ncontains a lexicon for the Rotokas language of Papua New Guinea.\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; from nltk.corpus import toolbox\\n&gt;&gt;&gt; lexicon = toolbox.xml(\\'rotokas.dic\\')\\n\\n\\n\\nThere are two ways to access the contents of the lexicon object, by\\nindexes and by paths.  Indexes use the familiar syntax, thus\\nlexicon[3] returns entry number 3 (which is actually the fourth\\nentry counting from zero); lexicon[3][0] returns its first field:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; lexicon[3][0]\\n&lt;Element \\'lx\\' at 0x10b2f6958&gt;\\n&gt;&gt;&gt; lexicon[3][0].tag\\n\\'lx\\'\\n&gt;&gt;&gt; lexicon[3][0].text\\n\\'kaa\\'\\n\\n\\n\\nThe second way to access the contents of the lexicon object uses\\npaths.  The lexicon is a series of record objects, each containing\\na series of field objects, such as lx and ps.  We can\\nconveniently address all of the lexemes using the path record/lx.\\nHere we use the findall() function to search for any matches to\\nthe path record/lx, and we access the text content of the element,\\nnormalizing it to lowercase.\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; [lexeme.text.lower() for lexeme in lexicon.findall(\\'record/lx\\')]\\n[\\'kaa\\', \\'kaa\\', \\'kaa\\', \\'kaakaaro\\', \\'kaakaaviko\\', \\'kaakaavo\\', \\'kaakaoko\\',\\n\\'kaakasi\\', \\'kaakau\\', \\'kaakauko\\', \\'kaakito\\', \\'kaakuupato\\', ..., \\'kuvuto\\']\\n\\n\\n\\nLet\\'s view the Toolbox data in XML format.  The write() method of\\nElementTree expects a file object.  We usually create one of these\\nusing Python\\'s built-in open() function.  In order to see the output\\ndisplayed on the screen, we can use a special pre-defined file object\\ncalled stdout  (standard output), defined in Python\\'s sys module.\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; import sys\\n&gt;&gt;&gt; from nltk.util import elementtree_indent\\n&gt;&gt;&gt; from xml.etree.ElementTree import ElementTree\\n&gt;&gt;&gt; elementtree_indent(lexicon)\\n&gt;&gt;&gt; tree = ElementTree(lexicon[3])\\n&gt;&gt;&gt; tree.write(sys.stdout, encoding=\\'unicode\\') \\n&lt;record&gt;\\n  &lt;lx&gt;kaa&lt;/lx&gt;\\n  &lt;ps&gt;N&lt;/ps&gt;\\n  &lt;pt&gt;MASC&lt;/pt&gt;\\n  &lt;cl&gt;isi&lt;/cl&gt;\\n  &lt;ge&gt;cooking banana&lt;/ge&gt;\\n  &lt;tkp&gt;banana bilong kukim&lt;/tkp&gt;\\n  &lt;pt&gt;itoo&lt;/pt&gt;\\n  &lt;sf&gt;FLORA&lt;/sf&gt;\\n  &lt;dt&gt;12/Aug/2005&lt;/dt&gt;\\n  &lt;ex&gt;Taeavi iria kaa isi kovopaueva kaparapasia.&lt;/ex&gt;\\n  &lt;xp&gt;Taeavi i bin planim gaden banana bilong kukim tasol long paia.&lt;/xp&gt;\\n  &lt;xe&gt;Taeavi planted banana in order to cook it.&lt;/xe&gt;\\n&lt;/record&gt;\\n\\n\\n\\n\\n\\n4.5&nbsp;&nbsp;&nbsp;Formatting Entries\\nWe can use the same idea we saw above to generate HTML tables instead of plain text.\\nThis would be useful for publishing a Toolbox lexicon on the web.\\nIt produces HTML elements &lt;table&gt;, &lt;tr&gt; (table row), and\\n&lt;td&gt; (table data).\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; html = \"&lt;table&gt;\\\\n\"\\n&gt;&gt;&gt; for entry in lexicon[70:80]:\\n...     lx = entry.findtext(\\'lx\\')\\n...     ps = entry.findtext(\\'ps\\')\\n...     ge = entry.findtext(\\'ge\\')\\n...     html += \"  &lt;tr&gt;&lt;td&gt;%s&lt;/td&gt;&lt;td&gt;%s&lt;/td&gt;&lt;td&gt;%s&lt;/td&gt;&lt;/tr&gt;\\\\n\" % (lx, ps, ge)\\n&gt;&gt;&gt; html += \"&lt;/table&gt;\"\\n&gt;&gt;&gt; print(html)\\n&lt;table&gt;\\n  &lt;tr&gt;&lt;td&gt;kakae&lt;/td&gt;&lt;td&gt;???&lt;/td&gt;&lt;td&gt;small&lt;/td&gt;&lt;/tr&gt;\\n  &lt;tr&gt;&lt;td&gt;kakae&lt;/td&gt;&lt;td&gt;CLASS&lt;/td&gt;&lt;td&gt;child&lt;/td&gt;&lt;/tr&gt;\\n  &lt;tr&gt;&lt;td&gt;kakaevira&lt;/td&gt;&lt;td&gt;ADV&lt;/td&gt;&lt;td&gt;small-like&lt;/td&gt;&lt;/tr&gt;\\n  &lt;tr&gt;&lt;td&gt;kakapikoa&lt;/td&gt;&lt;td&gt;???&lt;/td&gt;&lt;td&gt;small&lt;/td&gt;&lt;/tr&gt;\\n  &lt;tr&gt;&lt;td&gt;kakapikoto&lt;/td&gt;&lt;td&gt;N&lt;/td&gt;&lt;td&gt;newborn baby&lt;/td&gt;&lt;/tr&gt;\\n  &lt;tr&gt;&lt;td&gt;kakapu&lt;/td&gt;&lt;td&gt;V&lt;/td&gt;&lt;td&gt;place in sling for purpose of carrying&lt;/td&gt;&lt;/tr&gt;\\n  &lt;tr&gt;&lt;td&gt;kakapua&lt;/td&gt;&lt;td&gt;N&lt;/td&gt;&lt;td&gt;sling for lifting&lt;/td&gt;&lt;/tr&gt;\\n  &lt;tr&gt;&lt;td&gt;kakara&lt;/td&gt;&lt;td&gt;N&lt;/td&gt;&lt;td&gt;arm band&lt;/td&gt;&lt;/tr&gt;\\n  &lt;tr&gt;&lt;td&gt;Kakarapaia&lt;/td&gt;&lt;td&gt;N&lt;/td&gt;&lt;td&gt;village name&lt;/td&gt;&lt;/tr&gt;\\n  &lt;tr&gt;&lt;td&gt;kakarau&lt;/td&gt;&lt;td&gt;N&lt;/td&gt;&lt;td&gt;frog&lt;/td&gt;&lt;/tr&gt;\\n&lt;/table&gt;\\n\\n\\n\\n\\n\\n\\n5&nbsp;&nbsp;&nbsp;Working with Toolbox Data\\nGiven the popularity of Toolbox amongst linguists, we will discuss some further\\nmethods for working with Toolbox data.  Many of the methods discussed in previous\\nchapters, such as counting, building frequency distributions, tabulating co-occurrences,\\ncan be applied to the content of Toolbox entries.  For example, we can trivially\\ncompute the average number of fields for each entry:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; from nltk.corpus import toolbox\\n&gt;&gt;&gt; lexicon = toolbox.xml(\\'rotokas.dic\\')\\n&gt;&gt;&gt; sum(len(entry) for entry in lexicon) / len(lexicon)\\n13.635...\\n\\n\\n\\nIn this section we will discuss two tasks that arise in the context of documentary\\nlinguistics, neither of which is supported by the Toolbox software.\\n\\n5.1&nbsp;&nbsp;&nbsp;Adding a Field to Each Entry\\nIt is often convenient to add new fields that are derived automatically from\\nexisting ones.  Such fields often facilitate search and analysis.\\nFor instance, in 5.1 we define a function cv() which\\nmaps a string of consonants and vowels to the corresponding CV sequence,\\ne.g. kakapua would map to CVCVCVV.\\nThis mapping has four steps.  First, the string is converted to lowercase,\\nthen we replace any non-alphabetic characters [^a-z] with an underscore.\\nNext, we replace all vowels with V.  Finally, anything that is not\\na V or an underscore must be a consonant, so we replace it with a C.\\nNow, we can scan the lexicon and add a new cv field after every lx field.\\n5.1 shows what this does to a particular entry; note\\nthe last line of output, which shows the new cv field.\\n\\n\\n\\n\\n&nbsp;\\nfrom xml.etree.ElementTree import SubElement\\n\\ndef cv(s):\\n    s = s.lower()\\n    s = re.sub(r\\'[^a-z]\\',     r\\'_\\', s)\\n    s = re.sub(r\\'[aeiou]\\',    r\\'V\\', s)\\n    s = re.sub(r\\'[^V_]\\',      r\\'C\\', s)\\n    return (s)\\n\\ndef add_cv_field(entry):\\n    for field in entry:\\n        if field.tag == \\'lx\\':\\n            cv_field = SubElement(entry, \\'cv\\')\\n            cv_field.text = cv(field.text)\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; lexicon = toolbox.xml(\\'rotokas.dic\\')\\n&gt;&gt;&gt; add_cv_field(lexicon[53])\\n&gt;&gt;&gt; print(nltk.toolbox.to_sfm_string(lexicon[53]))\\n\\\\lx kaeviro\\n\\\\ps V\\n\\\\pt A\\n\\\\ge lift off\\n\\\\ge take off\\n\\\\tkp go antap\\n\\\\sc MOTION\\n\\\\vx 1\\n\\\\nt used to describe action of plane\\n\\\\dt 03/Jun/2005\\n\\\\ex Pita kaeviroroe kepa kekesia oa vuripierevo kiuvu.\\n\\\\xp Pita i go antap na lukim haus win i bagarapim.\\n\\\\xe Peter went to look at the house that the wind destroyed.\\n\\\\cv CVVCVCV\\n\\n\\nExample 5.1 (code_add_cv_field.py): Figure 5.1: Adding a new cv field to a lexical entry\\n\\n\\nNote\\nIf a Toolbox file is being continually updated, the program in\\ncode-add-cv-field will need to be run more than once.  It would\\nbe possible to modify add_cv_field() to modify the contents\\nof an existing entry.  However, it is a safer practice to use such\\nprograms to create enriched files for the purpose of data analysis,\\nwithout replacing the manually curated source files.\\n\\n\\n\\n5.2&nbsp;&nbsp;&nbsp;Validating a Toolbox Lexicon\\nMany lexicons in Toolbox format do not conform to any particular schema.\\nSome entries may include extra fields, or may order existing fields\\nin a new way.\\nManually inspecting thousands of lexical entries is not practicable.\\nHowever, we can easily identify frequent field sequences,\\nwith the help of a Counter:\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; from collections import Counter\\n&gt;&gt;&gt; field_sequences = Counter(\\':\\'.join(field.tag for field in entry) for entry in lexicon)\\n&gt;&gt;&gt; field_sequences.most_common()\\n[(\\'lx:ps:pt:ge:tkp:dt:ex:xp:xe\\', 41), (\\'lx:rt:ps:pt:ge:tkp:dt:ex:xp:xe\\', 37),\\n(\\'lx:rt:ps:pt:ge:tkp:dt:ex:xp:xe:ex:xp:xe\\', 27), (\\'lx:ps:pt:ge:tkp:nt:dt:ex:xp:xe\\', 20), ...]\\n\\n\\n\\nAfter inspecting these field sequences we could devise a context\\nfree grammar for lexical entries.  The grammar in 5.2\\nuses the CFG format we saw in 8..  Such a grammar models the implicit\\nnested structure of Toolbox entries, and builds a tree structure in which the\\nleaves of the tree are individual field names.  Finally, we iterate\\nover the entries and report their conformance with the grammar, as\\nshown in 5.2.\\nThose that are accepted by the grammar are prefixed with a \\'+\\' ,\\nand those that are rejected are prefixed with a \\'-\\' .\\nDuring the process of developing such a grammar\\nit helps to filter out some of the tags .\\n\\n\\n\\n\\n&nbsp;\\ngrammar = nltk.CFG.fromstring(\\'\\'\\'\\n  S -&gt; Head PS Glosses Comment Date Sem_Field Examples\\n  Head -&gt; Lexeme Root\\n  Lexeme -&gt; \"lx\"\\n  Root -&gt; \"rt\" |\\n  PS -&gt; \"ps\"\\n  Glosses -&gt; Gloss Glosses |\\n  Gloss -&gt; \"ge\" | \"tkp\" | \"eng\"\\n  Date -&gt; \"dt\"\\n  Sem_Field -&gt; \"sf\"\\n  Examples -&gt; Example Ex_Pidgin Ex_English Examples |\\n  Example -&gt; \"ex\"\\n  Ex_Pidgin -&gt; \"xp\"\\n  Ex_English -&gt; \"xe\"\\n  Comment -&gt; \"cmt\" | \"nt\" |\\n  \\'\\'\\')\\n\\ndef validate_lexicon(grammar, lexicon, ignored_tags):\\n    rd_parser = nltk.RecursiveDescentParser(grammar)\\n    for entry in lexicon:\\n        marker_list = [field.tag for field in entry if field.tag not in ignored_tags]\\n        if list(rd_parser.parse(marker_list)):\\n            print(\"+\", \\':\\'.join(marker_list)) \\n        else:\\n            print(\"-\", \\':\\'.join(marker_list)) \\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; lexicon = toolbox.xml(\\'rotokas.dic\\')[10:20]\\n&gt;&gt;&gt; ignored_tags = [\\'arg\\', \\'dcsv\\', \\'pt\\', \\'vx\\'] \\n&gt;&gt;&gt; validate_lexicon(grammar, lexicon, ignored_tags)\\n- lx:ps:ge:tkp:sf:nt:dt:ex:xp:xe:ex:xp:xe:ex:xp:xe\\n- lx:rt:ps:ge:tkp:nt:dt:ex:xp:xe:ex:xp:xe\\n- lx:ps:ge:tkp:nt:dt:ex:xp:xe:ex:xp:xe\\n- lx:ps:ge:tkp:nt:sf:dt\\n- lx:ps:ge:tkp:dt:cmt:ex:xp:xe:ex:xp:xe\\n- lx:ps:ge:ge:ge:tkp:cmt:dt:ex:xp:xe\\n- lx:rt:ps:ge:ge:tkp:dt\\n- lx:rt:ps:ge:eng:eng:eng:ge:tkp:tkp:dt:cmt:ex:xp:xe:ex:xp:xe:ex:xp:xe:ex:xp:xe:ex:xp:xe\\n- lx:rt:ps:ge:tkp:dt:ex:xp:xe\\n- lx:ps:ge:ge:tkp:dt:ex:xp:xe:ex:xp:xe\\n\\n\\nExample 5.2 (code_toolbox_validation.py): Figure 5.2: Validating Toolbox Entries Using a Context Free Grammar\\n\\nAnother approach would be to use a chunk parser (7.),\\nsince these are much more effective at identifying partial\\nstructures, and can report the partial structures that have\\nbeen identified.  In 5.3 we set up\\na chunk grammar for the entries of a lexicon, then parse each entry.\\nA sample of the output from this program is shown in 5.4.\\n\\n\\n\\n\\n&nbsp;\\ngrammar = r\"\"\"\\n      lexfunc: {&lt;lf&gt;(&lt;lv&gt;&lt;ln|le&gt;*)*}\\n      example: {&lt;rf|xv&gt;&lt;xn|xe&gt;*}\\n      sense:   {&lt;sn&gt;&lt;ps&gt;&lt;pn|gv|dv|gn|gp|dn|rn|ge|de|re&gt;*&lt;example&gt;*&lt;lexfunc&gt;*}\\n      record:   {&lt;lx&gt;&lt;hm&gt;&lt;sense&gt;+&lt;dt&gt;}\\n    \"\"\"\\n\\n\\n\\n\\n&nbsp;\\n&gt;&gt;&gt; from xml.etree.ElementTree import ElementTree\\n&gt;&gt;&gt; from nltk.toolbox import ToolboxData\\n&gt;&gt;&gt; db = ToolboxData()\\n&gt;&gt;&gt; db.open(nltk.data.find(\\'corpora/toolbox/iu_mien_samp.db\\'))\\n&gt;&gt;&gt; lexicon = db.parse(grammar, encoding=\\'utf8\\')\\n&gt;&gt;&gt; tree = ElementTree(lexicon)\\n&gt;&gt;&gt; with open(\"iu_mien_samp.xml\", \"wb\") as output:\\n...     tree.write(output)\\n\\n\\nExample 5.3 (code_chunk_toolbox.py): Figure 5.3: Chunking a Toolbox Lexicon: A chunk grammar describing the structure of\\nentries for a lexicon for Iu Mien, a language of China.\\n\\n\\n\\nFigure 5.4: XML Representation of a Lexical Entry, Resulting from Chunk Parsing a Toolbox Record\\n\\n\\n\\n\\n6&nbsp;&nbsp;&nbsp;Describing Language Resources using OLAC Metadata\\nMembers of the NLP community have a common need for\\ndiscovering language resources with high precision and recall.\\nThe solution which has been developed by the Digital Libraries\\ncommunity involves metadata aggregation.\\n\\n6.1&nbsp;&nbsp;&nbsp;What is Metadata?\\nThe simplest definition of metadata is \"structured data about data.\"\\nMetadata is descriptive information about an object or resource whether\\nit be physical or electronic. While the term metadata itself is relatively new,\\nthe underlying concepts behind metadata have been in use for as long as\\ncollections of information have been organized.\\nLibrary catalogs represent a well-established type of metadata;\\nthey have served as collection management and resource discovery\\ntools for decades. Metadata can be generated either\\n\"by hand\" or generated automatically using software.\\nThe Dublin Core Metadata Initiative began in 1995 to develop\\nconventions for resource discovery on the web.\\nThe Dublin Core metadata elements represent a broad,\\ninterdisciplinary consensus about the core set of elements\\nthat are likely to be widely useful to support resource discovery.\\nThe Dublin Core consists of 15 metadata elements, where each\\nelement is optional and repeatable: Title, Creator, Subject,\\nDescription, Publisher, Contributor, Date, Type, Format,\\nIdentifier, Source, Language, Relation, Coverage, Rights.\\nThis metadata set can be used to describe resources that\\nexist in digital or traditional formats.\\nThe Open Archives initiative (OAI) provides a common framework\\nacross digital repositories of scholarly materials regardless of their type,\\nincluding documents, data, software, recordings, physical artifacts,\\ndigital surrogates, and so forth.\\nEach repository consists of a network accessible server offering\\npublic access to archived items.  Each item has a unique identifier,\\nand is associated with a Dublin Core metadata record (and possibly additional\\nrecords in other formats).  The OAI defines a protocol for metadata search\\nservices to \"harvest\" the contents of repositories.\\n\\n\\n6.2&nbsp;&nbsp;&nbsp;OLAC: Open Language Archives Community\\nThe Open Language Archives Community (OLAC) is an international\\npartnership of institutions and individuals who are creating a\\nworldwide virtual library of language resources by:\\n(i) developing consensus on best current practice for\\nthe digital archiving of language resources, and\\n(ii) developing a network of interoperating repositories\\nand services for housing and accessing such resources.\\nOLAC\\'s home on the web is at http://www.language-archives.org/.\\nOLAC Metadata is a standard for describing language resources.\\nUniform description across repositories is ensured by limiting\\nthe values of certain metadata elements to the use of terms\\nfrom controlled vocabularies. OLAC metadata can be used to\\ndescribe data and tools, in both physical and digital formats.\\nOLAC metadata extends the Dublin Core Metadata Set,\\na widely accepted standard for describing resources of all types.\\nTo this core set, OLAC adds descriptors to cover fundamental\\nproperties of language resources, such as subject language and\\nlinguistic type.  Here\\'s an example of a complete OLAC record:\\n&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\\n&lt;olac:olac xmlns:olac=\"http://www.language-archives.org/OLAC/1.1/\"\\n           xmlns=\"http://purl.org/dc/elements/1.1/\"\\n           xmlns:dcterms=\"http://purl.org/dc/terms/\"\\n           xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\\n           xsi:schemaLocation=\"http://www.language-archives.org/OLAC/1.1/\\n                http://www.language-archives.org/OLAC/1.1/olac.xsd\"&gt;\\n  &lt;title&gt;A grammar of Kayardild. With comparative notes on Tangkic.&lt;/title&gt;\\n  &lt;creator&gt;Evans, Nicholas D.&lt;/creator&gt;\\n  &lt;subject&gt;Kayardild grammar&lt;/subject&gt;\\n  &lt;subject xsi:type=\"olac:language\" olac:code=\"gyd\"&gt;Kayardild&lt;/subject&gt;\\n  &lt;language xsi:type=\"olac:language\" olac:code=\"en\"&gt;English&lt;/language&gt;\\n  &lt;description&gt;Kayardild Grammar (ISBN 3110127954)&lt;/description&gt;\\n  &lt;publisher&gt;Berlin - Mouton de Gruyter&lt;/publisher&gt;\\n  &lt;contributor xsi:type=\"olac:role\" olac:code=\"author\"&gt;Nicholas Evans&lt;/contributor&gt;\\n  &lt;format&gt;hardcover, 837 pages&lt;/format&gt;\\n  &lt;relation&gt;related to ISBN 0646119966&lt;/relation&gt;\\n  &lt;coverage&gt;Australia&lt;/coverage&gt;\\n  &lt;type xsi:type=\"olac:linguistic-type\" olac:code=\"language_description\"/&gt;\\n  &lt;type xsi:type=\"dcterms:DCMIType\"&gt;Text&lt;/type&gt;\\n&lt;/olac:olac&gt;\\n\\nParticipating language archives publish their catalogs in an XML\\nformat, and these records are regularly \"harvested\" by OLAC services\\nusing the OAI protocol.  In addition to this software infrastructure,\\nOLAC has documented a series of best practices for describing\\nlanguage resources, through a process that involved extended\\nconsultation with the language resources community (e.g. see\\nhttp://www.language-archives.org/REC/bpr.html).\\nOLAC repositories can be searched using a query engine on the OLAC website.\\nSearching for \"German lexicon\" finds the following resources, amongst others:\\n\\nCALLHOME German Lexicon\\nhttp://www.language-archives.org/item/oai:www.ldc.upenn.edu:LDC97L18\\nMULTILEX multilingual lexicon\\nhttp://www.language-archives.org/item/oai:elra.icp.inpg.fr:M0001\\nSlelex Siemens Phonetic lexicon\\nhttp://www.language-archives.org/item/oai:elra.icp.inpg.fr:S0048\\n\\nSearching for \"Korean\" finds a newswire corpus, a treebank, a lexicon,\\na child-language corpus, interlinear glossed texts.  It also finds software\\nincluding a syntactic analyzer and a morphological analyzer.\\nObserve that the above URLs include a substring of the form:\\noai:www.ldc.upenn.edu:LDC97L18.  This is an OAI identifier,\\nusing a URI scheme registered with ICANN\\n(the Internet Corporation for Assigned Names and Numbers).\\nThese identifiers have the format oai:archive:local_id,\\nwhere oai is the name of the URI scheme,\\narchive is an archive identifier such as www.ldc.upenn.edu,\\nand local_id is the resource identifier assigned by the archive, e.g. LDC97L18.\\nGiven an OAI identifier for an OLAC resource, it is possible to retrieve\\nthe complete XML record for the resource using a URL of the following form:\\nhttp://www.language-archives.org/static-records/oai:archive:local_id\\n\\n\\n\\n6.3&nbsp;&nbsp;&nbsp;Disseminating Language Resources\\nThe Linguistic Data Consortium hosts the NLTK Data Repository,\\nan open-access archive where community members can upload corpora and\\nsaved models.  These resources can be easily accessed using NLTK\\'s\\ndownloader tool.\\n\\n\\n\\n7&nbsp;&nbsp;&nbsp;Summary\\n\\nFundamental data types, present in most corpora, are annotated texts\\nand lexicons.  Texts have a temporal structure, while lexicons have a\\nrecord structure.\\nThe lifecycle of a corpus includes data collection, annotation, quality\\ncontrol, and publication.  The lifecycle continues after publication as\\nthe corpus is modified and enriched during the course of research.\\nCorpus development involves a balance between capturing a representative\\nsample of language usage, and capturing enough material from any one\\nsource or genre to be useful; multiplying out the dimensions of\\nvariability is usually not feasible because of resource limitations.\\nXML provides a useful format for the storage and interchange of linguistic\\ndata, but provides no shortcuts for solving pervasive data modeling problems.\\nToolbox format is widely used in language documentation projects; we can\\nwrite programs to support the curation of Toolbox files, and to convert\\nthem to XML.\\nThe Open Language Archives Community (OLAC) provides an infrastructure for\\ndocumenting and discovering language resources.\\n\\n\\n\\n8&nbsp;&nbsp;&nbsp;Further Reading\\nExtra materials for this chapter are posted at http://nltk.org/, including links to freely\\navailable resources on the web.\\nThe primary sources of linguistic corpora are the Linguistic Data Consortium and\\nthe European Language Resources Agency, both with extensive online catalogs.\\nMore details concerning the major corpora mentioned in the chapter are available:\\nAmerican National Corpus (Reppen, Ide, &amp; Suderman, 2005),\\nBritish National Corpus ({BNC}, 1999),\\nThesaurus Linguae Graecae ({TLG}, 1999),\\nChild Language Data Exchange System (CHILDES) (MacWhinney, 1995),\\nTIMIT (S., Lamel, &amp; William, 1986).\\nTwo special interest groups of the Association for Computational Linguistics\\nthat organize regular workshops with published proceedings are\\nSIGWAC, which promotes the use of the web as a corpus and has\\nsponsored the CLEANEVAL task for removing HTML markup,\\nand SIGANN, which is encouraging efforts towards interoperability\\nof linguistic annotations.\\nFull details of the Toolbox data format are provided with the distribution (Buseman, Buseman, &amp; Early, 1996),\\nand with the latest distribution, freely available from http://www.sil.org/computing/toolbox/.\\nFor guidelines on the process of constructing a Toolbox lexicon see\\nhttp://www.sil.org/computing/ddp/.\\nMore examples of our efforts with the Toolbox\\nare documented in (Tamanji, Hirotani, &amp; Hall, 1999), (Robinson, Aumann, &amp; Bird, 2007).\\nDozens of other tools for linguistic data management are available, some\\nsurveyed by (Bird &amp; Simons, 2003).\\nSee also the proceedings of the \"LaTeCH\" workshops on\\nlanguage technology for cultural heritage data.\\nThere are many excellent resources for XML (e.g. http://zvon.org/)\\nand for writing Python programs to work with XML.  Many editors have XML modes.\\nXML formats for lexical information include\\nOLIF http://www.olif.net/\\nand LIFT http://code.google.com/p/lift-standard/.\\nFor a survey of linguistic annotation software, see the\\nLinguistic Annotation Page at http://www.ldc.upenn.edu/annotation/.\\nThe initial proposal for standoff annotation was (Thompson &amp; McKelvie, 1997).\\nAn abstract data model for linguistic annotations, called\\n\"annotation graphs\", was proposed in (Bird &amp; Liberman, 2001).\\nA general-purpose ontology for linguistic description (GOLD) is\\ndocumented at http://www.linguistics-ontology.org/.\\nFor guidance on planning and constructing a corpus, see (Meyer, 2002), (Farghaly, 2003)\\nMore details of methods for scoring inter-annotator agreement are available\\nin (Artstein &amp; Poesio, 2008), (Pevzner &amp; Hearst, 2002).\\nRotokas data was provided by Stuart Robinson, and Iu Mien data was provided by Greg Aumann.\\nFor more information about the Open Language Archives Community, visit\\nhttp://www.language-archives.org/, or see (Simons &amp; Bird, 2003).\\n\\n\\n9&nbsp;&nbsp;&nbsp;Exercises\\n\\n◑ In 5.1 the new field appeared at the bottom of the\\nentry.  Modify this program so that it inserts the new subelement right after\\nthe lx field.  (Hint: create the new cv field using Element(\\'cv\\'),\\nassign a text value to it, then use the insert() method of the parent element.)\\n\\n◑ Write a function that deletes a specified field from a lexical entry.\\n(We could use this to sanitize our lexical data before giving it to others,\\ne.g. by removing fields containing irrelevant or uncertain content.)\\n\\n◑ Write a program that scans an HTML dictionary file to find entries\\nhaving an illegal part-of-speech field, and reports the headword for\\neach entry.\\n\\n◑ Write a program to find any parts of speech (ps field) that\\noccurred less than ten times.  Perhaps these are typing mistakes?\\n\\n◑ We saw a method for discovering cases of whole-word reduplication.\\nWrite a function to find words that may contain partial\\nreduplication.  Use the re.search() method, and the following\\nregular expression: (..+)\\\\1\\n\\n◑ We saw a method for adding a cv field.  There is an interesting\\nissue with keeping this up-to-date when someone modifies the content\\nof the lx field on which it is based.  Write a version of this\\nprogram to add a cv field, replacing any existing cv field.\\n\\n◑ Write a function to add a new field syl which gives a count of\\nthe number of syllables in the word.\\n\\n◑ Write a function which displays the complete entry for a lexeme.\\nWhen the lexeme is incorrectly spelled it should display the entry\\nfor the most similarly spelled lexeme.\\n\\n◑ Write a function that takes a lexicon and finds which pairs of\\nconsecutive fields are most frequent (e.g. ps is often followed by pt).\\n(This might help us to discover some of the structure of a lexical entry.)\\n\\n◑ Create a spreadsheet using office software, containing one lexical entry per\\nrow, consisting of a headword, a part of speech, and a gloss.  Save the\\nspreadsheet in CSV format.  Write Python code to read the CSV file and print it in\\nToolbox format, using lx for the headword, ps for the part of speech,\\nand gl for the gloss.\\n\\n◑ Index the words of Shakespeare\\'s plays, with the help of nltk.Index.\\nThe resulting data structure should permit lookup on individual words such as music,\\nreturning a list of references to acts, scenes and speeches, of the form\\n[(3, 2, 9), (5, 1, 23), ...], where (3, 2, 9) indicates\\nAct 3 Scene 2 Speech 9.\\n\\n◑ Construct a conditional frequency distribution which records the word length\\nfor each speech in The Merchant of Venice, conditioned on the name of the character,\\ne.g. cfd[\\'PORTIA\\'][12] would give us the number of speeches by Portia\\nconsisting of 12 words.\\n\\n★ Obtain a comparative wordlist in CSV format, and write a program\\nthat prints those cognates having an edit-distance of at least three\\nfrom each other.\\n\\n★ Build an index of those lexemes which appear in example sentences.\\nSuppose the lexeme for a given entry is w.\\nThen add a single cross-reference field xrf to this entry, referencing\\nthe headwords of other entries having example sentences containing\\nw.  Do this for all entries and save the result as a toolbox-format file.\\n\\n◑ Write a recursive function to produce an XML representation for a\\ntree, with non-terminals represented as XML elements, and leaves represented\\nas text content, e.g.:\\n&lt;S&gt;\\n  &lt;NP type=\"SBJ\"&gt;\\n    &lt;NP&gt;\\n      &lt;NNP&gt;Pierre&lt;/NNP&gt;\\n      &lt;NNP&gt;Vinken&lt;/NNP&gt;\\n    &lt;/NP&gt;\\n    &lt;COMMA&gt;,&lt;/COMMA&gt;\\n\\n\\n\\n\\n\\nAbout this document...\\nUPDATED FOR NLTK 3.0.\\nThis is a chapter from Natural Language Processing with Python,\\nby Steven Bird, Ewan Klein and Edward Loper,\\nCopyright © 2019 the authors.\\nIt is distributed with the Natural Language Toolkit [http://nltk.org/],\\nVersion 3.0, under the terms of the\\nCreative Commons Attribution-Noncommercial-No Derivative Works 3.0 United States License\\n[http://creativecommons.org/licenses/by-nc-nd/3.0/us/].\\nThis document was built on\\nWed  4 Sep 2019 11:40:48 ACST\\n\\n\\n\\n\\n\\n',\n",
       " '\\n\\n\\n\\nfunction astext(node)\\n{\\n    return node.innerHTML.replace(/(]+)>)/ig,\"\")\\n                         .replace(/&gt;/ig, \">\")\\n                         .replace(/&lt;/ig, \"<\")\\n                         .replace(/&quot;/ig, \\'\"\\')\\n                         .replace(/&amp;/ig, \"&\");\\n}\\n\\nfunction copy_notify(node, bar_color, data)\\n{\\n    // The outer box: relative + inline positioning.\\n    var box1 = document.createElement(\"div\");\\n    box1.style.position = \"relative\";\\n    box1.style.display = \"inline\";\\n    box1.style.top = \"2em\";\\n    box1.style.left = \"1em\";\\n  \\n    // A shadow for fun\\n    var shadow = document.createElement(\"div\");\\n    shadow.style.position = \"absolute\";\\n    shadow.style.left = \"-1.3em\";\\n    shadow.style.top = \"-1.3em\";\\n    shadow.style.background = \"#404040\";\\n    \\n    // The inner box: absolute positioning.\\n    var box2 = document.createElement(\"div\");\\n    box2.style.position = \"relative\";\\n    box2.style.border = \"1px solid #a0a0a0\";\\n    box2.style.left = \"-.2em\";\\n    box2.style.top = \"-.2em\";\\n    box2.style.background = \"white\";\\n    box2.style.padding = \".3em .4em .3em .4em\";\\n    box2.style.fontStyle = \"normal\";\\n    box2.style.background = \"#f0e0e0\";\\n\\n    node.insertBefore(box1, node.childNodes.item(0));\\n    box1.appendChild(shadow);\\n    shadow.appendChild(box2);\\n    box2.innerHTML=\"Copied&nbsp;to&nbsp;the&nbsp;clipboard: \" +\\n                   \"\"+\\n                   data+\"\";\\n    setTimeout(function() { node.removeChild(box1); }, 1000);\\n\\n    var elt = node.parentNode.firstChild;\\n    elt.style.background = \"#ffc0c0\";\\n    setTimeout(function() { elt.style.background = bar_color; }, 200);\\n}\\n\\nfunction copy_codeblock_to_clipboard(node)\\n{\\n    var data = astext(node)+\"\\\\n\";\\n    if (copy_text_to_clipboard(data)) {\\n        copy_notify(node, \"#40a060\", data);\\n    }\\n}\\n\\nfunction copy_doctest_to_clipboard(node)\\n{\\n    var s = astext(node)+\"\\\\n   \";\\n    var data = \"\";\\n\\n    var start = 0;\\n    var end = s.indexOf(\"\\\\n\");\\n    while (end >= 0) {\\n        if (s.substring(start, start+4) == \">>> \") {\\n            data += s.substring(start+4, end+1);\\n        }\\n        else if (s.substring(start, start+4) == \"... \") {\\n            data += s.substring(start+4, end+1);\\n        }\\n        /*\\n        else if (end-start > 1) {\\n            data += \"# \" + s.substring(start, end+1);\\n        }*/\\n        // Grab the next line.\\n        start = end+1;\\n        end = s.indexOf(\"\\\\n\", start);\\n    }\\n    \\n    if (copy_text_to_clipboard(data)) {\\n        copy_notify(node, \"#4060a0\", data);\\n    }\\n}\\n    \\nfunction copy_text_to_clipboard(data)\\n{\\n    if (window.clipboardData) {\\n        window.clipboardData.setData(\"Text\", data);\\n        return true;\\n     }\\n    else if (window.netscape) {\\n        // w/ default firefox settings, permission will be denied for this:\\n        netscape.security.PrivilegeManager\\n                      .enablePrivilege(\"UniversalXPConnect\");\\n    \\n        var clip = Components.classes[\"@mozilla.org/widget/clipboard;1\"]\\n                      .createInstance(Components.interfaces.nsIClipboard);\\n        if (!clip) return;\\n    \\n        var trans = Components.classes[\"@mozilla.org/widget/transferable;1\"]\\n                       .createInstance(Components.interfaces.nsITransferable);\\n        if (!trans) return;\\n    \\n        trans.addDataFlavor(\"text/unicode\");\\n    \\n        var str = new Object();\\n        var len = new Object();\\n    \\n        var str = Components.classes[\"@mozilla.org/supports-string;1\"]\\n                     .createInstance(Components.interfaces.nsISupportsString);\\n        var datacopy=data;\\n        str.data=datacopy;\\n        trans.setTransferData(\"text/unicode\",str,datacopy.length*2);\\n        var clipid=Components.interfaces.nsIClipboard;\\n    \\n        if (!clip) return false;\\n    \\n        clip.setData(trans,null,clipid.kGlobalClipboard);\\n        return true;\\n    }\\n    return false;\\n}\\n//-->\\n\\n\\n\\n\\n\\n\\nAfterword: The Language Challenge\\n\\n\\n/*\\n:Author: Edward Loper, James Curran\\n:Copyright: This stylesheet has been placed in the public domain.\\n\\nStylesheet for use with Docutils.\\n\\nThis stylesheet defines new css classes used by NLTK.\\n\\nIt uses a Python syntax highlighting scheme that matches\\nthe colour scheme used by IDLE, which makes it easier for\\nbeginners to check they are typing things in correctly.\\n*/\\n\\n/* Include the standard docutils stylesheet. */\\n@import url(default.css);\\n\\n/* Custom inline roles */\\nspan.placeholder    { font-style: italic; font-family: monospace; }\\nspan.example        { font-style: italic; }\\nspan.emphasis       { font-style: italic; }\\nspan.termdef        { font-weight: bold; }\\n/*span.term           { font-style: italic; }*/\\nspan.category       { font-variant: small-caps; }\\nspan.feature        { font-variant: small-caps; }\\nspan.fval           { font-style: italic; }\\nspan.math           { font-style: italic; }\\nspan.mathit         { font-style: italic; }\\nspan.lex            { font-variant: small-caps; }\\nspan.guide-linecount{ text-align: right; display: block;}\\n\\n/* Python souce code listings */\\nspan.pysrc-prompt   { color: #9b0000; }\\nspan.pysrc-more     { color: #9b00ff; }\\nspan.pysrc-keyword  { color: #e06000; }\\nspan.pysrc-builtin  { color: #940094; }\\nspan.pysrc-string   { color: #00aa00; }\\nspan.pysrc-comment  { color: #ff0000; }\\nspan.pysrc-output   { color: #0000ff; }\\nspan.pysrc-except   { color: #ff0000; }\\nspan.pysrc-defname  { color: #008080; }\\n\\n\\n/* Doctest blocks */\\npre.doctest         { margin: 0; padding: 0; font-weight: bold; }\\ndiv.doctest         { margin: 0 1em 1em 1em; padding: 0; }\\ntable.doctest       { margin: 0; padding: 0;\\n                      border-top: 1px solid gray;\\n                      border-bottom: 1px solid gray; }\\npre.copy-notify     { margin: 0; padding: 0.2em; font-weight: bold;\\n                      background-color: #ffffff; }\\n\\n/* Python source listings */\\ndiv.pylisting       { margin: 0 1em 1em 1em; padding: 0; }\\ntable.pylisting     { margin: 0; padding: 0;\\n                      border-top: 1px solid gray; }\\ntd.caption { border-top: 1px solid black; margin: 0; padding: 0; }\\n.caption-label { font-weight: bold;  }\\ntd.caption p { margin: 0; padding: 0; font-style: normal;}\\n\\ntable tr td.codeblock { \\n  padding: 0.2em ! important; margin: 0;\\n  border-left: 1px solid gray;\\n  border-right: 2px solid gray;\\n  border-top: 0px solid gray;\\n  border-bottom: 1px solid gray;\\n  font-weight: bold; background-color: #eeffee;\\n}\\ntable pre span {\\n  white-space: pre-wrap;\\n}\\ntable tr td.doctest  { \\n  padding: 0.2em; margin: 0;\\n  border-left: 1px solid gray;\\n  border-right: 2px solid gray;\\n  border-top: 0px solid gray;\\n  border-bottom: 1px solid gray;\\n  font-weight: bold; background-color: #eeeeff;\\n}\\n\\ntd.codeblock table tr td.copybar {\\n    background: #40a060; border: 1px solid gray;\\n    font-family: monospace; padding: 0; margin: 0; }\\ntd.doctest table tr td.copybar {\\n    background: #4060a0; border: 1px solid gray;\\n    font-family: monospace; padding: 0; margin: 0; }\\n\\ntd.pysrc { padding-left: 0.5em; }\\n\\nimg.callout { border-width: 0px; }\\n\\ntable.docutils {\\n    border-style: solid;\\n    border-width: 1px;\\n    margin-top: 6px;\\n    border-color: grey;\\n    border-collapse: collapse; }\\n\\ntable.docutils th {\\n    border-style: none;\\n    border-width: 1px;\\n    border-color: grey;\\n    padding: 0 .5em 0 .5em; }\\n\\ntable.docutils td {\\n    border-style: none;\\n    border-width: 1px;\\n    border-color: grey; \\n    padding: 0 .5em 0 .5em; }\\n\\ntable.footnote td { padding: 0; }\\ntable.footnote { border-width: 0; }\\ntable.footnote td { border-width: 0; }\\ntable.footnote th { border-width: 0; }\\n\\ntable.noborder { border-width: 0; }\\n\\ntable.example pre { margin-top: 4px; margin-bottom: 0; }\\n\\n/* For figures & tables */\\np.caption { margin-bottom: 0; }\\ndiv.figure { text-align: center; }\\n\\n/* The index */\\ndiv.index { border: 1px solid black;\\n            background-color: #eeeeee; }\\ndiv.index h1 { padding-left: 0.5em; margin-top: 0.5ex;\\n               border-bottom: 1px solid black; }\\nul.index { margin-left: 0.5em; padding-left: 0; }\\nli.index { list-style-type: none; }\\np.index-heading { font-size: 120%; font-style: italic; margin: 0; }\\nli.index ul { margin-left: 2em; padding-left: 0; }\\n\\n/* \\'Note\\' callouts */\\ndiv.note\\n{\\n  border-right:   #87ceeb 1px solid;\\n  padding-right: 4px;\\n  border-top: #87ceeb 1px solid;\\n  padding-left: 4px;\\n  padding-bottom: 4px;\\n  margin: 2px 5% 10px;\\n  border-left: #87ceeb 1px solid;\\n  padding-top: 4px;\\n  border-bottom: #87ceeb 1px solid;\\n  font-style: normal;\\n  font-family: verdana, arial;\\n  background-color: #b0c4de;\\n}\\n\\ntable.avm { border: 0px solid black; width: 0; }\\ntable.avm tbody tr {border: 0px solid black; }\\ntable.avm tbody tr td { padding: 2px; }\\ntable.avm tbody tr td.avm-key { padding: 5px; font-variant: small-caps; }\\ntable.avm tbody tr td.avm-eq { padding: 5px; }\\ntable.avm tbody tr td.avm-val { padding: 5px; font-style: italic; }\\np.avm-empty { font-style: normal; }\\ntable.avm colgroup col { border: 0px solid black; }\\ntable.avm tbody tr td.avm-topleft \\n    { border-left: 2px solid #000080; border-top: 2px solid #000080; }\\ntable.avm tbody tr td.avm-botleft \\n    { border-left: 2px solid #000080; border-bottom: 2px solid #000080; }\\ntable.avm tbody tr td.avm-topright\\n    { border-right: 2px solid #000080; border-top: 2px solid #000080; }\\ntable.avm tbody tr td.avm-botright\\n    { border-right: 2px solid #000080; border-bottom: 2px solid #000080; }\\ntable.avm tbody tr td.avm-left\\n    { border-left: 2px solid #000080; }\\ntable.avm tbody tr td.avm-right\\n    { border-right: 2px solid #000080; }\\ntable.avm tbody tr td.avm-topbotleft\\n    { border: 2px solid #000080; border-right: 0px solid black; }\\ntable.avm tbody tr td.avm-topbotright\\n    { border: 2px solid #000080; border-left: 0px solid black; }\\ntable.avm tbody tr td.avm-ident\\n    { font-size: 80%; padding: 0; padding-left: 2px; vertical-align: top; }\\n.avm-pointer\\n{ border: 1px solid #008000; padding: 1px; color: #008000; \\n  background: #c0ffc0; font-style: normal; }\\n\\ntable.gloss { border: 0px solid black; width: 0; }\\ntable.gloss tbody tr { border: 0px solid black; }\\ntable.gloss tbody tr td { border: 0px solid black; }\\ntable.gloss colgroup col { border: 0px solid black; }\\ntable.gloss p { margin: 0; padding: 0; }\\n\\ntable.rst-example { border: 1px solid black; }\\ntable.rst-example tbody tr td { background: #eeeeee; }\\ntable.rst-example thead tr th { background: #c0ffff; }\\ntd.rst-raw { width: 0; }\\n\\n/* Used by nltk.org/doc/test: */\\ndiv.doctest-list { text-align: center; }\\ntable.doctest-list { border: 1px solid black;\\n  margin-left: auto; margin-right: auto;\\n}\\ntable.doctest-list tbody tr td { background: #eeeeee;\\n  border: 1px solid #cccccc; text-align: left; }\\ntable.doctest-list thead tr th { background: #304050; color: #ffffff;\\n  border: 1px solid #000000;}\\ntable.doctest-list thead tr a { color: #ffffff; }\\nspan.doctest-passed { color: #008000; }\\nspan.doctest-failed { color: #800000; }\\n\\n\\n\\n\\n\\nAfterword: The Language Challenge\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n<!-- Grammatical Category - e.g. NP and verb as technical terms\\n.. role:: gc\\n   :class: category -->\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n<!-- TODO: mention sentiment classification as an example of a trivial vs hard problem\\n(trivial with semantic orientation of adjectives; hard with interesting examples\\nof where reviews easily mislead this approach and motivate deeper analysis, e.g.\\nhttp://www.aclweb.org/anthology/W04-3253 -->\\nNatural language throws up some interesting computational challenges.\\nWe\\'ve explored many of these in the preceding chapters, including\\ntokenization, tagging, classification, information extraction,\\nand building syntactic and semantic representations.\\nYou should now be equipped to work with large datasets, to create\\nrobust models of linguistic phenomena, and to extend them into\\ncomponents for practical language technologies.  We hope that\\nthe Natural Language Toolkit (NLTK) has served to open up the exciting\\nendeavor of practical natural language processing to a broader\\naudience than before.\\nIn spite of all that has come before, language presents us with\\nfar more than a temporary challenge for computation.  Consider the following\\nsentences which attest to the riches of language:\\n\\n  (1)\\n  a.Overhead the day drives level and grey, hiding the sun by a flight of grey spears.  (William Faulkner, As I Lay Dying, 1935)\\n\\n  b.When using the toaster please ensure that the exhaust fan is turned on. (sign in dormitory kitchen)\\n\\n  c.Amiodarone weakly inhibited CYP2C9, CYP2D6, and CYP3A4-mediated activities with Ki values of 45.1-271.6 μM (Medline, PMID: 10718780)\\n\\n  d.Iraqi Head Seeks Arms (spoof news headline)\\n\\n  e.The earnest prayer of a righteous man has great power and wonderful results. (James 5:16b)\\n\\n  f.Twas brillig, and the slithy toves did gyre and gimble in the wabe (Lewis Carroll, Jabberwocky, 1872)\\n\\n  g.There are two ways to do this, AFAIK :smile:  (internet discussion archive)\\n\\nOther evidence for the riches of language is the vast array of disciplines\\nwhose work centers on language.  Some obvious disciplines include\\ntranslation, literary criticism, philosophy, anthropology and psychology.\\nMany less obvious disciplines investigate language use, including\\nlaw, hermeneutics, forensics, telephony, pedagogy, archaeology, cryptanalysis and speech\\npathology.  Each applies distinct methodologies to gather\\nobservations, develop theories and test hypotheses.  All serve to\\ndeepen our understanding of language and of the intellect that is\\nmanifested in language.\\nIn view of the complexity of language and the broad range of interest\\nin studying it from different angles, it\\'s clear that we have barely\\nscratched the surface here.  Additionally, within NLP itself,\\nthere are many important methods and applications that we haven\\'t\\nmentioned.\\nIn our closing remarks we will take a broader view of NLP,\\nincluding its foundations and the further directions you might\\nwant to explore.  Some of the topics are not well-supported by NLTK,\\nand you might like to rectify that problem by contributing new software\\nand data to the toolkit.\\n\\nLanguage Processing vs Symbol Processing\\nThe very notion that natural language could be treated in a\\ncomputational manner grew out of a research program, dating back to\\nthe early 1900s, to reconstruct mathematical reasoning using logic,\\nmost clearly manifested in work by Frege, Russell, Wittgenstein,\\nTarski, Lambek and Carnap.  This work led to the notion of language as\\na formal system amenable to automatic processing.  Three later\\ndevelopments laid the foundation for natural language processing.  The\\nfirst was formal language theory.  This defined a language as a set\\nof strings accepted by a class of automata, such as context-free\\nlanguages and pushdown automata, and provided the underpinnings for\\ncomputational syntax.\\nThe second development was symbolic logic. This provided a\\nformal method for capturing selected aspects of natural language that\\nare relevant for expressing logical proofs. A formal calculus in\\nsymbolic logic provides the syntax of a language, together with rules\\nof inference and, possibly, rules of interpretation in a set-theoretic\\nmodel; examples are propositional logic and First Order Logic.  Given\\nsuch a calculus, with a well-defined syntax and semantics, it becomes\\npossible to associate meanings with expressions of natural language by\\ntranslating them into expressions of the formal calculus. For example,\\nif we translate John saw Mary into a formula saw(j,m), we\\n(implicitly or explicitly) intepret the English verb saw as a\\nbinary relation, and John and Mary as denoting\\nindividuals.  More general statements like All birds fly require\\nquantifiers, in this case ∀, meaning for all: ∀x (bird(x) → fly(x)).  This use of logic provided\\nthe technical machinery to perform inferences that are an important\\npart of language understanding.\\nA closely related development was the principle of\\ncompositionality, namely that the meaning of a complex expression\\nis composed from the meaning of its parts and their mode of\\ncombination (10.).\\nThis principle provided a useful correspondence between\\nsyntax and semantics, namely that the meaning of a complex expression\\ncould be computed recursively.  Consider the sentence It is not true\\nthat p, where p is a proposition.  We can\\nrepresent the meaning of this sentence as not(p).  Similarly, we\\ncan represent the meaning of John saw Mary as saw(j, m).  Now we\\ncan compute the interpretation of It is not true that John saw Mary\\nrecursively, using the above information, to get\\nnot(saw(j,m)).\\nThe approaches just outlined share the premise that computing with\\nnatural language crucially relies on rules for manipulating symbolic\\nrepresentations. For a certain period in the development of NLP,\\nparticularly during the 1980s, this premise provided a common starting\\npoint for both linguists and practitioners of NLP, leading to a family\\nof grammar formalisms known as unification-based (or feature-based)\\ngrammar (cf. 9.), and to NLP applications implemented in the Prolog\\nprogramming language. Although grammar-based NLP is still a\\nsignificant area of research, it has become somewhat eclipsed in the\\nlast 15–20 years due to a variety of factors. One\\nsignificant influence came from automatic speech recognition. Although\\nearly work in speech processing adopted a model that emulated the\\nkind of rule-based phonological  processing typified\\nby the Sound Pattern of English (Chomsky &amp; Halle, 1968),\\nthis turned out to be hopelessly inadequate in dealing\\nwith the hard problem of recognizing actual speech in anything like\\nreal time. By contrast, systems which involved learning patterns from\\nlarge bodies of speech data were significantly more accurate,\\nefficient and robust. In addition, the speech community found that\\nprogress in building better systems was hugely assisted by the\\nconstruction of shared resources for quantitatively measuring\\nperformance against common test data. Eventually, much of the NLP\\ncommunity embraced a data intensive orientation to language\\nprocessing, coupled with a growing use of machine-learning techniques\\nand evaluation-led methodology.\\n\\n\\nContemporary Philosophical Divides\\nThe contrasting approaches to NLP described in the preceding section\\nrelate back to early metaphysical debates about rationalism\\nversus empiricism and realism versus idealism that\\noccurred in the Enlightenment period of Western philosophy.  These\\ndebates took place against a backdrop of orthodox thinking in which\\nthe source of all knowledge was believed to be divine revelation.\\nDuring this period of the seventeenth and eighteenth centuries,\\nphilosophers argued that human reason or sensory experience has\\npriority over revelation.  Descartes and Leibniz, amongst others, took\\nthe rationalist position, asserting that all truth has its origins in\\nhuman thought, and in the existence of \"innate ideas\" implanted in our\\nminds from birth.  For example, they argued that the principles of\\nEuclidean geometry were developed using human reason, and were not the\\nresult of supernatural revelation or sensory experience.  In contrast,\\nLocke and others took the empiricist view, that our primary source of\\nknowledge is the experience of our faculties, and that human reason\\nplays a secondary role in reflecting on that experience.  Often-cited\\nevidence for this position was Galileo\\'s discovery — based on\\ncareful observation of the motion of the planets — that the\\nsolar system is heliocentric and not geocentric.  In the context of\\nlinguistics, this debate leads to the following question: to what\\nextent does human linguistic experience, versus our innate \"language\\nfaculty\", provide the basis for our knowledge of language?  In NLP\\nthis issue surfaces in debates about the priority of corpus data\\nversus linguistic introspection in the construction of computational models.\\nA further concern, enshrined in the debate between realism and\\nidealism, was the metaphysical status of the constructs of a theory.\\nKant argued for a distinction between phenomena, the manifestations we\\ncan experience, and \"things in themselves\" which can never be\\nknown directly.  A linguistic realist would take a theoretical\\nconstruct like noun phrase to be a real world entity that exists\\nindependently of human perception and reason, and which actually\\ncauses the observed linguistic phenomena.  A linguistic idealist, on\\nthe other hand, would argue that noun phrases, along with more\\nabstract constructs like semantic representations, are intrinsically\\nunobservable, and simply play the role of useful fictions.  The way\\nlinguists write about theories often betrays a realist position, while\\nNLP practitioners occupy neutral territory or else lean towards the\\nidealist position.  Thus, in NLP, it is often enough if a theoretical\\nabstraction leads to a useful result; it does not matter whether this\\nresult sheds any light on human linguistic processing.\\nThese issues are still alive today, and show up in the distinctions\\nbetween symbolic vs statistical methods, deep vs shallow processing,\\nbinary vs gradient classifications, and scientific vs engineering\\ngoals.  However, such contrasts are now highly nuanced, and the debate\\nis no longer as polarized as it once was.  In fact, most of the\\ndiscussions — and most of the advances even — involve a\\n\"balancing act.\"  For example, one intermediate position is to assume\\nthat humans are innately endowed with analogical and memory-based\\nlearning methods (weak rationalism), and to use these methods to identify\\nmeaningful patterns in their sensory language experience (empiricism).\\nWe have seen many examples of this methodology throughout this book.\\nStatistical methods inform symbolic models any time corpus statistics\\nguide the selection of productions in a context free grammar,\\ni.e. \"grammar engineering.\"\\nSymbolic methods inform statistical models any time a corpus that\\nwas created using rule-based methods is used as a source of features\\nfor training a statistical language model,\\ni.e. \"grammatical inference.\"  The circle is closed.\\n\\n\\nNLTK Roadmap\\nThe Natural Language Toolkit is work-in-progress, and is being\\ncontinually expanded as people contribute code.  Some areas of\\nNLP and linguistics are not (yet) well supported in NLTK,\\nand contributions in these areas are especially welcome.\\nCheck http://nltk.org/ for news about developments after the publication\\ndate of the book.\\n\\n\\n\\n\\nPhonology and Morphology:\\n&nbsp;Computational approaches to the study of sound patterns and word structures\\ntypically use a finite state toolkit.  Phenomena such as suppletion and\\nnon-concatenative morphology are difficult to address using the string\\nprocessing methods we have been studying.  The technical challenge is\\nnot only to link NLTK to a high-performance finite state toolkit, but\\nto avoid duplication of lexical data and to link the morphosyntactic\\nfeatures needed by morph analyzers and syntactic parsers.\\n\\nHigh-Performance Components:\\n&nbsp;Some NLP tasks are too computationally intensive for pure Python\\nimplementations to be feasible.  However, in some cases the expense\\nonly arises when training models, not when using them to label inputs.\\nNLTK\\'s package system provides a convenient way to distribute\\ntrained models, even models trained using corpora that cannot be\\nfreely distributed.  Alternatives are to develop Python interfaces\\nto high-performance machine learning tools, or to expand the\\nreach of Python by using parallel programming techniques like MapReduce.\\n\\nLexical Semantics:\\n&nbsp;This is a vibrant area of current research, encompassing\\ninheritance models of the lexicon, ontologies, multiword expressions,\\netc, mostly outside the scope of NLTK as it stands.\\nA conservative goal would be to access lexical information\\nfrom rich external stores in support of tasks in\\nword sense disambiguation, parsing, and semantic interpretation.\\n\\nNatural Language Generation:\\n&nbsp;Producing coherent text from underlying representations of meaning\\nis an important part of NLP; a unification based approach to\\nNLG has been developed in NLTK, and there is scope for more\\ncontributions in this area.\\n\\nLinguistic Fieldwork:\\n&nbsp;A major challenge faced by linguists is to document thousands of\\nendangered languages, work which generates heterogeneous and\\nrapidly evolving data in large quantities.  More fieldwork\\ndata formats, including interlinear text formats and lexicon\\ninterchange formats, could be supported in NLTK,\\nhelping linguists to curate and analyze this data,\\nwhile liberating them to spend as much time as possible\\non data elicitation.\\n\\nOther Languages:\\n&nbsp;Improved support for NLP in languages other than English could\\ninvolve work in two areas: obtaining permission to distribute\\nmore corpora with NLTK\\'s data collection;\\nwriting language-specific HOWTOs for posting at http://nltk.org/howto,\\nillustrating the use of NLTK and discussing language-specific\\nproblems for NLP including character encodings, word segmentation,\\nand morphology.\\nNLP researchers with expertise in a particular language\\ncould arrange to translate this book and host a copy on the\\nNLTK website; this would go beyond translating the discussions\\nto providing equivalent worked examples using data in the\\ntarget language, a non-trivial undertaking.\\n\\nNLTK-Contrib:Many of NLTK\\'s core components were contributed by members of the\\nNLP community, and were initially housed in NLTK\\'s \"Contrib\"\\npackage, nltk_contrib.  The only requirement for software to\\nbe added to this package is that it must be written in Python,\\nrelevant to NLP, and given the same open source license as the\\nrest of NLTK.  Imperfect software is welcome, and will probably\\nbe improved over time by other members of the NLP community.\\n\\nTeaching Materials:\\n&nbsp;Since the earliest days of NLTK development, teaching materials have\\naccompanied the software, materials that have gradually expanded to\\nfill this book, plus a substantial quantity of online materials as well.\\nWe hope that instructors who supplement these materials with presentation slides,\\nproblem sets, solution sets, and more detailed treatments of the\\ntopics we have covered, will make them available, and will notify\\nthe authors so we can link them from http://nltk.org/.\\nOf particular value are materials that help NLP become a mainstream course in\\nthe undergraduate programs of computer science and linguistics departments,\\nor that make NLP accessible at the secondary level where there is\\nsignificant scope for including computational content in the language, literature,\\ncomputer science, and information technology curricula.\\n\\nOnly a Toolkit:As stated in the preface, NLTK is a toolkit, not a system.\\nMany problems will be tackled with a combination of NLTK,\\nPython, other Python libraries, and interfaces to external\\nNLP tools and formats.\\n\\n\\n\\n\\n\\nEnvoi...\\nLinguists are sometimes asked how many languages they speak,\\nand have to explain that this field actually concerns the\\nstudy of abstract structures that are shared by languages,\\na study which is more profound and elusive than learning to speak\\nas many languages as possible.\\nSimilarly, computer scientists are sometimes asked how many\\nprogramming languages they know, and have to explain that computer science\\nactually concerns the study of data structures and algorithms that can be\\nimplemented in any programming language,\\na study which is more profound and elusive than striving for\\nfluency in as many programming languages as possible.\\nThis book has covered many topics in the field of Natural Language Processing.\\nMost of the examples have used Python and English.\\nHowever, it would be unfortunate if readers concluded that\\nNLP is about how to write Python programs to manipulate English text,\\nor more broadly, about how to write programs (in any programming language)\\nto manipulate text (in any natural language).\\nOur selection of Python and English was expedient, nothing more.\\nEven our focus on programming itself was only a means to an end:\\nas a way to understand data structures and algorithms\\nfor representing and manipulating collections of linguistically annotated text,\\nas a way to build new language technologies to better serve\\nthe needs of the information society,\\nand ultimately as a pathway into deeper understanding of the vast riches\\nof human language.\\nBut for the present: happy hacking!\\n\\n\\nAbout this document...\\nUPDATED FOR NLTK 3.0.\\nThis is a chapter from Natural Language Processing with Python,\\nby Steven Bird, Ewan Klein and Edward Loper,\\nCopyright © 2019 the authors.\\nIt is distributed with the Natural Language Toolkit [http://nltk.org/],\\nVersion 3.0, under the terms of the\\nCreative Commons Attribution-Noncommercial-No Derivative Works 3.0 United States License\\n[http://creativecommons.org/licenses/by-nc-nd/3.0/us/].\\nThis document was built on\\nWed  4 Sep 2019 11:40:48 ACST\\n\\n\\n\\n\\n\\n']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10500"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(codebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10500"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectors[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### コードブック生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "codebook =  ['function', 'astext', '(', 'node', ')', '{', 'return', '.', 'innerhtml', 'replace', '(/(]+)>)/', 'ig', ',\"\")', '(/&', 'gt', ';/', ',', '\">\")', 'lt', '\"<\")', 'quot', '\\'\"\\')', 'amp', '\"&\");', '}', 'copy_notify', 'bar_color', 'data', '//', 'the', 'outer', 'box', ':', 'relative', '+', 'inline', 'positioning', 'var', 'box1', '=', 'document', 'createelement', '(\"', 'div', '\");', 'style', 'position', '\"', '\";', 'display', 'top', '2em', 'left', '1em', 'a', 'shadow', 'for', 'fun', 'absolute', '\"-', '1', '3em', 'background', '\"#', '404040', 'inner', 'box2', 'border', '1px', 'solid', '#', 'a0a0a0', '\"-.', 'white', 'padding', '\".', '4em', 'fontstyle', 'normal', 'f0e0e0', 'insertbefore', 'childnodes', 'item', '0', '));', 'appendchild', ');', '=\"', 'copied', '&', 'nbsp', ';', 'to', 'clipboard', '\"\"+', '+\"\";', 'settimeout', '()', 'removechild', '},', '1000', 'elt', 'parentnode', 'firstchild', 'ffc0c0', '200', 'copy_codeblock_to_clipboard', ')+\"\\\\', 'n', 'if', 'copy_text_to_clipboard', '))', '40a060', '\",', 'copy_doctest_to_clipboard', 's', '\"\";', 'start', 'end', 'indexof', '(\"\\\\', 'while', '>=', 'substring', '4', '==', '\">>>', '\")', '+=', 'else', '\"...', '/*', '-', '>', '}*/', 'grab', 'next', 'line', '4060a0', 'window', 'clipboarddata', 'setdata', 'text', 'true', 'netscape', 'w', '/', 'default', 'firefox', 'setting', 'permission', 'will', 'be', 'denied', 'this', 'security', 'privilegemanager', 'enableprivilege', 'universalxpconnect', 'clip', 'component', 'class', '[\"@', 'mozilla', 'org', 'widget', '\"]', 'createinstance', 'interface', 'nsiclipboard', '(!', 'trans', 'transferable', 'nsitransferable', 'adddataflavor', 'unicode', 'str', 'new', 'object', '();', 'len', 'support', 'string', 'nsisupportsstring', 'datacopy', 'settransferdata', 'length', '*', '2', 'clipid', 'false', 'null', 'kglobalclipboard', '//-->', 'preface', 'author', 'edward', 'loper', 'james', 'curran', 'copyright', 'stylesheet', 'ha', 'been', 'placed', 'in', 'public', 'domain', 'use', 'with', 'docutils', 'defines', 'cs', 'used', 'by', 'nltk', 'it', 'us', 'python', 'syntax', 'highlighting', 'scheme', 'that', 'match', 'colour', 'idle', 'which', 'make', 'easier', 'beginner', 'check', 'they', 'are', 'typing', 'thing', 'correctly', '*/', 'include', 'standard', '@', 'import', 'url', 'custom', 'role', 'span', 'placeholder', 'font', 'italic', 'family', 'monospace', 'example', 'emphasis', 'termdef', 'weight', 'bold', 'term', 'category', 'variant', 'small', 'cap', 'feature', 'fval', 'math', 'mathit', 'lex', 'guide', 'linecount', 'align', 'right', 'block', ';}', 'souce', 'code', 'listing', 'pysrc', 'prompt', 'color', '9b0000', 'more', '9b00ff', 'keyword', 'e06000', 'builtin', '940094', '00aa00', 'comment', 'ff0000', 'output', '0000ff', 'except', 'defname', '008080', 'doctest', 'pre', 'margin', 'table', 'gray', 'bottom', 'copy', 'notify', 'ffffff', 'source', 'pylisting', 'td', 'caption', 'black', 'label', 'p', 'tr', 'codeblock', '!', 'important', '2px', '0px', 'eeffee', 'space', 'wrap', 'eeeeff', 'copybar', '5em', 'img', 'callout', 'width', '6px', 'grey', 'collapse', 'th', 'none', 'footnote', 'noborder', '4px', 'figure', 'center', 'index', 'eeeeee', 'h1', '5ex', 'ul', 'li', 'list', 'type', 'heading', 'size', '120', '%;', \"'\", 'note', 'callouts', '87ceeb', '5', '%', '10px', 'verdana', 'arial', 'b0c4de', 'avm', 'tbody', 'key', '5px', 'eq', 'val', 'empty', 'colgroup', 'col', 'topleft', '000080', 'botleft', 'topright', 'botright', 'topbotleft', 'topbotright', 'ident', '80', 'vertical', 'pointer', '008000', 'c0ffc0', 'gloss', 'rst', 'thead', 'c0ffff', 'raw', 'doc', 'test', 'auto', 'cccccc', '304050', '000000', 'passed', 'failed', '800000', '<!--', 'grammatical', 'e', 'g', 'np', 'and', 'verb', 'technical', '..', '::', 'gc', '-->', 'is', 'book', 'about', 'natural', 'language', 'processing', 'we', 'mean', 'everyday', 'communication', 'human', 'like', 'english', 'hindi', 'or', 'portuguese', 'contrast', 'artificial', 'such', 'programming', 'mathematical', 'notation', 'have', 'evolved', 'pas', 'from', 'generation', 'hard', 'pin', 'down', 'explicit', 'rule', 'take', '—', 'nlp', 'short', 'wide', 'sense', 'cover', 'any', 'kind', 'of', 'computer', 'manipulation', 'at', 'one', 'extreme', 'could', 'simple', 'counting', 'word', 'frequency', 'compare', 'different', 'writing', 'other', 'involves', 'understanding', 'complete', 'utterance', 'least', 'extent', 'being', 'able', 'give', 'useful', 'response', 'them', 'technology', 'based', 'on', 'becoming', 'increasingly', 'widespread', 'phone', 'handheld', 'predictive', 'handwriting', 'recognition', 'web', 'search', 'engine', 'access', 'information', 'locked', 'up', 'unstructured', 'machine', 'translation', 'allows', 'u', 'retrieve', 'written', 'chinese', 'read', 'spanish', 'analysis', 'enables', 'detect', 'sentiment', 'tweet', 'blog', 'providing', 'sophisticated', 'stored', 'come', 'play', 'central', 'multilingual', 'society', 'provides', 'highly', 'accessible', 'introduction', 'field', 'can', 'individual', 'study', 'textbook', 'course', 'computational', 'linguistics', 'supplement', 'intelligence', 'mining', 'corpus', 'intensely', 'practical', 'containing', 'hundred', 'fully', 'worked', 'graded', 'exercise', 'together', 'an', 'open', 'library', 'called', 'toolkit', ').', 'includes', 'extensive', 'software', 'documentation', 'all', 'freely', 'downloadable', 'http', '://', '/.', 'distribution', 'provided', 'macintosh', 'unix', 'platform', 'strongly', 'encourage', 'you', 'download', 'try', 'out', 'along', 'way', 'audience', 'scientific', 'economic', 'social', 'cultural', 'reason', 'experiencing', 'rapid', 'growth', 'theory', 'method', 'deployed', 'variety', 'range', 'people', 'working', 'knowledge', 'within', 'industry', 'interaction', 'business', 'development', 'academia', 'area', 'humanity', 'computing', 'through', 'science', 'many', 'known', 'name', '.\")', 'intended', 'diverse', 'who', 'want', 'learn', 'how', 'write', 'program', 'analyze', 'regardless', 'previous', 'experience', '?', 'early', 'chapter', 'suitable', 'reader', 'no', 'prior', 'so', 'long', 'aren', 't', 'afraid', 'tackle', 'concept', 'develop', 'skill', 'full', 'yourself', 'need', 'general', 'see', 'resource', 'experienced', 'programmer', 'quickly', 'enough', 'using', 'get', 'immersed', 'relevant', 'carefully', 'explained', 'exemplified', 'appreciate', 'suitability', 'application', 'help', 'locate', 'discussion', 'already', 'dreaming', 'skim', 'dig', 'into', 'interesting', 'material', 'll', 'soon', 'applying', 'your', 'fascinating', 'real', 'grasp', 'value', 'idea', 'implementation', 'haven', 'learnt', 'teach', 'unlike', 'provide', 'illustration', 'approach', 'taken', 'also', 'principled', 'theoretical', 'underpinnings', 'don', 'shy', 'away', 'careful', 'linguistic', 'tried', 'pragmatic', 'striking', 'balance', 'between', 'identifying', 'connection', 'tension', 'finally', 'recognize', 'won', 'unless', 'pleasurable', 'entertaining', 'sometimes', 'whimsical', 'not', 'reference', 'work', 'coverage', 'selective', 'presented', 'tutorial', 'please', 'consult', 'substantial', 'quantity', 'searchable', 'available', 'advanced', 'content', 'introductory', 'intermediate', 'directed', 'algorithm', 'implemented', 'examine', 'linked', '/,', 'cited', 'what', 'digging', 'here', 'manipulate', 'these', 'describe', 'analyse', 'structure', 'format', 'evaluate', 'performance', 'technique', 'depending', 'motivation', 'interested', 'gain', 'set', 'iii', 'gained', 'reading', 'goal', 'art', 'engineering', 'manipulating', 'large', 'exploring', 'model', 'testing', 'empirical', 'claim', 'modeling', 'discovery', 'building', 'robust', 'system', 'perform', 'task', 'technological', 'organization', 'organized', 'order', 'conceptual', 'difficulty', 'starting', 'show', 'explore', 'body', 'tiny', '3', 'followed', 'structured', 'consolidates', 'topic', 'scattered', 'across', 'preceding', 'after', 'pace', 'pick', 'move', 'series', 'covering', 'fundamental', 'tagging', 'classification', 'extraction', '7', 'three', 'look', 'parse', 'sentence', 'syntactic', 'construct', 'representation', 'meaning', '8', '10', 'final', 'devoted', 'managed', 'effectively', '11', 'concludes', 'afterword', 'briefly', 'discussing', 'past', 'future', 'each', 'switch', 'presentation', 'driver', 'often', 'employ', 'introduced', 'systematically', 'their', 'purpose', 'before', 'delving', 'detail', 'why', 'just', 'learning', 'idiomatic', 'expression', 'foreign', 're', 'buy', 'nice', 'pastry', 'without', 'first', 'having', 'intricacy', 'question', 'formation', 'supporting', 'consolidating', 'according', 'following', '☼', 'easy', 'involve', 'minor', 'modification', 'supplied', 'sample', 'activity', '◑', 'aspect', 'depth', 'requiring', 'design', '★', 'difficult', 'ended', 'challenge', 'force', 'think', 'independently', 'should', 'skip', 'further', 'section', 'online', 'extra', 'version', 'there', 'yet', 'powerful', 'excellent', 'functionality', 'downloaded', 'free', 'installers', 'five', 'process', 'file', 'txt', 'print', 'ending', 'ing', ';&', '\"):', '...', 'split', '():', 'endswith', \"('\", \"'):\", 'illustrates', 'some', 'main', 'whitespace', 'nest', 'thus', 'fall', 'inside', 'scope', 'ensures', 'performed', 'second', 'oriented', 'variable', 'entity', 'certain', 'defined', 'attribute', 'than', 'sequence', 'character', 'operation', 'break', 'apply', 'period', 'i', '().', 'third', 'argument', 'expressed', 'parenthesis', 'instance', \"')\", 'had', 'indicate', 'wanted', 'something', 'most', 'importantly', 'readable', 'much', 'fairly', 'guess', 'doe', 'even', 'never', 'chose', 'because', 'shallow', 'curve', 'semantics', 'transparent', 'good', 'handling', 'interpreted', 'facilitates', 'interactive', 'exploration', 'permit', 'encapsulated', 'easily', 'dynamic', 'added', 'fly', 'typed', 'dynamically', 'facilitating', 'including', 'graphical', 'numerical', 'connectivity', 'heavily', 'research', 'education', 'around', 'world', 'praised', 'productivity', 'quality', 'maintainability', 'collection', 'success', 'story', 'posted', 'infrastructure', 'build', 'basic', 'representing', 'performing', 'part', 'speech', 'parsing', 'combined', 'solve', 'complex', 'problem', 'addition', 'website', 'api', 'every', 'module', 'specifying', 'parameter', 'giving', 'usage', 'updated', 'significant', 'change', 'statement', 'now', 'iterators', 'instead', 'save', 'memory', 'integer', 'division', 'floating', 'point', 'number', 'formatted', 'detailed', 'dev', 'whatsnew', 'html', 'utility', '2to3', 'py', 'convert', 'pervasive', 'initialised', 'fromstring', 'contextfreegrammar', 'cfg', 'weightedgrammar', 'pcfg', 'batch_tokenize', 'tokenize_sents', 'corresponding', 'batch', 'tagger', 'parser', 'classifier', 'removed', 'favour', 'external', 'package', 'maintained', 'adequately', 'github', 'com', 'wiki', 'porting', 'requirement', 'install', 'several', 'current', 'instruction', 'assumes', 'later', '6', '.)', 'subsequent', 'release', 'backward', 'compatible', 'contains', 'analyzed', 'processed', 'numpy', ':(', 'recommended', 'multidimensional', 'array', 'linear', 'algebra', 'required', 'probability', 'clustering', 'matplotlib', '2d', 'plotting', 'visualization', 'produce', 'graph', 'bar', 'chart', 'stanford', 'tool', ';(', 'scale', 'edu', '/).', 'networkx', 'optional', 'storing', 'network', 'consisting', 'edge', 'visualizing', 'semantic', 'graphviz', 'prover9', 'automated', 'theorem', 'prover', 'equational', 'logic', 'inference', 'wa', 'originally', 'created', '2001', 'department', 'university', 'pennsylvania', 'since', 'then', 'developed', 'expanded', 'dozen', 'contributor', 'adopted', 'serf', 'basis', 'project', 'viii', 'accessing', 'standardized', 'lexicon', 'tokenize', 'stem', 'tokenizers', 'stemmer', 'collocation', 'chi', 'squared', 'wise', 'mutual', 'tag', 'gram', 'backoff', 'brill', 'hmm', 'tnt', 'classify', 'cluster', 'tbl', 'decision', 'tree', 'maximum', 'entropy', 'naive', 'bayes', 'em', 'k', 'chunking', 'chunk', 'regular', 'named', 'ccg', 'unification', 'probabilistic', 'dependency', 'interpretation', 'sem', 'lambda', 'calculus', 'checking', 'evaluation', 'metric', 'precision', 'recall', 'agreement', 'coefficient', 'estimation', 'smoothed', 'app', 'chat', 'concordancer', 'wordnet', 'browser', 'chatbots', 'fieldwork', 'toolbox', 'sil', 'designed', 'four', 'primary', 'mind', 'simplicity', 'intuitive', 'framework', 'user', 'getting', 'bogged', 'tedious', 'house', 'keeping', 'usually', 'associated', 'annotated', 'consistency', 'uniform', 'consistent', 'guessable', 'extensibility', 'accommodated', 'alternative', 'competing', 'same', 'modularity', 'needing', 'understand', 'rest', 'contrasting', 'non', 'potentially', 'deliberately', 'avoided', 'encyclopedic', 'continue', 'evolve', 'efficient', 'meaningful', 'optimized', 'runtime', 'optimization', 'lower', 'level', 'c', '++.', 'would', 'le', 'avoid', 'clever', 'trick', 'believe', 'clear', 'preferable', 'ingenious', 'indecipherable', 'instructor', 'taught', 'confines', 'single', 'semester', 'undergraduate', 'postgraduate', 'found', 'both', 'side', 'subject', 'time', 'focus', 'exclusion', 'deprive', 'student', 'excitement', 'automatically', 'simply', 'linguist', 'do', 'manage', 'address', 'making', 'feasible', 'amount', 'practice', 'fraction', 'syllabus', 'deal', 'own', 'rather', 'dry', 'but', 'brings', 'life', 'possible', 'view', 'step', 'demonstration', 'performs', 'special', 'input', 'effective', 'deliver', 'entering', 'session', 'observing', 'modifying', 'issue', 'assignment', 'simplest', 'fragment', 'specified', 'answer', 'concrete', 'spectrum', 'flexible', 'graduate', 'widely', 'datasets', '),', 'extensible', 'architecture', 'additional', 'teaching', 'unique', 'comprehensive', 'context', 'apart', 'tight', 'coupling', 'those', 'completing', 'ready', 'attempt', 'jurafsky', 'martin', 'prentice', 'hall', '2008', 'present', 'unusual', 'beginning', 'trivial', 'introducing', 'control', 'comprehension', 'conditionals', 'idiom', 'once', 'place', 'systematic', 'loop', 'forth', 'ground', 'conventional', 'expecting', 'sake', 'two', 'plan', 'illustrated', 'ix', 'presumes', 'whereas', 'devote', 'remaining', '9', 'management', 'suggested', 'approximate', 'lecture', 'per', 'lexical', 'categorizing', 'extracting', 'analyzing', 'grammar', 'managing', 'total', '18', '36', 'convention', 'typographical', '--', 'indicates', 'paragraph', 'refer', 'filename', 'extension', 'constant', 'well', 'element', 'keywords', 'command', 'literally', 'replaced', 'determined', 'metavariables', 'icon', 'signifies', 'tip', 'suggestion', 'caution', 'warning', 'job', 'done', 'may', 'contact', 'youõre', 'reproducing', 'portion', 'require', 'selling', 'distributing', 'cd', 'rom', 'oõreilly', 'answering', 'citing', 'quoting', 'incorporating', 'productõs', 'attribution', 'title', 'publisher', 'isbn', 'ònatural', 'steven', 'bird', 'ewan', 'klein', 'o', 'reilly', 'medium', '978', '596', '51649', 'ó', 'feel', 'outside', 'fair', 'given', 'above', 'oreilly', 'acknowledgment', 'indebted', 'feedback', 'earlier', 'draft', 'doug', 'arnold', 'michaela', 'atterer', 'greg', 'aumann', 'kenneth', 'beesley', 'bethard', 'ondrej', 'bojar', 'chris', 'cieri', 'robin', 'cooper', 'grev', 'corbett', 'dan', 'garrette', 'jean', 'mark', 'gawron', 'hellmann', 'nitin', 'indurkhya', 'liberman', 'peter', 'ljunglöf', 'stefan', 'müller', 'munn', 'joel', 'nothman', 'adam', 'przepiorkowski', 'brandon', 'rhodes', 'stuart', 'robinson', 'jussi', 'salmela', 'kyle', 'schlansker', 'rob', 'speer', 'richard', 'sproat', 'thankful', 'colleague', 'participant', 'summer', 'school', 'brazil', 'india', 'usa', 'exist', 'member', 'developer', 'community', 'expertise', 'extending', 'grateful', 'national', 'foundation', 'consortium', 'clarence', 'dyason', 'fellowship', 'edinburgh', 'melbourne', 'our', 'thank', 'julie', 'steele', 'abby', 'fox', 'loranah', 'dimant', 'team', 'organizing', 'review', 'cheerfully', 'customizing', 'production', 'meticulous', 'editing', 'preparing', 'revised', 'edition', 'michael', 'korobov', 'leading', 'effort', 'port', 'antoine', 'trux', 'his', 'owe', 'huge', 'debt', 'gratitude', 'mimo', 'jee', 'love', 'patience', 'over', 'year', 'hope', 'child', 'andrew', 'alison', 'kirsten', 'leonie', 'maaike', 'catch', 'enthusiasm', 'computation', 'page', 'associate', 'professor', 'senior', 'he', 'completed', 'phd', 'phonology', '1990', 'supervised', 'moved', 'cameroon', 'conduct', 'grassfields', 'bantu', 'under', 'auspex', 'institute', 'recently', 'spent', 'director', 'where', 'led', 'r', 'd', 'create', 'database', 'established', 'group', 'curriculum', '2009', 'president', 'association', 'informatics', 'formal', 'cambridge', '1978', 'sussex', 'newcastle', 'upon', 'tyne', 'took', 'involved', 'establishment', '1993', 'closely', 'ever', '2000', '–', '2002', 'leave', 'act', 'manager', 'edify', 'corporation', 'santa', 'clara', 'responsible', 'spoken', 'dialogue', 'european', 'founding', 'coordinator', 'excellence', 'elsnet', 'went', 'ta', 'share', 'helped', 'documenting', 'epydoc', 'royalty', 'sale', 'xiv', 'july', '2007', '©', '2019', 'distributed', '[', '/],', 'creative', 'common', 'noncommercial', 'derivative', 'united', 'state', 'license', 'creativecommons', 'nc', 'nd', '/].', 'built', 'wed', 'sep', '40', '48', 'acst', 'todo', 'adopt', 'simpler', 'hacker', 'only', 'transposition', '?)', 'hand', 'million', 'assuming', 'achieve', 'combining', 'extract', 'phrase', 'sum', 'divided', 'quite', 'linguistically', 'motivated', 'necessarily', 'explaining', 'closer', 'flag', 'mix', 'front', 'authentic', 'taste', 'elementary', 'familiarity', 'repeat', 'miss', 'anything', 'completely', 'raise', 'addressed', 'very', 'familiar', 'day', 'treat', 'started', 'interpreter', 'friendly', 'directly', 'running', 'environment', 'mac', 'find', '→', 'macpython', 'run', 'shell', 'installed', 'blurb', '):', 'oct', '15', '2014', '22', '01', '37', 'gcc', 'apple', 'llvm', 'clang', '503', ')]', 'darwin', 'credit', 'unable', 'probably', 'visit', 'older', 'operator', 'round', 'fractional', 'result', 'downwards', 'expected', 'behavior', '__future__', 'waiting', 'when', 'copying', '\"&', ';\"', 'let', 'begin', 'calculator', 'finished', 'calculating', 'displaying', 'reappears', 'another', 'turn', 'enter', 'few', 'asterisk', '(*)', 'multiplication', 'slash', '(/)', 'bracketing', 'xxx', 'currently', 'boundary', 'esp', '\"^\"', 'pointing', 'demonstrate', 'interactively', 'experimenting', 'various', 'nonsensical', 'handle', 'stdin', ';\",', '^', 'syntaxerror', 'invalid', 'produced', 'error', 'doesn', 'plus', 'sign', 'occurred', ';,', 'stand', '\").', 'going', 'follow', 've', 'selecting', 'shown', 'downloading', 'browse', 'tab', 'downloader', 'grouped', 'select', 'labeled', 'obtain', 'consists', '30', 'compressed', '100mb', 'disk', '.,', 'nearly', 'ten', 'continues', 'expand', 'load', 'tell', '*.', 'say', '.\"', 'printing', 'welcome', 'message', 'again', 'care', 'spelling', 'punctuation', 'remember', ';.', '***', 'loading', 'text1', '...,', 'text9', 'sent1', 'sent9', \"()'\", 'sent', 'moby', 'dick', 'herman', 'melville', '1851', 'text2', 'sensibility', 'jane', 'austen', '1811', 'text3', 'genesis', 'text4', 'inaugural', 'text5', 'text6', 'monty', 'holy', 'grail', 'text7', 'wall', 'street', 'journal', 'text8', 'personal', 'man', 'thursday', 'chesterton', '1908', 'searching', 'concordance', 'occurrence', 'monstrous', 'placing', 'ong', 'former', 'came', 'towards', 'psalm', 'touching', 'bulk', 'whale', 'ork', 'heathenish', 'club', 'spear', 'were', 'thick', 'gazed', 'wondered', 'cannibal', 'savage', 'hav', 'survived', 'flood', 'mountainous', 'himmal', 'might', 'scout', 'fable', 'still', 'worse', 'de', 'radney', '.\\'\"', '55', 'picture', 'shall', 'ere', 'l', 'scene', 'connexion', 'am', 'fo', 'ght', 'rummaged', 'cabinet', 'telling', 'bone', 'oftentimes', 'cast', 'dead', 'particular', 'fast', 'arrow', 'ctrl', 'alt', 'modify', 'searched', 'included', 'affection', 'lived', 'back', '1789', 'nation', 'terror', 'god', 'differently', 'unconventional', 'im', 'ur', 'lol', 'uncensored', '!)', 'little', 'examining', 'richness', 'diversity', 'broader', 'saw', '___', 'appear', 'similar', 'appending', 'inserting', 'maddens', 'doleful', 'gamesome', 'subtly', 'uncommon', 'untoward', 'exasperate', 'loving', 'passing', 'mouldy', 'christian', 'mystifying', 'imperial', 'modifies', 'contemptible', 'heartily', 'exceedingly', 'remarkably', 'vast', 'great', 'amazingly', 'extremely', 'sweet', 'observe', 'her', 'positive', 'connotation', 'intensifier', 'common_contexts', 'shared', 'enclose', 'square', 'bracket', 'separate', 'comma', '([\"', '\"])', 'a_pretty', 'is_pretty', 'am_glad', 'be_glad', 'a_lucky', 'pair', 'occurs', 'however', 'determine', 'location', 'appears', 'positional', 'displayed', 'dispersion', 'plot', 'stripe', 'represents', 'row', 'entire', 'pattern', 'last', '220', 'constructed', 'joining', 'below', 'liberty', 'constitution', 'predict', 'quote', 'exactly', 'dispersion_plot', 'citizen', 'democracy', 'freedom', 'duty', 'america', 'presidential', 'investigate', 'installation', 'google', 'ngrams', 'generating', 'random', 'seen', 'generate', 'nothing', 'go', 'brother', 'hairy', 'whose', 'reach', 'unto', 'heaven', 'ye', 'sow', 'land', 'egypt', 'bread', 'month', 'earth', 'thy', 'wage', 'made', 'father', 'isaac', 'old', 'kissed', 'him', 'laban', 'cattle', 'midst', 'esau', 'born', 'phichol', 'chief', 'butler', 'son', 'she', 'reinstated', 'slow', 'gather', 'statistic', 'internet', 'room', 'although', 'lacking', 'randomly', 'generated', '``', 'off', 'correct', 'formatting', 'independent', 'chap', 'words_', 'vocabulary', 'obvious', 'fact', 'emerges', 'differ', 'count', 'jump', 'experiment', 'though', 'studied', 'trying', 'finding', 'finish', 'symbol', '44764', '44', '764', 'token', ':)', 'distinct', 'contain', 'pose', 'slightly', 'duplicate', 'collapsed', 'screen', 'sorted', \"['!\", \"',\", '\"\\'\",', \"'(',\", \"')',\", \"',',\", \"',)',\", \"'.\", \")',\", \"':',\", \"';',\", \"';)',\", \"'?\", 'abel', 'abelmizraim', 'abidah', 'abide', 'abimael', 'abimelech', 'abr', 'abrah', 'abraham', 'abram', 'accad', 'achbor', 'adah', '...]', '2789', 'wrapping', 'continuing', 'capitalized', 'precede', 'lowercase', 'discover', 'indirectly', 'asking', '789', 'form', 'specific', 'considered', 'generally', 'call', 'calculate', 'measure', 'equivalently', '16', 'average', '06230453042623537', 'compute', 'percentage', 'smote', '100', '4643016433938312', 'calculation', 'keep', 'retyping', 'formula', 'lexical_diversity', 'define', 'def', 'encountering', 'colon', 'expects', 'indented', 'indentation', 'hitting', 'blank', 'definition', 'specify', 'actual', 'reoccurs', 'similarly', 'know', 'ahead', '13477005109975562', \"'),\", 'recap', 'close', 'encountered', '(),', 'always', 'add', 'talking', 'mention', 'outset', 'newcomer', 'power', 'creativity', 'worry', 'bit', 'confusing', 'tabulating', 'repetitive', 'genre', 'brown', 'hobby', '82345', '11935', '145', 'humor', '21695', '5017', '231', 'fiction', '14470', '3233', '223', 'press', 'reportage', '100554', '14394', '143', 'romance', '70022', '8452', '121', 'religion', '39399', '6373', '162', 'reimport', '>>>', 'moment', 'assign', 'received', '*\"', 'granted', 'represent', 'case', 'opening', \"['\", 'me', 'ishmael', \"'.']\", 'equal', 'quoted', 'separated', 'surrounded', 'bracketed', 'store', 'inspect', 'ask', 'sent2', '…', '*).', 'dashwood', 'settled', 'sent3', 'ex1', \"'].\", ')),', \"').\", 'pleasant', 'surprise', 'adding', 'creates', 'everything', \"']\", 'concatenation', 'combine', 'concatenate', 'either', 'sent4', 'fellow', \"'-',\", 'senate', 'representative', 'append', 'itself', 'indexing', 'helpful', 'namely', 'showing', '|', 'obviate', 'contrived', 'word1', 'word2', 'etc', 'especially', 'slicing', 'reasonable', 'zero', 'doing', 'offset', 'cell', '+-', '-+-', '-+', 'represented', 'combination', 'ordinary', '1st', '173rd', '14', '278th', 'printed', 'analogously', 'identify', 'instruct', '173', ']', 'awaken', 'converse', 'sublists', 'manageable', 'piece', '16715', '16735', 'u86', 'thats', 'gamefly', 'actually', 'game', 'buying', '1600', '1625', 'anarcho', 'syndicalist', 'commune', 'sort', 'executive', 'officer', 'week', 'subtlety', 'word3', 'word4', 'word5', 'word6', 'word7', 'word8', 'word9', 'word10', 'notice', '],', 'forward', 'leaf', 'initially', 'typical', 'modern', 'hang', 'mastered', 'century', '19xy', '20th', 'live', 'country', 'floor', 'numbered', 'walking', 'flight', 'stair', 'accidentally', 'too', 'traceback', 'recent', 'indexerror', 'syntactically', 'brief', 'explanation', 'verify', 'slice', 'm', 'omit', '[:', '141525', ':]', 'among', 'merit', 'happiness', 'elinor', 'marianne', 'ranked', 'considerable', 'sister', 'living', 'almost', 'sight', 'disagreement', 'themselves', 'producing', 'coolness', 'husband', 'assigning', 'put', 'consequence', 'generates', 'minute', 'saved', 'lot', '250', '000', 'did', 'ourselves', 'defining', 'follows', 'misleading', 'moving', 'my_sent', 'xyzzy', 'must', 'letter', 'underscore', 'bravely', 'sir', 'rode', 'camelot', 'noun_phrase', 'multiple', 'happens', '\"...\"', 'matter', 'continuation', 'choose', 'remind', 'anyone', 'meant', 'blindly', 'restriction', 'cannot', 'reserved', 'hold', 'vocab', 'vocab_size', '19317', 'choice', 'identifier', 'optionally', 'digit', 'abc23', 'fine', '23abc', 'cause', 'sensitive', 'myvar', 'my_var', 'insert', 'hyphen', 'my', 'wrong', 'interprets', '\"-\"', 'minus', 'mont', 'montymonty', \"'!'\", \"!'\", 'join', \"(['\", \"'])\", 'bring', 'bear', 'began', 'compile', 'automatic', 'characteristic', 'predicting', 'whether', 'got', 'sure', 'saying', 'said', '[-', 'expect', 'informative', 'imagine', '50', 'frequent', 'tally', 'thousand', 'laborious', 'appearing', 'observable', 'event', 'freqdist', 'fdist1', '260819', 'outcome', 'most_common', \"[(',',\", '18713', '13721', \"('.\", '6862', '6536', '6024', '4569', '4542', \"(';',\", '4072', '3916', '2982', '(\"\\'\",', '2684', \"('-',\", '2552', '2459', '2209', '2124', '1739', '1695', '1661', '1659', '1632', '1620', '(\\'\"\\',', '1478', '1462', '1414', '1280', \"('!\", '1269', '1231', '1137', '1113', '1103', \"('--',\", '1070', '1058', '1052', '1030', '1005', '918', '906', '889', '841', '767', '760', '715', '705', '697', '680', '646', '640', \"('?\", '637', '627', '624', 'invoke', 'counted', '260', '819', 'frequently', 'occurring', 'uppercase', 'nameerror', '900', 'plumbing', 'proportion', 'cumulative', 'account', 'half', 'occur', 'hapaxes', 'lexicographer', 'cetological', 'contraband', 'expostulation', 'others', 'seems', 'rare', 'seeing', 'neither', 'nor', 'infrequent', 'grained', 'selection', 'perhaps', 'adapt', 'property', 'express', 'interest', '1a', 'v', '∈', ')}', 'b', '1b', 'executable', 'long_words', 'circumnavigation', 'physiognomically', 'apprehensiveness', 'cannibalistically', 'characteristically', 'circumnavigating', 'comprehensiveness', 'hermaphroditical', 'indiscriminately', 'indispensableness', 'irresistibleness', 'preternaturalness', 'responsibility', 'simultaneousness', 'subterraneousness', 'supernaturalness', 'superstitiousness', 'uncomfortableness', 'uncompromisedness', 'undiscriminating', 'uninterpenetratingly', 'greater', 'ignored', 'discus', 'changing', 'condition', 'difference', '...]?', 'characterize', 'reflect', 'constitutionally', 'transcontinental', 'informal', 'boooooooooooglyyyyyy', 'yuuuuuuuuuuuummmmmmmmmmmm', 'succeeded', 'typify', 'better', 'promising', 'eliminates', 'antiphilosophists', 'longer', 'seven', 'fdist5', \"['#\", '19teens', \"'#\", 'talkcity_adults', \"'((((((((((',\", \"'........',\", 'cute', '.-', 'as', 'everyone', 'football', 'innocent', 'listening', 'seriously', 'tomorrow', 'watching', 'bearing', 'modest', 'milestone', 'bigram', 'unusually', 'red', 'wine', 'resistant', 'substitution', 'sens', 'maroon', 'sound', 'definitely', 'odd', 'accomplished', \"']))\", \"[('\", \"')]\", 'omitted', '...]),', 'generator', '0x10fb8b3a8', 'essentially', 'pay', 'attention', 'ago', 'federal', 'government', 'american', 'vice', 'almighty', 'magistrate', 'justice', 'bless', 'indian', 'tribe', 'political', 'party', 'drinker', 'quiet', 'night', 'smoker', 'age', 'financially', 'secure', 'weekend', 'po', 'rship', 'married', 'mum', 'permanent', 'relationship', 'slim', 'emerge', 'larger', 'creating', 'fdist', '19', '({', '50223', '47933', '42345', '38513', '26597', '17111', '14399', '9966', '6428', '3528', '...})', 'deriving', 'quarter', '20', 'twenty', 'wonder', '[(', '1873', '12', '1053', '13', '567', '177', '70', '17', 'max', 'freq', '19255882431878046', 'roughly', '%)', 'pursue', 'summarizes', 'description', 'increment', 'iterate', 'greatest', 'tabulate', '|=', 'fdist2', 'update', 'touched', 'normalization', 'taking', 'far', 'ability', 'potential', 'automation', 'behalf', 'executing', 'met', 'repeatedly', 'looping', 'until', 'satisfied', ';=,', 'relational', 'comparison', ';=', '\"=\"', '!=', 'news', 'changed', 'sent7', 'undefined', 'pierre', 'vinken', '61', 'board', 'nonexecutive', 'nov', '29', \"[',',\", \".']\", 'yield', 'listed', 'startswith', 'islower', 'cased', 'isupper', 'isalpha', 'alphabetic', 'isalnum', 'alphanumeric', 'isdigit', 'istitle', 'titlecased', 'initial', 'capital', 'ableness', 'gnt', 'entirely', \"'))\", 'comfortableness', 'honourableness', 'immutableness', 'sovereignty', '())', 'aaaaaaaaah', 'aaaaaaaah', 'aaaaaah', 'aaaah', 'aaaaugh', 'aaagh', 'c1', 'c2', 'conjunction', 'disjunction', 'explain', \"'-'\", 'wd', 'cie', 'cei', 'operating', 'upper', \"['[',\", \"']',\", 'etymology', 'f', '...],', 'operates', 'described', 'fixed', 'habitually', 'bothering', 'mastering', 'fluent', '17231', 'double', 'capitalization', 'wiped', 'eliminate', 'filtering', '()))', '16948', 'complicated', 'purely', '?).', 'confident', 'nested', 'execute', 'conditional', '].', 'cat', 'invoked', 'executed', 'indent', 'recognized', 'print_function', \"']:\", 'executes', 'circular', 'fashion', 'relates', 'action', 'elif', 'titlecase', 'multiline', 'invaluable', 'comfortable', \"='\", 'newline', 'tricky', 'ancient', 'ceiling', 'conceit', 'conceited', 'conceive', 'conscience', 'conscientious', 'conscientiously', 'deceitful', 'deceive', '>>', 'misc', 'babelize_shell', 'exploiting', 'opportunity', 'nitty', 'gritty', 'paint', 'bigger', 'navigate', 'universe', 'crucial', 'popularity', 'shortcoming', 'luck', 'tourist', 'site', 'philadelphia', 'pittsburgh', 'limited', 'budget', 'expert', 'digital', 'slr', 'camera', 'prediction', 'steel', 'market', 'credible', 'commentator', 'summarization', 'carried', 'robustness', 'beyond', 'capability', 'philosophical', 'standing', 'intelligent', 'major', 'behaviour', 'become', 'mature', 'unrestricted', 'prospect', 'emerged', 'plausible', 'disambiguation', 'consider', 'ambiguous', 'serve', 'dish', 'food', 'drink', 'office', 'ball', 'plate', 'meal', 'device', 'served', 'unlikely', 'shifted', 'sport', 'crockery', 'invent', 'bizarre', 'image', 'tennis', 'pro', 'frustration', 'china', 'tea', 'laid', 'beside', 'court', 'disambiguate', 'nearby', 'related', 'contextual', 'effect', 'agentive', 'cup', 'stove', 'locative', 'submit', 'friday', 'temporal', 'submitting', '3c', 'italicized', 'interpret', 'lost', 'searcher', 'mountain', 'afternoon', 'pronoun', 'resolution', 'deeper', 'whom', 'harder', 'thief', 'stole', 'painting', 'stealing', '4c', 'sold', 'caught', 'subsequently', 'antecedent', 'tackling', 'anaphora', 'noun', 'refers', 'labeling', 'agent', 'patient', 'instrument', 'relating', 'demonstrates', 'translate', 'accurately', 'conveying', 'original', 'translating', 'french', 'forced', 'gender', 'il', 'masculine', 'elles', 'feminine', 'depends', 'voleurs', 'ont', 'volé', 'peintures', 'été', 'trouvés', 'tard', 'trouvées', 'establishing', 'mt', 'ultimately', 'seeking', 'high', 'root', 'cold', 'war', 'promise', 'sponsorship', 'today', 'integrated', 'serious', 'starkly', 'revealed', 'equilibrium', 'reached', 'alice', 'spring', 'wie', 'lang', 'vor', 'dem', 'folgenden', 'flug', 'zu', 'springen', 'sie', 'bevor', 'der', 'folgende', 'tun', 'tut', 'sprung', 'leap', 'translates', 'german', ';),', 'preposition', 'translated', 'phrasing', 'indicated', 'proper', 'misinterpreted', 'translationparty', 'target', 'faced', 'collecting', 'massive', 'parallel', 'publish', 'possibly', 'bilingual', 'dictionary', 'alignment', 'dialog', 'history', 'turing', 'responding', 'naturally', 'distinguish', 'commercial', 'narrowly', 'saving', 'private', 'ryan', 'playing', 'theater', 'paramount', 'madison', '00', 'driving', 'restaurant', 'incorporated', 'asks', 'movie', 'determines', 'didn', 'endowed', 'interact', 'asked', '?,', 'unhelpfully', 'respond', 'yes', 'assumption', 'ensure', 'request', 'handled', 'screening', 'service', 'pipeline', 'parsed', 'planned', 'realized', 'suitably', 'inflected', 'inform', 'stage', 'commonly', 'assumed', 'diagram', 'map', 'via', 'middle', 'reverse', 'converting', 'static', 'repository', 'draw', 'primitive', 'conversation', 'chatbot', 'textual', 'entailment', 'brought', 'recognizing', 'rte', 'scenario', 'suppose', 'evidence', 'hypothesis', 'sandra', 'goudie', 'defeated', 'purnell', 'elected', 'parliament', 'election', 'winning', 'seat', 'coromandel', 'defeating', 'labour', 'candidate', 'pushing', 'incumbent', 'green', 'mp', 'jeanette', 'fitzsimons', 'accept', 'conclusion', 'allow', 'competitor', 'brute', 'intensive', 'consequently', 'person', 'david', 'golinkin', 'editor', 'eighteen', '150', 'responsa', 'article', 'sermon', 'supported', 'someone', 'ii', 'conclude', 'limitation', 'despite', 'advance', 'reasoning', 'manner', 'wait', 'solved', 'meantime', 'necessary', 'severe', 'accordingly', 'progress', ',\"', 'superficial', 'indeed', 'equip', 'contribute', 'aspiration', 'summary', 'appearance', ')).', 'operate', 'x', 'derive', 'collapsing', 'distinction', 'ignoring', '()).', ':.', 'assigned', 'reused', 'mult', 'y', 'mixed', 'consolidated', '/),', 'link', 'wikipedia', 'acquaint', 'moin', 'beginnersguide', 'miscellaneous', 'answered', 'faq', 'delve', 'subscribe', 'mailing', 'announced', 'covered', 'fred', 'damerau', 'ed', '2010', 'handbook', 'chapman', 'crc', 'dale', 'moisl', 'somers', 'daniel', 'mitkov', 'ruslan', '2003', 'oxford', 'international', 'acl', 'www', 'aclweb', '/)', 'host', 'regional', 'conference', 'workshop', 'anthology', 'literature', 'indexed', 'finegan2007', '_', 'grady', 'et', 'al', '2004', 'osu', 'languagelog', 'popular', 'occasional', 'post', 'alphabet', '26', '**', '141167095653376', 'applied', 'score', 'lexically', 'protagonist', 'willoughby', 'played', 'male', 'female', 'novel', 'couple', 'my_string', 'pressing', 'multiplying', 'joined', 'fix', '[\"', 'favorite', 'phrase1', 'phrase2', 'whole', ')?', 'typically', '\"[', '\"][', '][', 'alphabetical', 'sunset', 'trial', 'sent8', '()?', 'decreasing', 'script', 'meet', '...].', 'ise', 'z', 'pt', 'sell', 'sea', 'shore', 'sh', 'percent', 'calculates', 'ch01', 'rst2', '1889', 'backlink', 'unknown', 'rich', 'poor', 'ethnologue', 'tokenization', 'fail', 'mi', 'synonym', 'homonym', 'lemma', 'cf', '201', 'mapping', 'morphology', 'exploratory', 'categorization', 'refinement', 'reload', 'redefining', '170', 'repeating', 'unfamiliar', 'substituting', 'hows', 'mentioned', 'examined', 'convenience', 'glued', 'treated', 'accessed', 'examines', 'gutenberg', 'electronic', 'archive', '25', 'hosted', 'fileids', 'emma', 'persuasion', 'bible', 'kjv', 'blake', 'poem', 'bryant', 'burgess', 'busterbrown', 'carroll', 'edgeworth', 'parent', 'moby_dick', 'milton', 'paradise', 'shakespeare', 'caesar', 'hamlet', 'macbeth', 'whitman', '192427', 'showed', 'carry', 'concordancing', 'nine', 'obtained', 'surprize', 'cumbersome', 'fileid', 'compact', 'nearest', 'num_chars', 'num_words', 'num_sents', 'num_vocab', ')))', '28', '34', '79', '23', '21', '52', 'recurrent', 'really', 'divide', 'macbeth_sentences', \"[['[',\", 'tragedie', 'william', '1603', \"']'],\", 'actus', 'primus', \"'],\", '1116', 'toile', 'trouble', 'fire', 'burne', 'cauldron', 'bubble', 'longest_len', \"[['\", 'doubtfull', 'stood', 'swimmer', 'cling', 'choake', 'mercilesse', 'macdonwald', '...]]', 'richer', 'forum', 'overheard', 'york', 'pirate', 'carribean', 'advertisement', 'webtext', ')[:', '65', \"'...')\", 'cookie', 'cooky', 'se', 'wind', 'clop', 'king', 'arthur', 'whoa', 'guy', 'evening', 'asian', 'girl', 'chest', 'ted', 'elliott', 'terr', 'sexy', 'seek', 'attrac', 'lady', 'discreet', 'encoun', 'lovely', 'delicate', 'fragrant', 'rhone', 'polished', 'leather', 'strawb', 'instant', 'messaging', 'collected', 'naval', 'detection', 'predator', 'anonymized', 'replacing', 'usernames', 'generic', 'usernnn', 'manually', 'edited', 'remove', 'date', 'chatroom', 'teen', 'adult', '20s_706posts', 'xml', '706', 'gathered', '2006', 'nps_chat', '123', 'hot', 'pic', 'mirror', '1961', '500', 'categorized', 'editorial', 'icame', 'uib', 'bcm', 'los', 'id', 'a16', 'ca16', 'chicago', 'tribune', 'b02', 'cb02', 'monitor', 'c17', 'cc17', 'magazine', 'd12', 'cd12', 'underwood', 'probing', 'ethic', 'realtor', 'e36', 'ce36', 'norling', 'renting', 'car', 'europe', 'f25', 'cf25', 'lore', 'boroff', 'jewish', 'teenage', 'culture', 'g22', 'cg22', 'belles_lettres', 'reiner', 'coping', 'runaway', 'h15', 'ch15', 'civil', 'defence', 'mobilization', 'fallout', 'shelter', 'j17', 'cj19', 'learned', 'mosteller', 'statistical', 'k04', 'ck04', 'du', 'bois', 'l13', 'cl13', 'mystery', 'hitchens', 'footstep', 'm01', 'cm01', 'science_fiction', 'heinlein', 'stranger', 'strange', 'n14', 'cn15', 'adventure', 'rattlesnake', 'ridge', 'p12', 'cp12', 'callaghan', 'passion', 'rome', 'r06', 'cr06', 'thurber', 'comedy', 'fulton', 'county', 'grand', 'jury', \"=['\", \"'...],\", 'convenient', 'studying', 'inquiry', 'stylistics', 'modal', 'news_text', '94', '87', '93', '38', '53', '389', 'wh', 'unpick', 'ignore', 'concentrate', 'cfd', 'conditionalfreqdist', '86', '66', '82', '59', '78', '54', '71', '268', '58', '131', '83', '264', '49', '74', '193', '51', '45', '43', 'predicted', 'reuters', '788', 'totaling', 'classified', '90', 'training', '14826', 'drawn', '14828', '14829', '14832', 'acq', 'alum', 'barley', 'bop', 'carcass', 'castor', 'oil', 'cocoa', 'coconut', 'coffee', 'copper', 'copra', 'cake', 'corn', 'cotton', 'cpi', 'cpu', 'crude', 'dfl', 'dlr', 'overlap', '9865', 'grain', 'wheat', '9880', 'money', 'fx', '15618', '15649', '15676', '15728', '15871', '14858', '15033', '15043', '15106', '15287', '15341', '15648', 'handful', \"')[:\", 'cereal', 'export', 'bid', 'requested', 'licence', 'thai', 'trade', 'deficit', 'widens', 'looked', 'fig', 'ax', 'dimension', 'washington', '1793', '1797', '()]', '1801', '1805', '1809', '1813', '1817', '1821', 'extracted', '])', 'kept', 'plotted', 'trend', 'observed', 'normalized', 'annotation', 'howto', 'compiler', 'francis', 'kucera', '15m', 'tagged', 'ce', 'treebanks', 'clic', 'ub', '1m', 'catalan', 'pereira', 'warren', 'geographic', 'cmu', 'pronouncing', '127k', 'entry', 'conll', '270k', 'chunked', '700k', 'dutch', 'sel', '150k', 'basque', 'treebank', 'narad', 'penn', 'framenet', 'fillmore', 'baker', '10k', '170k', 'floresta', 'diana', 'santos', '9k', 'gazetteer', 'city', '200k', 'hart', 'newby', '2m', 'cspan', 'kumaran', '60k', 'bangla', 'marathi', 'telugu', 'macmorpho', 'nilc', 'usp', 'brazilian', 'pang', 'lee', '2k', 'polarity', 'kantrowitz', 'ross', '8k', 'nist', '1999', 'info', 'extr', 'garofolo', '63k', 'newswire', 'sgml', 'markup', 'nombank', 'meyers', '115k', 'proposition', '1400', 'frame', 'forsyth', 'martell', 'bond', 'aligned', 'pp', 'attachment', 'ratnaparkhi', '28k', 'prepositional', 'modifier', 'bank', 'palmer', '113k', '3300', 'roth', '6k', '3m', 'roget', 'thesaurus', 'dagan', 'semcor', 'ru', 'mihalcea', '880k', 'senseval', 'pedersen', '600k', 'sentiwordnet', 'esuli', 'sebastiani', '145k', 'bosak', 'union', '485k', 'stopwords', 'porter', '400', 'swadesh', 'wiktionary', 'comparative', 'wordlists', '24', 'switchboard', 'ldc', 'phonecalls', 'transcribed', 'univ', 'decl', '480k', '300', '40k', 'timit', 'audio', 'transcript', 'speaker', 'verbnet', '5k', 'hierarchically', 'wordlist', 'openoffice', '960k', '20k', 'affix', 'miller', 'fellbaum', 'encoding', 'cess_esp', 'el', 'grupo', 'estatal', 'electricit', '\\\\', 'xe9_de_france', 'um', 'revivalismo', 'refrescante', '7_e_meio', 'प', 'ू', 'र', '्', 'ण', 'रत', 'ि', 'ब', 'ं', 'ध', 'हट', 'ा', 'ओ', 'इर', 'क', 'स', 'य', 'ु', 'त', 'udhr', 'abkhaz', 'cyrillic', 'abkh', 'utf8', 'achehnese', 'latin1', 'achuar', 'shiwiar', 'adja', 'afaan_oromo_oromiffa', 'afrikaans', 'aguaruna', 'akuapem_twi', 'albanian_shqip', 'amahuaca', 'javanese', \"')[\", 'saben', 'umat', 'manungsa', 'lair', 'kanthi', 'hak', 'universal', 'declaration', 'boolean', 'chickasaw', 'german_deutsch', 'greenlandic_inuktikut', 'hungarian_magyar', 'ibibio_efik', \"'-\", 'six', 'fewer', 'ibibio', '60', 'inuktitut', 'raw_text', 'unfortunately', 'insufficient', 'industrial', 'developing', 'piecemeal', 'endangered', 'summarized', 'lack', 'correspond', 'notably', 'topical', 'occasionally', 'isolated', '([', '=[', 'f1', 'f2', 'f3', 'abspath', 'stream', 'path', 'locally', 'readme', 'illustrate', 'buster', 'thornton', '1920', 'fishing', 'yawned', 'lay', 'bed', 'watched', 'morning', 'sunbeam', 'creeping', 'plaintextcorpusreader', 'directory', 'usr', 'dict', 'whatever', 'corpus_root', 'initializer', \"'[\", 'abc', ']/.', '*\\\\.', \"'/\", \"*')\", 'connective', 'propernames', 'web2', 'web2a', 'local', ':\\\\', 'bracketparsecorpusreader', 'file_pattern', 'contained', 'subfolders', 'penntreebank', 'mrg', 'wsj', '\".*/', 'wsj_', 'ptb', 'wsj_0001', 'wsj_0002', 'wsj_0003', 'wsj_0004', '49208', 'wsj_2013', 'mr', 'noriega', 'smooth', 'shah', 'iran', 'nicaragua', '\"\\'', 'anastasio', 'somoza', 'ferdinand', 'marcos', 'philippine', 'bloody', 'haiti', 'baby', 'duvalier', 'mylist', 'generalize', 'maintain', 'achieved', 'depicts', '161', '192', 'genre_word', '170576', '_start', '\"\\'\\'\"),', \"'.')]\", '_end', 'usual', '_conditions', 'satisfy', '3899', '3736', '2758', '1776', '1502', '1335', '1186', \"('``',\", '1045', '(\"\\'\\'\",', '1044', '993', '951', '875', '702', '692', '690', '651', '583', '573', '559', '496', \"']['\", 'initialize', 'tabulation', 'reproduced', 'occured', 'exploit', '1865', 'lincoln', 'lowercased', 'derived', 'limit', 'selected', '638', '185', '525', '883', '997', '1166', '1283', '1440', '1558', '1638', '171', '263', '614', '717', '894', '1013', '1110', '1213', '1275', 'newsworthy', 'romantic', 'monday', 'noticed', 'multi', ']),', 'permitted', 'introducted', 'consecutive', 'cryptic', 'generate_model', 'reset', 'likely', '());', 'inspecting', 'tends', 'stuck', 'cfdist', 'num', \"({'\", 'creature', \"':\", 'substance', \"',':\", 'soul', '})', 'code_random_text', 'obtains', 'record', 'seed', 'counter', 'cfdist1', 'cfdist2', 'reusing', 'retyped', 'mess', 'reuse', 'compose', 'menu', 'shortly', '================================', 'restart', 'revising', 'paste', 'descriptive', 'separating', 'monty_python', 'interacting', 'plural', 'singular', 'reliable', 'localize', 'needed', 'behave', 'equivalent', 'my_text_data', 'arbitrary', 'word_count', 'diversity_score', '[:-', 'sx', 'ch', 'en', 'fairy', 'woman', 'code_plural', 'delete', 'latest', 'collect', 'previously', 'text_proc', 'importing', 'wish', 'fan', 'fen', 'obviously', 'edit', 'existing', 'confusion', 'imported', 'folder', 'secondary', 'enriched', 'my_text', 'word_freq', 'preparation', 'terminology', 'headword', 'spell', 'checker', 'spelt', 'unusual_words', 'text_vocab', 'english_vocab', 'abbeyland', 'abhorred', 'abounded', 'abridgement', 'abused', 'abuse', 'accent', 'accepting', 'accommodation', 'accompanied', 'accounted', 'accustomary', 'ache', 'acknowledging', 'aaaaaaaaaaaaaaaaa', 'aaahhhh', 'abortion', 'abou', 'abourted', 'ab', 'ack', 'acros', 'actualy', 'adduser', 'adjusts', 'adoted', 'adreniline', 'ad', 'afe', 'affair', 'affari', 'affect', 'afk', 'agaibn', 'code_unusual', 'computes', 'leaving', 'filter', 'presence', 'fails', 'myself', 'ours', 'yours', 'yourselves', 'himself', 'hers', 'herself', 'theirs', 'against', 'during', 'content_fraction', '7364374824583169', 'puzzle', 'grid', 'chosen', 'solving', 'iterates', 'obligatory', 'constraint', 'trickier', 'solution', 'twice', 'puzzle_letters', 'egivrvonl', 'glover', 'gorlin', 'govern', 'grovel', 'involver', 'lienor', 'linger', 'lovering', 'noiler', 'overling', 'region', 'renvoi', 'revolving', 'ringle', 'roving', 'violer', 'virole', 'male_names', 'female_names', 'abbey', 'abbie', 'addie', 'adrian', 'adrien', 'ajay', 'alex', 'alexis', 'alfie', 'ali', 'alix', 'allie', 'allyn', 'andie', 'andrea', 'andy', 'angel', 'angie', 'ariel', 'ashley', 'aubrey', 'augustine', 'austin', 'averil', 'h', 'equally', 'spreadsheet', 'synthesizer', 'cmudict', '133737', '42371', '42379', ']:', 'fir', 'er1', 'ay1', 'er0', 'firearm', 'aa2', 'fireball', 'ao2', 'phonetic', 'contrastive', 'pronunciation', 'syllable', 'arpabet', 'tuple', 'individually', ':,', 'pron', 'ph1', 'ph2', 'ph3', 'pait', 'ey1', 'pat', 'ae1', 'pate', 'patt', 'peart', 'peat', 'iy1', 'peet', 'peete', 'pert', 'pet', 'eh1', 'pete', 'pett', 'piet', 'piette', 'pit', 'ih1', 'pitt', 'pot', 'aa1', 'pote', 'ow1', 'pott', 'pout', 'aw1', 'puett', 'uw1', 'purt', 'uh1', 'putt', 'ah1', 'scan', 'looking', 'assigns', 'sounding', 'nick', 'rhyming', 'ih0', 'atlantic', 'audiotronics', 'avionics', 'beatnik', 'calisthenics', 'centronics', 'chamonix', 'chetniks', 'clinic', 'conic', 'cryogenics', 'cynic', 'diasonics', 'dominic', 'ebonics', 'electronics', '\\'\",', 'nics', 'niks', 'nix', 'ntic', 'silent', 'mismatch', 'summarize', 'autumn', 'column', 'condemn', 'damn', 'goddamn', 'hymn', 'solemn', 'gn', 'kn', 'mn', 'pn', 'stress', 'char', \"']]\", 'abbreviated', 'abbreviating', 'accelerated', 'accelerating', 'accelerator', 'accentuated', 'accentuating', 'accommodating', 'accommodative', 'accumulated', 'accumulating', 'accumulative', 'abbreviation', 'abomination', 'abortifacient', 'academician', 'accreditation', 'accumulation', 'acetylcholine', 'adjudication', 'doubly', 'minimally', 'p3', \"]+'-'+\", 'template', '()):', 'wordstring', '\"...\")', 'patch', 'pautsch', 'peach', 'perch', 'petsch', 'petsche', 'piche', 'piech', 'pietsch', 'pitch', 'pac', 'pack', 'paek', 'paik', 'pak', 'pake', 'paque', 'peak', 'peake', 'pech', 'peck', 'peek', 'perc', 'perk', 'pahl', 'pail', 'paille', 'pal', 'pale', 'pall', 'paul', 'paule', 'paull', 'peal', 'peale', 'pearl', 'paign', 'pain', 'paine', 'pan', 'pane', 'pawn', 'payne', 'peine', 'pen', 'penh', 'pine', 'pinn', 'paap', 'paape', 'pap', 'pape', 'papp', 'paup', 'peep', 'pep', 'pip', 'pipe', 'pipp', 'poop', 'pop', 'pope', 'paar', 'par', 'pare', 'parr', 'pear', 'peer', 'pier', 'poore', 'por', 'pore', 'porr', 'pour', 'peace', 'pearse', 'pea', 'perce', 'pers', 'perse', 'pesce', 'piss', 'piett', 'peru', 'peugh', 'pew', 'plew', 'plue', 'prew', 'pru', 'prue', 'prugh', 'pshew', 'pugh', 'iterating', 'prondict', 'keyerror', 'existent', 'missing', 'tweak', 'absent', 'ph', ']]', 'ah0', 'ng', 'jh', 'eh0', 'tabular', 'identified', 'iso', '639', 'bg', 'ca', 'cu', 'fr', 'hr', 'la', 'mk', 'nl', 'pl', 'ro', 'sk', 'sl', 'sr', 'sw', 'uk', 'thou', 'big', 'cognate', 'fr2en', 'je', 'tu', 'vous', 'chien', 'dog', 'jeter', 'throw', 'translator', 'de2en', 'es2en', 'hund', 'perro', 'germanic', '139', '140', '141', '142', ')[', 'sagen', 'zeggen', 'decir', 'dire', 'dizer', 'dicere', 'sing', 'singen', 'zingen', 'cantar', 'chanter', 'canere', 'spielen', 'spelen', 'jugar', 'jouer', 'jogar', 'brincar', 'ludere', 'float', 'schweben', 'zweven', 'flotar', 'flotter', 'flutuar', 'boiar', 'fluctuare', 'shoebox', 'replaces', 'traditional', 'card', 'repeatable', 'rotokas', 'kaa', 'gag', '\":', 'dic', 'ge', 'tkp', 'nek', 'pa', 'dcsv', 'vx', 'sc', \"'???\", 'dt', '2005', 'ex', 'apoka', 'ira', 'kaaroi', 'aioa', 'ia', 'reoreopaoro', 'xp', 'kaikai', 'bilong', 'bikos', 'na', 'toktok', 'xe', 'gagging', \"')]),\", 'consist', 'tok', 'pisin', 'loose', 'island', 'bougainville', 'papua', 'guinea', 'contributed', 'notable', 'inventory', 'phoneme', 'rotokas_language', 'wn', 'datastructures', 'imposes', 'heavy', 'cognitive', 'notion', 'pretty', '`', 'synset', '`:', ':,\"', 'semantically', '155', '287', '117', '659', 'motorcar', 'automobile', 'stay', 'benz', 'credited', 'invention', 'remained', 'unchanged', 'synonymous', 'lemma_names', 'signify', 'train', 'carriage', 'gondola', 'elevator', 'prose', 'motor', 'vehicle', 'wheel', 'propelled', 'internal', 'combustion', 'ambiguity', 'pairing', 'unambiguous', '02', '03', '04', 'cable_car', 'railcar', 'railway_car', 'railroad_car', 'elevator_car', 'involving', 'hierarchy', 'abstract', 'gas', 'guzzler', 'hatchback', 'hypernym', 'hyponym', 'relation', 'superordinate', 'subordinate', 'immediate', 'types_of_motorcar', 'ambulance', 'model_t', 'suv', 'stanley_steamer', 'beach_waggon', 'beach_wagon', 'bus', 'cab', 'compact_car', 'convertible', 'coupe', 'cruiser', 'electric', 'electric_automobile', 'electric_car', 'estate_car', 'gas_guzzler', 'hack', 'hardtop', 'heap', 'horseless_carriage', 'rod', 'hot_rod', 'jalopy', 'jeep', 'landrover', 'limo', 'limousine', 'loaner', 'minicar', 'minivan', 'pace_car', 'patrol_car', 'phaeton', 'police_car', 'police_cruiser', 'prowl_car', 'race_car', 'racer', 'racing_car', 'roadster', 'runabout', 'saloon', 'secondhand_car', 'sedan', 'sport_car', 'sport_utility', 'sport_utility_vehicle', 'sports_car', 'squad_car', 'station_waggon', 'station_wagon', 'stock_car', 'subcompact', 'subcompact_car', 'taxi', 'taxicab', 'tourer', 'touring_car', 'seater', 'waggon', 'wagon', 'visiting', 'wheeled_vehicle', 'container', 'motor_vehicle', 'hypernym_paths', 'physical_entity', 'artifact', 'instrumentality', 'self', 'propelled_vehicle', 'conveyance', 'root_hypernyms', 'relate', 'meronym', 'holonym', 'trunk', 'crown', 'part_meronyms', 'heartwood', 'sapwood', 'substance_meronyms', 'forest', 'member_holonyms', 'burl', '07', 'limb', 'stump', 'intricate', 'mint', '05', 'north', 'temperate', 'plant', 'genus', 'mentha', 'aromatic', 'mauve', 'flower', 'fresh', 'candied', 'candy', 'flavored', '06', 'coined', 'authority', 'part_holonyms', 'substance_holonyms', 'stepping', 'entail', 'walk', 'eat', 'chew', 'swallow', 'tease', 'arouse', 'disappoint', 'antonymy', 'supply', 'antonym', 'demand', 'rush', 'horizontal', 'inclined', 'staccato', 'legato', 'dir', 'harmony', \"')).\", 'similarity', 'traverse', 'knowing', 'low', 'right_whale', 'orca', 'minke', 'minke_whale', 'tortoise', 'lowest_common_hypernyms', 'baleen_whale', 'vertebrate', 'baleen', 'quantify', 'generality', 'min_depth', 'incorporate', 'insight', 'path_similarity', 'shortest', 'connects', '(-', 'returned', 'comparing', 'decrease', 'inanimate', '16666666666666666', '07692307692307693', '043478260869565216', 'hierarhical', 'entered', 'funct', 'documented', 'extensively', 'published', 'agency', 'elra', 'higher', 'fee', 'brat', 'nlplab', 'olac', 'metadata', 'homepage', 'posting', 'biber', 'conrad', 'reppen', '1998', 'mcenery', 'meyer', 'sampson', 'mccarthy', 'scott', 'tribble', 'quantitative', 'baayen', 'gries', 'wood', 'fletcher', 'hughes', '1986', 'psycholinguistics', 'retrieval', 'globalwordnet', 'budanitsky', 'hirst', 'phonetics', 'sorting', 'state_union', 'men', 'happened', 'member_meronyms', 'arise', 'suggest', 'strunk', 'nevertheless', 'advise', 'best', 'bartleby', 'strunk3', 'considering', 'fossilized', 'prejudice', '\\'\"', 'itre', 'ci', 'upenn', '/~', 'myl', '001913', 'bbc', 'vicky', 'pollard', 'behind', 'co', 'hi', '6173441', 'stm', 'yeah', '003993', 'impressionistic', 'closed', 'exhibit', 'all_synsets', 'supergloss', 'ratio', 'lowest', 'adjacent', 'omitting', 'absence', 'hedge', 'zipf', 'law', 'inversely', 'proportional', 'rank', '×', '50th', '150th', 'pylab', 'confirm', 'hint', 'logarithmic', 'abcdefg', '\"),', 'accumulate', 'light', 'intelligible', 'strength', 'weakness', 'hybrid', 'observation', 'find_language', 'latin', 'branching', 'factor', 'polysemy', 'adjective', 'adverb', 'predefined', 'ranking', 'experimentally', 'charles', 'gem', 'jewel', 'journey', 'voyage', 'boy', 'lad', 'coast', 'asylum', 'madhouse', 'magician', 'wizard', 'midday', 'noon', 'furnace', 'fruit', 'cock', 'crane', 'implement', 'monk', 'oracle', 'cemetery', 'woodland', 'rooster', 'hill', 'graveyard', 'slave', 'chord', 'smile', 'glass', 'ch03', 'global', 'pprint', 'word_tokenize', 'invocation', 'clunky', 'regexp_tokenize', 'compiled', 'vowel', 'findall', 'undoubtedly', 'unlimited', 'stemming', 'consolidate', 'dispense', 'onwards', 'assume', 'catalog', 'ascii', 'finnish', 'italian', '2554', 'crime', 'punishment', 'urllib', 'urlopen', 'decode', \"'&\", '1176893', '75', 'ebook', 'fyodor', 'dostoevsky', 'downloads', 'proxy', 'detected', \"{'\", 'someproxy', '3128', \"'}\", 'proxyhandler', '176', '893', ').)', 'feed', '254354', '1024', '1062', 'exceptionally', 'young', 'garret', 'lodged', 'walked', 'slowly', 'hesitation', 'bridge', 'katerina', 'ivanovna', 'pyotr', 'petrovitch', 'pulcheria', 'alexandrovna', 'avdotya', 'romanovna', 'rodion', 'romanovitch', 'marfa', 'petrovna', 'sofya', 'semyonovna', 'tm', 'porfiry', 'amalia', 'nikodim', 'fomitch', 'ilya', 'dmitri', 'prokofitch', 'andrey', 'semyonovitch', 'hay', 'header', 'scanned', 'corrected', 'footer', 'reliably', 'resort', 'manual', 'inspection', 'trimming', '5338', 'rfind', '1157743', 'overwrite', 'brush', 'reality', 'unwanted', 'dealing', 'easiest', 'blonde', 'die', 'urban', 'legend', 'health', '2284783', ';!', 'doctype', '\"-//', 'w3c', 'dtd', 'transitional', 'glory', 'meta', 'javascript', 'beautifulsoup', 'crummy', '/:', 'bs4', 'get_text', \"'|',\", 'concerning', 'navigation', '110', '390', 'gene', 'hey', 'hair', 'caused', 'recessive', 'blond', 'disadvantage', 'chance', 'disappear', 'thin', 'thought', 'unannotated', 'advantage', 'furthermore', 'smaller', 'programmatically', 'absolutely_', 'yahoo', 'xref', 'data_', '.]', 'hit', 'absolutely', 'adore', 'prefer', '289', '905', '644', '460', '158', '62', '600', '198', '97', 'allowable', 'severely', 'restricted', 'arbitrarily', 'wildcards', 'inconsistent', 'geographical', 'duplicated', 'boosted', 'unpredictably', 'breaking', 'locating', 'ameliorated', 'apis', 'blogosphere', 'register', 'pypi', 'feedparser', 'llog', 'nll', '/?', 'atom', 'log', 'bf', 'chatting', 'prc', 'thinking', 'au', 'courant', 'dui4xiang4', \"'\\\\\", 'u5c0d', 'u8c61', '\\'(\"\\',', \"'/',\", 'friend', '\\'\"\\',', 'frustrating', 'therefore', 'snippet', 'poignantly', 'nb', 'feeling', 'monkey', 'patching', 'fake', 'io', 'stringio', 'fake_open', 'mode', '.\\\\', 'nfruit', 'banana', 'fake_urlopen', \"('<!\", '\"\\')', 'plain', 'offer', 'gone', 'couldn', 'pyshell', 'toplevel', 'ioerror', 'errno', 'listdir', \"('.')\", 'controlling', 'opened', 'marking', 'newlines', 'keyboard', 'suppressing', 'strip', 'demonstrated', 'pdf', 'msword', 'binary', 'specialized', 'pypdf', 'pywin32', 'particularly', 'challenging', 'conversion', 'drive', 'capturing', 'capture', 'discussed', 'tokenized', 'converted', 'properly', 'int', 'normalizing', 'attributeerror', 'query', \"?'\", 'beatles', 'john', 'george', 'ringo', 'typeerror', 'studiously', 'avoiding', 'focused', 'backslash', 'escape', 'literal', 'otherwise', 'report', 'circus', 'flying', \"\\\\'\", 'couplet', 'thee', '\"\\\\', ':\"', 'rough', 'shake', 'darling', 'bud', 'lease', 'hath', ':\")', 'sonnet', 'triple', '\"\"\"', ':\"\"\"', \"'''\", \":'''\", 'pasted', 'multiply', 'veryveryvery', \"'',\", 'subtraction', 'unsupported', 'operand', '-:', 'muddle', 'told', '-)', 'quotation', 'pythonholy', 'remark', 'ameliorate', 'negative', 'colorless', 'sleep', 'furiously', '117092', '87996', '77916', '69326', '65617', 'q', 'j', 'sb', 'unpacking', 'somewhere', 'visualize', 'continuous', 'pull', 'pyth', 'stopping', 'applies', 'stop', ':-', \"...'.\", 'valueerror', 'rindex', 'glue', 'wherever', 'splitlines', 'uppercased', 'trailing', 'concatenating', 'brian', 'granularity', 'correspondingly', 'downstream', 'conversely', 'terminal', 'lennon', 'del', '0th', 'immutable', 'mutable', 'modified', 'speaking', 'realizing', 'extended', 'ø', 'danish', 'norwegian', 'ő', 'hungarian', 'ñ', 'breton', 'ň', 'czech', 'slovak', 'overview', 'uxxxx', 'xxxx', 'hexadecimal', 'encoded', 'byte', 'subset', 'utf', 'mechanism', 'decoding', 'perspective', 'glyph', 'paper', 'polish', 'lat2', 'suggests', 'biblioteka_pruska', '8859', 'locates', 'unicode_samples', 'latin2', 'pruska', 'biblioteka', 'państwowa', 'jej', 'dawne', 'zbiory', 'znane', 'pod', 'nazwą', 'berlinka', 'skarb', 'kultury', 'sztuki', 'niemieckiej', 'przewiezione', 'przez', 'niemców', 'koniec', 'wojny', 'światowej', 'dolny', 'śląsk', 'zostały', 'odnalezione', '1945', 'terytorium', 'polski', 'trafiły', 'biblioteki', 'jagiellońskiej', 'krakowie', 'obejmują', 'ponad', 'tys', 'zabytkowych', 'archiwaliów', 'manuskrypty', 'goethego', 'mozarta', 'beethovena', 'bacha', 'underlying', 'codepoints', 'encode', 'unicode_escape', '\\\\\\\\', 'u0144stwowa', 'nazw', 'u0105', 'niemc', 'xf3w', 'u015bwiatowej', 'u015al', 'u0105sk', 'zosta', 'u0142y', 'trafi', 'jagiello', 'u0144skiej', 'obejmuj', 'archiwali', \".'\", 'preceded', 'u0144', 'dislayed', 'ń', 'xf3', 'corresponds', '128', '255', 'ordinal', 'ord', '324', '0144', 'hex', 'appropriate', 'nacute', 'determining', 'rendered', 'failing', 'configure', 'locale', 'render', 'xc5', 'x84', 'unicodedata', 'prefixing', '+),', 'readlines', '127', \"('{}\", '+{:', '04x', \"{}'.\", 'xc3', 'xb3', '00f3', 'acute', 'x9b', '015b', 'x9a', '015a', 'xc4', 'x85', '0105', 'ogonek', 'x82', '0142', 'stroke', 'ś', 'ą', 'ł', 'alternatively', 'u015bl', \"('\\\\\", 'u015b', \"*',\", 'inputting', '-*-', 'coding', \"-*-'\", 'big5', 'requires', 'preference', 'courier', 'mail', 'pipermail', 'february', '247783', 'evaluating', 'hangul', 'codec', 'emacs', 'specification', 'valid', 'accepted', 'detecting', 'matching', 'describing', 'chevron', '«', '».', 'preprocess', '$».', 'dollar', \"$',\", 'abaissed', 'abandoned', 'abased', 'abashed', 'abatised', 'abed', 'aborted', 'wildcard', 'crossword', 'sixth', \"('^..\", \"..$',\", 'abjectly', 'adjuster', 'dejected', 'dejectly', 'injector', 'majestic', 'caret', '$', '«..', '..»?', 'specifies', '«^', '-?', '$»', 'email', \"('^\", 'closure', 't9', 'mobile', 'keystroke', 'textonyms', 'hole', 'golf', '4653', '«^[', 'ghi', 'mno', 'jlk', ']$»:', \"('^[\", \"]$',\", 'gold', ']»,', '«[', 'constrains', 'fourth', 'constrained', 'hig', 'nom', 'ljk', 'fed', ']$»', 'matched', 'finger', 'twister', 'pad', 'ghijklmno', ']+$»,', 'concisely', 'fj', ']+$»', 'corner', 'chat_words', \"+$',\", 'miiiiiiiiiiiiinnnnnnnnnnneeeeeeeeee', 'miiiiiinnnnnnnnnneeeeeeee', 'mine', 'mmmmmmmmiiiiiiiiinnnnnnnnneeeeeeee', \"]+$',\", 'ah', 'ahah', 'ahahah', 'ahh', 'ahhahahaha', 'ahhh', 'ahhhh', 'ahhhhhh', 'ahhhhhhhhhhhhhh', 'haaa', 'hah', 'haha', 'hahaaa', 'hahah', 'hahaha', 'hahahaa', 'hahahah', 'hahahaha', '*,', '*$»', '+$»,', 'min', 'mmmmm', 'referred', 'kleene', '«[^', 'aeiouaeiou', ']»', '«^[^', ':):):),', 'grrr', 'cyb3r', 'zzzzzzzz', 'illustrating', '\\\\,', '{},', '|:', ']+\\\\.', '0085', '56', '84', '95', '99', '125', '1650', \"]+\\\\$$',\", \"$']\", ']{', \"}$',\", '1614', '1637', '1787', '1901', '1903', '1917', '1925', '1929', '1933', ']+-[', 'lap', ',}-[', '}-[', ']{,', 'butter', 'gun', 'toting', 'loan', \"('(\", \")$',\", '%-', 'owned', 'absorbed', 'adopting', 'advancing', 'deprived', '\\\\.', 'braced', 'ai', 'oo', '»,', 'wit', 'wet', 'woot', 'instructive', 'z0', ']*', '+,', ']+', ']?', ',}', '{,', ')+', 'specially', 'backspace', 'band', 'habit', \"'...'\", 'complication', 'regexp', 'overlapping', 'supercalifragilisticexpialidocious', 'aeiou', \"]',\", 'fd', \",}',\", '549', 'ea', '476', 'ie', '331', 'ou', '329', '261', '253', 'ee', '217', '174', 'ua', '109', '106', 'ue', '105', 'ui', '31', '(?,', 'noted', 'redundant', 'becomes', 'dclrtn', 'inalienable', 'inlnble', 'retaining', 'consonant', \"''.\", \"'^[\", ']+|[', ']+$|[^', \"]'\", 'compress', 'english_udhr', 'tokenwrap', ']))', 'unvrsl', 'hmn', 'rghts', 'prmble', 'whrs', 'rcgntn', 'inhrnt', 'dgnty', 'eql', 'mmbrs', 'fmly', 'fndtn', 'frdm', 'jstce', 'pce', 'wrld', 'dsrgrd', 'cntmpt', 'hve', 'rsltd', 'brbrs', 'whch', 'outrgd', 'cnscnce', 'mnknd', 'advnt', 'bngs', 'shll', 'enjy', 'spch', 'ka', 'si', 'rotokas_words', 'cv', 'ptksvr', '418', '148', '420', '187', '63', '89', '47', '27', 'partial', 'complementary', 'conceivably', 'drop', 'pronounced', 'su', 'kasuari', 'cassowary', 'borrowed', 'allowing', 'cv_index', 'cv_word_pairs', 'kaapo', 'kaapopato', 'kaipori', 'kaiporipie', 'kaiporivira', 'kapo', 'kapoa', 'kapokao', 'kapokapo', 'kapokapoa', 'kapokapora', ']».', 'ri', 'laptop', 'versa', 'minded', 'suffix', 'ly', 'iou', 'ive', 'ment', \"'^.\", '*(', 'gave', 'arcane', \"'^.*(?\", 'parenthesize', \"'^(.\", '*)(', 'processe', 'incorrectly', 'star', 'greedy', 'consume', '*?,', \"'^(.*?\", ')(', \")?$',\", \"'')]\", 'spot', \")?$'\", 'dennis', 'listen', 'lying', 'pond', 'sword', 'supreme', 'derives', 'mandate', 'mass', 'farcical', 'aquatic', 'ceremony', '.\"\"\"', 'distribut', 'basi', 'execut', 'deriv', 'acceptable', 'angle', '*&', 'monied', 'bro', '(&', ';)', ';\")', 'nervous', 'dangerous', 'pious', 'queer', 'cape', 'butterless', 'fiendish', 'furious', 'dismasted', 'younger', 'brave', 'twizted', '.*&', ';{', ',}\")', 'lmao', 're_show', 'annotates', 'nemo', 'phenomenon', 'tied', 'hobbies_learned', ';\\\\', 'speed', 'water', 'liquid', 'tomb', 'landmark', 'statue', 'monument', 'road', 'military', 'compilation', 'iron', 'metal', 'taxonomy', 'labor', 'exclude', 'ontology', 'correcting', 'suffers', 'risky', 'doubled', '*([', 'bdgptk', '])\\\\', '1ed', 'transforming', 'stripping', 'correction', 'resulting', 'lemmatization', 'shelf', 'crafting', 'irregular', 'lancaster', 'lie', 'porterstemmer', 'lancasterstemmer', 'denni', 'strang', 'suprem', 'mandat', 'farcic', 'aquat', 'ceremoni', 'den', 'wom', 'ba', 'pow', 'mand', 'som', 'farc', 'aqu', 'suit', 'enumerate', 'indexedtext', '__init__', '_text', '_stemmer', '_index', '((', '_stem', 'wc', 'lcontext', 'rcontext', 'ldisplay', \"'{:&\", \"}}'.\", ':],', 'rdisplay', \"'{:{\", 'beat', 'retreat', 'minstrel', 'singing', 'bravest', 'nay', 'oh', 'wounded', 'doctor', 'immediately', 'clap', 'piglet', 'danger', 'cave', 'gorge', 'eternal', 'peril', 'tim', 'caerbannog', 'fifty', 'strewn', 'fight', 'til', 'code_stemmer_indexing', 'lemmatizer', 'slower', 'wnl', 'wordnetlemmatizer', 'lemmatize', 'decimal', 'mapped', 'acronym', 'aaa', 'improves', 'accuracy', 'tokenizing', 'cutting', 'identifiable', 'unit', 'constitute', 'delay', 'wonderland', '\"\"\"\\'', 'duchess', \",'\", 'hopeful', 'tone', 'pepper', 'kitchen', 'soup', 'maybe', 'tempered', ',\\'...\"\"\"', '[\"\\'', ',\\'\",', \",',\", \"'(\", 'nthough', \"),',\", 'nwell', ',\\'...\"]', \"]+',\", ']+»', '(\\\\', 'rewritten', \"+',\", 'prefix', 'instructs', 'backslashed', 'splitting', \",'.\", 'za', '9_', 'complement', \"['',\", \"'']\", 'xx', 'extend', 'wider', '«\\\\', '+|\\\\', '*»', 'apostrophe', \"+([-']\\\\\", '+)*».', \"[-']\\\\\", '+;', \"+(?:[-']\\\\\", \"+)*|'|[-.\", '(]+|\\\\', '*\",', '[\"\\'\",', \"'--',\", \"'...']\", '«[-.', '(]+»', 'ellipsis', 'separately', '[^', 'tokenizer', 'avoids', 'treatment', 'readability', '(?', 'verbose', 'embedded', 'poster', 'cost', \"...'\", \"'''(?\", 'regexps', '(?:[', ']\\\\.', '+(?', ':-\\\\', '+)*', '\\\\$?\\\\', '+(?:\\\\.\\\\', '+)?%?', 'currency', '\\\\.\\\\.\\\\.', '[][.,;\"\\'?', '():-', '`]', \"'$\", 'gap', 'reporting', 'decide', 'treebank_raw', 'contraction', 'normalize', 'lookup', 'segmentation', 'radically', 'presupposes', '250994070456922', 'segment', 'punkt', 'segmenter', 'kiss', 'segmenting', 'sent_tokenize', '[\\'\"', 'nonsense', '\"\\',', 'gregory', 'rational', 'nattempted', 'paradox', 'clerk', 'navvy', 'railway', 'sad', 'tired', 'ntell', 'ticket', 'sloane', 'station', 'victoria', 'wild', 'rapture', ',\\\\', 'eye', 'eden', 'unaccountably', 'unpoetical', 'replied', 'poet', 'syme', 'lucian', 'simultaneously', 'terminate', 'visual', '爱国人', 'ai4', 'guo2', 'ren2', '爱国', '人', '爱', '国人', 'arises', 'hearer', 'learner', 'hearing', 'doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy', 'annotating', '.).', 'pause', 'doyouseethekittyseethedoggydoyoulikethekittylikethedoggy', 'seg1', '0000000000000001000000000010000000000000000100000000000', 'seg2', '0100100100100001001001000010100100010010000100010010000', 'shorter', 'broken', 'segmented', 'segs', '[]', ')):', ':])', 'kitty', 'doggy', 'code_segment', 'reconstruct', 'hypothetical', 'reproduce', 'acquiring', 'brent', '1995', 'objective', 'scoring', 'optimize', 'delimiter', 'derivation', 'reconstructed', 'marker', 'text_size', 'lexicon_size', 'seg3', '0000100100000011001000000110000100010000001100010000001', 'doyou', 'thekitt', 'thedogg', '64', 'code_evaluate', 'reconstructing', 'minimizes', 'thekitty', 'randint', 'flip', 'flip_n', ')-', 'anneal', 'iteration', 'cooling_rate', 'temperature', 'best_segs', '5000', 'doyouseetheki', 'tty', 'thedoggy', 'doyouliketh', 'ekittylike', 'doy', 'ouseetheki', 'ttysee', 'ulikethekittylike', '57', 'seetheki', 'liketh', 'seethekit', 'tysee', 'likethekittylike', 'seethekittysee', '0000100100000001001000000010000100010000000100010000000', 'code_anneal', 'deterministic', 'simulated', 'annealing', 'perturb', 'lowered', 'perturbation', 'reduced', 'degree', 'criterion', 'reformatting', 'silly', \"';'.\", \";.'\", 'wecalledhimtortoisebecausehetaughtus', 'spacer', 'calling', 'enjoys', 'privilege', 'hello', 'nworld', 'naming', 'recreate', 'benefit', 'clue', 'snake', \"'-&\", \";',\", \"=';\", '-&', 'alternating', \"('{}-&\", \";{};'.\", \"'{}-&\", \";'\", 'curly', \"'{}'\", 'replacement', 'embed', 'unpack', \";'.\", \"'{}'.\", '{}', \"'{}\", 'sandwich', 'lunch', 'consumed', 'superfluous', '2262', 'unexpected', \"{}'\", \"}',\", \"}'.\", 'spam', 'fritter', 'pancake', 'snack', 'lining', 'variable_', \"'%-*\", '_width', \"':'\", '{:', 'padded', 'justified', 'specifier', 'option', \"'{:\", '41', \"}'\", '2310', '{:.', '4f', \"'{:.\", 'pi', '1416', 'smart', \"'%'\", '3205', '9375', '%}\".', '1867', \"%'\", 'tabulated', 'exercising', 'separation', \"('{:\", \"('{:&\", 'code_modal_tabulate', \"}}'\", 'bound', 'customize', 'accommodate', 'output_file', 'identical', 'conveniently', 'overflow', \"'('\", \"'),',\", 'textwrap', 'clarity', 'onto', 'fill', '[\"{}', '{}\".', 'wrapped', 'linebreak', 'redefine', \"'%\", 's_', '(%', 'onty', \"'/'.\", 'inadequate', 'bundle', 'appeared', 'canonical', 'citation', 'lexeme', '\\\\.,', '\\\\|,', '\\\\$,', 'lose', 'arg_tuple', 'howtos', 'mertz', 'kuchling', 'amk', 'regex', 'friedl', 'facility', 'ned', 'batchelder', 'nedbatchelder', 'unipain', 'beazley', 'pyvideo', 'video', 'pycon', 'spolsky', 'minimum', 'positively', 'excuse', 'joelonsoftware', 'sighan', 'acquisition', 'niyogi', 'multiword', 'alone', 'baldwin', 'kim', 'heuristic', 'approximation', 'optimum', 'discrete', 'analogy', 'metallurgy', 'discovering', 'hearst', '1992', 'colourless', 'morphological', \"'[:-\", 'inserted', 'ning', 'ality', 'un', 'heat', 'direction', '[::-', '+(\\\\.\\\\', '+)?', '([^', '][^', '])*', '+|[^\\\\', 'determiner', 'arithmetic', \"/').\", 'sole', 'tokenizes', 'monetary', 'rewrite', 'newspaper', 'word_len', 'choosing', 'versus', \"')?\", 'prog', 'clause', 'exclamation', 'fuzzy', 'webpage', 'weather', 'forecast', 'town', 'categorize', 'preserved', 'improve', '|\\\\', '+».', 'hack3r', '|,', '5w33t', '!,', 'ate', 'pig', 'transformation', 'ay', 'ingstray', 'idleay', 'pig_latin', 'preserve', 'qu', 'ietquay', 'yellow', 'chooses', 'aehh', 'uncontrolled', 'sneezing', 'maniacal', 'laughter', 'heheeh', 'eha', 'numeric', 'medline', 'cortisol', 'serum', '+/-', '%,', 'respectively', 'compound', 'fifteen', '\"?', 'wouldn', 'possibility', 'motivate', 'μw', 'μs', 'ari', \"'.'].\", '[].', 'newly', 'formed', 'bland', 'inexpressible', 'infuriating', 'legitimate', 'chomsky', 'famous', 'eoldrnnnna', 'nationality', 'canadian', 'australian', 'canada', 'australia', 'list_of_adjectival_forms_of_place_names', '002733', 'lolcat', 'lolspeak', 'lolcatbible', 'php', 'how_to_speak_lolcat', 'sub', 'consulting', '-\\\\', 'nterm', 'identifies', 'hyphenated', 'remain', 'encyclo', 'npedia', 'soundex', 'respective', 'rural', 'confabulation', 'elocution', 'sequoia', 'tenacious', 'unidirectional', 'vsequences', \"(''.\", 'aiuio', 'eaiou', 'eouio', 'euoia', 'oauaio', 'uiieioa', 'ancestor', 'correlation', 'spearman_correlation', 'unseen', 'discovers', 'useragent', 'spoofing', '#.', 'soso', 'ghits', '()``', '``.', '``,', 'ch04', 'transitive', 'connected', 'synonymy', 'w04', '3253', 'wrestling', 'pitfall', 'concise', 'introduce', 'dictated', 'revert', 'seem', 'deserving', 'surprising', 'foo', 'behaves', 'affected', 'bodkin', 'updating', '3133', 'holding', 'extends', '[[],', '[],', '[]]', '[[]]', 'propagate', 'overwrote', 'wasn', 'referenced', 'overwriting', '[:].', 'deepcopy', 'equality', 'identity', '==,', 'snake_nest', 'pairwise', 'interloper', '4557855488', '4557854763', 'reveals', 'nonempty', 'evaluated', 'evaluates', 'opposed', 'situation', 'animal', 'bare', 'cond1', 'cond2', 'fish', 'anywhere', 'porpoise', 'tuples', 'enclosed', 'sliced', 'fem', 'grouping', 'snark', '\\',\".', \"(('\", \"',))\", 'turned', 'spectroroute', 'ute', 'computed', 'implicitly', 'aggregating', 'reversed', '))).', 'randomize', 'shuffle', \"':'.\", 'lorry', ',:', 'arrange', 'precedence', 'temporary', 'tmp', 'rearrange', 'handy', 'zip', 'prep', 'det', '...&', 'lazy', '0x10d005448', 'putting', 'cut', '%.', 'training_data', 'test_data', 'wordlens', 'discard', 'commonality', 'di', \":',\", \"@']),\", 'qf', 'predetermined', 'orthographic', 'sampa', 'phon', 'ucl', 'ac', 'home', \"');\", 'venetian', 'blind', \"'];\", 'vbd', 't3', '\\'\\'\\'\"', 'humpty', 'dumpty', 'scornful', '.\"\\'\\'\\'', \"['``',\", '\"\\'\\'\",', ')])', 'notational', 'storage', 'allocated', 'streamed', 'lexicographic', 'undisputed', 'volume', 'donald', 'knuth', 'literate', 'ramification', 'layout', 'procedural', 'declarative', 'subtle', 'spacing', 'needle', 'designer', '0008', 'maximizing', 'recommendation', 'messed', 'brace', ']):', 'chore', 'highlight', 'aware', 'pythoneditors', 'implication', 'efficiency', 'influencing', '401545438271973', 'track', 'meaningless', 'dictating', '401', 'forever', 'sat', 'mat', 'understood', 'constitutes', 'word_list', 'instantly', 'recognizable', ']).', 'most_common_words', '(\"%', '3d', '2f', '%%', '42', '67', 'tempting', 'longest', \"''\", 'unextinguishable', 'maxlen', 'transubstantiate', 'inextinguishable', 'incomprehensible', 'overhead', 'instantaneous', 'concern', 'successive', 'trigram', '[[', '()],', \"'},\", '()]]', 'incorrect', '[[{', '}],', '[{', '}]]', 'elegant', '*?&', 'code_get_text', 'cleaned', 'whenever', 'clutter', 'docstring', 'loaded', '__main__', 'reusable', 'tested', 'risk', 'forget', 'bug', 'increased', 'reliability', 'transparently', 'mechanic', 'parenthesized', 'msg', 'communicates', 'procedure', 'my_sort1', '()),', 'my_sort2', 'my_sort3', 'touch', 'bad', 'set_up', 'reflected', 'visible', 'concerned', 'collision', 'resolve', 'respect', 'succeed', 'lgb', 'enable', 'introduces', 'portability', 'reusability', 'declare', 'iterator', 'defensive', 'knight', '([\"\\'', 'ti', 'scratch', 'sensible', 'complain', 'clearly', 'slight', 'improvement', 'diagnostic', 'propagated', 'unpredictable', 'assert', 'basestring', 'generalizes', 'isinstance', 'halt', 'execution', 'additionally', 'assertion', 'logical', 'docstrings', 'functional', 'decomposition', 'grows', 'analogous', 'essay', 'expressing', 'abstraction', 'fetch', 'load_corpus', 'maintainable', 'reimplement', 'freq_words', 'gov', 'charter', 'constitution_transcript', \"'=',\", 'congress', 'code_freq_words1', 'poorly', 'elsewhere', 'populated', 'refactor', 'simplify', 'dropping', 'code_freq_words2', 'usability', 'improved', 'signal', 'decomposing', 'adequate', '0257', 'sphinx', 'richly', 'param', ']}.', 'adj', 'ordered', 'rtype', 'num_correct', 'code_sphinx', 'exception', 'last_letter', 'extract_property', 'prop', 'invoking', 'treating', 'supposing', 'latter', 'cmp', 'initializing', 'returning', 'aggregated', 'search1', 'search2', 'zz', 'grizzly', 'fizzled', 'rizzuto', 'huzzahs', 'dazzler', 'jazz', 'pezza', 'code_search_examples', 'continued', 'stopped', 'encounter', 'allocate', 'permutation', 'seq', 'perm', ':]):', 'police', 'buffalo', 'recursion', 'haskell', 'alongside', 'is_content_word', 'retains', \"')))\", '75081116158339', 'favored', 'throughout', 'confused', \"='&\", 'alicealicealicealicealice', 'unnamed', 'args', 'kwargs', '(*', 'african', 'song', 'hen', 'turtle', 'dove', 'shorthand', 'fw', 'optionality', 'happy', '(\".', '=\"\")', 'debugging', 'acquired', 'manifestation', 'shooting', 'describes', 'organize', 'logically', 'facilitate', 'separator', 'extn', 'inf', 'emulate', '__file__', 'distance', 'lib', 'python2', 'pyc', 'edloper', 'gmail', 'stevenbird1', 'tom', 'lippincott', 'columbia', '/&', 'demo', '_helper', 'hide', 'externally', '__all__', 'edit_distance', 'jaccard_distance', 'stable', 'depicted', 'my_program', 'localized', 'dividing', 'growing', 'designing', 'mastery', 'damage', 'creep', 'unnoticed', 'fixing', 'impression', 'reassurance', 'spontaneous', 'fault', 'flippancy', 'aside', 'faulty', 'decomposed', \"('.').\", 'broke', 'rsplit', 'rightmost', 'intact', 'released', '297', 'misunderstanding', '98', '\"%', '.%', '02d', '.\",', 'find_words', 'wordlength', '=[]):', 'omg', 'teh', 'sitted', 'progressed', 'stack', 'trace', 'pinpointing', 'reduce', 'smallest', 'offending', 'purport', 'debugger', 'breakpoints', 'pdb', 'mymodule', 'myfunction', \"()')\", 'breakpoint', 'arose', ')\")', ')&', ';()', 'prompting', 'assertionerror', 'notification', 'bugfix', 'isn', 'trap', 'magically', 'articulate', 'undo', 'resolved', 'suite', 'regression', 'regress', 'unintended', 'ensuring', 'sync', 'strategy', 'algorithmic', 'adapting', 'elaborate', 'prevalent', 'conquer', 'attack', 'pile', 'merge', 'recursively', 'simplifies', 'factorial', 'factorial1', '*=', 'recursive', 'ordering', 'base', 'factorial2', 'deeply', 'rooted', 'size1', 'iterative', 'layer', 'maintains', 'size2', 'procedurally', 'happening', 'abbreviate', '190', 'trie', 'held', 'hien', 'chair', 'flesh', 'chic', 'stylish', 'nicer', \"'}},\", \"'}}},\", \"'}}}\", \"'}}}}}\", 'code_trie', 'nesting', 'pushed', 'tradeoff', 'significantly', 'auxiliary', 'faster', \"'*\", '...\")', 'movie_reviews', 'abspaths', 'idx', 'quit', 'raw_input', 'code_search_documents', 'invert', 'preprocessed', 'tagged_corpus', 'wm', '[[(', 'code_strings_to_ints', 'maintaining', 'membership', 'timeit', 'timer', 'setup', 'simulate', '100000', 'setup_list', ')\"', 'setup_set', '))\"', '78092288971', '0037260055542', 'mere', '0037', 'magnitude', 'planning', 'scheduling', 'remainder', 'pingala', '5th', 'wrote', 'treatise', 'sanskrit', 'prosody', 'chandas', 'shastra', 'virahanka', '6th', 'meter', 'marked', 'v4', 'ssl', 'sss', '}.', 'prefixed', 'v2', 'v3', 'ss', 'virahanka1', '[\"\"]', 'virahanka2', '[[\"\"],', '\"]]', 'virahanka3', '={', ':[\"\"],', ':[\"', '\"]}):', 'memoize', 'virahanka4', 'code_virahanka', 'iv', 'memoization', 'v1', 'wasteful', 'v20', '181', 'v40', '245', '986', 'filling', 'crucially', 'wasted', 'wastage', 'decorator', 'housekeeping', 'cluttering', 'recalculating', 'realize', 'matlab', 'sourceforge', 'net', 'lined', 'arange', 'pyplot', 'rgbcmyk', 'blue', 'cyan', 'magenta', 'bar_chart', 'ind', 'bar_groups', ']],', 'xticks', 'loc', 'ylabel', 'code_modal_plot', 'count_words_by_tag', 'tagged_words', 'visitor', 'agg', 'backend', 'raster', 'pixel', 'savefig', 'directs', 'png', \"('&\", \";')\", 'src', '\"/&', 'lanl', 'initializes', 'traversal', 'nx', 'shortest_path_distance', 'add_edge', 'hyponym_graph', 'graph_draw', 'draw_graphviz', 'node_size', 'node_color', 'with_labels', 'code_networkx', 'darkest', 'csv', 'sli', 'wo', 'intr', 'lifting', 'foot', 'wake', 'weik', 'intrans', 'cease', 'input_file', 'rb', \"...']\", 'dimensional', 'cube', 'transpose', '([[', ']])', 'matrix', 'latent', 'implicit', 'linalg', ',-', 'vt', 'svd', '([[-', '4472136', '89442719', '32455532', '16227766', '70710678', 'gaussian', 'agglomerative', 'dendrogram', 'mysql', 'pylucene', 'etree', 'imaplib', 'incremented', 'unnecessary', 'enumerated', 'essential', 'namespace', 'declared', 'recomputation', 'scratched', 'surface', 'stdtypes', 'importance', 'itertools', 'multimedia', 'guzdial', 'sys', 'harel', 'levitin', 'guidance', 'hunt', 'thomas', 'mcconnell', 'flanked', '__getitem__', 'winded', \"'?'].\", 'transform', \"'!'].\", 'sliding', 'limiting', 'pointed', 'inequality', 'montague', 'lexicographical', 'normalizes', 'helper', 'cmp_len', '[:],', 'word_table', \"[['']\", 'word_vowels', 'novel10', 'gematria', 'hidden', 'essene', 'gemcal', 'htm', 'letter_vals', '800', '666', 'shorten', 'proximity', 'itemgetter', 'uniquely', 'vanguard', 'vang', '-,', 'linkage', 'levenshtein', 'norvig', 'combinatorial', 'mathematics', 'c0', 'cn', 'σ0', 'cicn', 'nth', 'increase', 'zhao', 'zobel', 'authorship', 'identification', 'clintoneast', 'alphabetically', '---', 'ic', 'uniqueness', 'discarding', 'compression', 'justify', 'approximately', 'evenly', 'extractive', 'highest', 'orientation', 'p97', '1023', 'statistically', 'improbable', 'amazon', 'gp', 'sipshelp', '002679', 'keys_', 'raised', '...``', 'concatenated', '\":\"', 'outstanding', 'tagged_sents', 'disambiguates', 'grammarian', 'classifying', 'tagset', 'attache', 'pos_tag', 'cc', 'nn', 'jj', 'coordinating', 'queried', 'upenn_tagset', \".*').\", '.??', '?.', 'refuse', 'prp', 'vbp', 'vb', 'tense', 'deny', 'trash', 'homophone', 'pronounce', 'ski', 'race', 'commonplace', 'obscure', 'justification', 'bought', 'w1w', 'w2', 'heard', 'felt', 'mostly', 'worth', 'clothes', 'scrobbling', 'scrobble', 'str2tuple', 'tagged_token', 'commented', 'ap', 'nns', ',/,', 'ppo', 'atlanta', 'tl', 'purchasing', 'vbg', 'wdt', 'pps', '``/``', 'ber', 'ql', 'operated', 'vbn', 'inure', 'jjt', 'abx', \"''/''\", './.', 'nr', 'investigation', 'dti', 'irregularity', 'conll2000', 'confidence', 'nnp', \"(',',\", \"','),\", 'tagsets', 'sinica_treebank', 'ä', 'neu', 'åæ', 'nad', 'åç', 'nba', 'মহ', 'ি', 'ষ', 'ে', 'র', 'সন', '্', 'ত', 'া', 'ন', \"(':',\", 'sym', 'mac_morpho', 'jersei', 'atinge', 'xe9dia', 'conll2002', 'sao', 'paulo', 'vmi', \"('(',\", 'fpa', 'cess_cat', 'da0ms0', 'tribunal_suprem', 'np0000o', 'presenting', 'trained', 'simplified', 'adp', 'adposition', 'adv', 'conj', 'africa', 'numeral', '1991', 'prt', 'particle', 'ersatz', 'esprit', 'dunno', 'gr8', 'univeristy', 'brown_news_tagged', 'tag_fd', '30640', '12355', '11928', '11389', '6706', '3349', '2717', '2535', '2264', '2166', 'vd', 'vn', 'scotland', 'yesterday', 'recount', 'colonization', 'mary', 'impressed', 'teacher', 'word_tag_pairs', 'noun_preceders', 'confirms', 'referent', 'adjunct', 'fell', 'dot', 'stock', 'suddenly', 'stone', 'mouse', 'cheese', 'pizza', 'gusto', 'word_tag_fd', 'wt', 'rose', 'reported', 'paired', 'cfd1', 'cfd2', 'compared', 'priced', 'fined', 'paid', 'traded', 'clarify', 'participle', 'surrounding', 'accused', 'idx1', 'kicked', 'swiftly', 'idx2', 'head', 'vbz', \"']),\", 'predicate', 'falling', 'classifies', 'uncertain', 'watch', 'schoolhouse', 'rock', 'youtube', 'unsimplified', 'possessive', 'hl', 'headline', 'findtags', 'tag_prefix', 'tagged_text', 'tagdict', '137', '88', '85', '72', '[(\"', 'company', '$-', 'navy', 'army', 'gallery', 'league', 'sp', 'salary', 'eva', 'aya', 'ovum', '68', 'fort', 'dr', 'oak', 'basin', '101', '69', '46', 'janitor', 'taxpayer', 'dealer', 'idol', 'giant', 'bros', '.\\'\",', 'writer', 'offense', 'sacrifice', 'fund', 'master', 'communist', 'code_findtags', 'constructing', 'brown_learned_text', 'analytically', 'apt', 'became', 'brown_lrnd_tagged', ';\").', 'w1', 't1', 't2', 'w3', \"')):\", 'tagged_sent', 'protect', 'allowed', 'code_three_word_phrase', 'grant', 'cnj', 'near', 'correlate', 'associative', 'hash', 'hashmap', 'phonebook', 'ip', 'morph', 'analyzer', 'morpheme', 'specifically', 'hasn', 'legal', 'inherently', '\":\",', 'overwritten', 'normally', \"{['\", 'unhashable', 'defaultdict', '\")).', '_automatically', 'struggle', 'guarantee', 'unk', 'v1000', 'alice2', 'rabbit', 'sitting', '\",\\'\",', '\"?\\'\"', '1001', 'tolerate', 'incrementally', 'emulating', 'tallying', 'code_dictionary', '8336', 'schematic', 'my_dictionary', 'item_key', 'last_letters', 'abactinally', 'abandonedly', 'abasedly', 'abashedly', 'abashlessly', 'abbreviately', 'abdominally', 'abhorrently', 'abidingly', 'abiogenetically', 'abiologically', 'zy', 'blazy', 'bleezy', 'blowzy', 'boozy', 'breezy', 'bronzy', 'buzzy', 'chazy', 'anagram', 'aeilnrt', 'entrail', 'latrine', 'ratline', 'reliant', 'retinal', 'trenail', \"((''.\", 'initialization', ')][', 'iterated', 'inverting', '32', 'virtue', 'thine', 'mortal', 'pos2', 'realistic', 'peacefully', \"'})\", 'inverted', '([(', 'k1', 'k2', '...])', 'd1', 'd2', 'brown_tagged_sents', 'brown_sents', 'banal', 'establishes', 'baseline', 'egg', 'ham', 'sam', 'default_tagger', 'defaulttagger', 'unsurprisingly', 'eighth', '13089484257215028', 'gerund', '3rd', 'ould', 'md', \"*\\\\'\", \"$'),\", \"'^-?[\", ']+(\\\\.', \"]+)?$',\", 'cardinal', 'fifth', 'regexp_tagger', 'regexptagger', \"[('``',\", '20326391789486245', '«.', 'partially', 'automate', 'unigramtagger', 'most_freq_words', 'likely_tags', 'baseline_tagger', '45578495136941344', 'untagged', \"'``'),\", 'bedz', 'voter', 'word_freqs', 'words_by_freq', 'perfs', 'bo', 'varying', 'xlabel', 'code_baseline_tagger', 'rapidly', 'eventually', 'reaching', 'plateau', 'theme', 'sd', 'greatly', 'multiplied', 'impartial', 'judge', 'assessed', 'regarded', 'mistake', 'lead', 'guideline', 'undertaking', 'inevitably', 'depend', 'creator', 'neutral', 'maximize', 'usefulness', 'unigram', 'cafe', 'unigram_tagger', 'apartment', 'terrace', 'beg', 'entrance', 'bez', 'direct', '9349006503968017', 'memorized', 'perfect', 'useless', '%:', '4160', 'train_sents', 'test_sents', '811721', 'unigrams', 'isolation', 'priori', 'generalization', 'tn', 'circled', 'shaded', 'ngramtagger', 'bigram_tagger', 'bigramtagger', 'unseen_sent', '4203', 'population', 'congo', 'innumerable', 'dialect', 'manages', 'badly', 'overall', '102063', 'specificity', 'sparse', 'cross', 'accurate', 't0', '844513', 'trigramtagger', 'initialized', 'retain', 'cutoff', 'pkl', 'pickle', 'dump', 'wb', 'enterprise', 'maze', 'regulatory', 'rp', 'empirically', 'ambiguous_contexts', '049297702068029296', 'legitimately', 'test_tags', 'gold_tags', 'confusionmatrix', 'dropped', 'annotator', 'finer', 'deciding', 'church', 'bloothooft', '1996', 'imperfection', 'rise', '1990s', 'employed', 'strike', 'considers', 'impractical', 'conditioned', 'inductive', 'inventor', 'successively', 'transforms', 'compiles', 'transformational', 'bough', 'branch', 'twig', 'sky', 'canvas', 'broad', 'vocational', 'rehabilitation', 'phase', 'scored', 'corrects', 'interpretable', 'brill_demo', '6555', '------------------+-------------------------------------------------------', \"-'\", \"'*-\", '--------------------------+------------------------+--------------------------', 'guest', 'honor', 'speedway', 'hauled', 'dway', 'crew', 'official', 'indianapolis', 'fortun', 'ter', 'fortune', 'dro', 'drooled', 'schoolboy', 'olboys', 'code_brill_demo', 'instantiated', 'residual', 'belongs', 'ness', 'ill', 'illness', 'establish', 'morphologically', 'ongoing', 'incomplete', 'eating', 'suspicion', 'mainly', 'formalize', 'underpin', 'intuition', 'verjaardag', 'birthday', 'zij', 'vandaag', 'jarig', 'exact', 'acquire', 'cyberslacker', 'fatoush', 'blamestorm', 'sars', 'cantopop', 'bupkis', 'noughties', 'muggle', 'robata', 'belonging', 'gradually', 'morpho', 'receive', 'excursion', 'ungrammatical', 'processor', 'morphosyntactic', 'bem', 'ben', 'carrying', 'finely', 'variation', 'unavoidable', \"'}.\", 'repair', 'petrov', 'da', 'mcdonald', '2012', 'contiguous', 'brown_tagset', 'directive', 'indicating', 'emphasized', 'synthesis', 'ssml', 'wf', 'wnsn', 'measured', 'shape', 'configuration', 'contour', 'conformation', 'turin', 'italiano', 'progetto', 'realizzazione', 'primo', 'ordin', 'porto', 'turistico', 'dell', 'albania', 'communicative', 'user117', 'dude', 'ynquestion', 'user120', 'bye', 'gonna', 'user122', 'user2', 'slap', 'trout', 'user121', 'pm', 'tryin', 'spoof', 'british', 'waffle', 'falkland', 'juvenile', 'defendant', 'contest', 'opponent', 'clock', 'chase', 'xyz', \"']?\", 'deleting', 'deleted', 'interchanged', 'affixtagger', 'substituted', 'typesseq', 'removing', 'minimize', 'regard', \"')],\", 'eg', 'lab', 'qualifier', 'estimate', '102', 'varied', 'hadn', 'circumstance', 'happen', 'untouched', 'contribution', 'axis', 'semilogx', 'gradient', 'wordi', 'tagi', 'discriminate', 'epistemic', 'deontic', 'altogether', '$)', 'ignores', 'abney', 'impossibility', 'lidstone', 'laplace', 'anti', 'ngram', 'prevent', 'estimating', 'fold', 'validation', 'inherits', 'encapsulates', 'talk', 'x3', 'generative', 'joint', 'abstractly', 'ml', '\"(', 'lx', 'viewed', 'determinize', '12345', 'tend', 'indicative', 'salient', 'focusing', 'politics', 'river', 'financial', 'institution', 'tilting', 'depositing', 'jointly', 'extractor', 'distinctive', 'precisely', 'gender_features', ']}', 'shrek', 'booleans', 'prepare', 'labeled_names', \"')])\", 'featuresets', 'train_set', 'test_set', 'naivebayesclassifier', 'neo', 'trinity', '2199', 'conforms', 'expectation', '77', 'distinguishing', 'show_most_informative_features', '33', 'likelihood', 'retrain', 'apply_features', 'enormous', 'impact', 'decent', 'thorough', 'guided', 'sink', 'confuse', 'mentioning', 'valued', 'gender_features2', 'first_letter', 'abcdefghijklmnopqrstuvwxyz', '({})\".', \")':\", '...}', 'code_gender_features_overfitting', 'overfits', 'overfitting', 'relatively', 'relying', 'idiosyncrasy', 'problematic', 'overfit', '768', 'productive', 'refining', 'subdivided', 'train_names', '1500', 'devtest_names', 'test_names', 'devtest_set', 'tricking', 'adjusted', '={:&', 'abigail', 'cindelyn', 'katheryn', 'kathryn', 'aldrich', 'mitch', 'yn', 'predominantly', 'adjust', 'suffix1', 'suffix2', ':]}', 'rebuilding', 'dataset', '76', '%):', '782', 'repeated', 'trust', 'unused', 'categorizes', 'all_words', 'word_features', 'document_features', 'document_words', \"({})'.\", 'cv957_8737', 'waste', 'code_document_classify_fd', '81', 'neg', 'seagal', 'wonderfully', 'damon', 'code_document_classify_use', 'apparently', 'binning', 'tf', 'idf', 'most_informative_features', 'ek', 'crafted', 'suffix_fdist', ':]]', 'common_suffixes', 'er', \"'``',\", 'ion', 'pos_features', 'tinted', 'impossible', 'rely', 'exclusively', 'highlighted', 'decisiontreeclassifier', '62705121829935351', 'interpretability', 'pseudocode', '(,)', \"','\", '(.)', \"'.'\", 'ran', 'doctests', 'nondeterministic', \"$'\", '\",\".', 'certainly', '\".\").', 'linking', ']+(.', '->', 'augmenting', 'leverage', 'functioning', 'revise', 'dependent', '{\"', ')\":', '][-', 'prev', '()[', 'untagged_sent', 'untag', '78915962207856782', 'code_suffix_pos_tag', 'detector', 'learns', 'gubernatorial', 'reasonably', 'augment', 'proceed', 'consecutivepostagger', 'taggeri', 'featureset', '79796012981', 'code_consecutive_pos_tagger', 'commit', 'iteratively', 'inconsistency', 'markov', 'trillion', '3010', 'efficiently', 'chain', 'terminates', 'merged', 'punct_features', 'punct', \"'.?!']\", '936026936026936', 'segment_sentences', \"'.?!'\", 'code_classification_based_segmenter', 'straightforward', 'performative', 'forgive', 'bet', 'climb', 'greeting', 'clarification', 'emotion', 'continuer', 'xml_posts', '()[:', '10000', 'dialogue_act_features', '())]', 'yanswer', 'parviz', 'davudi', 'meeting', 'shanghai', 'organisation', 'sco', 'fledgling', 'bind', 'russia', 'soviet', 'republic', 'asia', 'terrorism', 'llc', 'nelson', 'beaver', 'chester', 'jennie', 'stewart', 'holder', 'carolina', 'analytical', 'laboratory', 'successful', 'ideal', 'captured', 'hyp_extra', 'motivates', 'ne', 'filtered', 'intro', 'rtefeatureextractor', '??', 'rte_features', 'rtepair', 'word_overlap', 'word_hyp_extra', 'ne_overlap', 'ne_hyp_extra', 'code_rte_features', 'bag', 'throwing', 'rte3_dev', \"'])[\", 'text_words', 'hyp_words', 'rte_classify', 'impressive', 'scaling', 'numerically', 'pure', 'unreasonable', 'recommend', 'interfacing', 'stuff', 'speak', 'additive', 'nature', 'parallelization', 'mapreduce', 'toy', 'trustworthy', 'guiding', 'misleadingly', 'balanced', 'skew', 'err', 'safety', 'consideration', 'reflects', 'somewhat', 'file_ids', 'stringent', 'predicts', 'interpreting', 'hardly', 'measuring', 'inter', 'attempting', 'irrelevant', 'outweighs', 'tp', '/(', 'fp', 'fn', 'harmonic', 'recreates', 'subdivide', 'diagonal', '|)', 'tag_list', 'apply_tagger', 'cm', 'pretty_format', 'sort_by_count', 'show_percents', 'truncate', '----+----------------------------------------------------------------+', '%&', ';|', 'reserve', 'varies', 'vary', 'skeptical', 'upside', 'dichotomous', 'flowchart', 'communicate', 'selects', 'arrive', 'conventionally', 'picking', 'decides', 'achieves', 'picked', 'sufficient', 'grow', 'leftmost', 'disorganized', '−', 'σl', 'labelsp', 'log2p', 'probs', '811', 'code_entropy', 'calculated', 'weighted', 'coherent', '>.', 'suited', 'hierarchical', 'categorical', 'phylogeny', 'prune', 'checked', 'automotive', 'murder', 'hasword', 'exponentially', 'repetition', 'weak', 'predictor', 'incremental', 'descended', 'overcomes', 'dark', 'indicator', 'strong', 'closest', 'voting', 'contributes', 'unrealistic', 'simplifying', 'independence', 'bayesian', 'maximizes', ')/', 'suffices', 'prodf', 'featuresp', ')`', 'equation', 'smoothing', 'toward', 'fit', 'basically', 'heldout', 'orange', 'height', 'bell', 'variance', 'naivete', 'dependence', 'correlated', 'deserves', 'precise', 'σx', ')|', 'σlabel', 'refine', 'optimal', 'generalized', 'gi', 'considerably', 'conjugate', 'cg', 'bfgs', ':?', 'applicable', 'receives', 'unlabeled', 'product', 'prodjoint', 'unwarranted', 'principle', 'dominates', 'vi', 'fewest', 'coming', 'vii', 'argmax_label', 'strictly', 'price', 'topographical', 'skyline', 'powerpoint', 'copyrighted', 'topo', 'largely', 'neural', 'opaque', 'ass', 'deemed', 'sufficiently', 'explanatory', 'interchangeable', 'polar', 'detest', 'postulate', 'causal', 'construction', 'worrying', 'helping', 'tune', 'unrealistically', 'optimistic', 'weka', 'mallet', 'tadm', 'megam', 'alpaydin', 'mathematically', 'intense', 'hastie', 'tibshirani', 'friedman', 'daelemans', 'bosch', 'feldman', 'sanger', 'segaran', 'wei', 'manning', 'schutze', 'raghavan', 'coded', 'naively', 'increasing', 'kiusalaas', 'agirre', 'edmonds', 'melamed', 'croft', 'metzler', 'strohman', 'driven', 'sponsored', 'competition', 'ace', 'aquaint', 'waikato', 'nz', '002003', 'publication', 'xin', 'l2r', 'uiuc', 'danr', 'liro05a', 'adaptation', 'unsupervised', 'weakly', 'ref', '6900', 'chip', 'indication', 'infrequently', 'ppattachment', 'ppattach', 'noun1', 'noun2', 'chairman', 'inst', \".')\", 'nattach', 'connect', 'researcher', 'jar', 'cupboard', 'campus', 'letterman', 'ch06', '1264', 'truly', 'staggering', 'complexity', 'located', 'predictable', 'orgname', 'locationname', 'omnicom', 'ddb', 'needham', 'kaplan', 'thaler', 'bbdo', 'south', 'georgia', 'pacific', '?\"', 'locs', 'e1', 'rel', 'e2', \"=='\", 'ieer', 'nyt19980315', 'packaged', 'corp', 'arrived', 'hertz', 'channel', 'worldwide', 'corporate', 'advertising', 'brand', 'soft', 'toilet', 'tissue', 'sparkle', 'towel', 'ken', 'haldin', 'spokesman', 'glean', 'reap', 'sql', 'resume', 'harvesting', 'patent', 'scanning', 'electronically', 'biology', 'medicine', '.:', 'prove', \"']).\", 'ie_preprocess', 'participate', 'definite', 'ni', 'indefinite', 'recording', 'omits', 'chunker', 'chunkers', 'hardware', 'fragmented', 'nnps', 'nominal', 'graphically', 'barked', '\")]', '{&', ';?&', ';*&', ';}\"', 'cp', 'regexpparser', 'code_chunkex', 'delimited', 'sharp', 'dive', 'policy', 'jjr', 'panamanian', 'dictator', 'manuel', ';+.', 'mansion', '%/', 'fastest', 'jjs', 'chunkparser', 'flat', '\\\\$&', ';+}', 'rapunzel', '$\"),', 'golden', 'code_chunker1', 'escaped', '$.', 'permissive', ';+}.', 'tracing', 'interrogate', \";}')\", 'subtree', 'subtrees', 'overtake', 'encapsulate', 'find_chunks', ',}}\"', 'chinking', 'chink', 'periphery', 'remains', 'excise', '}&', ';+{', 'code_chinker', 'chinker', 'talked', 'anyway', 'chunked_sents', 'chunk_types', '5810414336070245', 'ch07', 'seemed', 'befits', 'status', 'iob', 'suffixed', 'constituent', 'manipulated', 'expanding', 'vp', 'conllstr2tree', 'moreover', 'carlyle', 'merchant', 'banking', '100th', './', './.)', 'delivered', '(\"\")', 'chunkparse', ';[', 'cdjnp', ';+}\"', 'unigramchunker', 'chunkparseri', 'constructor', 'train_data', 'tree2conlltags', 'pos_tags', 'tagged_pos_tags', 'chunktags', 'chunktag', 'conlltags', 'conlltags2tree', 'code_unigram_chunker', 'unigram_chunker', '92', 'achieving', 'postags', \"[('#',\", \"('$',\", \"(')',\", 'pdt', 'rbr', 'uh', 'wp', 'wrb', 'discovered', '$,', 'bother', 'bigramchunker', 'bigram_chunker', 'joey', 'farmer', 'rice', 'maxentclassifier', 'wrapper', 'consecutivenpchunktagger', 'npchunk_features', 'consecutivenpchunker', '[[((', 'code_classifier_chunker', 'config_megam', 'prevword', 'prevpos', 'hypothesized', 'reduction', 'rate', 'lookahead', 'nextword', 'nextpos', '+%', 'tags_since_dt', \"'+'.\", '96', '91', 'cascaded', ';+$}', 'sit', ')))))', 'code_cascaded_chunker', 'headed', '_saw', ')))))))', 'cascading', 'deep', 'cascade', 'reachable', 'distinguished', 'standardly', 'metaphor', 'sibling', 'chased', '))))', 'homogeneous', 'discourse', 'tree1', 'tree2', 'tree3', 'tree4', 'zoom', 'postscript', 'inclusion', \")))')\", 'code_traverse', 'duck', 'gpe', 'geo', 'province', 'eddy', 'bonte', 'obama', 'murray', 'mount', 'everest', 'june', '175', 'gbp', 'pct', 'stonehenge', 'east', 'midlothian', 'ner', 'prelude', 'qa', 'recovering', 'isolate', 'minimal', 'retrieved', 'passage', 'prominent', 'attraction', 'alexandria', 'getty', 'prone', 'sanchez', 'dominican', 'vietnam', 'existence', 'contemporary', 'dior', 'yankee', 'infielder', 'posed', 'cecil', 'escondido', 'village', 'woordvoerder', 'van', 'diezelfde', 'hogeschool', 'punc', 'ne_chunk', 'brooke', 'mossman', '...)', 'purchased', 'ldc2003t11', 'approaching', 'α', 'intervenes', '!\\\\', '.+', 'disregard', 'supervising', 'transition', \"'.*\\\\\", 'bin', \")')\", 'parsed_docs', 'nyt_19980315', 'extract_rels', 'rtuple', 'whyy', 'mcglashan', 'sarrail', 'firm', 'san', 'mateo', 'arlington', 'brookings', 'idealab', 'incubator', 'angeles', 'waterloo', 'wgbh', 'boston', 'bastille', 'opera', 'paris', 'transportation', 'committee', 'secured', '];', 'excluding', 'filler', 'devise', 'clausal', 'relsym', 'vnv', 'zijn', 'werd', 'wordt', 'worden', '\"))', 'cornet_d', 'elzius', 'buitenlandse_handel', 'johan_rottiers', 'kardinaal_van_roey_instituut', 'annie_lennox', 'eurythmics', 'lcon', 'rcon', 'intervene', 'populate', '../', 'muc', '(``', 'garcia', 'alvarado', '``),', 'killed', '``).', 'intervening', 'due', 'pioneering', 'vinartus', 'spa', '97a', '1975', 'tukey', 'bio', 'ramshaw', 'marcus', 'ananiadou', 'mcnaught', 'generalizing', 'solely', 'receiving', 'assistant', 'devising', 'coordinated', 'august', 'supervisor', 'adjudicator', 'chunkscore', 'missed', 'regexpchunk', 'merging', 'erroneous', \"')...\", 'treebank_chunk', 'tree2conllstr', 'chunk2brackets', 'chunk2iob', 'bracket2iob', 'iob2bracket', 'resp', 'speculate', 'anymore', 'preliminary', 'decided', 'computationally', 'expensive', 'mdash', 'ingredient', 'cope', 'finite', 'dilemma', 'stressed', 'daily', 'gigantic', 'uttered', 'telegraph', '2387900', 'argue', 'imaginary', 'judgement', 'reject', 'agree', 'perfectly', 'usain', 'bolt', '100m', 'jamaica', 'observer', 'andre', 'ingenuity', 'winnie', 'pooh', 'milne', 'joy', 'ship', 'liked', 'terrible', 'hour', 'imprisonment', 'owl', 'flown', 'comfort', 'aunt', 'seagull', 'quietly', 'slipping', 'hanging', 'toe', 'luckily', 'sudden', 'loud', 'squawk', 'woke', 'jerk', 'brain', 'captain', 'mate', 'rescue', 'indefinitely', 'concoct', 'intertwined', 'competent', 'ubiquitous', 'groucho', 'marx', 'cracker', '1930', 'hunting', 'shot', 'elephant', 'pajama', 'groucho_grammar', '(\"\"\"', '\"\"\")', 'chartparser', '))))))', 'depict', '3b', 'fighting', 'tiresome', 'blame', 'investigating', 'formedness', 'degenerate', 'childrens', '22816', 'roared', 'slip', 'worst', 'clumsy', 'whoever', 'intuitively', 'salad', 'coordinate', 'coordination', 'conjoined', 'aps', 'conjoin', 'substitutability', 'rendering', 'fat', 'brook', 'substitute', 'grammaticality', 'reproduces', 'topmost', 'bounded', 'admitted', 'grammar1', 'bob', 'telescope', 'park', 'rd_parser', 'recursivedescentparser', 'code_cfg1', 'righthand', 'descent', 'rdparser', 'autostep', 'button', 'structurally', 'attached', 'balcony', 'overlooking', 'cfgs', 'mygrammar', 'disallowed', 'new_york', 'indirect', 'grammar2', 'propn', 'chatterer', 'joe', 'squirrel', 'angry', 'frightened', 'tall', 'code_cfg2', '10a', '10b', 'beware', 'conform', 'licensed', 'fringe', 'psycholinguistic', 'submitted', 'undergo', 'shift', 'subgoals', 'expansion', 'expands', 'hence', 'consults', 'enlarge', 'backtracks', 'dangling', 'backtrack', 'send', 'infinite', 'backtracking', 'rebuilt', 'proceeds', 'popping', 'push', 'popped', 'reducing', 'srparser', 'shifting', 'succeeds', 'shiftreduceparser', 'guaranteed', 'exists', 'verbosely', 'sr_parser', 'sr_parse', 'undone', 'resolving', 'conflict', 'favoring', 'lr', 'doublerightarrow', 'wrongly', '12c', 'pointless', '12a', '12b', '⇒*', 'trapped', 'preprocesses', 'suffer', 'completeness', 'remedy', 'subconstituent', 'wfst', 'reminiscent', 'triangular', 'denote', 'a0a1', '`+', 'ch08', 'rh', 'init_wfst', 'numtokens', 'lh', 'complete_wfst', 'mid', 'nt1', 'nt2', '(\"[%', '[%', '==&', ']\"', ')],', 'nwfst', '((\"%-', '4d', '(\"%-', 'wfst0', 'wfst1', 'code_wfst', 'acceptor', 'nonterminal', 'propose', 'structural', 'slighly', 'asymmetric', 'tensed', 'arc', 'sbj', 'nmod', 'groucho_dep_grammar', 'dependencygrammar', 'projective', 'crossing', 'descendent', 'pdp', 'projectivedependencyparser', 'proposed', 'appealing', 'embody', 'formalism', 'valency', '15d', '16d', '.*', 'imagination', 'historical', 'tradition', 'psg', 'overly', 'constraining', 'subcategories', 'subcategorized', 'tv', 'excluded', 'intransitive', 'datv', 'dative', 'sv', 'sentential', 'contrasted', 'modifer', '17d', 'scaled', 'succinct', 'modularize', 'distribute', 'collaborative', 'lfg', 'pargram', 'hpsg', 'lingo', 'lexicalized', 'adjoining', 'xtag', 'parsed_sents', '(,', ',)', 'adjp', ',))', 'clr', '(.', '.))', 'child_nodes', 'code_sentential_complement', \"]['\", 'amongst', 'rejected', 'pe08', 'prepared', 'large_grammars', 'sinica', '3450', 'pernicious', 'astronomical', 'sbar', '132', '429', '430', '862', '796', '786', '208', '012', '1012', 'effortlessly', 'patil', '1982', 'spelled', 'overwhelmed', 'gibberish', 'klavans', 'resnik', 'hundredth', 'hectare', 'sq', 'designating', 'drawing', 'paddock', 'anticipated', 'explains', 'horrendous', 'inefficiency', 'seemingly', 'innocuous', 'mysterious', 'sheer', 'recipient', '19a', '19b', 'heebie', 'jeebies', 'dtv', \"')\\\\\", 'print_node', '%\\\\', 'chef', 'ovation', 'advertiser', 'discount', 'politician', 'consumer', 'straight', 'scoop', 'crisis', 'mitsui', 'tech', 'medical', 'mitsubishi', 'foster', 'gift', 'suspend', 'trading', 'futu', 'quick', 'approval', 'billion', 'supplemental', 'appr', 'obligation', 'cal', 'qualified', 'rating', '``...', 'veto', 'code_give', 'tendency', 'animacy', 'contributing', 'surveyed', 'bresnan', 'jack', 'code_pcfg1', 'impose', 'obeys', 'viterbi_parser', 'viterbiparser', \"']):\", '064', 'characterization', 'α1', 'αn', 'grammatically', 'inefficient', 'substructure', 'globally', 'radford', '1988', 'gentle', 'unbounded', '1965', 'jacob', 'rosenbaum', '1970', 'explored', 'jackendoff', '1977', 'prime', 'typographically', 'demanding', 'burton', 'robert', '1997', 'practically', 'constituency', 'exemplification', 'huddleston', 'pullum', 'levin', 'www2', 'parc', 'istl', 'nltt', 'delph', 'partner', 'prohibition', 'dana', 'cheered', 'underlining', 'boost', 'cleverer', 'buffalo_buffalo_buffalo_buffalo_buffalo_buffalo_buffalo_buffalo', 'listener', 'list_of_homophonous_phrases', 'garden', 'garden_path_sentence', 'draw_trees', 'influence', 'claimed', 'regularity', 'youngish', 'nikolay', 'parfenovich', 'sincere', 'liking', 'discriminated', 'procurator', 'karamazov', '[[[', ',]', 'n3', 'ascent', 'complementation', 'dumped', 'filled', 'inheriting', 'parsei', 'recurse', '4th', 'callback', 'unify', 'dicts', 'mixture', 'featurevaluetuple', '=(', \")]'\", 'featurevalueset', \"}]'\", 'descend', 'binding', 'featurevalueunion', \"'{?\", '+?', 'subcat', 'doable', ':-(', 'revisited', 'subsection', 'subcategorization', 'shouldn', 'reorganised', 'generalise', '(?)', 'generalised', 'inv', 'genitive', 'pity', 'gpsg', 'garbled', 'ereferences', 'bibliography', 'grosz', 'stickel', '1983', 'dahl', 'saint', 'dizier', '1985', 'flexibility', 'atomic', 'decompose', 'orth', 'orthography', 'exhaustive', 'obj', 'agt', 'lex2fs', 'subj', '=&', 'experiencer', 'exp', 'surprised', 'hoc', 'elegantly', 'offered', 'expressiveness', 'demonstrative', '2b', 'deletion', 'controller', 'paradigm', '2nd', 'agrees', 'sg', 'ch09', '252', '257', 'blocking', 'np_sg', 'vp_sg', 'np_pl', 'vp_pl', 'det_sg', 'n_sg', 'det_pl', 'n_pl', 'v_sg', 'v_pl', 'vps', 'counterpart', 'aesthetically', 'unappealing', 'doubling', 'unattractive', 'blowing', 'referring', 'spoke', 'informally', '=?', 'admit', '11a', '11b', '13a', '13b', 'prohibited', 'incompatibility', 'choosy', 'underspecified', 'letting', 'economical', 'explicitly', 'feat0', 'fcfg', 'show_cfg', 'book_grammars', '###################', ']-&', 'jody', 'pres', 'disappears', 'disappeared', 'code_feat0cfg', 'experimentation', 'load_parser', '|.', 'chil', '.|', 'init', '|[----]', '[----]', '[----]|', '|[----&', '{?', '[----&', '[---------]|', '|[==============]|', 'code_featurecharttrace', 'admissible', 'flow', 'upwards', 'assembles', 'instantiate', 'subpart', 'aux', '=+]', 'abbreviates', '=+', '=-,', 'attaching', 'radical', 'agr', 'gnd', 'avms', 'athough', 'visually', 'pleasing', 'stick', 'significance', 'bundled', 'cop', 'featstruct', 'fs1', 'acc', 'fs2', '(\"[', '\\']]\"))', 'telno', 'divert', 'groundwork', 'acyclic', 'dag', 'rue', 'pascal', 'spouse', 'sharing', 'reentrancy', '(\"\"\"[', ')]]\"\"\"))', 'coindex', ')]\"))', ')[],', '->(', ')]\")', '_ex', 'reentrant03', 'subsumption', '23a', '23b', '23c', '.[', 'fs0', 'subsumes', '⊑', 'reentrancies', 'incommensurable', 'subsumed', '25b', '25a', '25c', 'formally', '⊔', 'symmetric', 'repr', 'unifying', 'π', 'interacts', '\\']]]\"\"\")', ']]]\")', 'unified', ')]]\"\"\")', 'stated', 'address1', '\\']]\")', 'address2', ']\")', 'augmented', 'adopts', 'mnemonic', 'correspondence', 'belong', 'comp', 'categorial', 'patr', 'encodes', ';]', 'comprises', 'discharged', 'traditionally', 'verbal', 'factoring', 'abstracting', 'phrasal', '34a', '34b', 'projection', 'maximal', '35', 'inversion', 'switched', 'interrogative', 'rarely', '39', 'positioned', '[+', 'slot', 'ungrammaticality', 'music', '45a', 'preposed', '45b', '__', 'hip', 'hop', 'occurence', 'termed', 'constellation', 'percolated', 'dominated', 'fortunately', 'reducible', 'feat1', ']/?', 'code_slashcfg', 'percolate', 'slashed', ']/', '[]/', 'admits', 'masc', 'neut', 'gen', 'dat', 'nominative', 'accusative', 'helfen', '1693', '1698', '1702', '1707', 'comprising', 'objcase', 'hunde', 'hunden', 'katze', 'katzen', 'ich', 'mich', 'mir', 'wir', 'ihr', 'komme', 'kommst', 'kommt', 'kommen', 'sehe', 'mag', 'siehst', 'magst', 'sieht', 'folge', 'helfe', 'folgst', 'hilfst', 'folgt', 'hilft', 'sehen', 'moegen', 'moegt', 'folgen', 'helft', 'code_germancfg', 'governs', 'failure', 'fol', 'kat', '|[---]', '[---]', '[---]|', '|[---&', '[---&', 'scanner', 'admitting', ']].', 'realization', 'constrain', '[+/-', 'entrant', 'earliest', 'phonological', 'labial', 'voice', 'chomskyan', 'advocated', 'gazdar', 'elaborated', 'primarily', 'shieber', 'algebraic', 'attempted', 'negation', 'pioneered', 'kasper', 'johnson', 'argues', 'integral', 'huang', 'chen', '1989', 'sag', 'wasow', 'cl', 'uni', 'bremen', 'bib', 'permissible', 'anomalous', 'deficiency', 'stipulate', 'subtype', 'emele', 'zajac', 'examination', 'carpenter', 'copestake', 'implementing', 'copious', 'nerbonne', 'netter', '1994', '{\\\\\"', 'ller', 'integration', 'sings', 'precious', '2028', '2033', '2038', '2043', 'earleychartparser', 'reentrance', '~', '08', ']]\")', 'fs3', 'fs4', 'fs5', 'fs6', 'fs7', 'fs8', 'fs9', 'fs10', 'code_featstructures', 'subsume', 'heute', 'cart', 'sand', 'conjugation', 'redundantly', 'whereby', '--&', 'book_1ed', 'facing', 'tracker', 'face', 'obstacle', 'workflow', 'lifecycle', 'crawling', 'texas', 'mit', 'acoustic', 'eight', 'educational', 'greasy', 'wash', 'oily', 'rag', 'phonetically', 'diphones', 'comparability', '160', 'recorded', 'transcription', 'customary', 'dr1', 'fvmh0', 'sa1', \"#',\", 'iy', 'hv', 'ae', 'dcl', 'aa', 'kcl', 'ux', 'tcl', 'gcl', 'epi', 'dx', 'ao', 'ih', \"#']\", 'word_times', '7812', '10610', '14496', '15791', '20720', '25647', '26906', '32668', '37890', '38531', '42417', '43091', '46052', '50522', 'timitdict', 'transcription_dict', 'ao1', 'axr', 'england', 'demographic', 'permitting', 'vocal', 'spkrinfo', 'speakerinfo', 'vmh0', 'sex', 'trn', 'recdate', 'birthdate', 'ht', 'wht', 'envisaged', 'sociolinguistics', 'judgment', 'revision', 'subdirectory', 'aks0', 'wav', 'schematically', 'subfields', 'amid', 'fictional', 'narrative', 'biased', 'incorporates', 'mini', 'cycle', 'converter', 'creation', 'unfolds', 'elicitation', 'archival', 'computerization', 'boon', 'decade', 'routinely', 'experimental', 'norm', 'funded', 'curation', 'anc', 'bnc', 'reliance', 'mundane', 'regularly', 'devised', 'validated', 'specialist', 'uncertainty', 'consistently', 'reveal', 'differing', 'adjudicated', 'exercised', 'exceptional', 'kappa', '333', 'rectangle', 's1', 's2', 's3', 'windowdiff', 'awarding', 'scorer', '00000010000000001000000', '00000001000000010000000', '00010000000000000001000', '571', 'slide', 'summed', 'shrink', 'sensitivity', 'evolution', 'disfluency', 'intonation', 'recycling', 'desire', 'replication', 'naturalistic', 'renaming', 'retokenizing', 'enrich', 'onerous', 'enriching', 'integrate', 'confronts', 'aligning', 'chaotic', 'centrally', 'curated', 'periodic', 'interval', 'submission', 'publishing', 'standoff', 'rearranged', 'silently', 'obtaining', 'sigwac', 'reproducible', 'desired', 'gnu', 'wget', 'crawler', 'heritrix', 'mime', 'cyclic', 'latency', 'overloading', 'banned', 'validate', 'maximized', 'authoring', 'macro', 'capable', 'verifying', 'proprietary', '...\".', 'msonormal', 'mso', 'spacerun', '[&', 'spelle', '0pt', '.&', 'legal_pos', 'used_pos', 'illegal', ';([', ']+)&', '1252', 'illegal_pos', 'iceberg', 'maintainer', 'lexical_data', 'html_file', '_entry', 'dict1', 'writerows', 'code_html2csv', 'microsoft', 'gzip', '+\".', 'gz', '\",\"', 'f_out', 'rosettaproject', 'fledged', 'validity', 'declaring', 'schema', 'dominant', 'structuring', 'unworkable', '...\"', 'defn', 'defns', 'defn_words', \"['...',\", 'arrives', 'isomorphic', 'transliterate', 'digested', 'discarded', 'defn_word', 'idx_file', 'idx_words', 'idx_line', '\"{}:', 'vexing', 'unavoidably', 'loosing', 'inject', 'tripping', 'unambiguously', 'coreference', 'rhetorical', \"'/&\", 'forge', 'expressive', 'rootedness', 'connectedness', 'acyclicity', 'invalidate', 'obliterate', 'litter', 'filespaces', 'founded', 'premise', 'focussing', 'viz', 'innovation', 'filesystem', 'disturbing', 'insulates', 'dissemination', 'expedient', 'treasure', 'embodied', 'oral', 'nuance', 'threatened', 'remnant', 'subspecies', 'therapeutic', 'breathtaking', 'colorful', 'tapestry', 'stretching', 'extinction', 'facet', 'heritage', 'recognizers', 'priority', 'voiced', 'curating', 'vexed', 'owns', 'literary', 'tandem', 'continually', 'overriding', 'confusible', 'kw', \"('[\", \"'(.\", ')\\\\', 'signature', 'repl', \"('[^\", '))[:', 'illefent', 'lfnt', 'ebsekwieous', 'bskws', 'nuculerr', 'nclr', 'anicular', 'inocular', 'nucellar', 'nuclear', 'unicolor', 'uniocular', 'unocular', 'fuzzy_spell', 'sig', 'olefiant', 'oliphant', 'elephanta', 'obsequious', 'nucular', 'gleaning', 'thanks', 'cetacean', 'mammal', 'streamlined', 'breathing', 'blowhole', 'closing', ';;', 'nicely', 'past_tense', 'akin', 'existential', 'modeled', 'abstracted', 'panacea', 'elementtree', 'merchant_file', '163', ';?', '\"?&', 'href', ';!--', 'venice', 'stagedir', 'antonio', 'salarino', 'salanio', 'sooth', ':&', 'getchildren', '0x10ac43d18', '_element', '0x10ac43c28', 'persona', '0x10ac43bd8', 'scndescr', '0x10b067f98', 'playsubt', '0x10af37048', '0x10af37098', '0x10b936368', '0x10b934b88', '0x10cfd8188', '0x10cfadb38', 'subtitle', '0x10cfd8228', '0x10cfb02c8', '0x10cfb0318', 'portia', '0x10cfb0368', 'mercy', 'strain', 'romeo', 'juliet', 'doth', 'fading', 'air', 'merry', 'hear', 'ear', 'trusted', 'madam', 'musician', 'wren', 'navigating', 'speaker_seq', 'speaker_freq', 'top5', 'shylock', 'bassanio', '73', 'gratiano', 'lorenzo', 'oth', 'speaker_seq2', 'anto', 'bass', 'grat', 'shyl', '153', 'exchange', 'largest', '0x10b2f6958', 'kaakaaro', 'kaakaaviko', 'kaakaavo', 'kaakaoko', 'kaakasi', 'kaakau', 'kaakauko', 'kaakito', 'kaakuupato', 'kuvuto', 'stdout', 'util', 'elementtree_indent', 'isi', 'cooking', 'kukim', 'itoo', 'sf', 'flora', 'aug', 'taeavi', 'iria', 'kovopaueva', 'kaparapasia', 'planim', 'gaden', 'tasol', 'paia', 'planted', 'cook', 'findtext', ';%', 'kakae', ';??', '?&', 'kakaevira', 'kakapikoa', 'kakapikoto', 'newborn', 'kakapu', 'sling', 'kakapua', 'kakara', 'arm', 'kakarapaia', 'kakarau', 'frog', 'trivially', '635', 'documentary', 'cvcvcvv', 'subelement', \"'[^\", 'v_', 'add_cv_field', 'cv_field', 'to_sfm_string', 'kaeviro', 'lift', 'antap', 'motion', 'nt', 'plane', 'jun', 'pita', 'kaeviroroe', 'kepa', 'kekesia', 'oa', 'vuripierevo', 'kiuvu', 'lukim', 'haus', 'win', 'bagarapim', 'destroyed', 'cvvcvcv', 'code_add_cv_field', 'safer', 'validating', 'practicable', 'field_sequences', \"(':'.\", 'rt', 'conformance', \"'+'\", \"('''\", 'sem_field', 'eng', 'ex_pidgin', 'ex_english', 'cmt', \"''')\", 'validate_lexicon', 'ignored_tags', 'marker_list', '(\"+\",', '(\"-\",', 'arg', 'code_toolbox_validation', 'lexfunc', 'lf', ';(&', 'lv', 'ln', ';*)*}', 'rf', 'xv', 'xn', ';*}', 'sn', 'gv', 'dv', 'dn', 'rn', 'hm', ';+&', 'toolboxdata', 'db', 'iu_mien_samp', 'code_chunk_toolbox', 'iu', 'mien', 'aggregation', 'physical', 'dublin', 'core', 'initiative', 'interdisciplinary', 'consensus', 'oai', 'scholarly', 'surrogate', 'server', 'offering', 'archived', 'protocol', 'harvest', 'partnership', 'virtual', 'archiving', 'interoperating', 'housing', 'ensured', 'controlled', 'descriptor', 'xmlns', '/\"', 'purl', 'dc', 'dcterms', 'xsi', 'xmlschema', 'schemalocation', 'xsd', 'kayardild', 'tangkic', 'evans', 'nicholas', 'gyd', '3110127954', 'berlin', 'mouton', 'gruyter', 'hardcover', '837', '0646119966', 'language_description', 'dcmitype', 'participating', 'harvested', 'consultation', 'rec', 'bpr', 'callhome', 'ldc97l18', 'multilex', 'icp', 'inpg', 'm0001', 'slelex', 'siemens', 's0048', 'korean', 'interlinear', 'glossed', 'uri', 'registered', 'icann', 'local_id', 'disseminating', 'upload', 'variability', 'interchange', 'shortcut', 'ide', 'suderman', 'lingua', 'graecae', 'tlg', 'childes', 'macwhinney', 'lamel', 'proceeding', 'promotes', 'cleaneval', 'sigann', 'encouraging', 'interoperability', 'buseman', 'ddp', 'tamanji', 'hirotani', 'simon', 'latech', 'zvon', 'olif', 'survey', 'proposal', 'thompson', 'mckelvie', 'farghaly', 'artstein', 'poesio', 'pevzner', 'deletes', 'sanitize', 'reduplication', '(..+)\\\\', 'syl', 'gl', \"'][\", 'xrf', 'referencing', ';,&', 'mislead', 'equipped', 'exciting', 'endeavor', 'spite', 'attest', 'hiding', 'sun', 'faulkner', 'dying', '1935', 'toaster', 'exhaust', 'dormitory', 'amiodarone', 'inhibited', 'cyp2c9', 'cyp2d6', 'cyp3a4', 'mediated', 'ki', '271', 'μm', 'pmid', '10718780', 'iraqi', 'earnest', 'prayer', 'righteous', 'wonderful', '16b', 'twas', 'brillig', 'slithy', 'toves', 'gyre', 'gimble', 'wabe', 'lewis', 'jabberwocky', '1872', 'afaik', 'discipline', 'criticism', 'philosophy', 'anthropology', 'psychology', 'hermeneutics', 'forensics', 'telephony', 'pedagogy', 'archaeology', 'cryptanalysis', 'pathology', 'methodology', 'deepen', 'intellect', 'manifested', 'barely', 'rectify', 'grew', 'dating', '1900s', 'frege', 'russell', 'wittgenstein', 'tarski', 'lambek', 'carnap', 'amenable', 'automaton', 'pushdown', 'symbolic', 'proof', 'theoretic', 'propositional', 'intepret', 'denoting', 'quantifier', '∀,', '∀', 'machinery', 'compositionality', 'composed', 'outlined', 'relies', '1980s', 'practitioner', 'prolog', 'eclipsed', 'emulated', 'typified', 'halle', '1968', 'hopelessly', 'hugely', 'assisted', 'quantitatively', 'embraced', 'coupled', 'metaphysical', 'debate', 'rationalism', 'empiricism', 'realism', 'idealism', 'enlightenment', 'western', 'backdrop', 'orthodox', 'believed', 'divine', 'revelation', 'seventeenth', 'eighteenth', 'philosopher', 'argued', 'sensory', 'descartes', 'leibniz', 'rationalist', 'asserting', 'truth', 'origin', 'innate', 'implanted', 'birth', 'euclidean', 'geometry', 'supernatural', 'locke', 'empiricist', 'faculty', 'reflecting', 'galileo', 'planet', 'solar', 'heliocentric', 'geocentric', 'introspection', 'enshrined', 'kant', 'realist', 'perception', 'idealist', 'intrinsically', 'unobservable', 'betrays', 'occupy', 'territory', 'lean', 'shed', 'alive', 'nuanced', 'polarized', 'balancing', 'innately', 'analogical', 'circle', 'roadmap', 'suppletion', 'concatenative', 'duplication', 'expense', 'vibrant', 'encompassing', 'inheritance', 'conservative', 'nlg', 'heterogeneous', 'evolving', 'curate', 'liberating', 'spend', 'contrib', 'housed', 'nltk_contrib', 'imperfect', 'mainstream', 'tackled', 'envoi', 'profound', 'elusive', 'scientist', 'striving', 'fluency', 'unfortunate', 'concluded', 'broadly', 'pathway', 'hacking']\n"
     ]
    }
   ],
   "source": [
    "codebook = collect_words_eng(sentence)\n",
    "print('codebook = ',codebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 文書ベクトル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vectors[0] = [13, 3, 57, 14, 23, 104, 11, 436, 2, 5, 1, 5, 1, 4, 4, 4, 300, 1, 1, 1, 1, 1, 2, 1, 97, 3, 2, 40, 5, 189, 1, 2, 222, 3, 11, 3, 2, 16, 8, 37, 5, 3, 8, 10, 6, 40, 4, 20, 19, 2, 16, 7, 21, 7, 121, 8, 84, 1, 2, 2, 26, 4, 21, 6, 1, 1, 11, 62, 26, 48, 46, 1, 2, 2, 32, 1, 2, 1, 5, 1, 1, 1, 1, 62, 1, 2, 19, 1, 1, 10, 7, 219, 133, 2, 1, 1, 2, 5, 1, 2, 1, 3, 1, 1, 1, 1, 1, 2, 6, 17, 3, 2, 2, 5, 1, 14, 1, 13, 10, 2, 2, 3, 1, 5, 21, 2, 1, 2, 3, 4, 1, 12, 229, 1, 2, 1, 2, 11, 2, 4, 2, 2, 26, 2, 2, 1, 31, 2, 1, 1, 8, 15, 26, 1, 59, 1, 1, 1, 1, 4, 10, 8, 3, 3, 18, 2, 3, 3, 11, 2, 3, 5, 1, 1, 1, 3, 4, 11, 8, 3, 1, 10, 9, 1, 3, 1, 1, 1, 26, 2, 2, 1, 1, 1, 2, 5, 7, 5, 2, 2, 2, 4, 14, 7, 1, 92, 1, 2, 7, 38, 5, 2, 2, 25, 22, 48, 31, 2, 56, 3, 1, 3, 41, 1, 1, 1, 6, 3, 1, 1, 1, 5, 33, 1, 1, 1, 10, 3, 6, 2, 1, 2, 1, 4, 25, 1, 29, 11, 4, 3, 30, 2, 1, 6, 8, 4, 3, 4, 4, 4, 5, 1, 1, 1, 1, 1, 1, 5, 11, 3, 3, 1, 17, 3, 10, 1, 22, 1, 14, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 14, 5, 24, 48, 13, 10, 3, 1, 3, 2, 2, 29, 4, 14, 1, 5, 25, 2, 2, 4, 17, 12, 1, 1, 1, 1, 2, 7, 1, 1, 14, 1, 3, 2, 4, 3, 4, 1, 5, 3, 2, 8, 3, 1, 1, 2, 2, 13, 3, 1, 2, 1, 2, 20, 6, 1, 4, 3, 1, 1, 1, 1, 1, 30, 18, 2, 3, 1, 1, 1, 2, 2, 1, 12, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 1, 5, 4, 3, 1, 2, 4, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 164, 1, 1, 2, 1, 1, 1, 67, 42, 6, 20, 62, 26, 32, 2, 1, 1, 5, 2, 1, 1, 22, 1, 1, 3, 10, 21, 1, 1, 14, 2, 1, 24, 2, 1, 1, 2, 1, 1, 1, 11, 27, 2, 3, 1, 8, 4, 2, 130, 8, 1, 26, 6, 2, 3, 4, 1, 10, 2, 1, 3, 4, 9, 1, 2, 1, 1, 1, 1, 4, 3, 2, 5, 1, 5, 7, 5, 24, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 2, 8, 1, 3, 1, 4, 1, 1, 7, 1, 3, 1, 1, 1, 6, 1, 1, 1, 1, 1, 2, 1, 2, 4, 2, 1, 1, 1, 5, 3, 1, 5, 3, 17, 1, 1, 2, 12, 8, 11, 1, 2, 2, 9, 1, 7, 1, 3, 1, 2, 4, 12, 2, 21, 3, 7, 6, 10, 9, 7, 4, 11, 5, 8, 2, 1, 16, 16, 6, 3, 1, 1, 1, 2, 1, 1, 26, 2, 2, 3, 2, 6, 3, 3, 1, 1, 1, 2, 1, 2, 1, 4, 10, 1, 1, 4, 7, 2, 6, 8, 2, 1, 1, 4, 2, 3, 4, 3, 4, 9, 9, 1, 7, 1, 1, 1, 3, 2, 8, 11, 4, 21, 6, 1, 2, 3, 4, 2, 26, 1, 5, 4, 3, 7, 1, 1, 5, 1, 1, 5, 2, 4, 1, 1, 2, 3, 7, 4, 1, 1, 2, 2, 10, 4, 1, 3, 1, 1, 1, 2, 1, 5, 5, 1, 2, 2, 1, 1, 1, 7, 4, 13, 2, 1, 1, 10, 1, 1, 1, 4, 1, 7, 1, 2, 2, 1, 5, 1, 3, 1, 9, 1, 3, 1, 1, 1, 1, 2, 17, 3, 1, 1, 1, 2, 1, 1, 1, 3, 2, 1, 2, 1, 1, 1, 1, 14, 2, 6, 1, 1, 3, 1, 4, 2, 3, 1, 1, 5, 5, 2, 1, 2, 1, 8, 1, 1, 1, 2, 1, 4, 1, 3, 2, 9, 1, 1, 10, 3, 1, 2, 2, 2, 2, 2, 1, 2, 2, 1, 2, 4, 3, 4, 2, 3, 1, 3, 2, 2, 1, 1, 2, 4, 2, 2, 1, 10, 1, 1, 1, 5, 1, 1, 3, 3, 5, 1, 1, 20, 3, 2, 1, 2, 1, 2, 1, 3, 1, 1, 1, 2, 1, 2, 5, 4, 1, 4, 2, 1, 2, 4, 2, 2, 1, 2, 3, 3, 1, 1, 1, 1, 4, 1, 1, 1, 1, 2, 1, 6, 1, 4, 2, 4, 1, 1, 1, 5, 1, 2, 1, 2, 2, 1, 6, 1, 2, 1, 2, 1, 1, 1, 6, 8, 1, 1, 3, 1, 2, 1, 1, 3, 1, 2, 3, 1, 1, 3, 2, 1, 1, 1, 1, 3, 1, 1, 3, 1, 2, 1, 1, 2, 4, 1, 1, 2, 3, 1, 5, 3, 2, 1, 1, 4, 1, 3, 1, 2, 2, 4, 2, 3, 2, 6, 2, 1, 4, 3, 1, 2, 2, 1, 1, 8, 1, 1, 1, 1, 3, 2, 1, 1, 1, 3, 2, 4, 2, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 2, 1, 1, 1, 1, 1, 1, 4, 1, 2, 1, 1, 1, 5, 3, 1, 1, 2, 1, 1, 3, 1, 1, 1, 1, 2, 3, 1, 4, 1, 3, 1, 1, 1, 2, 1, 1, 3, 3, 1, 1, 1, 7, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 3, 3, 2, 1, 1, 2, 2, 2, 3, 1, 1, 5, 1, 1, 3, 2, 2, 5, 5, 2, 8, 2, 2, 1, 1, 1, 1, 1, 2, 2, 1, 2, 2, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 3, 1, 1, 1, 2, 1, 1, 2, 3, 3, 1, 1, 1, 2, 3, 1, 1, 1, 1, 1, 2, 1, 1, 1, 4, 3, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 2, 4, 4, 1, 1, 1, 1, 1, 2, 1, 3, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 2, 8, 2, 1, 1, 2, 12, 4, 3, 3, 2, 1, 3, 1, 1, 1, 2, 2, 2, 2, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 2, 1, 1, 1, 1, 1, 4, 5, 1, 1, 1, 1, 1, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 5, 2, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 2, 1, 3, 2, 2, 1, 1, 1, 1, 1, 3, 1, 1, 1, 8, 1, 1, 1, 1, 5, 1, 1, 1, 1, 3, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 4, 1, 1, 1, 2, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 3, 1, 2, 2, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 2, 2, 1, 5, 2, 2, 2, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 7, 4, 7, 5, 3, 3, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 2, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 2, 1, 2, 1, 1, 2, 1, 1, 3, 1, 1, 1, 6, 3, 4, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 2, 1, 9, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "----\n",
      "vectors[1] = [41, 3, 303, 13, 144, 105, 14, 880, 2, 6, 1, 5, 1, 4, 594, 4, 703, 1, 22, 1, 1, 1, 6, 1, 97, 3, 2, 33, 5, 804, 1, 2, 378, 3, 22, 3, 2, 17, 8, 73, 7, 3, 15, 10, 6, 44, 4, 65, 19, 4, 20, 7, 27, 7, 460, 8, 160, 3, 2, 2, 96, 4, 19, 6, 1, 1, 11, 62, 26, 48, 46, 1, 2, 2, 32, 4, 2, 1, 5, 1, 1, 1, 35, 81, 1, 2, 23, 1, 1, 337, 156, 591, 333, 2, 1, 1, 2, 46, 1, 2, 1, 3, 1, 1, 1, 1, 1, 2, 11, 99, 3, 14, 2, 15, 1, 76, 1, 27, 24, 2, 2, 4, 1, 6, 52, 4, 1, 8, 4, 8, 1, 12, 265, 1, 2, 1, 13, 27, 2, 4, 2, 2, 149, 10, 2, 79, 36, 4, 1, 1, 1, 55, 56, 1, 140, 1, 1, 1, 1, 4, 11, 5, 3, 3, 16, 2, 4, 3, 5, 2, 3, 5, 1, 1, 1, 2, 4, 14, 5, 2, 52, 4, 23, 1, 3, 1, 22, 14, 52, 2, 3, 1, 1, 1, 0, 8, 3, 2, 2, 1, 3, 4, 26, 22, 1, 286, 3, 2, 39, 96, 6, 1, 2, 16, 80, 38, 133, 6, 119, 9, 1, 2, 142, 2, 1, 3, 23, 19, 3, 2, 8, 20, 81, 14, 6, 5, 10, 7, 2, 1, 17, 1, 1, 5, 24, 3, 29, 9, 5, 3, 47, 1, 1, 6, 9, 14, 3, 4, 5, 4, 5, 1, 1, 1, 1, 2, 1, 5, 22, 17, 3, 1, 26, 2, 10, 11, 22, 1, 42, 1, 3, 1, 1, 1, 1, 1, 2, 12, 1, 3, 1, 1, 13, 6, 24, 51, 13, 12, 1, 1, 3, 2, 2, 29, 4, 14, 1, 11, 25, 2, 9, 12, 17, 12, 1, 8, 2, 1, 2, 7, 1, 1, 11, 1, 3, 2, 5, 4, 4, 1, 5, 7, 2, 28, 3, 1, 1, 2, 2, 77, 28, 1, 9, 1, 2, 469, 30, 1, 4, 42, 2, 1, 1, 1, 1, 30, 18, 5, 3, 1, 1, 5, 2, 2, 1, 12, 1, 1, 1, 1, 1, 1, 2, 1, 1, 3, 1, 5, 4, 3, 1, 2, 3, 22, 2, 1, 1, 1, 1, 1, 1, 12, 3, 23, 18, 2, 280, 4, 2, 1, 1, 1, 13, 205, 45, 22, 11, 58, 13, 201, 7, 0, 1, 3, 26, 7, 0, 67, 0, 1, 6, 23, 12, 1, 7, 54, 0, 1, 62, 0, 1, 0, 0, 0, 1, 16, 25, 12, 6, 1, 17, 1, 12, 5, 417, 5, 0, 41, 33, 0, 8, 10, 9, 232, 24, 2, 14, 2, 16, 2, 15, 3, 0, 1, 0, 8, 6, 11, 9, 2, 17, 4, 1, 57, 1, 0, 1, 0, 0, 0, 0, 0, 3, 9, 2, 8, 11, 1, 31, 0, 14, 10, 2, 27, 0, 5, 0, 6, 0, 4, 0, 5, 0, 0, 1, 0, 0, 1, 4, 2, 0, 0, 0, 1, 0, 0, 1, 1, 94, 3, 0, 1, 1, 4, 7, 0, 4, 0, 11, 0, 3, 8, 2, 1, 1, 0, 4, 9, 49, 3, 0, 13, 2, 31, 1, 0, 0, 1, 35, 1, 2, 14, 14, 4, 17, 2, 0, 1, 1, 1, 0, 140, 3, 24, 22, 4, 10, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 7, 1, 3, 11, 2, 2, 9, 3, 0, 0, 1, 1, 0, 1, 0, 4, 6, 2, 16, 7, 46, 0, 1, 1, 5, 13, 5, 49, 9, 12, 2, 0, 7, 0, 64, 1, 19, 1, 0, 6, 0, 27, 22, 0, 28, 0, 0, 7, 2, 3, 3, 5, 20, 5, 22, 3, 0, 1, 1, 3, 46, 17, 0, 3, 1, 0, 0, 0, 0, 5, 15, 0, 5, 5, 0, 0, 0, 20, 5, 8, 14, 0, 0, 30, 0, 1, 1, 13, 1, 0, 0, 1, 0, 0, 7, 1, 0, 4, 20, 0, 0, 0, 9, 0, 1, 3, 3, 0, 0, 1, 0, 14, 2, 0, 0, 2, 1, 0, 1, 0, 0, 1, 0, 38, 2, 18, 0, 0, 1, 1, 2, 4, 1, 3, 0, 6, 0, 7, 2, 1, 0, 0, 0, 2, 1, 1, 0, 37, 0, 19, 1, 33, 2, 0, 6, 0, 1, 0, 7, 1, 1, 1, 0, 47, 1, 0, 2, 5, 0, 0, 0, 4, 2, 1, 2, 0, 0, 0, 0, 5, 2, 31, 5, 15, 0, 1, 0, 12, 0, 2, 3, 11, 3, 4, 1, 51, 11, 0, 0, 7, 0, 2, 5, 7, 0, 4, 2, 0, 1, 0, 0, 0, 1, 16, 2, 9, 0, 27, 2, 0, 1, 10, 12, 13, 0, 0, 1, 1, 7, 0, 0, 0, 0, 3, 0, 28, 0, 0, 0, 9, 0, 2, 5, 12, 2, 22, 0, 2, 6, 17, 1, 1, 22, 1, 12, 0, 0, 0, 8, 26, 3, 0, 14, 0, 0, 0, 0, 29, 16, 3, 3, 0, 0, 0, 14, 0, 13, 1, 1, 1, 0, 0, 5, 0, 5, 2, 5, 1, 8, 2, 6, 16, 3, 3, 5, 19, 4, 1, 2, 0, 1, 1, 0, 1, 7, 5, 0, 23, 2, 1, 434, 0, 34, 7, 4, 3, 55, 1, 0, 48, 1, 1, 0, 5, 1, 3, 0, 2, 2, 11, 0, 27, 0, 1, 9, 0, 31, 13, 19, 7, 0, 3, 1, 15, 7, 3, 5, 0, 14, 1, 9, 7, 1, 0, 7, 16, 0, 0, 3, 0, 1, 19, 4, 1, 0, 10, 0, 0, 0, 0, 4, 0, 1, 0, 3, 1, 3, 0, 3, 1, 0, 1, 1, 0, 0, 5, 3, 2, 0, 0, 3, 0, 1, 3, 0, 0, 2, 0, 6, 0, 1, 0, 0, 8, 2, 0, 3, 4, 3, 1, 0, 1, 1, 4, 4, 2, 0, 7, 2, 1, 5, 0, 3, 2, 0, 6, 21, 23, 0, 7, 3, 1, 1, 4, 0, 1, 22, 0, 1, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 2, 2, 0, 0, 2, 6, 2, 8, 0, 12, 16, 4, 2, 1, 0, 1, 7, 1, 1, 1, 0, 0, 0, 1, 0, 0, 4, 0, 0, 1, 0, 0, 0, 9, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 2, 15, 0, 2, 0, 0, 1, 0, 9, 17, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 13, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 2, 0, 0, 0, 0, 11, 0, 0, 0, 0, 0, 12, 3, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 4, 3, 0, 5, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 5, 6, 6, 0, 17, 9, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 8, 3, 1, 2, 25, 4, 0, 0, 0, 0, 9, 4, 0, 28, 0, 6, 3, 0, 1, 1, 0, 0, 0, 8, 2, 0, 33, 0, 0, 4, 4, 8, 0, 2, 2, 6, 0, 0, 1, 0, 0, 1, 0, 4, 0, 1, 1, 9, 0, 0, 0, 0, 0, 0, 76, 0, 2, 1, 0, 4, 1, 13, 1, 0, 0, 2, 0, 4, 0, 2, 2, 1, 2, 2, 0, 0, 6, 0, 0, 6, 2, 2, 5, 7, 3, 0, 6, 2, 0, 0, 0, 0, 28, 0, 0, 0, 0, 3, 0, 0, 12, 0, 0, 0, 0, 1, 6, 0, 3, 1, 0, 0, 9, 1, 0, 2, 0, 0, 4, 1, 2, 0, 0, 0, 4, 16, 0, 6, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 6, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 14, 2, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 19, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 1, 0, 0, 0, 6, 0, 0, 1, 0, 0, 0, 0, 1, 12, 9, 3, 3, 0, 0, 0, 0, 0, 0, 0, 4, 3, 1, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 1, 2, 12, 2, 1, 1, 3, 0, 0, 2, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 2, 0, 0, 1, 0, 0, 1, 1, 1, 0, 2, 0, 2, 0, 0, 0, 0, 0, 0, 4, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 2, 2, 46, 1, 1, 6, 1, 1, 2, 3, 3, 1, 1, 1, 1, 3, 1, 1, 2, 1, 1, 1, 1, 2, 2, 17, 1, 2, 2, 3, 1, 1, 2, 3, 9, 2, 1, 2, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 2, 1, 3, 1, 4, 1, 1, 1, 10, 1, 2, 2, 4, 26, 1, 1, 2, 1, 1, 19, 2, 1, 8, 2, 4, 1, 6, 1, 6, 1, 2, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 3, 2, 1, 10, 1, 1, 8, 1, 3, 1, 3, 3, 25, 2, 5, 1, 18, 4, 2, 1, 1, 4, 1, 11, 13, 5, 7, 1, 1, 3, 1, 1, 1, 4, 1, 1, 1, 1, 1, 2, 1, 1, 7, 2, 2, 5, 4, 2, 2, 2, 2, 14, 3, 4, 7, 3, 1, 1, 3, 4, 2, 9, 1, 7, 1, 1, 2, 2, 1, 3, 1, 5, 1, 3, 1, 1, 1, 18, 3, 3, 2, 1, 4, 9, 1, 5, 6, 1, 1, 7, 3, 4, 2, 12, 7, 2, 2, 1, 30, 6, 3, 18, 2, 2, 23, 9, 9, 3, 4, 3, 12, 4, 2, 3, 2, 16, 6, 13, 5, 11, 5, 15, 6, 6, 3, 2, 2, 2, 2, 1, 2, 1, 3, 1, 2, 7, 7, 22, 1, 1, 1, 1, 1, 1, 1, 1, 8, 1, 1, 1, 1, 11, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 16, 1, 1, 6, 1, 1, 1, 1, 1, 6, 3, 2, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 10, 1, 3, 1, 1, 3, 1, 2, 2, 2, 10, 1, 2, 1, 4, 3, 1, 1, 1, 2, 1, 1, 3, 1, 2, 5, 1, 5, 2, 7, 8, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 6, 2, 1, 1, 1, 3, 2, 1, 2, 6, 3, 4, 2, 2, 1, 1, 1, 1, 1, 11, 7, 8, 3, 1, 1, 1, 1, 4, 13, 1, 4, 3, 2, 2, 10, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 4, 6, 7, 3, 10, 1, 2, 2, 1, 2, 4, 1, 1, 1, 1, 1, 1, 2, 2, 1, 5, 1, 2, 6, 1, 2, 1, 1, 1, 1, 1, 1, 4, 1, 1, 5, 1, 1, 1, 2, 1, 1, 3, 1, 1, 2, 2, 3, 5, 1, 1, 3, 2, 23, 2, 3, 1, 2, 26, 8, 4, 2, 1, 2, 5, 2, 5, 1, 2, 2, 22, 1, 8, 2, 1, 5, 2, 1, 1, 23, 1, 384, 2, 1, 1, 13, 1, 14, 2, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 7, 1, 1, 3, 2, 1, 8, 1, 1, 1, 2, 10, 5, 1, 3, 19, 2, 2, 1, 2, 2, 2, 5, 10, 1, 2, 2, 1, 2, 1, 1, 12, 9, 7, 1, 8, 1, 7, 4, 1, 2, 2, 2, 2, 1, 1, 4, 1, 1, 4, 1, 1, 1, 4, 3, 3, 1, 2, 1, 1, 2, 1, 2, 1, 4, 1, 1, 7, 1, 1, 1, 1, 1, 2, 1, 1, 1, 3, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 3, 4, 2, 4, 1, 1, 1, 2, 13, 2, 40, 14, 12, 12, 10, 1, 1, 1, 1, 2, 4, 2, 3, 2, 1, 1, 1, 6, 4, 3, 3, 3, 1, 1, 2, 1, 2, 23, 1, 4, 1, 3, 1, 3, 2, 1, 2, 2, 3, 3, 1, 2, 2, 16, 1, 1, 6, 4, 2, 2, 4, 1, 6, 2, 1, 1, 2, 6, 2, 2, 3, 1, 1, 1, 3, 1, 1, 1, 4, 1, 3, 36, 2, 1, 1, 1, 4, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 3, 3, 3, 1, 3, 10, 2, 2, 1, 2, 1, 1, 1, 1, 2, 1, 1, 3, 1, 1, 1, 1, 8, 1, 1, 4, 3, 3, 3, 1, 1, 2, 1, 8, 3, 2, 2, 1, 2, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 3, 1, 1, 1, 1, 1, 1, 4, 3, 1, 1, 5, 1, 3, 7, 4, 4, 15, 3, 1, 3, 1, 3, 3, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 5, 2, 2, 3, 3, 3, 1, 1, 1, 3, 1, 2, 1, 1, 1, 2, 1, 2, 1, 3, 2, 1, 1, 1, 1, 1, 1, 1, 7, 3, 1, 1, 1, 1, 1, 3, 3, 1, 5, 1, 1, 5, 5, 1, 4, 4, 1, 10, 8, 2, 2, 2, 1, 1, 1, 13, 10, 3, 3, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 5, 3, 4, 1, 1, 1, 1, 5, 3, 3, 6, 4, 1, 1, 1, 1, 1, 5, 2, 1, 1, 1, 2, 1, 1, 3, 1, 3, 3, 2, 1, 6, 1, 1, 8, 1, 1, 2, 3, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 5, 1, 1, 2, 19, 5, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 4, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 10, 1, 2, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 2, 1, 18, 1, 1, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 9, 2, 1, 1, 1, 5, 1, 1, 1, 1, 1, 1, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 4, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 2, 1, 3, 3, 2, 1, 1, 3, 5, 1, 2, 2, 2, 10, 1, 2, 2, 5, 2, 2, 3, 5, 2, 1, 2, 2, 1, 4, 3, 2, 3, 3, 1, 1, 2, 4, 1, 3, 3, 2, 2, 1, 1, 1, 1, 1, 3, 3, 1, 1, 1, 1, 1, 1, 3, 3, 2, 1, 1, 1, 4, 3, 3, 1, 1, 1, 1, 1, 6, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 3, 2, 2, 1, 4, 1, 2, 1, 1, 1, 1, 2, 2, 3, 2, 4, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 3, 1, 1, 3, 1, 2, 1, 1, 1, 2, 6, 2, 3, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 3, 2, 1, 2, 2, 2, 2, 1, 2, 2, 1, 1, 1, 1, 3, 1, 1, 1, 6, 2, 1, 1, 1, 10, 6, 11, 1, 1, 4, 2, 5, 2, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 2, 1, 2, 1, 1, 2, 4, 2, 2, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 15, 3, 6, 6, 3, 2, 2, 6, 6, 4, 5, 5, 4, 4, 4, 3, 1, 2, 1, 3, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 2, 2, 3, 3, 3, 4, 4, 2, 1, 2, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 2, 2, 1, 1, 3, 1, 1, 2, 6, 3, 3, 2, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 3, 4, 1, 1, 1, 1, 1, 3, 1, 1, 2, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 4, 1, 4, 1, 1, 2, 1, 1, 2, 1, 1, 2, 2, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 2, 2, 4, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 2, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 2, 1, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "----\n",
      "vectors[2] = [67, 3, 312, 14, 114, 104, 25, 1363, 2, 7, 1, 5, 1, 4, 686, 4, 631, 1, 8, 1, 1, 1, 10, 1, 97, 3, 2, 35, 5, 671, 1, 2, 332, 3, 22, 4, 2, 16, 8, 133, 18, 3, 12, 10, 6, 36, 3, 58, 20, 8, 18, 7, 22, 7, 452, 8, 193, 1, 2, 2, 101, 4, 17, 6, 1, 1, 11, 62, 26, 48, 51, 1, 2, 4, 32, 3, 2, 1, 5, 1, 1, 1, 6, 81, 1, 2, 20, 1, 1, 359, 174, 587, 219, 2, 1, 1, 2, 96, 1, 2, 1, 3, 1, 1, 1, 2, 1, 2, 117, 48, 3, 18, 2, 18, 1, 75, 1, 15, 19, 2, 2, 7, 1, 5, 51, 14, 1, 6, 3, 5, 1, 12, 358, 1, 2, 1, 5, 13, 2, 5, 2, 2, 143, 7, 2, 47, 94, 3, 3, 1, 1, 33, 37, 1, 100, 1, 1, 1, 1, 4, 9, 6, 3, 3, 20, 2, 3, 3, 5, 2, 3, 5, 1, 1, 1, 2, 4, 13, 13, 2, 26, 4, 10, 1, 3, 1, 12, 9, 55, 2, 3, 1, 1, 1, 0, 5, 2, 2, 1, 1, 2, 4, 20, 8, 1, 270, 1, 1, 33, 82, 5, 1, 2, 22, 64, 107, 83, 3, 30, 1, 1, 2, 97, 4, 1, 3, 27, 8, 2, 2, 7, 18, 86, 3, 6, 1, 10, 9, 2, 1, 33, 1, 1, 3, 24, 1, 29, 9, 6, 3, 44, 1, 1, 6, 6, 8, 44, 4, 9, 4, 1, 1, 1, 1, 1, 1, 1, 5, 24, 5, 3, 1, 26, 2, 10, 6, 24, 1, 37, 1, 4, 1, 1, 1, 1, 1, 2, 11, 1, 1, 1, 1, 13, 6, 24, 55, 13, 10, 3, 1, 3, 8, 2, 29, 4, 14, 2, 22, 25, 2, 4, 5, 17, 12, 1, 5, 1, 1, 2, 7, 1, 1, 11, 1, 3, 2, 4, 3, 4, 1, 5, 13, 2, 9, 3, 1, 1, 2, 3, 48, 20, 1, 2, 1, 2, 1031, 16, 1, 4, 23, 4, 1, 1, 1, 1, 30, 18, 5, 3, 1, 1, 1, 2, 2, 1, 12, 1, 1, 1, 1, 1, 1, 3, 3, 1, 3, 1, 7, 4, 3, 1, 17, 2, 26, 5, 1, 1, 1, 1, 1, 1, 8, 1, 28, 18, 2, 251, 10, 1, 2, 1, 1, 8, 136, 10, 21, 4, 53, 13, 137, 5, 0, 0, 4, 21, 28, 2, 64, 2, 1, 0, 22, 2, 0, 1, 44, 0, 1, 59, 2, 2, 1, 5, 0, 1, 8, 17, 4, 3, 1, 10, 1, 14, 10, 396, 0, 0, 27, 40, 1, 9, 7, 5, 274, 49, 3, 13, 4, 24, 2, 1, 2, 0, 1, 2, 7, 1, 11, 10, 0, 17, 1, 3, 34, 0, 0, 0, 5, 0, 0, 0, 0, 6, 3, 1, 19, 12, 0, 16, 0, 3, 3, 1, 15, 0, 0, 0, 5, 5, 2, 0, 1, 2, 0, 6, 1, 1, 2, 3, 1, 0, 1, 2, 5, 0, 1, 0, 5, 78, 4, 5, 0, 1, 0, 2, 0, 0, 0, 189, 0, 1, 6, 2, 0, 0, 0, 1, 2, 25, 4, 1, 17, 1, 38, 9, 0, 0, 2, 26, 3, 3, 21, 21, 7, 39, 1, 0, 1, 0, 0, 0, 71, 0, 9, 17, 2, 11, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 17, 0, 6, 3, 1, 1, 0, 3, 0, 0, 0, 0, 0, 1, 0, 2, 5, 1, 18, 12, 49, 0, 1, 0, 3, 6, 5, 25, 11, 27, 0, 0, 6, 1, 28, 1, 9, 1, 8, 10, 0, 21, 6, 0, 16, 1, 0, 13, 0, 0, 3, 2, 11, 7, 26, 21, 0, 0, 0, 0, 48, 18, 0, 1, 0, 0, 0, 0, 0, 1, 14, 1, 6, 1, 0, 0, 0, 19, 1, 2, 10, 1, 0, 27, 0, 1, 0, 9, 3, 0, 0, 0, 0, 2, 1, 0, 2, 2, 20, 0, 0, 0, 5, 0, 1, 1, 8, 0, 0, 0, 1, 14, 1, 2, 0, 1, 1, 2, 0, 0, 0, 4, 0, 17, 0, 19, 0, 0, 1, 0, 2, 2, 2, 1, 0, 9, 0, 13, 0, 0, 0, 1, 0, 1, 5, 1, 0, 21, 0, 14, 1, 25, 0, 0, 11, 1, 0, 0, 1, 0, 0, 1, 0, 22, 0, 0, 4, 1, 1, 0, 0, 7, 1, 1, 1, 0, 0, 0, 0, 0, 0, 3, 1, 7, 0, 2, 3, 9, 0, 0, 0, 2, 6, 5, 0, 37, 8, 2, 0, 10, 0, 3, 0, 2, 1, 3, 1, 1, 0, 0, 0, 2, 0, 9, 9, 16, 0, 19, 2, 3, 0, 10, 11, 8, 1, 0, 0, 1, 8, 0, 0, 0, 0, 1, 1, 49, 0, 0, 0, 9, 1, 1, 3, 8, 1, 6, 0, 2, 4, 19, 1, 0, 3, 0, 4, 0, 0, 0, 4, 20, 9, 0, 3, 0, 0, 0, 4, 16, 7, 3, 0, 0, 0, 2, 22, 0, 15, 0, 1, 0, 0, 6, 0, 0, 1, 0, 2, 0, 5, 0, 7, 11, 2, 2, 9, 14, 2, 1, 0, 2, 0, 5, 0, 2, 8, 44, 69, 16, 6, 0, 516, 2, 91, 2, 3, 3, 173, 3, 0, 44, 1, 0, 0, 5, 0, 7, 0, 0, 0, 2, 2, 14, 15, 1, 11, 1, 14, 2, 9, 2, 1, 0, 2, 22, 12, 3, 3, 0, 1, 1, 52, 3, 4, 0, 1, 23, 0, 0, 4, 0, 1, 9, 2, 0, 0, 5, 0, 0, 1, 0, 1, 0, 0, 0, 5, 0, 3, 0, 1, 0, 0, 0, 1, 0, 0, 5, 3, 1, 0, 0, 4, 1, 1, 4, 0, 1, 0, 0, 22, 0, 3, 1, 0, 2, 1, 1, 0, 15, 13, 0, 1, 0, 4, 2, 1, 1, 1, 7, 9, 1, 9, 1, 6, 1, 2, 1, 9, 15, 0, 4, 4, 0, 2, 2, 0, 0, 12, 2, 1, 0, 0, 4, 0, 0, 9, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 1, 0, 0, 0, 1, 0, 0, 7, 0, 0, 0, 2, 2, 0, 0, 0, 10, 2, 1, 1, 4, 6, 7, 0, 1, 0, 0, 16, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 3, 0, 4, 3, 0, 0, 0, 4, 0, 1, 2, 0, 0, 1, 0, 4, 4, 1, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 11, 2, 7, 0, 0, 1, 0, 8, 18, 2, 0, 3, 0, 0, 0, 0, 3, 0, 4, 0, 14, 1, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 1, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 1, 5, 1, 0, 1, 5, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 6, 0, 31, 1, 0, 0, 8, 1, 2, 5, 1, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 5, 3, 4, 0, 0, 0, 0, 0, 1, 0, 0, 13, 0, 0, 1, 0, 1, 2, 0, 0, 0, 0, 1, 0, 2, 0, 0, 0, 0, 10, 0, 3, 0, 7, 4, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 9, 0, 0, 1, 3, 5, 0, 0, 19, 0, 0, 0, 0, 0, 1, 4, 2, 18, 0, 12, 2, 0, 1, 0, 2, 0, 0, 6, 3, 0, 12, 0, 1, 3, 0, 4, 0, 2, 1, 4, 0, 0, 0, 1, 0, 0, 2, 1, 3, 2, 8, 1, 0, 0, 0, 0, 1, 0, 43, 0, 0, 2, 2, 1, 0, 6, 2, 0, 0, 3, 0, 1, 0, 1, 1, 0, 0, 2, 2, 4, 4, 0, 0, 2, 5, 0, 2, 6, 4, 2, 8, 2, 0, 0, 0, 0, 28, 1, 2, 0, 0, 1, 0, 0, 5, 0, 1, 0, 0, 4, 26, 0, 0, 1, 0, 1, 0, 3, 2, 1, 0, 0, 0, 0, 1, 6, 2, 1, 4, 1, 0, 3, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 10, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 3, 1, 1, 2, 0, 0, 0, 0, 0, 0, 0, 2, 0, 13, 10, 0, 2, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 17, 0, 0, 0, 1, 0, 0, 0, 0, 0, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 17, 4, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 1, 0, 12, 0, 18, 0, 8, 1, 3, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 1, 2, 1, 3, 1, 0, 0, 0, 0, 0, 1, 2, 1, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 1, 1, 2, 2, 65, 1, 1, 8, 1, 1, 2, 5, 3, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 3, 0, 0, 0, 7, 0, 0, 0, 2, 0, 0, 1, 1, 3, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 6, 0, 5, 1, 1, 8, 0, 1, 0, 0, 0, 18, 0, 0, 5, 0, 1, 0, 16, 1, 6, 0, 2, 80, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 2, 0, 1, 2, 5, 0, 4, 0, 2, 0, 1, 0, 15, 1, 1, 0, 13, 3, 0, 0, 0, 0, 0, 7, 5, 2, 4, 0, 0, 1, 1, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 1, 1, 0, 0, 0, 2, 1, 0, 4, 0, 0, 0, 0, 0, 4, 1, 4, 0, 5, 2, 0, 0, 0, 2, 3, 0, 1, 5, 2, 0, 0, 1, 14, 0, 0, 1, 1, 4, 1, 2, 2, 2, 0, 0, 0, 5, 1, 3, 0, 5, 0, 0, 1, 2, 0, 0, 0, 0, 0, 15, 1, 1, 0, 2, 0, 0, 1, 1, 15, 0, 0, 6, 0, 17, 0, 0, 4, 0, 1, 0, 1, 1, 1, 0, 1, 1, 2, 6, 0, 1, 4, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 9, 0, 0, 2, 0, 3, 0, 0, 2, 0, 0, 0, 7, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 12, 0, 1, 0, 0, 3, 1, 3, 0, 0, 1, 3, 1, 0, 3, 0, 0, 1, 0, 0, 0, 0, 2, 1, 1, 4, 0, 6, 0, 3, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 2, 0, 0, 0, 0, 0, 0, 2, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 24, 0, 7, 1, 4, 3, 0, 0, 0, 20, 0, 1, 2, 1, 6, 6, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 8, 3, 2, 0, 0, 0, 4, 10, 2, 6, 3, 4, 2, 0, 4, 0, 0, 3, 0, 0, 4, 0, 0, 0, 3, 0, 0, 3, 0, 0, 2, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 2, 0, 0, 0, 4, 1, 1, 4, 0, 3, 0, 2, 1, 2, 0, 0, 2, 0, 5, 0, 3, 0, 0, 14, 0, 2, 2, 0, 1, 1, 0, 1, 0, 0, 0, 7, 0, 4, 8, 0, 1, 0, 0, 0, 7, 0, 961, 0, 0, 0, 10, 0, 6, 1, 4, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 33, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 12, 7, 0, 0, 3, 0, 4, 0, 4, 8, 1, 3, 1, 0, 0, 0, 0, 0, 1, 0, 5, 11, 9, 0, 1, 0, 1, 0, 0, 0, 10, 4, 1, 0, 3, 1, 0, 0, 50, 0, 1, 0, 11, 4, 2, 1, 0, 0, 0, 0, 0, 1, 0, 0, 2, 0, 49, 38, 4, 0, 0, 0, 4, 0, 0, 0, 2, 0, 0, 0, 0, 1, 1, 1, 0, 19, 1, 1, 0, 4, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 2, 4, 1, 97, 1, 0, 3, 1, 0, 0, 0, 0, 1, 2, 3, 0, 0, 0, 0, 0, 0, 0, 1, 0, 24, 0, 0, 1, 0, 1, 48, 2, 0, 0, 2, 0, 0, 0, 0, 0, 0, 4, 2, 2, 0, 0, 0, 0, 0, 0, 0, 4, 1, 1, 0, 0, 3, 1, 1, 0, 0, 0, 0, 2, 0, 0, 0, 3, 0, 0, 0, 1, 0, 0, 34, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 3, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 5, 3, 1, 5, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 2, 0, 0, 0, 2, 1, 1, 1, 0, 0, 0, 1, 0, 8, 2, 7, 0, 4, 1, 0, 0, 0, 0, 1, 0, 0, 5, 1, 0, 0, 1, 2, 0, 0, 1, 2, 0, 2, 0, 1, 3, 0, 4, 1, 3, 2, 3, 0, 0, 0, 0, 6, 19, 1, 0, 0, 0, 0, 0, 6, 1, 0, 0, 0, 5, 2, 0, 0, 0, 0, 2, 0, 1, 2, 2, 0, 4, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 6, 19, 0, 5, 0, 0, 1, 1, 0, 1, 0, 1, 0, 3, 8, 1, 0, 0, 3, 5, 0, 1, 0, 1, 1, 6, 11, 0, 0, 2, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 2, 0, 0, 3, 1, 0, 0, 0, 0, 0, 4, 4, 0, 5, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 9, 1, 1, 4, 0, 2, 2, 19, 0, 0, 8, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 4, 0, 38, 6, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 11, 0, 0, 3, 0, 0, 6, 0, 4, 0, 0, 0, 0, 7, 23, 0, 0, 2, 0, 0, 0, 0, 0, 0, 4, 2, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 0, 2, 0, 0, 0, 5, 0, 0, 0, 0, 2, 0, 0, 0, 4, 2, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 11, 0, 0, 2, 0, 9, 0, 3, 0, 0, 1, 1, 3, 0, 0, 0, 0, 0, 0, 4, 0, 0, 1, 10, 0, 0, 3, 2, 0, 1, 2, 1, 0, 0, 0, 0, 0, 0, 1, 4, 0, 0, 0, 1, 2, 0, 1, 29, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 3, 0, 0, 0, 2, 0, 0, 0, 1, 0, 0, 2, 0, 0, 0, 0, 6, 0, 0, 0, 0, 8, 0, 0, 0, 0, 0, 0, 3, 3, 0, 0, 4, 0, 0, 0, 0, 0, 1, 0, 1, 0, 14, 1, 0, 2, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 26, 2, 0, 3, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 2, 0, 0, 0, 1, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 1, 0, 0, 2, 0, 0, 1, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 2, 0, 1, 3, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 16, 0, 0, 0, 0, 0, 1, 0, 9, 0, 0, 2, 0, 4, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 0, 0, 0, 0, 19, 0, 0, 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 0, 0, 2, 2, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 5, 0, 0, 1, 1, 4, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 2, 0, 0, 2, 0, 0, 0, 0, 8, 0, 0, 0, 2, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 2, 0, 0, 6, 0, 0, 0, 4, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 9, 9, 0, 0, 4, 1, 0, 6, 1, 3, 0, 0, 0, 6, 0, 0, 0, 0, 9, 10, 5, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 7, 1, 1, 0, 0, 2, 0, 0, 0, 0, 0, 1, 1, 1, 2, 0, 2, 3, 0, 1, 2, 1, 0, 0, 1, 0, 0, 0, 0, 1, 2, 3, 1, 1, 3, 9, 3, 39, 3, 1, 3, 1, 1, 2, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 3, 1, 3, 5, 1, 27, 3, 5, 6, 1, 33, 14, 4, 2, 6, 3, 3, 2, 6, 5, 2, 2, 2, 2, 2, 2, 9, 2, 2, 4, 2, 1, 2, 1, 2, 1, 1, 1, 1, 27, 2, 1, 3, 4, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 5, 1, 1, 1, 1, 1, 1, 1, 6, 1, 1, 1, 6, 1, 1, 1, 2, 6, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 2, 1, 3, 2, 1, 4, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 2, 2, 5, 2, 2, 2, 4, 1, 1, 4, 3, 1, 2, 1, 1, 1, 1, 7, 5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 57, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 5, 5, 3, 3, 5, 2, 4, 1, 1, 1, 9, 2, 1, 1, 2, 2, 1, 2, 1, 1, 2, 1, 29, 16, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 18, 1, 1, 3, 1, 8, 2, 1, 1, 1, 2, 1, 1, 7, 1, 1, 1, 4, 1, 2, 1, 1, 1, 1, 5, 2, 1, 1, 1, 1, 1, 4, 4, 2, 2, 2, 2, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 4, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 14, 1, 4, 1, 1, 1, 1, 6, 1, 1, 1, 1, 14, 1, 2, 1, 1, 2, 2, 1, 1, 1, 6, 7, 1, 35, 6, 1, 1, 1, 1, 1, 1, 1, 5, 1, 4, 1, 1, 1, 3, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 13, 1, 1, 9, 1, 4, 8, 1, 1, 4, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 3, 1, 1, 8, 1, 1, 1, 1, 2, 2, 6, 1, 1, 1, 1, 1, 5, 1, 1, 1, 1, 1, 2, 1, 1, 3, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 11, 2, 1, 1, 4, 1, 13, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 2, 2, 1, 2, 4, 2, 2, 2, 3, 2, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 3, 1, 3, 1, 1, 1, 1, 3, 6, 3, 3, 3, 1, 1, 10, 1, 3, 1, 5, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 4, 2, 3, 9, 3, 6, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 2, 2, 3, 3, 3, 1, 1, 7, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 6, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 2, 3, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 4, 1, 6, 1, 1, 1, 1, 10, 2, 1, 4, 6, 3, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 15, 4, 1, 1, 1, 1, 1, 4, 1, 2, 2, 2, 1, 3, 9, 2, 3, 1, 1, 1, 1, 2, 2, 1, 2, 3, 1, 1, 1, 2, 1, 1, 1, 2, 1, 4, 3, 1, 2, 1, 1, 1, 3, 3, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 4, 1, 1, 1, 1, 3, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 2, 1, 4, 11, 7, 4, 4, 1, 1, 1, 2, 8, 6, 2, 2, 1, 1, 23, 3, 3, 3, 2, 2, 2, 4, 2, 2, 2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 1, 1, 5, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 4, 2, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 9, 3, 5, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 2, 2, 2, 1, 1, 1, 2, 1, 4, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 6, 2, 1, 1, 1, 1, 2, 3, 2, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 3, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 3, 2, 1, 1, 3, 1, 1, 1, 1, 5, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 2, 1, 1, 5, 2, 4, 1, 1, 2, 1, 2, 2, 1, 1, 1, 1, 2, 2, 1, 3, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 43, 1, 1, 1, 1, 1, 2, 2, 102, 1, 1, 3, 1, 1, 1, 1, 16, 13, 1, 2, 2, 2, 1, 1, 1, 2, 1, 3, 1, 2, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 16, 7, 7, 3, 1, 1, 1, 1, 1, 11, 1, 1, 1, 2, 16, 8, 9, 1, 1, 1, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 2, 3, 1, 2, 2, 2, 2, 2, 1, 1, 1, 3, 3, 2, 2, 3, 2, 2, 3, 4, 3, 1, 2, 1, 1, 1, 17, 3, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 4, 2, 2, 2, 2, 2, 2, 2, 1, 2, 8, 1, 1, 1, 1, 5, 4, 1, 5, 4, 2, 3, 1, 1, 1, 4, 1, 1, 5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 2, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 5, 4, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 3, 3, 1, 1, 2, 2, 1, 2, 1, 1, 1, 2, 2, 3, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "----\n",
      "vectors[3] = [39, 3, 461, 13, 172, 112, 30, 1350, 2, 13, 1, 5, 1, 4, 990, 4, 823, 1, 46, 1, 1, 1, 5, 1, 106, 3, 2, 38, 9, 929, 1, 4, 416, 6, 57, 4, 2, 16, 8, 216, 26, 3, 19, 10, 6, 36, 9, 143, 20, 10, 18, 7, 29, 7, 729, 8, 247, 2, 3, 2, 83, 4, 17, 6, 1, 1, 11, 62, 26, 48, 61, 1, 2, 8, 33, 4, 2, 1, 6, 1, 1, 1, 22, 109, 1, 2, 20, 1, 1, 502, 157, 826, 433, 2, 1, 1, 2, 87, 1, 3, 1, 3, 1, 1, 1, 3, 1, 2, 61, 85, 3, 19, 2, 34, 1, 174, 1, 25, 42, 2, 2, 6, 1, 22, 46, 3, 1, 15, 3, 7, 1, 12, 395, 1, 2, 1, 12, 69, 2, 6, 2, 2, 208, 3, 2, 91, 73, 8, 1, 1, 1, 59, 85, 1, 150, 1, 1, 1, 1, 4, 7, 18, 3, 3, 20, 2, 3, 3, 6, 2, 3, 5, 1, 1, 1, 38, 23, 15, 20, 2, 24, 12, 203, 1, 3, 1, 10, 16, 45, 2, 4, 1, 1, 1, 0, 3, 2, 2, 1, 1, 2, 4, 17, 15, 1, 355, 3, 3, 85, 123, 5, 2, 2, 31, 68, 88, 156, 3, 98, 6, 1, 2, 184, 43, 1, 12, 40, 14, 1, 1, 4, 23, 107, 4, 10, 5, 10, 9, 9, 1, 29, 18, 1, 2, 24, 4, 32, 9, 5, 3, 62, 1, 1, 6, 6, 5, 13, 4, 20, 4, 3, 1, 5, 1, 1, 1, 1, 5, 27, 2, 3, 1, 23, 3, 10, 7, 22, 1, 53, 1, 1, 1, 1, 1, 1, 4, 2, 27, 1, 4, 1, 1, 13, 7, 24, 55, 13, 10, 3, 1, 3, 12, 2, 29, 4, 15, 1, 18, 25, 2, 12, 7, 17, 12, 1, 17, 2, 1, 2, 7, 1, 1, 37, 1, 3, 2, 4, 5, 4, 1, 5, 16, 3, 46, 3, 1, 1, 2, 2, 80, 30, 4, 5, 1, 2, 1127, 30, 2, 4, 31, 9, 1, 1, 1, 1, 30, 18, 7, 3, 1, 1, 6, 2, 2, 1, 12, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 5, 4, 3, 1, 56, 3, 6, 2, 1, 1, 1, 4, 1, 1, 15, 1, 73, 38, 2, 346, 2, 1, 6, 1, 1, 15, 221, 7, 22, 3, 27, 31, 272, 12, 0, 0, 4, 56, 14, 0, 85, 1, 1, 2, 26, 4, 0, 8, 65, 0, 1, 105, 0, 3, 0, 1, 0, 5, 11, 19, 4, 4, 3, 2, 2, 53, 9, 458, 0, 0, 49, 37, 0, 17, 6, 1, 266, 11, 6, 20, 6, 40, 1, 2, 2, 2, 3, 0, 6, 6, 16, 9, 2, 22, 0, 7, 72, 0, 0, 0, 1, 0, 0, 0, 0, 19, 53, 11, 15, 6, 0, 20, 0, 2, 3, 3, 56, 0, 4, 4, 35, 3, 1, 0, 2, 0, 0, 4, 0, 0, 1, 4, 0, 0, 1, 0, 4, 1, 1, 1, 5, 125, 14, 2, 0, 0, 0, 0, 0, 1, 0, 56, 0, 2, 11, 0, 0, 1, 0, 5, 7, 68, 33, 6, 15, 1, 59, 11, 1, 3, 2, 52, 1, 0, 28, 27, 3, 8, 0, 0, 0, 0, 0, 0, 110, 1, 19, 28, 4, 18, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 28, 0, 2, 16, 10, 1, 2, 5, 0, 0, 0, 0, 0, 2, 0, 1, 3, 1, 14, 9, 11, 1, 1, 0, 7, 35, 5, 25, 30, 21, 0, 0, 6, 0, 36, 2, 15, 4, 2, 25, 0, 32, 5, 0, 78, 0, 0, 4, 1, 0, 3, 1, 28, 2, 42, 2, 0, 0, 2, 8, 89, 21, 0, 2, 2, 2, 0, 0, 0, 8, 10, 0, 6, 3, 0, 0, 0, 45, 3, 13, 6, 0, 0, 43, 0, 1, 0, 26, 4, 0, 2, 0, 0, 1, 5, 0, 8, 1, 30, 0, 1, 0, 13, 0, 0, 1, 11, 1, 1, 0, 0, 16, 2, 1, 0, 2, 0, 7, 1, 0, 0, 6, 0, 55, 2, 21, 0, 0, 1, 1, 1, 1, 0, 1, 0, 4, 3, 34, 1, 0, 1, 2, 0, 3, 0, 2, 0, 26, 0, 17, 2, 32, 1, 0, 2, 48, 10, 1, 4, 2, 0, 4, 0, 21, 0, 0, 10, 1, 0, 0, 1, 6, 2, 0, 0, 0, 0, 1, 0, 1, 0, 18, 6, 14, 0, 1, 1, 15, 0, 4, 5, 3, 4, 0, 0, 105, 12, 2, 0, 0, 0, 4, 3, 9, 0, 3, 2, 0, 2, 3, 0, 0, 1, 17, 9, 17, 1, 25, 0, 2, 8, 5, 21, 14, 6, 0, 0, 0, 8, 0, 0, 0, 0, 0, 0, 28, 0, 1, 0, 12, 0, 0, 0, 16, 1, 16, 0, 3, 4, 20, 0, 0, 95, 0, 68, 0, 0, 0, 10, 25, 5, 0, 3, 0, 0, 0, 2, 28, 17, 3, 1, 0, 0, 0, 3, 1, 20, 0, 0, 0, 0, 8, 5, 0, 2, 0, 5, 0, 22, 1, 15, 16, 2, 3, 10, 22, 2, 2, 0, 3, 1, 2, 0, 1, 16, 87, 30, 61, 6, 13, 646, 0, 65, 31, 1, 2, 85, 0, 3, 47, 1, 20, 0, 4, 1, 14, 5, 0, 0, 14, 1, 21, 2, 2, 2, 1, 19, 27, 124, 16, 9, 3, 4, 80, 18, 5, 11, 0, 11, 13, 56, 2, 6, 2, 3, 16, 0, 2, 5, 0, 4, 10, 4, 1, 0, 17, 0, 0, 0, 0, 4, 1, 4, 1, 4, 0, 2, 0, 1, 0, 1, 6, 3, 0, 0, 18, 2, 2, 0, 1, 1, 0, 2, 3, 0, 0, 2, 0, 4, 0, 2, 1, 0, 5, 5, 0, 0, 12, 5, 0, 0, 1, 2, 17, 4, 0, 1, 6, 11, 1, 5, 0, 0, 2, 2, 4, 8, 25, 0, 12, 8, 0, 12, 3, 2, 11, 32, 2, 1, 0, 0, 29, 1, 0, 7, 14, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 1, 3, 0, 5, 0, 1, 0, 0, 0, 0, 6, 3, 0, 0, 0, 5, 1, 0, 0, 1, 31, 10, 0, 0, 0, 0, 14, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 15, 0, 0, 1, 0, 1, 0, 0, 2, 0, 0, 6, 2, 0, 3, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 9, 0, 2, 3, 0, 0, 0, 14, 23, 0, 0, 0, 0, 0, 0, 5, 10, 0, 5, 0, 7, 5, 13, 2, 16, 6, 0, 0, 2, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 4, 1, 1, 72, 1, 0, 0, 0, 0, 1, 0, 1, 0, 3, 0, 0, 1, 3, 0, 0, 0, 0, 1, 4, 0, 4, 1, 0, 0, 1, 0, 0, 5, 0, 4, 1, 1, 0, 5, 1, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 15, 0, 0, 3, 0, 0, 23, 1, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 7, 3, 12, 0, 10, 0, 3, 1, 0, 0, 3, 1, 0, 0, 0, 3, 0, 21, 0, 0, 0, 15, 11, 1, 0, 13, 2, 0, 0, 2, 0, 3, 7, 0, 39, 0, 1, 1, 0, 5, 2, 1, 0, 3, 8, 3, 0, 30, 0, 0, 8, 1, 11, 0, 0, 14, 12, 0, 0, 2, 2, 0, 0, 3, 1, 2, 0, 5, 3, 0, 0, 4, 2, 1, 0, 71, 0, 0, 1, 0, 2, 2, 4, 2, 0, 0, 0, 0, 0, 1, 2, 2, 0, 0, 2, 2, 0, 1, 0, 0, 3, 3, 0, 0, 5, 6, 0, 9, 0, 0, 0, 0, 0, 23, 0, 2, 0, 0, 0, 0, 0, 19, 0, 1, 0, 0, 8, 3, 0, 4, 2, 0, 0, 6, 2, 0, 4, 0, 6, 2, 1, 2, 5, 0, 1, 12, 5, 0, 4, 3, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 9, 21, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 2, 4, 0, 0, 0, 1, 1, 1, 1, 20, 0, 0, 0, 0, 0, 2, 0, 3, 1, 9, 18, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 9, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 8, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 27, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 5, 0, 7, 3, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 16, 0, 45, 12, 9, 0, 1, 2, 0, 4, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 2, 2, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 2, 0, 1, 2, 1, 82, 1, 1, 2, 1, 1, 1, 1, 3, 1, 1, 1, 1, 5, 1, 1, 5, 3, 1, 2, 1, 1, 0, 20, 0, 1, 1, 1, 1, 0, 0, 14, 11, 2, 0, 1, 0, 0, 0, 1, 0, 4, 0, 0, 0, 0, 0, 0, 5, 0, 4, 1, 2, 0, 28, 2, 5, 1, 0, 14, 0, 2, 2, 0, 0, 39, 9, 0, 2, 0, 1, 0, 40, 0, 4, 0, 1, 1, 1, 0, 0, 0, 0, 0, 17, 0, 0, 0, 7, 0, 0, 8, 1, 0, 28, 0, 2, 5, 1, 0, 48, 0, 14, 1, 19, 7, 0, 0, 0, 1, 0, 9, 14, 8, 5, 0, 0, 2, 0, 0, 0, 5, 0, 9, 0, 0, 0, 0, 0, 0, 8, 0, 4, 8, 9, 4, 1, 1, 3, 8, 4, 2, 2, 0, 0, 1, 0, 4, 1, 5, 1, 9, 0, 1, 7, 0, 1, 6, 0, 3, 2, 1, 0, 0, 1, 3, 0, 1, 2, 0, 3, 5, 0, 5, 1, 2, 0, 4, 6, 1, 2, 9, 2, 4, 0, 0, 0, 0, 0, 0, 0, 0, 11, 2, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 58, 4, 12, 0, 1, 2, 1, 0, 0, 6, 1, 1, 0, 12, 4, 2, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 10, 0, 0, 6, 0, 1, 0, 0, 1, 0, 4, 2, 10, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 1, 9, 0, 3, 0, 0, 2, 0, 3, 0, 1, 4, 0, 1, 0, 0, 0, 0, 0, 0, 13, 0, 1, 2, 1, 0, 0, 0, 9, 0, 5, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 4, 0, 4, 0, 0, 0, 0, 3, 3, 6, 5, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 14, 1, 0, 7, 1, 6, 0, 2, 0, 0, 4, 2, 27, 19, 0, 1, 0, 6, 0, 0, 0, 11, 3, 0, 0, 0, 0, 0, 0, 0, 1, 0, 5, 0, 0, 2, 9, 1, 3, 5, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 3, 1, 0, 1, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 1, 1, 1, 3, 1, 3, 1, 5, 6, 1, 11, 0, 1, 0, 6, 0, 4, 0, 1, 10, 0, 2, 7, 0, 3, 3, 1, 13, 0, 0, 0, 38, 0, 1, 11, 0, 1, 0, 0, 4, 9, 0, 780, 5, 1, 2, 23, 0, 26, 0, 4, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 11, 2, 2, 0, 0, 1, 7, 4, 1, 0, 0, 20, 3, 0, 1, 11, 0, 1, 0, 9, 3, 0, 4, 2, 0, 4, 0, 2, 3, 0, 0, 0, 16, 16, 0, 1, 0, 0, 2, 0, 2, 0, 9, 1, 0, 1, 17, 0, 0, 6, 0, 3, 1, 5, 6, 7, 0, 2, 0, 0, 7, 1, 0, 2, 0, 1, 0, 6, 13, 3, 0, 0, 0, 2, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 0, 9, 0, 2, 0, 0, 0, 4, 16, 3, 59, 1, 0, 8, 0, 3, 2, 0, 1, 3, 5, 1, 0, 0, 0, 0, 0, 0, 0, 1, 2, 10, 0, 0, 1, 1, 3, 36, 3, 3, 6, 1, 0, 0, 1, 0, 0, 10, 0, 5, 1, 2, 0, 73, 0, 0, 0, 0, 0, 0, 5, 4, 11, 5, 3, 2, 0, 0, 0, 3, 3, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 52, 0, 0, 0, 0, 10, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 1, 3, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 14, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 0, 0, 0, 0, 0, 0, 8, 9, 9, 3, 0, 0, 0, 0, 16, 22, 4, 10, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 2, 0, 0, 0, 0, 1, 0, 5, 2, 1, 0, 6, 0, 0, 0, 0, 14, 31, 1, 0, 0, 0, 0, 0, 3, 7, 0, 1, 0, 1, 0, 1, 0, 0, 0, 8, 0, 2, 3, 0, 0, 6, 0, 2, 9, 0, 0, 0, 2, 0, 0, 0, 1, 4, 8, 1, 0, 0, 1, 0, 0, 0, 0, 24, 2, 2, 0, 0, 0, 1, 2, 0, 0, 2, 1, 3, 4, 15, 5, 0, 0, 0, 3, 2, 0, 1, 0, 0, 0, 0, 4, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 3, 0, 3, 0, 7, 0, 0, 25, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, 1, 0, 6, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 2, 1, 0, 0, 3, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 4, 0, 0, 0, 2, 0, 0, 1, 7, 4, 2, 0, 1, 0, 0, 0, 0, 0, 0, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 1, 10, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0, 15, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 2, 0, 4, 0, 0, 0, 0, 4, 0, 6, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 2, 1, 0, 1, 0, 0, 1, 2, 0, 2, 0, 0, 1, 5, 2, 0, 0, 1, 5, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 4, 0, 0, 0, 0, 2, 1, 0, 0, 0, 24, 0, 0, 5, 0, 0, 0, 2, 0, 0, 4, 1, 0, 0, 0, 3, 0, 3, 0, 0, 0, 2, 0, 4, 7, 12, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 11, 12, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 2, 0, 0, 1, 0, 0, 0, 0, 0, 4, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 2, 1, 0, 0, 4, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 4, 2, 0, 0, 0, 0, 0, 1, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 3, 0, 0, 0, 1, 0, 0, 0, 0, 0, 9, 1, 0, 0, 3, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 3, 0, 0, 2, 3, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 4, 0, 0, 0, 1, 3, 1, 0, 6, 0, 0, 0, 2, 0, 0, 0, 6, 2, 0, 0, 2, 0, 0, 1, 0, 0, 3, 0, 0, 0, 3, 0, 1, 0, 0, 0, 3, 0, 1, 0, 11, 1, 0, 4, 0, 2, 1, 1, 0, 0, 20, 0, 0, 0, 2, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 16, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 8, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 0, 3, 3, 1, 0, 4, 0, 0, 6, 2, 0, 0, 0, 3, 9, 0, 0, 0, 1, 0, 0, 2, 0, 0, 2, 0, 2, 0, 0, 1, 0, 0, 2, 0, 3, 0, 0, 15, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 13, 0, 0, 0, 0, 0, 1, 0, 0, 3, 0, 0, 2, 0, 0, 0, 15, 0, 0, 0, 0, 3, 4, 0, 4, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 3, 0, 18, 1, 1, 2, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 2, 2, 1, 1, 1, 0, 3, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 4, 0, 1, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 11, 10, 0, 0, 0, 0, 0, 0, 0, 2, 3, 0, 9, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 6, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 3, 1, 0, 0, 3, 0, 1, 0, 2, 1, 4, 1, 3, 0, 4, 0, 4, 2, 1, 1, 3, 2, 1, 4, 2, 1, 1, 1, 2, 1, 2, 1, 1, 3, 1, 3, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 10, 1, 0, 0, 0, 1, 0, 5, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 1, 0, 0, 0, 4, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 7, 0, 0, 0, 5, 0, 0, 17, 0, 0, 0, 0, 0, 78, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 9, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 4, 2, 1, 0, 0, 0, 0, 0, 2, 1, 0, 0, 2, 0, 0, 2, 0, 0, 1, 0, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 6, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 1, 0, 0, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 1, 0, 0, 0, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 3, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 0, 0, 1, 0, 0, 0, 2, 0, 0, 0, 0, 2, 0, 0, 0, 3, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 3, 0, 9, 0, 2, 0, 0, 1, 0, 4, 5, 0, 9, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 3, 1, 1, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 1, 0, 2, 0, 0, 0, 1, 0, 2, 1, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 3, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 5, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 17, 0, 0, 1, 0, 0, 0, 0, 0, 2, 4, 2, 0, 0, 0, 0, 0, 0, 18, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 8, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 2, 4, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 2, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 12, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 8, 0, 0, 0, 0, 0, 3, 1, 1, 2, 1, 1, 0, 5, 0, 0, 1, 2, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 16, 0, 4, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 1, 6, 11, 1, 1, 7, 1, 20, 31, 1, 1, 5, 3, 1, 1, 5, 2, 8, 1, 1, 3, 4, 3, 3, 7, 4, 11, 1, 2, 2, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 7, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 4, 1, 1, 2, 4, 2, 1, 1, 1, 3, 2, 1, 5, 2, 1, 1, 2, 1, 1, 2, 2, 3, 2, 2, 1, 4, 2, 5, 1, 2, 1, 2, 2, 1, 1, 1, 1, 6, 1, 2, 1, 1, 1, 2, 1, 2, 1, 1, 1, 3, 1, 2, 1, 1, 1, 1, 1, 1, 3, 3, 2, 3, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 3, 4, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 16, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 2, 3, 2, 4, 4, 2, 3, 2, 1, 2, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 3, 1, 1, 10, 1, 2, 2, 2, 1, 1, 1, 3, 2, 3, 1, 1, 1, 2, 10, 3, 2, 8, 4, 1, 10, 2, 10, 8, 5, 3, 5, 1, 1, 1, 8, 5, 2, 1, 2, 12, 6, 1, 8, 4, 2, 1, 4, 4, 4, 4, 4, 4, 1, 1, 1, 5, 1, 2, 1, 1, 2, 2, 1, 2, 2, 3, 1, 1, 2, 1, 2, 1, 1, 1, 6, 3, 2, 2, 1, 1, 1, 1, 1, 2, 5, 1, 1, 2, 2, 2, 4, 1, 1, 1, 2, 2, 1, 2, 1, 4, 1, 1, 1, 1, 1, 3, 1, 2, 1, 2, 5, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 3, 1, 2, 10, 5, 1, 9, 1, 2, 1, 7, 1, 6, 2, 2, 1, 1, 1, 1, 5, 2, 2, 1, 2, 2, 2, 2, 8, 1, 2, 2, 2, 2, 2, 2, 2, 3, 6, 6, 3, 6, 3, 3, 2, 2, 2, 2, 1, 2, 1, 2, 1, 2, 2, 2, 1, 2, 2, 2, 2, 2, 1, 1, 7, 3, 23, 1, 1, 2, 3, 4, 4, 2, 3, 4, 5, 1, 1, 1, 1, 1, 3, 1, 2, 1, 4, 1, 1, 1, 1, 1, 4, 3, 1, 2, 4, 4, 1, 1, 1, 1, 5, 1, 4, 1, 3, 2, 1, 2, 1, 1, 1, 1, 5, 1, 1, 2, 6, 1, 2, 1, 2, 1, 1, 2, 2, 1, 2, 2, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 6, 1, 1, 1, 2, 1, 4, 1, 1, 1, 1, 2, 1, 2, 3, 2, 1, 1, 5, 1, 1, 5, 1, 2, 2, 1, 3, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 5, 1, 1, 3, 3, 2, 1, 1, 2, 5, 2, 1, 1, 2, 2, 2, 1, 6, 3, 3, 2, 1, 7, 1, 2, 2, 3, 1, 1, 1, 1, 1, 1, 2, 1, 7, 1, 1, 1, 1, 2, 1, 1, 2, 1, 3, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 5, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 5, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 6, 3, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 4, 2, 1, 3, 2, 1, 1, 2, 1, 2, 1, 1, 1, 1, 7, 1, 1, 11, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 3, 1, 5, 1, 1, 1, 4, 2, 1, 2, 1, 7, 4, 1, 1, 1, 1, 2, 2, 1, 2, 1, 4, 3, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 3, 6, 3, 1, 2, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 6, 6, 1, 1, 1, 4, 3, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 2, 4, 1, 1, 11, 9, 8, 8, 8, 1, 1, 3, 1, 1, 1, 2, 2, 1, 1, 4, 2, 1, 1, 3, 3, 1, 1, 1, 1, 5, 6, 7, 8, 7, 4, 3, 4, 6, 4, 4, 5, 2, 4, 2, 3, 3, 1, 2, 5, 3, 5, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 6, 1, 1, 2, 1, 1, 2, 1, 1, 3, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 4, 3, 2, 1, 1, 5, 10, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 3, 2, 2, 1, 3, 3, 2, 2, 2, 3, 3, 1, 2, 3, 1, 1, 1, 1, 1, 2, 4, 1, 1, 1, 2, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 2, 1, 1, 3, 2, 2, 1, 1, 4, 5, 1, 1, 2, 1, 1, 1, 1, 6, 1, 6, 6, 12, 6, 6, 6, 7, 1, 3, 2, 2, 4, 1, 3, 1, 2, 1, 1, 3, 1, 3, 2, 1, 1, 1, 1, 4, 3, 2, 1, 1, 1, 3, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 3, 7, 1, 1, 3, 2, 4, 1, 2, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 6, 1, 18, 1, 1, 1, 11, 2, 2, 1, 2, 1, 1, 2, 2, 2, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 4, 1, 2, 2, 2, 2, 1, 1, 1, 4, 8, 4, 6, 3, 1, 1, 4, 23, 3, 1, 1, 2, 2, 1, 3, 2, 1, 1, 2, 2, 4, 1, 1, 1, 2, 1, 1, 2, 2, 3, 1, 12, 2, 2, 1, 1, 1, 1, 5, 2, 2, 2, 1, 2, 3, 2, 8, 3, 1, 1, 1, 14, 1, 2, 2, 1, 2, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 2, 2, 3, 1, 1, 1, 1, 1, 1, 1, 12, 1, 1, 1, 1, 4, 1, 1, 3, 1, 1, 1, 2, 1, 3, 1, 1, 1, 8, 1, 1, 2, 2, 4, 1, 2, 2, 1, 1, 1, 1, 7, 4, 8, 5, 1, 1, 1, 2, 1, 1, 9, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 4, 2, 2, 2, 4, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 5, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 2, 2, 2, 1, 1, 2, 4, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 1, 1, 1, 1, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 4, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "----\n",
      "vectors[4] = [223, 3, 488, 23, 207, 127, 75, 1263, 2, 5, 1, 5, 1, 4, 817, 6, 967, 1, 32, 1, 1, 1, 4, 1, 100, 3, 2, 48, 5, 950, 1, 2, 400, 5, 45, 3, 2, 16, 8, 257, 17, 3, 17, 10, 6, 49, 10, 93, 19, 7, 20, 7, 24, 7, 715, 8, 223, 3, 2, 2, 130, 4, 17, 6, 1, 1, 11, 62, 26, 48, 62, 1, 2, 2, 32, 7, 2, 1, 5, 1, 1, 1, 44, 114, 1, 2, 22, 5, 2, 416, 128, 633, 429, 2, 1, 1, 2, 95, 1, 20, 4, 3, 1, 1, 1, 2, 1, 2, 96, 109, 3, 21, 2, 14, 1, 127, 1, 26, 17, 2, 2, 18, 1, 9, 117, 28, 1, 10, 13, 12, 1, 12, 305, 1, 2, 1, 15, 43, 2, 4, 2, 2, 85, 12, 2, 42, 87, 6, 1, 3, 1, 49, 94, 1, 158, 1, 1, 1, 1, 4, 9, 8, 3, 3, 21, 2, 5, 3, 11, 2, 3, 5, 1, 1, 1, 5, 8, 21, 62, 2, 55, 7, 52, 1, 3, 1, 33, 21, 78, 2, 9, 1, 1, 1, 0, 9, 3, 3, 1, 1, 4, 4, 33, 10, 2, 368, 1, 1, 81, 89, 6, 1, 2, 25, 77, 55, 201, 14, 128, 6, 1, 2, 166, 2, 1, 1, 44, 21, 6, 1, 16, 20, 92, 7, 6, 4, 10, 4, 6, 5, 37, 7, 1, 3, 24, 2, 29, 9, 4, 3, 69, 1, 1, 6, 6, 8, 14, 4, 5, 4, 9, 1, 1, 1, 1, 4, 1, 5, 12, 7, 3, 1, 64, 5, 10, 2, 28, 1, 45, 1, 6, 1, 1, 1, 1, 4, 2, 17, 1, 2, 1, 1, 19, 6, 24, 53, 13, 15, 13, 1, 3, 4, 2, 29, 4, 15, 1, 24, 25, 2, 9, 11, 17, 12, 1, 12, 2, 1, 2, 7, 2, 1, 17, 1, 3, 2, 4, 7, 4, 1, 5, 19, 2, 17, 3, 1, 1, 2, 2, 159, 39, 1, 15, 1, 2, 591, 16, 1, 4, 29, 24, 1, 1, 1, 1, 30, 18, 22, 3, 1, 1, 27, 2, 2, 1, 12, 1, 1, 1, 1, 1, 1, 4, 1, 2, 3, 1, 5, 9, 3, 1, 13, 10, 25, 2, 1, 1, 1, 5, 1, 1, 6, 1, 52, 35, 1, 314, 10, 1, 3, 1, 1, 11, 256, 10, 23, 9, 29, 12, 213, 6, 0, 0, 1, 17, 0, 0, 51, 0, 3, 0, 34, 52, 0, 0, 57, 0, 5, 54, 0, 2, 0, 5, 1, 1, 31, 14, 7, 2, 0, 9, 0, 31, 12, 475, 9, 0, 43, 39, 1, 13, 10, 2, 208, 14, 6, 25, 4, 29, 3, 5, 6, 0, 1, 0, 2, 2, 7, 8, 0, 18, 0, 6, 77, 0, 0, 1, 0, 0, 0, 0, 0, 3, 6, 0, 5, 14, 0, 33, 0, 3, 0, 4, 21, 0, 7, 0, 26, 0, 5, 0, 6, 0, 0, 0, 1, 3, 9, 4, 0, 0, 0, 0, 8, 1, 2, 2, 5, 102, 3, 2, 0, 0, 0, 0, 0, 0, 0, 30, 0, 0, 12, 1, 0, 0, 0, 4, 8, 73, 12, 17, 22, 2, 56, 5, 3, 5, 11, 31, 0, 0, 29, 29, 6, 3, 6, 0, 0, 0, 0, 0, 74, 0, 13, 17, 4, 36, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 13, 0, 5, 14, 3, 3, 4, 5, 0, 0, 0, 4, 0, 0, 0, 2, 8, 2, 16, 8, 44, 1, 3, 1, 2, 12, 2, 20, 30, 82, 1, 0, 4, 1, 14, 0, 11, 0, 5, 16, 0, 24, 5, 0, 33, 0, 0, 6, 2, 2, 5, 1, 20, 9, 33, 1, 1, 6, 1, 2, 56, 15, 0, 0, 3, 1, 0, 1, 0, 2, 24, 2, 6, 6, 0, 1, 0, 34, 0, 5, 14, 2, 1, 28, 0, 0, 0, 78, 2, 2, 0, 1, 0, 1, 9, 3, 17, 1, 29, 0, 1, 0, 4, 0, 1, 0, 2, 1, 0, 1, 1, 11, 1, 0, 0, 3, 0, 0, 2, 0, 0, 6, 0, 62, 32, 24, 0, 0, 2, 0, 2, 4, 1, 1, 0, 6, 2, 18, 0, 1, 0, 14, 1, 0, 0, 0, 0, 25, 0, 25, 0, 28, 1, 0, 16, 10, 3, 1, 8, 0, 0, 1, 0, 46, 1, 0, 7, 0, 2, 1, 2, 6, 1, 1, 8, 1, 1, 0, 0, 7, 0, 6, 3, 18, 0, 0, 1, 23, 0, 0, 7, 7, 0, 9, 1, 70, 3, 10, 0, 5, 0, 1, 0, 4, 0, 1, 2, 6, 0, 4, 0, 0, 0, 40, 19, 13, 1, 9, 2, 7, 0, 4, 16, 18, 3, 0, 0, 2, 5, 1, 0, 1, 1, 0, 0, 52, 0, 2, 0, 10, 0, 1, 0, 12, 8, 11, 0, 6, 4, 29, 0, 1, 16, 0, 17, 0, 0, 0, 10, 28, 12, 0, 2, 0, 1, 0, 5, 21, 10, 7, 2, 0, 1, 2, 3, 0, 16, 3, 3, 0, 8, 9, 0, 0, 0, 5, 2, 0, 21, 2, 3, 11, 1, 4, 12, 26, 1, 0, 1, 5, 0, 0, 0, 1, 19, 42, 3, 46, 0, 0, 550, 0, 135, 12, 3, 0, 45, 0, 3, 48, 9, 5, 1, 7, 2, 16, 9, 0, 4, 16, 1, 49, 1, 1, 14, 0, 12, 40, 4, 14, 6, 1, 3, 54, 10, 4, 31, 2, 11, 8, 32, 7, 2, 0, 4, 11, 1, 12, 8, 0, 0, 17, 4, 3, 0, 12, 0, 0, 1, 2, 8, 1, 0, 0, 2, 0, 5, 0, 3, 13, 0, 1, 1, 1, 0, 4, 4, 5, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 9, 0, 0, 0, 0, 6, 1, 0, 3, 7, 1, 1, 3, 6, 3, 40, 0, 1, 1, 3, 51, 2, 46, 2, 2, 2, 3, 6, 28, 23, 1, 11, 1, 8, 6, 0, 0, 3, 21, 1, 3, 2, 0, 24, 0, 0, 16, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 1, 2, 0, 1, 0, 1, 10, 0, 0, 0, 8, 2, 0, 1, 0, 9, 5, 1, 0, 5, 20, 8, 0, 0, 0, 0, 7, 0, 0, 8, 0, 0, 1, 19, 1, 1, 8, 0, 4, 12, 0, 2, 7, 6, 16, 22, 3, 0, 2, 2, 0, 2, 1, 9, 1, 0, 4, 6, 3, 3, 5, 0, 0, 0, 0, 0, 0, 0, 0, 20, 0, 3, 1, 0, 0, 0, 16, 33, 1, 0, 0, 0, 0, 2, 0, 1, 0, 0, 0, 14, 1, 0, 0, 0, 0, 0, 0, 0, 0, 20, 2, 0, 0, 0, 0, 0, 2, 0, 0, 3, 3, 0, 1, 0, 1, 4, 0, 0, 2, 4, 0, 0, 0, 1, 5, 0, 6, 0, 3, 1, 7, 0, 0, 0, 0, 0, 0, 0, 1, 0, 13, 1, 0, 0, 0, 0, 3, 7, 0, 2, 1, 0, 1, 2, 1, 0, 0, 0, 0, 8, 0, 0, 1, 0, 0, 0, 0, 0, 4, 0, 36, 0, 0, 9, 4, 0, 2, 1, 0, 2, 0, 3, 0, 4, 3, 0, 0, 0, 9, 9, 27, 0, 15, 2, 5, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 20, 0, 0, 0, 7, 10, 4, 0, 30, 1, 0, 0, 0, 0, 5, 8, 0, 26, 0, 1, 4, 0, 0, 2, 3, 0, 0, 1, 4, 0, 23, 0, 0, 9, 2, 14, 0, 3, 8, 14, 1, 0, 0, 2, 0, 2, 3, 14, 1, 2, 3, 0, 0, 0, 1, 0, 2, 0, 23, 0, 0, 1, 0, 3, 0, 8, 2, 0, 0, 5, 0, 1, 0, 0, 0, 0, 0, 0, 4, 0, 4, 2, 0, 1, 10, 1, 3, 11, 9, 0, 12, 0, 0, 1, 0, 0, 44, 0, 2, 0, 0, 0, 0, 2, 8, 0, 0, 0, 0, 3, 4, 0, 0, 0, 1, 0, 14, 0, 0, 2, 0, 3, 0, 1, 3, 2, 1, 2, 7, 17, 1, 5, 0, 2, 1, 0, 0, 0, 0, 0, 5, 1, 1, 4, 16, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 2, 5, 1, 1, 2, 0, 0, 0, 0, 0, 0, 1, 5, 0, 10, 12, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 28, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 14, 1, 1, 6, 0, 0, 0, 0, 0, 0, 0, 4, 3, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 20, 0, 5, 17, 11, 2, 0, 3, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 2, 2, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 3, 3, 151, 1, 1, 7, 1, 1, 3, 6, 5, 1, 1, 3, 1, 13, 1, 2, 5, 1, 1, 1, 2, 0, 0, 22, 0, 1, 1, 0, 0, 0, 2, 2, 1, 9, 1, 3, 0, 0, 0, 1, 1, 2, 1, 0, 0, 0, 1, 1, 9, 0, 1, 0, 3, 0, 5, 5, 0, 0, 0, 7, 0, 1, 3, 0, 0, 12, 0, 0, 4, 0, 2, 0, 63, 0, 4, 0, 1, 11, 0, 0, 0, 0, 0, 0, 13, 0, 0, 0, 2, 0, 0, 11, 0, 0, 48, 0, 13, 6, 0, 0, 46, 2, 0, 0, 13, 3, 0, 0, 0, 1, 0, 21, 5, 1, 0, 0, 0, 3, 0, 0, 0, 0, 0, 1, 0, 0, 0, 2, 0, 0, 3, 1, 1, 1, 0, 0, 0, 0, 3, 13, 3, 0, 0, 0, 0, 0, 2, 4, 0, 4, 1, 7, 0, 1, 2, 0, 0, 0, 0, 1, 1, 7, 0, 0, 0, 2, 0, 0, 2, 0, 3, 1, 0, 3, 0, 3, 0, 1, 4, 18, 0, 0, 2, 2, 0, 3, 3, 0, 0, 5, 0, 0, 25, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 34, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 13, 0, 0, 6, 0, 0, 0, 0, 0, 1, 2, 0, 34, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 1, 0, 0, 0, 9, 0, 0, 0, 1, 7, 0, 0, 0, 0, 1, 0, 0, 3, 0, 0, 0, 1, 1, 0, 0, 0, 4, 0, 4, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 1, 0, 0, 0, 5, 0, 0, 0, 0, 0, 1, 0, 1, 3, 1, 7, 0, 0, 0, 0, 0, 0, 0, 11, 0, 19, 0, 10, 2, 0, 2, 0, 3, 0, 0, 7, 0, 1, 4, 0, 1, 0, 2, 0, 3, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 1, 0, 7, 11, 5, 3, 4, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 2, 0, 1, 1, 5, 14, 3, 0, 0, 0, 0, 8, 1, 1, 0, 1, 18, 0, 2, 2, 0, 2, 2, 0, 1, 0, 0, 0, 25, 0, 3, 4, 0, 0, 3, 0, 0, 22, 0, 379, 0, 2, 2, 9, 0, 11, 0, 1, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 3, 0, 0, 0, 13, 3, 0, 0, 25, 2, 0, 1, 3, 4, 0, 15, 0, 0, 4, 0, 1, 3, 0, 0, 0, 10, 42, 0, 0, 1, 0, 4, 0, 1, 10, 5, 1, 0, 1, 3, 0, 0, 12, 0, 2, 3, 27, 7, 11, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 17, 13, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 7, 0, 10, 0, 0, 0, 2, 12, 2, 86, 0, 0, 7, 2, 2, 3, 0, 0, 11, 2, 1, 4, 1, 0, 0, 0, 0, 0, 4, 2, 3, 0, 0, 7, 0, 1, 42, 0, 6, 0, 0, 0, 0, 0, 0, 0, 8, 5, 4, 1, 0, 6, 7, 0, 0, 0, 0, 0, 1, 1, 0, 0, 3, 0, 1, 0, 0, 0, 4, 0, 0, 0, 0, 3, 0, 1, 0, 4, 0, 0, 85, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 20, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 10, 30, 0, 0, 1, 1, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 5, 1, 7, 3, 2, 0, 6, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 7, 5, 0, 0, 2, 2, 1, 2, 0, 0, 2, 0, 1, 3, 1, 4, 5, 0, 0, 0, 0, 11, 9, 4, 0, 0, 0, 0, 0, 6, 6, 0, 0, 1, 4, 0, 0, 1, 1, 0, 3, 0, 3, 3, 4, 0, 4, 4, 1, 0, 0, 0, 0, 2, 0, 0, 0, 10, 0, 0, 5, 1, 0, 0, 0, 0, 0, 0, 6, 7, 5, 3, 0, 2, 0, 0, 0, 0, 7, 0, 2, 1, 3, 7, 5, 0, 0, 2, 4, 0, 0, 0, 0, 0, 0, 20, 0, 0, 0, 5, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 4, 0, 8, 1, 0, 0, 0, 0, 0, 3, 1, 0, 0, 1, 2, 0, 0, 2, 1, 0, 13, 0, 0, 0, 13, 0, 1, 11, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 3, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 7, 0, 0, 1, 0, 0, 8, 0, 0, 0, 1, 3, 5, 2, 0, 8, 0, 0, 0, 0, 0, 0, 1, 0, 2, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 3, 4, 4, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 2, 0, 4, 0, 4, 0, 0, 1, 1, 4, 1, 0, 0, 0, 0, 0, 1, 0, 9, 0, 0, 0, 0, 1, 1, 0, 0, 10, 3, 0, 0, 0, 0, 0, 1, 0, 3, 3, 0, 1, 5, 7, 0, 3, 5, 8, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 2, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 5, 0, 0, 0, 0, 1, 1, 0, 0, 0, 12, 0, 1, 3, 2, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 21, 1, 0, 7, 16, 2, 2, 1, 0, 0, 2, 1, 0, 0, 0, 4, 8, 0, 1, 0, 0, 6, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 2, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 5, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 4, 1, 0, 0, 0, 0, 1, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 15, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 4, 0, 4, 0, 1, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 2, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 0, 3, 3, 3, 2, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 6, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 4, 0, 1, 0, 0, 0, 0, 0, 3, 0, 0, 0, 2, 0, 1, 6, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 6, 0, 1, 3, 2, 0, 0, 0, 0, 0, 3, 0, 4, 0, 9, 0, 0, 5, 0, 1, 0, 4, 0, 0, 7, 2, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 9, 2, 0, 0, 0, 0, 0, 2, 0, 2, 0, 3, 0, 0, 0, 3, 0, 1, 0, 0, 0, 1, 0, 2, 0, 1, 2, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 4, 11, 1, 0, 0, 3, 1, 4, 0, 0, 13, 1, 0, 0, 0, 1, 2, 0, 1, 0, 0, 2, 0, 0, 0, 0, 0, 1, 0, 5, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 2, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 2, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 28, 0, 0, 1, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 3, 0, 2, 0, 2, 0, 2, 0, 0, 0, 0, 0, 2, 0, 0, 2, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 9, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 10, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 1, 10, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 3, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 3, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 5, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 5, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 5, 2, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 5, 14, 0, 0, 41, 0, 0, 0, 0, 0, 0, 1, 0, 2, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 2, 2, 5, 10, 0, 2, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 2, 2, 1, 0, 0, 0, 0, 0, 0, 1, 3, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 1, 3, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 21, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 1, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 3, 15, 0, 20, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 26, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 1, 0, 0, 1, 1, 14, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 2, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 6, 3, 0, 0, 0, 4, 8, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 2, 0, 0, 0, 0, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 4, 0, 2, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 4, 1, 0, 1, 0, 2, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 1, 1, 0, 0, 0, 0, 0, 0, 5, 5, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 3, 0, 0, 0, 1, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 7, 9, 0, 0, 0, 1, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 3, 3, 0, 0, 0, 0, 0, 0, 0, 2, 16, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 2, 10, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 3, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 3, 0, 3, 0, 2, 3, 0, 7, 0, 0, 4, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 5, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 1, 10, 0, 0, 1, 2, 0, 0, 0, 1, 2, 4, 0, 0, 2, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 2, 6, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 7, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 2, 0, 4, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 3, 0, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 4, 0, 6, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 15, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 2, 0, 0, 7, 1, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 16, 0, 0, 1, 0, 1, 1, 1, 0, 2, 0, 3, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 1, 0, 10, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 3, 1, 1, 19, 1, 1, 2, 2, 2, 2, 1, 1, 1, 2, 2, 2, 1, 1, 1, 1, 2, 3, 2, 2, 1, 24, 1, 2, 4, 1, 2, 1, 2, 2, 1, 4, 6, 1, 2, 2, 8, 2, 1, 25, 1, 1, 3, 1, 4, 1, 2, 1, 14, 10, 1, 4, 1, 1, 3, 1, 1, 1, 1, 6, 1, 1, 2, 2, 4, 1, 1, 10, 5, 6, 1, 1, 1, 1, 6, 1, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 2, 1, 1, 2, 1, 2, 2, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 3, 5, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 5, 1, 1, 2, 1, 1, 1, 1, 1, 7, 1, 1, 6, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 7, 6, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 6, 2, 1, 1, 3, 1, 2, 2, 1, 1, 2, 1, 2, 4, 1, 8, 1, 1, 1, 3, 1, 1, 11, 1, 1, 1, 1, 1, 8, 1, 1, 2, 2, 2, 2, 1, 1, 5, 1, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 5, 2, 1, 1, 1, 1, 1, 4, 1, 1, 3, 1, 1, 5, 2, 1, 2, 1, 9, 2, 1, 1, 4, 2, 2, 2, 1, 1, 1, 5, 1, 1, 1, 1, 13, 2, 1, 1, 2, 2, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 3, 1, 2, 1, 1, 3, 1, 2, 5, 5, 2, 2, 1, 1, 1, 6, 1, 1, 1, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 2, 1, 7, 5, 4, 1, 7, 7, 7, 1, 1, 5, 1, 1, 2, 1, 1, 1, 1, 1, 4, 7, 3, 2, 2, 10, 3, 3, 3, 1, 3, 1, 1, 1, 1, 4, 1, 1, 1, 2, 1, 2, 2, 1, 1, 1, 1, 2, 10, 1, 1, 3, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 3, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 11, 3, 1, 6, 12, 3, 1, 2, 1, 1, 1, 2, 1, 1, 8, 2, 7, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 2, 4, 3, 4, 1, 1, 1, 1, 15, 3, 1, 2, 2, 1, 5, 3, 10, 1, 2, 1, 1, 1, 2, 30, 1, 1, 4, 3, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 3, 2, 1, 1, 1, 3, 1, 1, 1, 1, 1, 3, 2, 1, 1, 1, 4, 5, 4, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 2, 17, 1, 1, 1, 1, 4, 2, 4, 6, 6, 1, 2, 7, 4, 1, 6, 2, 4, 1, 1, 6, 1, 1, 1, 1, 3, 5, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 12, 1, 1, 1, 1, 2, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 2, 3, 3, 1, 1, 1, 1, 1, 3, 2, 1, 2, 2, 1, 1, 1, 1, 1, 1, 8, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 4, 1, 3, 5, 1, 1, 1, 2, 1, 2, 1, 2, 2, 2, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 4, 2, 1, 1, 1, 2, 1, 1, 1, 1, 5, 1, 2, 2, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "----\n",
      "vectors[5] = [15, 3, 354, 13, 147, 104, 15, 1140, 2, 10, 1, 5, 1, 4, 683, 6, 727, 1, 13, 1, 1, 1, 6, 1, 97, 3, 2, 59, 5, 791, 1, 2, 323, 7, 19, 4, 2, 16, 8, 170, 7, 3, 31, 10, 6, 36, 5, 48, 20, 8, 16, 7, 27, 7, 614, 8, 204, 1, 2, 2, 99, 4, 19, 6, 1, 1, 11, 62, 26, 48, 55, 1, 2, 2, 32, 3, 2, 1, 5, 1, 1, 1, 24, 76, 1, 2, 19, 4, 1, 360, 205, 600, 348, 2, 1, 1, 2, 58, 1, 2, 2, 3, 1, 1, 1, 1, 1, 2, 87, 68, 3, 11, 2, 26, 1, 87, 1, 13, 14, 2, 2, 6, 1, 5, 37, 8, 1, 5, 7, 5, 1, 12, 391, 1, 2, 1, 5, 5, 2, 5, 2, 2, 60, 6, 2, 15, 170, 37, 1, 4, 1, 45, 87, 1, 113, 1, 1, 1, 1, 4, 8, 23, 3, 3, 10, 2, 4, 3, 6, 2, 3, 5, 1, 1, 1, 2, 5, 24, 13, 2, 6, 5, 15, 1, 3, 1, 4, 12, 60, 2, 2, 1, 1, 1, 0, 2, 2, 2, 1, 1, 2, 4, 22, 17, 1, 354, 1, 3, 45, 110, 5, 1, 2, 32, 50, 113, 128, 13, 20, 3, 1, 3, 137, 3, 1, 2, 44, 14, 1, 1, 6, 28, 91, 1, 6, 8, 10, 1, 11, 1, 17, 1, 1, 4, 24, 1, 30, 9, 5, 3, 58, 5, 1, 6, 6, 5, 44, 5, 7, 4, 4, 1, 2, 1, 1, 1, 1, 5, 18, 2, 3, 1, 13, 3, 10, 1, 22, 1, 32, 1, 1, 1, 1, 1, 1, 2, 2, 9, 1, 2, 1, 1, 13, 6, 24, 63, 13, 10, 1, 1, 3, 9, 2, 29, 4, 14, 2, 12, 25, 2, 6, 12, 17, 12, 1, 3, 1, 1, 2, 7, 1, 1, 11, 1, 4, 4, 4, 44, 4, 1, 5, 14, 2, 20, 3, 1, 1, 2, 2, 73, 18, 1, 30, 1, 2, 629, 25, 1, 4, 34, 4, 1, 1, 1, 1, 30, 18, 64, 3, 1, 3, 5, 2, 2, 1, 12, 1, 1, 1, 1, 1, 1, 2, 1, 1, 3, 1, 6, 4, 3, 1, 3, 2, 8, 2, 1, 1, 1, 1, 1, 1, 5, 4, 59, 43, 17, 246, 55, 1, 5, 1, 1, 5, 209, 7, 16, 5, 23, 12, 188, 4, 0, 0, 6, 24, 11, 2, 55, 1, 4, 0, 31, 3, 0, 2, 41, 0, 0, 50, 0, 1, 0, 2, 0, 23, 6, 7, 6, 1, 0, 8, 1, 20, 11, 432, 1, 0, 53, 30, 0, 7, 8, 0, 290, 20, 3, 16, 0, 29, 2, 2, 1, 0, 3, 0, 6, 2, 5, 17, 0, 13, 1, 9, 62, 0, 0, 2, 0, 0, 0, 0, 0, 4, 5, 0, 10, 20, 0, 25, 0, 1, 0, 1, 16, 1, 0, 1, 3, 1, 7, 1, 2, 1, 0, 9, 1, 1, 1, 8, 0, 2, 0, 0, 5, 1, 0, 0, 0, 94, 2, 3, 1, 2, 0, 2, 0, 4, 0, 77, 0, 0, 1, 2, 0, 0, 0, 2, 3, 65, 7, 2, 14, 1, 42, 1, 0, 0, 8, 29, 2, 0, 6, 6, 0, 7, 0, 0, 0, 0, 0, 0, 30, 0, 16, 26, 4, 26, 0, 0, 0, 0, 0, 4, 0, 0, 0, 1, 30, 1, 7, 5, 3, 2, 5, 3, 0, 0, 1, 1, 0, 1, 0, 2, 4, 0, 13, 10, 9, 0, 0, 0, 3, 6, 3, 33, 7, 11, 1, 2, 8, 0, 54, 3, 12, 1, 3, 21, 1, 13, 1, 0, 14, 0, 0, 1, 3, 0, 2, 1, 6, 8, 45, 1, 0, 0, 2, 0, 41, 19, 0, 0, 1, 1, 0, 0, 0, 0, 18, 0, 1, 2, 0, 0, 0, 12, 1, 2, 3, 1, 2, 18, 0, 0, 0, 62, 23, 0, 0, 1, 0, 2, 1, 0, 4, 1, 18, 0, 1, 0, 3, 0, 1, 1, 5, 0, 0, 1, 2, 16, 0, 1, 0, 3, 0, 0, 0, 0, 0, 5, 0, 30, 2, 16, 3, 0, 0, 0, 1, 2, 1, 1, 0, 3, 1, 2, 0, 0, 0, 3, 1, 3, 0, 1, 0, 38, 0, 7, 0, 38, 2, 0, 4, 5, 19, 30, 6, 1, 0, 0, 0, 18, 0, 0, 4, 2, 0, 0, 0, 10, 1, 23, 6, 0, 0, 0, 0, 1, 0, 6, 3, 9, 0, 0, 0, 14, 0, 0, 1, 7, 2, 0, 2, 63, 0, 1, 0, 1, 0, 0, 10, 9, 0, 4, 0, 1, 0, 1, 61, 3, 0, 17, 9, 20, 0, 34, 7, 5, 1, 4, 9, 5, 1, 0, 0, 1, 5, 0, 0, 1, 0, 11, 0, 33, 0, 0, 6, 12, 5, 2, 0, 20, 1, 11, 0, 5, 6, 12, 4, 0, 15, 0, 4, 1, 1, 0, 3, 16, 8, 0, 9, 0, 0, 0, 7, 24, 13, 1, 2, 0, 0, 2, 1, 0, 20, 0, 0, 0, 0, 10, 1, 0, 1, 0, 6, 0, 11, 0, 6, 6, 0, 5, 3, 19, 0, 1, 0, 1, 0, 2, 0, 3, 17, 12, 2, 11, 2, 4, 512, 0, 69, 4, 5, 1, 253, 2, 3, 46, 1, 0, 0, 6, 5, 1, 0, 0, 1, 6, 1, 2, 0, 2, 3, 0, 14, 11, 1, 1, 1, 0, 0, 33, 5, 3, 2, 0, 1, 5, 35, 4, 0, 2, 6, 36, 0, 1, 5, 0, 10, 12, 13, 3, 1, 5, 0, 3, 0, 0, 4, 0, 0, 1, 1, 1, 7, 0, 1, 0, 5, 4, 0, 0, 0, 11, 1, 0, 0, 0, 0, 0, 2, 3, 0, 0, 1, 0, 5, 0, 0, 1, 0, 1, 2, 1, 1, 54, 53, 0, 3, 1, 6, 7, 4, 0, 3, 6, 3, 4, 11, 1, 3, 3, 1, 4, 2, 25, 0, 6, 2, 3, 4, 0, 0, 1, 20, 0, 1, 0, 0, 1, 0, 0, 6, 3, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 196, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 6, 9, 0, 0, 3, 16, 9, 0, 0, 0, 0, 4, 0, 2, 0, 0, 0, 0, 4, 0, 0, 3, 1, 0, 0, 0, 2, 0, 4, 0, 0, 1, 0, 4, 0, 2, 0, 0, 0, 0, 5, 0, 2, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 20, 1, 2, 0, 1, 3, 0, 10, 22, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 227, 33, 26, 15, 0, 0, 2, 1, 1, 1, 3, 0, 0, 0, 0, 0, 3, 1, 0, 14, 2, 0, 0, 0, 0, 0, 0, 6, 0, 0, 6, 0, 1, 3, 1, 0, 2, 0, 3, 2, 0, 2, 1, 0, 0, 0, 0, 3, 11, 1, 0, 0, 0, 0, 0, 1, 0, 0, 2, 0, 7, 1, 3, 0, 3, 0, 0, 0, 0, 0, 0, 22, 0, 0, 0, 3, 0, 3, 1, 0, 0, 0, 1, 0, 3, 0, 0, 0, 0, 3, 3, 16, 0, 14, 6, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 5, 0, 0, 0, 4, 6, 0, 3, 15, 0, 0, 0, 0, 0, 6, 10, 1, 32, 0, 2, 0, 0, 2, 3, 3, 0, 2, 3, 7, 0, 16, 0, 0, 12, 0, 5, 4, 3, 7, 6, 0, 0, 0, 0, 0, 0, 5, 0, 1, 1, 4, 5, 0, 0, 0, 0, 0, 0, 130, 0, 0, 0, 0, 0, 0, 55, 2, 0, 0, 2, 0, 0, 1, 1, 1, 0, 0, 1, 7, 0, 2, 0, 2, 0, 0, 0, 3, 7, 8, 0, 5, 1, 2, 1, 0, 0, 11, 0, 3, 0, 1, 0, 0, 1, 9, 0, 0, 0, 0, 2, 9, 2, 0, 1, 2, 0, 0, 1, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 4, 6, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 2, 1, 12, 0, 0, 0, 0, 0, 0, 0, 9, 0, 0, 0, 0, 0, 1, 0, 1, 2, 0, 0, 0, 1, 1, 1, 1, 3, 0, 0, 0, 0, 0, 0, 0, 1, 0, 20, 15, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 17, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 15, 5, 0, 4, 0, 0, 0, 0, 0, 1, 0, 0, 2, 0, 0, 0, 5, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 3, 0, 0, 1, 0, 0, 13, 0, 12, 17, 25, 0, 0, 0, 0, 0, 3, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 1, 0, 0, 3, 1, 2, 2, 67, 1, 1, 7, 1, 1, 1, 10, 2, 1, 4, 1, 1, 2, 1, 1, 1, 1, 1, 2, 0, 1, 0, 20, 0, 1, 0, 4, 4, 1, 2, 0, 6, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 4, 1, 0, 8, 2, 1, 2, 1, 1, 0, 1, 0, 1, 0, 21, 6, 0, 6, 0, 0, 0, 18, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 22, 0, 0, 5, 5, 0, 0, 2, 0, 0, 5, 0, 5, 1, 0, 0, 26, 0, 4, 0, 22, 4, 0, 0, 0, 0, 0, 10, 11, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 6, 0, 0, 2, 2, 0, 0, 0, 3, 10, 0, 1, 0, 1, 1, 0, 1, 2, 3, 0, 0, 14, 0, 0, 0, 0, 1, 0, 0, 6, 0, 0, 0, 0, 0, 8, 1, 0, 0, 0, 4, 2, 0, 5, 0, 2, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 13, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 2, 3, 0, 0, 0, 5, 5, 2, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 19, 0, 0, 3, 1, 0, 0, 0, 0, 3, 1, 0, 1, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 11, 0, 0, 0, 0, 6, 0, 2, 0, 0, 13, 0, 3, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 8, 0, 8, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 4, 6, 0, 0, 0, 0, 1, 1, 1, 2, 3, 0, 0, 0, 0, 0, 0, 0, 0, 19, 1, 13, 4, 1, 2, 0, 1, 0, 6, 0, 1, 0, 0, 10, 4, 0, 0, 0, 6, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 10, 1, 1, 17, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 1, 0, 0, 4, 0, 0, 2, 0, 6, 9, 2, 0, 1, 0, 9, 0, 5, 0, 1, 15, 0, 5, 1, 0, 1, 5, 0, 0, 0, 0, 0, 26, 0, 11, 3, 0, 0, 1, 0, 0, 19, 0, 546, 3, 0, 0, 3, 0, 17, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 21, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 25, 1, 0, 4, 4, 0, 0, 0, 1, 1, 0, 0, 3, 0, 6, 0, 0, 1, 0, 0, 0, 7, 5, 0, 0, 0, 0, 0, 0, 0, 4, 8, 1, 0, 2, 2, 0, 0, 185, 0, 3, 4, 9, 3, 9, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 1, 53, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 18, 2, 0, 0, 7, 16, 0, 56, 1, 0, 0, 0, 0, 0, 0, 0, 7, 6, 2, 0, 0, 0, 0, 0, 0, 0, 4, 1, 0, 0, 1, 0, 0, 3, 45, 0, 4, 0, 1, 0, 0, 1, 0, 0, 4, 3, 2, 1, 2, 1, 44, 0, 0, 0, 0, 0, 0, 0, 0, 5, 2, 0, 0, 0, 0, 0, 2, 3, 1, 0, 0, 1, 0, 0, 0, 4, 0, 0, 32, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 5, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 2, 0, 2, 3, 4, 1, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 1, 2, 2, 3, 0, 0, 0, 0, 0, 0, 5, 0, 3, 0, 2, 2, 0, 1, 0, 0, 0, 0, 2, 0, 0, 0, 0, 3, 0, 0, 0, 1, 4, 1, 1, 0, 1, 1, 0, 1, 4, 0, 2, 4, 0, 0, 0, 0, 6, 2, 0, 0, 0, 0, 0, 0, 3, 4, 0, 0, 0, 1, 0, 1, 0, 0, 1, 7, 0, 0, 2, 0, 0, 2, 0, 0, 0, 0, 2, 0, 0, 1, 0, 0, 0, 0, 2, 0, 0, 0, 1, 0, 0, 0, 1, 4, 0, 1, 0, 0, 1, 0, 6, 1, 1, 4, 1, 3, 0, 8, 1, 1, 0, 1, 0, 14, 2, 2, 0, 0, 0, 5, 10, 0, 0, 0, 10, 0, 0, 0, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 6, 1, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 2, 1, 1, 0, 3, 3, 2, 0, 37, 0, 0, 12, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 7, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 24, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 40, 12, 1, 0, 0, 0, 1, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 3, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 77, 0, 0, 0, 0, 0, 0, 0, 7, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 2, 2, 1, 1, 0, 2, 0, 4, 0, 0, 0, 0, 3, 1, 0, 0, 0, 0, 0, 1, 1, 1, 2, 2, 0, 0, 5, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 19, 3, 0, 0, 2, 2, 0, 2, 0, 0, 0, 1, 0, 2, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 11, 0, 0, 0, 0, 13, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0, 0, 0, 0, 0, 4, 0, 0, 1, 4, 0, 0, 3, 3, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 10, 2, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 5, 0, 0, 0, 0, 0, 33, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 2, 0, 3, 0, 0, 0, 0, 0, 1, 15, 5, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 4, 0, 1, 0, 1, 0, 0, 4, 0, 0, 2, 0, 0, 0, 0, 0, 81, 0, 2, 0, 0, 0, 0, 1, 1, 1, 0, 2, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 1, 0, 1, 0, 0, 2, 0, 1, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 88, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 10, 0, 0, 6, 0, 0, 0, 0, 0, 1, 0, 0, 0, 2, 0, 0, 1, 3, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 0, 0, 4, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 3, 0, 0, 3, 0, 1, 0, 2, 0, 0, 2, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 2, 0, 1, 1, 0, 0, 0, 0, 3, 1, 2, 0, 10, 1, 2, 12, 0, 2, 0, 10, 0, 0, 3, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 10, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 5, 1, 0, 0, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 1, 0, 0, 4, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 16, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 2, 1, 0, 2, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 3, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 2, 0, 0, 0, 2, 0, 2, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 7, 3, 3, 5, 0, 0, 4, 1, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 14, 8, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 34, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 2, 0, 1, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 9, 0, 1, 0, 0, 0, 2, 2, 0, 0, 0, 0, 46, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 18, 0, 0, 0, 0, 4, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 3, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 14, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 1, 0, 0, 2, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 10, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 5, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 3, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 1, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 15, 0, 2, 0, 0, 0, 10, 1, 0, 22, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 0, 0, 7, 5, 0, 0, 1, 0, 3, 0, 3, 0, 0, 0, 0, 0, 1, 0, 10, 0, 1, 0, 1, 2, 0, 0, 1, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 6, 11, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 3, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 2, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 1, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 17, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 9, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 9, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 3, 0, 2, 0, 0, 3, 1, 0, 2, 0, 7, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 10, 0, 2, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 20, 9, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 4, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 3, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 2, 3, 0, 1, 0, 12, 0, 0, 0, 0, 0, 1, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 1, 20, 20, 14, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 2, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 3, 0, 1, 0, 0, 0, 0, 2, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 10, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 2, 0, 0, 0, 12, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 3, 0, 0, 0, 16, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 7, 0, 0, 0, 0, 25, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 1, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 2, 0, 1, 0, 1, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 17, 0, 0, 0, 0, 2, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 26, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 9, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 2, 0, 1, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 39, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 26, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 11, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 6, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 6, 1, 1, 2, 35, 1, 2, 9, 134, 18, 1, 1, 3, 1, 1, 1, 7, 4, 2, 13, 2, 1, 1, 1, 1, 1, 3, 1, 1, 1, 5, 2, 7, 1, 1, 1, 1, 1, 2, 1, 4, 4, 2, 1, 44, 11, 1, 2, 14, 1, 4, 3, 1, 1, 4, 2, 1, 10, 1, 1, 1, 2, 3, 1, 1, 1, 1, 1, 1, 13, 11, 6, 7, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 6, 7, 1, 28, 5, 1, 2, 1, 6, 1, 1, 1, 1, 1, 1, 7, 3, 3, 2, 2, 1, 1, 1, 1, 1, 1, 1, 7, 7, 5, 2, 1, 1, 2, 1, 2, 2, 2, 1, 1, 1, 3, 1, 3, 1, 1, 1, 1, 3, 1, 2, 3, 1, 1, 1, 6, 3, 1, 1, 1, 1, 1, 2, 7, 1, 1, 3, 5, 1, 3, 1, 3, 1, 1, 3, 1, 1, 1, 1, 1, 1, 3, 3, 7, 3, 2, 2, 2, 3, 1, 2, 1, 1, 6, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 3, 10, 16, 2, 1, 2, 1, 1, 1, 3, 2, 8, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 2, 1, 1, 1, 1, 22, 1, 1, 1, 1, 20, 2, 3, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 3, 1, 2, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 6, 2, 2, 2, 2, 2, 2, 2, 1, 2, 1, 1, 1, 1, 2, 1, 1, 7, 1, 3, 1, 1, 1, 1, 1, 1, 4, 4, 11, 6, 1, 1, 1, 2, 2, 2, 3, 5, 1, 1, 1, 3, 2, 1, 2, 1, 3, 1, 1, 1, 2, 1, 4, 3, 2, 1, 1, 1, 1, 9, 2, 3, 6, 1, 2, 2, 2, 1, 2, 2, 2, 1, 3, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 5, 2, 1, 1, 1, 1, 1, 1, 2, 2, 23, 5, 5, 2, 2, 3, 2, 5, 3, 1, 1, 1, 1, 1, 1, 5, 4, 1, 1, 1, 1, 2, 2, 1, 1, 2, 4, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 2, 1, 2, 1, 1, 2, 1, 1, 3, 2, 2, 1, 2, 2, 2, 6, 1, 2, 1, 1, 2, 2, 1, 1, 1, 3, 1, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 2, 2, 3, 5, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "----\n",
      "vectors[6] = [18, 3, 473, 27, 242, 104, 37, 1166, 2, 5, 1, 5, 1, 4, 400, 4, 836, 1, 19, 1, 1, 1, 11, 1, 100, 3, 2, 76, 5, 932, 1, 4, 346, 3, 22, 3, 2, 16, 8, 226, 51, 3, 8, 10, 6, 35, 3, 120, 19, 3, 20, 7, 24, 7, 534, 8, 259, 1, 2, 2, 125, 4, 18, 6, 1, 1, 11, 62, 26, 48, 46, 1, 2, 3, 32, 9, 2, 1, 5, 1, 1, 1, 9, 172, 1, 2, 20, 1, 1, 249, 165, 475, 401, 2, 1, 1, 2, 15, 1, 2, 1, 3, 1, 1, 1, 1, 1, 2, 39, 90, 3, 17, 2, 9, 1, 63, 1, 25, 21, 2, 2, 6, 1, 5, 45, 27, 1, 2, 7, 5, 1, 12, 328, 1, 2, 1, 15, 4, 2, 3, 2, 2, 31, 31, 2, 15, 42, 2, 1, 1, 1, 55, 139, 1, 114, 1, 1, 1, 1, 4, 7, 18, 3, 3, 11, 2, 11, 3, 4, 2, 3, 5, 1, 1, 1, 2, 4, 32, 5, 2, 18, 3, 5, 1, 3, 1, 3, 19, 53, 2, 21, 1, 1, 1, 0, 2, 2, 2, 1, 1, 2, 4, 25, 13, 1, 277, 1, 6, 45, 88, 6, 2, 2, 50, 87, 68, 137, 6, 11, 2, 2, 2, 281, 3, 1, 1, 71, 24, 2, 1, 14, 31, 138, 1, 1, 6, 10, 8, 2, 1, 17, 1, 1, 4, 24, 1, 29, 9, 4, 3, 66, 1, 1, 10, 6, 7, 21, 5, 18, 4, 304, 1, 3, 1, 1, 2, 1, 5, 15, 2, 3, 1, 3, 4, 10, 1, 25, 1, 52, 1, 1, 1, 1, 1, 1, 1, 2, 4, 1, 2, 1, 1, 18, 5, 24, 49, 13, 11, 2, 1, 3, 2, 2, 29, 4, 15, 218, 55, 25, 2, 1, 17, 17, 12, 1, 4, 1, 1, 2, 7, 1, 1, 11, 1, 3, 2, 4, 3, 4, 1, 5, 22, 2, 10, 3, 1, 1, 2, 3, 31, 17, 1, 33, 1, 4, 209, 21, 1, 4, 45, 114, 1, 1, 1, 1, 30, 18, 2, 3, 1, 1, 1, 2, 3, 1, 12, 1, 1, 1, 1, 1, 1, 5, 1, 2, 3, 1, 6, 4, 3, 1, 1, 1, 67, 2, 1, 1, 1, 1, 1, 1, 33, 1, 18, 13, 3, 211, 11, 2, 2, 1, 1, 36, 244, 4, 42, 4, 20, 8, 214, 7, 0, 0, 2, 12, 0, 0, 48, 0, 2, 0, 31, 1, 1, 0, 45, 0, 1, 40, 0, 2, 0, 5, 5, 0, 12, 18, 3, 1, 5, 13, 1, 15, 1, 410, 0, 0, 46, 46, 2, 12, 15, 3, 151, 14, 2, 16, 1, 27, 0, 6, 3, 2, 3, 1, 2, 3, 11, 7, 1, 17, 2, 22, 102, 0, 0, 0, 0, 0, 1, 0, 0, 0, 5, 0, 1, 34, 0, 20, 0, 17, 2, 2, 18, 0, 0, 0, 1, 0, 6, 0, 0, 0, 0, 0, 0, 1, 0, 2, 0, 2, 0, 0, 5, 6, 0, 2, 1, 121, 11, 1, 1, 2, 0, 0, 0, 0, 0, 55, 0, 0, 5, 0, 0, 0, 0, 1, 2, 48, 1, 1, 7, 1, 32, 1, 0, 0, 0, 28, 0, 0, 10, 10, 0, 19, 1, 0, 0, 0, 0, 0, 21, 0, 3, 12, 2, 10, 0, 0, 0, 0, 0, 5, 0, 0, 0, 1, 19, 0, 8, 1, 2, 2, 3, 1, 0, 4, 0, 10, 0, 1, 0, 0, 3, 1, 16, 8, 70, 0, 3, 1, 0, 5, 6, 47, 0, 1, 0, 2, 5, 1, 22, 2, 13, 1, 1, 9, 5, 16, 2, 0, 27, 0, 0, 4, 1, 0, 1, 0, 10, 4, 17, 0, 0, 0, 1, 2, 36, 12, 0, 16, 1, 0, 0, 1, 0, 1, 8, 0, 3, 6, 0, 0, 0, 19, 3, 1, 11, 0, 2, 8, 0, 2, 0, 65, 2, 2, 3, 0, 0, 0, 9, 1, 11, 6, 8, 0, 0, 1, 9, 0, 1, 0, 9, 1, 0, 0, 0, 17, 2, 1, 0, 3, 0, 0, 2, 0, 0, 1, 0, 47, 3, 10, 0, 0, 1, 0, 2, 2, 0, 2, 0, 6, 2, 3, 0, 0, 0, 16, 0, 4, 0, 0, 0, 36, 0, 12, 0, 51, 1, 0, 2, 8, 9, 14, 18, 1, 0, 3, 7, 140, 3, 0, 3, 2, 0, 0, 0, 16, 0, 122, 4, 1, 0, 4, 0, 5, 0, 11, 11, 36, 0, 5, 2, 10, 0, 0, 3, 6, 2, 0, 0, 45, 0, 2, 0, 7, 0, 4, 1, 3, 0, 1, 0, 0, 0, 0, 10, 50, 3, 14, 8, 13, 0, 35, 0, 6, 0, 3, 14, 18, 5, 0, 0, 0, 4, 0, 0, 1, 0, 2, 1, 69, 0, 0, 0, 12, 5, 0, 1, 9, 3, 4, 0, 2, 4, 13, 29, 0, 5, 0, 6, 0, 2, 0, 6, 16, 2, 0, 15, 0, 0, 0, 1, 9, 6, 4, 1, 0, 0, 0, 2, 0, 2, 4, 2, 0, 0, 2, 2, 0, 8, 1, 7, 3, 30, 2, 9, 10, 0, 0, 1, 24, 1, 5, 2, 0, 0, 1, 0, 0, 8, 0, 3, 27, 6, 1, 330, 0, 47, 3, 1, 22, 25, 0, 1, 31, 0, 0, 0, 9, 0, 0, 1, 0, 3, 4, 0, 0, 4, 0, 11, 1, 29, 19, 1, 2, 0, 2, 2, 65, 3, 0, 2, 1, 0, 17, 17, 5, 5, 2, 3, 34, 0, 1, 15, 4, 13, 10, 6, 3, 3, 14, 1, 1, 1, 1, 11, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 3, 0, 1, 0, 0, 2, 0, 0, 2, 0, 0, 1, 0, 1, 0, 0, 0, 0, 28, 9, 2, 3, 21, 20, 1, 6, 1, 2, 19, 1, 0, 0, 13, 2, 3, 23, 0, 0, 1, 5, 0, 3, 4, 0, 6, 0, 3, 0, 2, 0, 6, 21, 0, 0, 17, 0, 2, 0, 0, 8, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 15, 0, 167, 0, 0, 2, 5, 0, 0, 0, 0, 0, 0, 0, 1, 2, 3, 0, 1, 2, 24, 2, 1, 0, 0, 0, 25, 0, 0, 2, 0, 1, 0, 1, 1, 0, 4, 41, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 2, 0, 0, 0, 1, 2, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 13, 0, 1, 1, 0, 0, 0, 15, 62, 2, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 56, 0, 2, 1, 0, 0, 24, 0, 0, 74, 52, 23, 55, 32, 31, 0, 5, 0, 0, 3, 3, 0, 0, 1, 1, 1, 0, 0, 0, 7, 22, 2, 6, 6, 1, 0, 2, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 7, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 7, 1, 5, 0, 0, 5, 0, 0, 0, 3, 1, 13, 0, 0, 6, 1, 0, 3, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 7, 14, 4, 12, 0, 19, 5, 3, 0, 0, 1, 3, 0, 0, 0, 0, 0, 0, 10, 0, 0, 0, 6, 6, 4, 0, 18, 1, 0, 0, 0, 0, 9, 11, 1, 15, 0, 0, 11, 0, 14, 0, 0, 0, 0, 3, 8, 0, 46, 0, 0, 20, 0, 4, 0, 1, 2, 88, 3, 0, 0, 1, 0, 0, 3, 2, 3, 0, 2, 9, 0, 0, 0, 0, 4, 6, 40, 0, 0, 2, 0, 0, 1, 11, 1, 0, 0, 20, 0, 0, 5, 2, 2, 0, 0, 4, 3, 0, 0, 0, 0, 0, 0, 0, 0, 12, 1, 0, 0, 1, 0, 1, 0, 2, 13, 1, 1, 0, 0, 1, 0, 3, 13, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 6, 0, 1, 0, 0, 0, 9, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 2, 3, 12, 0, 0, 0, 2, 4, 0, 0, 1, 0, 0, 3, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 2, 0, 1, 0, 0, 0, 0, 0, 0, 1, 58, 5, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 29, 0, 0, 0, 0, 0, 0, 0, 2, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 1, 0, 0, 0, 0, 14, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 18, 0, 3, 9, 5, 0, 0, 1, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 2, 0, 0, 0, 17, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 3, 1, 2, 4, 59, 1, 1, 8, 1, 1, 1, 2, 2, 1, 2, 2, 1, 5, 1, 1, 2, 1, 1, 0, 1, 1, 0, 7, 0, 1, 8, 0, 1, 3, 1, 4, 1, 3, 3, 5, 1, 1, 4, 1, 6, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 20, 0, 0, 3, 0, 0, 0, 1, 0, 1, 0, 18, 0, 0, 6, 0, 1, 0, 37, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 1, 5, 0, 0, 0, 0, 0, 14, 0, 1, 0, 1, 0, 38, 0, 3, 3, 5, 10, 0, 0, 6, 0, 0, 25, 1, 0, 2, 0, 0, 0, 0, 0, 0, 12, 0, 9, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 20, 3, 0, 0, 1, 0, 0, 0, 1, 0, 9, 5, 6, 0, 0, 0, 0, 0, 6, 13, 1, 2, 4, 0, 0, 0, 7, 1, 2, 0, 0, 1, 3, 0, 3, 6, 0, 0, 2, 3, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 17, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 9, 0, 0, 3, 1, 0, 0, 0, 5, 3, 0, 0, 12, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 22, 0, 0, 0, 0, 2, 0, 3, 0, 0, 4, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 3, 0, 6, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 7, 0, 0, 0, 2, 0, 0, 1, 3, 1, 0, 0, 0, 0, 0, 0, 0, 11, 7, 22, 6, 0, 6, 0, 0, 0, 0, 0, 0, 3, 1, 25, 2, 0, 6, 0, 4, 0, 0, 6, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 12, 4, 11, 0, 5, 0, 0, 9, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 10, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 10, 0, 2, 3, 0, 2, 21, 0, 8, 0, 0, 0, 2, 4, 0, 0, 17, 0, 1, 3, 0, 0, 4, 0, 1, 0, 0, 0, 15, 2, 2, 6, 0, 2, 0, 0, 1, 1, 0, 79, 0, 0, 0, 1, 0, 2, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 2, 6, 1, 1, 5, 14, 6, 0, 0, 1, 0, 4, 4, 0, 7, 0, 2, 1, 0, 0, 0, 8, 16, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 9, 0, 0, 2, 0, 1, 0, 3, 2, 7, 0, 4, 0, 0, 2, 0, 1, 0, 1, 0, 1, 4, 17, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 12, 0, 4, 0, 0, 0, 1, 19, 1, 8, 1, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 5, 0, 5, 0, 2, 0, 0, 0, 0, 1, 7, 2, 0, 1, 0, 1, 53, 0, 0, 0, 0, 1, 7, 0, 1, 3, 0, 3, 3, 0, 0, 0, 1, 6, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 48, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 1, 13, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 4, 0, 0, 0, 0, 1, 0, 1, 0, 14, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 2, 0, 1, 4, 0, 0, 5, 0, 1, 2, 1, 0, 1, 1, 0, 0, 0, 8, 12, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 15, 0, 0, 0, 0, 1, 1, 1, 2, 0, 0, 0, 3, 0, 0, 0, 0, 2, 0, 3, 1, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0, 0, 0, 0, 1, 6, 0, 2, 0, 1, 0, 1, 1, 2, 18, 2, 1, 0, 0, 7, 2, 12, 0, 1, 5, 0, 0, 0, 0, 1, 0, 6, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 3, 2, 0, 0, 1, 1, 1, 0, 1, 0, 0, 6, 1, 1, 0, 12, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 3, 7, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0, 0, 2, 4, 0, 1, 4, 1, 0, 5, 0, 1, 0, 0, 0, 0, 0, 2, 2, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 8, 0, 0, 3, 3, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 13, 0, 4, 0, 2, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 2, 3, 2, 0, 0, 0, 0, 0, 1, 0, 3, 1, 0, 0, 0, 0, 0, 1, 10, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 2, 13, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 28, 0, 0, 6, 3, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 1, 0, 12, 5, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 22, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 10, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 3, 6, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 2, 0, 0, 0, 4, 0, 0, 0, 0, 0, 1, 12, 2, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 2, 4, 0, 0, 0, 1, 1, 0, 1, 0, 4, 0, 0, 0, 30, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 9, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 1, 0, 0, 4, 0, 0, 0, 2, 0, 2, 0, 2, 0, 6, 15, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 1, 1, 0, 3, 1, 0, 0, 0, 1, 11, 4, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 3, 1, 6, 1, 0, 0, 0, 0, 0, 2, 0, 1, 0, 3, 9, 0, 5, 7, 0, 4, 3, 11, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 4, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 3, 0, 6, 2, 1, 1, 2, 0, 0, 0, 0, 0, 0, 0, 1, 1, 11, 0, 1, 0, 2, 0, 2, 0, 0, 0, 3, 0, 0, 3, 0, 1, 0, 8, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 1, 2, 0, 1, 1, 0, 17, 0, 0, 0, 0, 1, 29, 0, 0, 0, 0, 35, 33, 0, 2, 0, 0, 3, 0, 1, 11, 0, 0, 0, 0, 1, 6, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 1, 0, 0, 1, 9, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 1, 0, 2, 0, 0, 0, 0, 0, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 0, 0, 2, 0, 0, 2, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 2, 0, 0, 0, 1, 0, 0, 0, 1, 0, 10, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 7, 0, 0, 0, 3, 0, 81, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 1, 5, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 10, 0, 0, 1, 0, 3, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 3, 3, 0, 0, 0, 1, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 2, 0, 0, 0, 0, 3, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 7, 0, 0, 0, 27, 0, 0, 0, 0, 0, 0, 0, 0, 10, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 1, 0, 0, 0, 0, 3, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 5, 0, 0, 0, 0, 4, 1, 0, 0, 1, 0, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 10, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 27, 0, 0, 0, 1, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 16, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 3, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 3, 0, 0, 0, 1, 1, 1, 3, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 1, 0, 11, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 2, 0, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 4, 3, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 1, 0, 5, 0, 0, 0, 0, 0, 0, 0, 2, 7, 7, 0, 2, 0, 9, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 10, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 5, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 8, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 21, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 15, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 32, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 1, 0, 0, 0, 6, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 7, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 4, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 9, 4, 0, 0, 0, 0, 0, 2, 0, 0, 1, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 2, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 3, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 3, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 23, 0, 0, 3, 0, 0, 0, 0, 5, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 3, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 2, 3, 0, 0, 6, 0, 0, 0, 0, 0, 4, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 2, 0, 0, 0, 0, 2, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 3, 1, 0, 1, 1, 0, 0, 0, 7, 3, 0, 0, 1, 0, 2, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 1, 3, 7, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 8, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0, 2, 0, 6, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 8, 0, 1, 0, 0, 1, 0, 14, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 5, 3, 0, 0, 2, 1, 9, 16, 1, 3, 1, 1, 2, 2, 2, 5, 3, 1, 1, 1, 1, 5, 4, 1, 1, 2, 41, 1, 1, 15, 1, 1, 2, 1, 9, 1, 28, 27, 21, 11, 1, 1, 1, 1, 1, 1, 1, 4, 3, 23, 1, 4, 1, 2, 1, 1, 1, 1, 1, 1, 2, 3, 1, 1, 2, 8, 3, 1, 1, 2, 7, 1, 3, 3, 2, 1, 1, 2, 2, 3, 2, 4, 2, 4, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 3, 1, 3, 1, 1, 1, 5, 1, 1, 1, 2, 2, 3, 2, 3, 1, 1, 1, 2, 5, 2, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 5, 3, 3, 1, 1, 2, 9, 1, 3, 1, 1, 1, 1, 1, 1, 3, 4, 2, 5, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 6, 2, 6, 6, 9, 1, 4, 3, 1, 1, 2, 1, 1, 2, 2, 1, 2, 1, 4, 1, 1, 1, 1, 1, 5, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 2, 1, 2, 1, 1, 1, 2, 2, 5, 1, 1, 2, 1, 1, 2, 1, 1, 1, 4, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 4, 2, 4, 1, 1, 4, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 3, 3, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 4, 1, 1, 4, 1, 4, 1, 1, 1, 1, 1, 6, 1, 4, 2, 1, 1, 1, 1, 2, 3, 1, 3, 2, 2, 1, 1, 1, 1, 2, 9, 1, 1, 1, 2, 1, 2, 1, 5, 1, 2, 3, 1, 2, 1, 1, 1, 1, 2, 1, 2, 1, 2, 1, 3, 2, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 6, 6, 1, 1, 1, 2, 1, 2, 1, 1, 1, 2, 3, 1, 1, 2, 1, 1, 3, 1, 4, 2, 1, 2, 1, 1, 2, 4, 1, 1, 1, 2, 1, 3, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 4, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 4, 1, 1, 3, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 4, 3, 3, 2, 8, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "----\n",
      "vectors[7] = [18, 3, 329, 22, 153, 107, 19, 843, 2, 6, 1, 5, 1, 4, 282, 4, 495, 1, 42, 1, 1, 1, 8, 1, 103, 3, 2, 34, 5, 463, 1, 5, 366, 7, 14, 3, 2, 16, 8, 133, 16, 3, 46, 10, 6, 35, 6, 114, 19, 4, 16, 7, 23, 7, 308, 8, 133, 1, 2, 2, 66, 4, 17, 6, 1, 1, 11, 62, 26, 48, 64, 1, 2, 2, 32, 5, 2, 1, 6, 1, 1, 1, 1, 68, 1, 2, 18, 5, 1, 186, 108, 404, 224, 2, 1, 1, 2, 24, 1, 2, 1, 3, 1, 1, 1, 1, 1, 2, 21, 37, 3, 26, 2, 57, 1, 57, 1, 22, 16, 2, 2, 6, 1, 5, 26, 8, 1, 5, 3, 9, 1, 12, 397, 1, 2, 1, 7, 12, 2, 6, 2, 2, 47, 6, 2, 7, 269, 4, 1, 1, 1, 39, 56, 1, 78, 1, 1, 1, 1, 4, 7, 20, 3, 3, 26, 2, 3, 3, 6, 2, 3, 5, 1, 1, 1, 2, 4, 20, 3, 2, 2, 1, 13, 1, 3, 1, 1, 2, 49, 2, 4, 1, 1, 1, 0, 2, 2, 2, 1, 1, 2, 4, 15, 11, 1, 236, 1, 3, 33, 44, 5, 2, 2, 20, 55, 76, 91, 9, 7, 1, 1, 4, 114, 9, 1, 1, 26, 8, 3, 1, 1, 8, 60, 2, 2, 2, 10, 3, 5, 1, 9, 1, 1, 2, 26, 1, 29, 9, 5, 3, 45, 1, 1, 6, 6, 5, 5, 4, 4, 4, 18, 1, 1, 1, 1, 1, 1, 5, 13, 2, 7, 1, 3, 3, 10, 1, 22, 1, 25, 1, 2, 1, 1, 1, 1, 3, 2, 7, 1, 3, 1, 1, 13, 6, 24, 50, 13, 10, 1, 1, 3, 4, 2, 29, 4, 14, 14, 6, 25, 2, 1, 5, 17, 12, 1, 1, 1, 1, 2, 7, 1, 1, 11, 1, 3, 3, 4, 3, 4, 1, 5, 17, 3, 8, 3, 1, 1, 2, 2, 17, 33, 1, 4, 1, 2, 163, 14, 1, 4, 20, 39, 1, 1, 1, 1, 30, 18, 1, 3, 1, 1, 1, 2, 2, 1, 12, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 5, 4, 3, 1, 4, 5, 9, 2, 1, 1, 1, 1, 1, 1, 13, 1, 17, 14, 131, 156, 6, 1, 4, 2, 1, 13, 121, 2, 12, 6, 6, 2, 120, 0, 0, 0, 1, 15, 1, 0, 36, 0, 0, 0, 30, 0, 0, 1, 25, 0, 0, 33, 0, 0, 0, 5, 0, 22, 9, 0, 1, 0, 0, 0, 4, 18, 2, 260, 3, 0, 23, 31, 0, 6, 13, 0, 46, 4, 2, 6, 1, 11, 2, 3, 3, 0, 0, 0, 2, 8, 0, 5, 0, 10, 0, 20, 27, 0, 0, 1, 0, 0, 0, 0, 11, 1, 9, 0, 5, 30, 0, 7, 2, 3, 0, 4, 13, 1, 1, 0, 3, 0, 3, 1, 1, 1, 0, 1, 0, 0, 1, 4, 0, 0, 0, 0, 3, 0, 0, 0, 0, 49, 2, 1, 0, 2, 0, 0, 0, 1, 1, 47, 0, 0, 6, 0, 0, 0, 0, 1, 2, 36, 3, 1, 5, 1, 19, 0, 0, 2, 0, 9, 3, 0, 5, 5, 0, 0, 3, 0, 0, 0, 0, 0, 14, 0, 12, 12, 2, 11, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 16, 0, 1, 0, 1, 0, 0, 2, 0, 1, 4, 0, 0, 1, 0, 0, 1, 0, 7, 0, 12, 0, 0, 0, 5, 6, 1, 12, 7, 4, 0, 0, 6, 0, 12, 2, 13, 1, 0, 7, 0, 9, 4, 0, 30, 0, 0, 0, 5, 1, 1, 0, 3, 3, 14, 1, 0, 0, 0, 2, 38, 6, 0, 0, 0, 1, 0, 0, 0, 1, 4, 0, 1, 3, 0, 0, 0, 17, 2, 3, 2, 0, 0, 15, 0, 0, 0, 2, 0, 0, 2, 0, 0, 1, 0, 0, 10, 0, 19, 0, 0, 0, 3, 0, 0, 0, 3, 0, 0, 0, 0, 14, 6, 0, 0, 5, 3, 2, 0, 0, 0, 4, 0, 18, 0, 10, 3, 0, 2, 0, 1, 0, 0, 0, 0, 6, 0, 3, 0, 1, 0, 1, 0, 0, 0, 1, 0, 8, 0, 12, 0, 32, 1, 0, 23, 15, 14, 12, 4, 0, 2, 2, 0, 9, 0, 0, 3, 2, 1, 1, 0, 2, 1, 3, 0, 0, 0, 0, 0, 2, 1, 16, 1, 14, 0, 12, 1, 3, 0, 1, 1, 5, 1, 1, 0, 27, 8, 4, 0, 0, 0, 0, 0, 1, 0, 3, 0, 0, 0, 0, 8, 0, 16, 6, 10, 11, 14, 80, 4, 2, 16, 3, 7, 5, 0, 0, 0, 0, 1, 0, 0, 1, 0, 2, 0, 22, 0, 0, 0, 5, 0, 1, 0, 6, 1, 2, 0, 0, 3, 10, 3, 0, 13, 0, 7, 2, 0, 0, 0, 15, 3, 0, 11, 0, 0, 0, 3, 11, 4, 2, 0, 0, 0, 1, 1, 0, 6, 0, 4, 1, 0, 8, 4, 0, 1, 0, 4, 0, 12, 0, 5, 10, 0, 4, 1, 11, 2, 1, 0, 0, 0, 0, 0, 0, 6, 6, 6, 37, 0, 2, 208, 0, 101, 2, 2, 0, 61, 2, 0, 12, 0, 1, 0, 1, 1, 3, 0, 0, 2, 5, 0, 0, 53, 0, 3, 0, 15, 31, 1, 1, 0, 5, 0, 59, 3, 1, 5, 0, 0, 2, 6, 0, 1, 1, 0, 12, 0, 0, 3, 0, 0, 5, 1, 0, 0, 3, 0, 0, 0, 0, 2, 0, 0, 0, 1, 0, 2, 0, 1, 0, 3, 0, 0, 0, 0, 4, 3, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 1, 4, 1, 0, 11, 2, 1, 1, 39, 33, 5, 1, 0, 2, 7, 1, 0, 0, 6, 2, 1, 2, 1, 0, 1, 1, 2, 1, 10, 0, 3, 1, 0, 1, 2, 0, 2, 3, 0, 0, 0, 0, 0, 2, 0, 9, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 29, 4, 14, 4, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 3, 4, 0, 0, 2, 8, 2, 1, 0, 0, 0, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 1, 2, 0, 2, 0, 0, 0, 5, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 19, 0, 1, 0, 0, 1, 0, 11, 26, 2, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 103, 9, 0, 1, 0, 0, 2, 1, 0, 0, 46, 0, 0, 1, 0, 0, 0, 32, 157, 14, 27, 0, 0, 0, 0, 0, 4, 0, 0, 0, 4, 0, 12, 11, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 1, 0, 0, 1, 1, 0, 0, 2, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 8, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 3, 9, 0, 11, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 2, 8, 0, 0, 6, 3, 0, 0, 1, 0, 2, 4, 0, 14, 0, 0, 0, 0, 2, 0, 0, 0, 1, 2, 5, 0, 18, 0, 0, 3, 0, 2, 0, 0, 3, 4, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 2, 10, 0, 0, 0, 0, 0, 0, 17, 0, 5, 2, 0, 1, 0, 4, 0, 0, 0, 5, 0, 0, 3, 1, 1, 0, 0, 2, 4, 0, 7, 2, 0, 0, 0, 0, 0, 3, 4, 0, 3, 1, 0, 0, 0, 0, 22, 0, 2, 0, 0, 0, 0, 0, 6, 2, 0, 0, 0, 6, 0, 0, 3, 0, 23, 1, 0, 1, 0, 0, 0, 0, 2, 0, 2, 1, 0, 0, 7, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 3, 0, 0, 0, 2, 0, 0, 0, 9, 0, 0, 2, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 38, 0, 1, 0, 0, 0, 0, 1, 3, 0, 7, 3, 0, 0, 0, 1, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 12, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 2, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 14, 1, 5, 1, 9, 3, 0, 5, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 6, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 2, 1, 69, 1, 1, 2, 1, 1, 1, 3, 3, 1, 1, 1, 1, 3, 1, 1, 2, 1, 1, 0, 0, 1, 0, 9, 0, 1, 0, 1, 0, 1, 0, 6, 34, 0, 1, 2, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 9, 0, 2, 0, 0, 0, 0, 2, 0, 0, 0, 9, 0, 0, 4, 0, 0, 0, 31, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 2, 0, 0, 0, 0, 0, 10, 0, 1, 0, 1, 0, 7, 0, 8, 4, 5, 10, 0, 0, 0, 0, 0, 6, 6, 0, 0, 0, 0, 0, 0, 0, 1, 3, 0, 2, 0, 0, 0, 1, 0, 0, 0, 0, 3, 0, 4, 0, 0, 0, 1, 5, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 7, 0, 0, 0, 0, 1, 1, 3, 1, 2, 1, 0, 0, 0, 3, 0, 0, 0, 1, 2, 1, 0, 3, 0, 0, 0, 3, 0, 0, 0, 1, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 18, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 6, 6, 6, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 2, 1, 3, 0, 0, 4, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 12, 0, 0, 0, 0, 11, 0, 5, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 3, 2, 1, 0, 0, 0, 2, 0, 1, 3, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 8, 6, 17, 1, 0, 0, 0, 0, 0, 0, 1, 3, 28, 4, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 2, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 5, 1, 3, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0, 1, 1, 0, 5, 0, 0, 2, 0, 0, 0, 23, 1, 0, 7, 0, 1, 0, 0, 0, 2, 0, 74, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 10, 9, 1, 0, 1, 0, 12, 0, 0, 0, 0, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 8, 15, 0, 0, 1, 0, 0, 0, 0, 2, 2, 1, 0, 0, 1, 0, 0, 47, 0, 0, 2, 1, 1, 5, 0, 3, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 2, 0, 0, 0, 2, 9, 0, 5, 0, 0, 0, 0, 0, 0, 0, 1, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 2, 1, 30, 0, 3, 0, 4, 0, 0, 0, 0, 0, 3, 1, 0, 1, 0, 0, 11, 0, 0, 0, 0, 0, 1, 0, 1, 3, 1, 0, 0, 0, 0, 0, 3, 2, 1, 0, 0, 0, 0, 0, 0, 6, 0, 0, 39, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 3, 2, 2, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0, 0, 0, 0, 0, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 2, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 3, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 6, 0, 0, 0, 2, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 6, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 12, 0, 1, 40, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 38, 0, 0, 0, 0, 5, 0, 0, 5, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 3, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 11, 0, 0, 3, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 3, 0, 0, 3, 12, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 2, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 4, 0, 2, 0, 0, 0, 0, 6, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 0, 2, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 47, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 1, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 12, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 4, 2, 0, 0, 2, 0, 2, 0, 2, 0, 0, 4, 0, 0, 0, 1, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 8, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 2, 0, 0, 1, 3, 0, 1, 0, 1, 0, 1, 0, 1, 2, 5, 0, 0, 1, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 2, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 11, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 5, 0, 0, 2, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 2, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 10, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 0, 0, 0, 0, 0, 1, 0, 0, 0, 4, 0, 1, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 13, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 1, 0, 0, 1, 2, 0, 0, 0, 0, 21, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 15, 1, 13, 0, 3, 0, 0, 0, 6, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 17, 0, 0, 0, 3, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 2, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 1, 0, 5, 0, 0, 0, 7, 0, 0, 1, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 12, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 15, 0, 0, 0, 0, 0, 0, 0, 0, 0, 64, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 9, 0, 0, 0, 0, 0, 1, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 19, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 20, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 37, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 3, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 3, 1, 0, 0, 1, 1, 2, 0, 0, 1, 2, 0, 1, 0, 0, 0, 1, 4, 0, 0, 3, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 8, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 2, 0, 2, 1, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 3, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 4, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 13, 1, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 2, 1, 0, 0, 0, 3, 0, 2, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 10, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 3, 2, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 25, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 16, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 1, 5, 97, 41, 0, 0, 0, 0, 0, 0, 0, 12, 1, 19, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 13, 2, 0, 13, 0, 0, 7, 1, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 3, 0, 0, 0, 0, 11, 0, 30, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 5, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 10, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 6, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 3, 1, 2, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 16, 12, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 3, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 3, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 1, 2, 1, 0, 0, 0, 0, 0, 2, 0, 0, 1, 1, 4, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 2, 1, 2, 1, 4, 3, 3, 3, 3, 8, 8, 11, 11, 1, 2, 2, 6, 2, 1, 3, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 2, 1, 1, 3, 1, 1, 1, 1, 58, 15, 2, 1, 3, 1, 1, 6, 6, 14, 4, 4, 2, 21, 12, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 3, 1, 2, 1, 4, 1, 3, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 3, 2, 1, 1, 1, 1, 5, 10, 1, 1, 1, 1, 1, 1, 1, 1, 1, 9, 7, 1, 3, 1, 1, 1, 25, 1, 1, 1, 1, 26, 2, 1, 1, 1, 1, 1, 6, 2, 1, 1, 8, 1, 1, 1, 5, 4, 3, 4, 4, 4, 4, 4, 8, 7, 5, 1, 3, 2, 1, 2, 1, 1, 2, 1, 1, 1, 3, 1, 2, 1, 1, 3, 2, 1, 2, 2, 2, 1, 2, 7, 6, 1, 1, 1, 6, 14, 1, 1, 1, 2, 2, 6, 2, 2, 1, 1, 1, 2, 1, 5, 2, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 7, 1, 1, 1, 3, 3, 3, 6, 1, 1, 1, 1, 1, 1, 5, 2, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 10, 1, 1, 1, 3, 2, 2, 2, 1, 1, 1, 3, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 2, 1, 1, 3, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "----\n",
      "vectors[8] = [25, 3, 382, 23, 165, 104, 16, 1035, 2, 10, 1, 5, 1, 4, 259, 4, 526, 1, 1, 1, 1, 1, 7, 1, 97, 3, 2, 26, 5, 753, 1, 3, 329, 5, 27, 4, 2, 16, 8, 95, 5, 3, 7, 10, 6, 35, 8, 141, 19, 8, 33, 7, 51, 7, 537, 8, 141, 1, 2, 2, 94, 4, 17, 6, 1, 1, 11, 62, 26, 48, 46, 1, 2, 3, 32, 5, 2, 1, 5, 1, 1, 1, 20, 99, 1, 2, 22, 3, 1, 115, 101, 445, 294, 2, 1, 1, 2, 33, 1, 2, 1, 3, 1, 1, 1, 1, 1, 2, 56, 45, 3, 10, 2, 9, 1, 115, 1, 27, 23, 2, 2, 9, 1, 10, 68, 7, 1, 4, 3, 5, 1, 12, 327, 1, 2, 1, 8, 5, 2, 7, 2, 2, 25, 4, 2, 1, 61, 2, 1, 2, 1, 39, 82, 1, 117, 1, 1, 1, 1, 4, 7, 11, 3, 3, 13, 2, 4, 3, 6, 2, 3, 5, 1, 1, 1, 2, 4, 8, 17, 2, 9, 1, 10, 1, 3, 1, 12, 6, 58, 2, 3, 1, 1, 1, 0, 2, 2, 2, 1, 1, 2, 4, 21, 17, 1, 299, 2, 2, 26, 97, 5, 2, 2, 17, 60, 54, 118, 6, 7, 7, 1, 2, 138, 8, 1, 1, 33, 9, 3, 1, 5, 12, 78, 1, 3, 2, 10, 1, 3, 1, 9, 1, 1, 3, 28, 1, 29, 9, 4, 3, 49, 1, 1, 8, 6, 7, 26, 4, 6, 4, 2, 1, 1, 1, 1, 1, 1, 5, 21, 3, 3, 1, 3, 2, 10, 1, 22, 1, 32, 1, 1, 1, 1, 1, 1, 1, 2, 10, 1, 1, 1, 1, 13, 7, 24, 64, 13, 21, 1, 1, 3, 2, 2, 29, 4, 14, 18, 31, 25, 2, 5, 3, 17, 12, 1, 5, 1, 1, 2, 7, 3, 1, 14, 1, 3, 3, 4, 4, 4, 1, 5, 19, 2, 12, 3, 1, 1, 2, 2, 11, 6, 1, 4, 1, 2, 232, 14, 1, 4, 26, 13, 1, 1, 1, 1, 30, 18, 13, 3, 1, 1, 1, 2, 2, 1, 12, 1, 1, 1, 1, 1, 1, 1, 2, 1, 3, 1, 5, 4, 3, 1, 1, 1, 3, 2, 1, 1, 1, 2, 1, 1, 11, 18, 20, 17, 201, 211, 44, 2, 2, 1, 1, 14, 201, 3, 15, 8, 22, 4, 154, 3, 0, 0, 4, 12, 16, 0, 39, 0, 4, 0, 22, 5, 0, 3, 48, 0, 0, 38, 1, 5, 1, 14, 1, 6, 12, 5, 2, 1, 0, 0, 12, 12, 5, 336, 0, 0, 30, 30, 0, 3, 19, 0, 64, 1, 4, 9, 4, 18, 2, 1, 8, 1, 1, 0, 6, 4, 27, 2, 0, 11, 0, 8, 59, 0, 1, 0, 0, 0, 0, 0, 2, 4, 3, 0, 4, 5, 0, 35, 0, 0, 0, 4, 13, 1, 3, 1, 3, 0, 4, 1, 0, 0, 0, 0, 0, 1, 0, 3, 1, 0, 0, 0, 8, 1, 0, 2, 0, 95, 3, 2, 1, 0, 2, 2, 0, 0, 1, 29, 0, 1, 3, 0, 0, 0, 0, 5, 0, 70, 0, 0, 10, 1, 39, 4, 1, 0, 1, 22, 2, 0, 13, 13, 1, 2, 2, 0, 0, 0, 0, 0, 36, 1, 15, 14, 4, 14, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 11, 0, 1, 10, 1, 1, 1, 4, 1, 1, 0, 1, 1, 0, 0, 2, 4, 0, 4, 4, 2, 0, 2, 0, 6, 4, 0, 30, 15, 11, 5, 1, 1, 0, 35, 0, 18, 2, 2, 12, 0, 16, 6, 1, 28, 0, 0, 2, 4, 0, 0, 0, 12, 2, 21, 1, 0, 0, 0, 1, 25, 7, 0, 0, 1, 3, 0, 0, 0, 3, 9, 0, 1, 3, 0, 0, 0, 15, 4, 2, 0, 1, 0, 14, 0, 0, 0, 0, 3, 2, 1, 0, 0, 2, 1, 0, 13, 1, 17, 0, 0, 0, 2, 0, 1, 0, 5, 0, 0, 1, 0, 6, 0, 0, 0, 1, 1, 1, 0, 0, 0, 2, 0, 31, 1, 6, 9, 0, 2, 0, 2, 1, 0, 0, 0, 3, 0, 1, 1, 1, 1, 8, 1, 1, 0, 2, 0, 29, 0, 12, 0, 34, 3, 1, 47, 0, 0, 3, 8, 1, 2, 1, 1, 15, 0, 0, 9, 7, 0, 0, 0, 5, 0, 3, 0, 0, 0, 1, 0, 1, 0, 3, 1, 7, 0, 1, 0, 8, 1, 3, 1, 5, 3, 0, 0, 66, 2, 0, 0, 0, 0, 1, 1, 5, 0, 1, 0, 0, 0, 1, 0, 0, 0, 19, 5, 13, 42, 125, 16, 8, 3, 12, 8, 4, 1, 0, 0, 0, 3, 0, 0, 0, 1, 0, 0, 21, 0, 1, 0, 5, 0, 0, 2, 21, 2, 11, 0, 3, 6, 9, 0, 0, 0, 0, 2, 1, 0, 0, 13, 20, 3, 0, 7, 0, 0, 0, 5, 21, 13, 2, 2, 0, 0, 0, 6, 0, 15, 4, 7, 1, 1, 7, 2, 0, 3, 0, 7, 1, 5, 0, 4, 10, 1, 2, 2, 25, 1, 0, 0, 0, 0, 14, 0, 0, 12, 4, 1, 22, 1, 0, 152, 0, 68, 6, 2, 0, 9, 0, 2, 18, 3, 0, 0, 3, 0, 1, 2, 0, 0, 8, 1, 0, 0, 1, 4, 0, 10, 29, 0, 6, 1, 3, 0, 39, 4, 2, 0, 0, 0, 2, 4, 4, 1, 0, 3, 7, 0, 0, 7, 0, 0, 12, 7, 4, 0, 5, 0, 0, 0, 0, 2, 0, 2, 0, 1, 0, 3, 0, 2, 4, 3, 0, 0, 0, 0, 2, 5, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 7, 0, 5, 1, 0, 14, 0, 0, 0, 12, 2, 37, 0, 1, 2, 10, 1, 0, 0, 3, 3, 1, 2, 0, 1, 1, 2, 2, 1, 6, 0, 5, 1, 0, 0, 1, 0, 5, 11, 1, 0, 0, 0, 1, 1, 0, 6, 2, 0, 0, 8, 0, 14, 0, 6, 0, 0, 6, 0, 0, 100, 0, 0, 0, 1, 1, 0, 0, 0, 1, 4, 0, 1, 0, 6, 0, 0, 1, 4, 22, 11, 0, 0, 0, 2, 8, 2, 0, 0, 0, 1, 0, 0, 2, 0, 2, 9, 0, 0, 0, 0, 0, 7, 4, 2, 16, 0, 1, 0, 2, 1, 0, 0, 5, 0, 0, 4, 6, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 23, 0, 4, 0, 1, 0, 0, 6, 16, 1, 0, 0, 0, 1, 0, 1, 6, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 103, 1, 0, 0, 0, 0, 2, 2, 3, 0, 1, 0, 0, 4, 28, 4, 0, 1, 0, 2, 1, 0, 0, 4, 1, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 9, 1, 0, 0, 0, 0, 0, 2, 1, 1, 0, 0, 1, 0, 0, 0, 2, 0, 13, 0, 0, 5, 0, 0, 10, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 5, 12, 0, 14, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 6, 9, 23, 7, 10, 1, 0, 0, 1, 0, 2, 2, 2, 18, 0, 2, 0, 0, 2, 0, 0, 0, 2, 4, 5, 0, 22, 0, 0, 14, 1, 8, 3, 0, 0, 30, 1, 0, 0, 0, 0, 0, 2, 0, 2, 0, 2, 4, 0, 0, 1, 0, 2, 0, 20, 0, 0, 1, 0, 1, 1, 17, 1, 0, 0, 2, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 2, 0, 0, 0, 11, 2, 1, 2, 0, 1, 0, 0, 0, 23, 0, 4, 0, 0, 1, 0, 1, 4, 0, 0, 0, 0, 0, 15, 0, 0, 2, 156, 0, 1, 3, 0, 1, 0, 0, 2, 1, 1, 1, 1, 0, 14, 1, 0, 2, 0, 6, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 9, 0, 0, 0, 1, 1, 0, 0, 4, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 2, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 9, 11, 0, 0, 0, 0, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 12, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 85, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 9, 0, 1, 0, 0, 0, 0, 0, 0, 12, 3, 2, 9, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 10, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 13, 0, 0, 10, 4, 0, 0, 2, 0, 0, 2, 0, 0, 10, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 3, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 1, 98, 1, 1, 4, 1, 1, 1, 3, 4, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 2, 1, 0, 0, 26, 0, 0, 20, 1, 1, 0, 0, 1, 51, 1, 0, 1, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 6, 0, 1, 0, 0, 0, 0, 3, 0, 0, 0, 31, 7, 0, 6, 0, 0, 0, 31, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 1, 5, 0, 0, 1, 0, 0, 5, 1, 1, 0, 1, 0, 27, 0, 0, 0, 8, 3, 0, 0, 0, 1, 0, 4, 11, 2, 0, 0, 0, 0, 0, 0, 2, 6, 1, 0, 0, 0, 0, 0, 0, 1, 4, 0, 2, 0, 0, 0, 0, 0, 0, 1, 1, 2, 0, 0, 0, 2, 0, 1, 2, 3, 2, 15, 1, 0, 0, 1, 0, 0, 4, 0, 0, 0, 1, 0, 0, 2, 0, 0, 0, 3, 4, 2, 0, 9, 1, 0, 0, 1, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 17, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 12, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 4, 0, 0, 4, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 4, 0, 2, 0, 0, 3, 0, 1, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 42, 0, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 2, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 9, 0, 1, 3, 0, 1, 0, 0, 0, 0, 4, 3, 7, 6, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 5, 3, 2, 2, 1, 0, 3, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 6, 0, 0, 4, 1, 4, 2, 0, 0, 0, 0, 1, 1, 6, 0, 0, 0, 0, 4, 3, 0, 0, 4, 1, 9, 0, 0, 0, 18, 0, 1, 1, 0, 0, 0, 1, 0, 3, 0, 19, 0, 0, 0, 0, 0, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 18, 0, 3, 3, 1, 0, 0, 0, 3, 1, 0, 0, 0, 0, 2, 0, 0, 1, 0, 0, 0, 5, 7, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 4, 0, 0, 1, 0, 0, 1, 6, 0, 5, 1, 1, 0, 0, 2, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 1, 1, 2, 0, 0, 3, 8, 0, 5, 5, 0, 0, 0, 0, 1, 1, 2, 3, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 4, 0, 6, 0, 8, 0, 0, 0, 0, 0, 0, 1, 0, 1, 3, 1, 74, 0, 0, 0, 0, 1, 0, 0, 0, 2, 0, 0, 16, 0, 0, 0, 3, 3, 1, 1, 0, 3, 0, 0, 0, 4, 0, 0, 76, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 0, 4, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 9, 0, 2, 0, 0, 1, 0, 1, 3, 1, 2, 1, 0, 0, 0, 0, 5, 1, 0, 0, 0, 0, 0, 0, 3, 4, 1, 3, 0, 2, 0, 0, 0, 2, 1, 6, 0, 2, 0, 0, 0, 7, 0, 0, 0, 0, 5, 0, 1, 0, 0, 0, 0, 0, 19, 1, 1, 0, 0, 0, 0, 0, 0, 3, 1, 0, 0, 10, 0, 0, 0, 0, 0, 9, 1, 1, 1, 9, 0, 1, 0, 2, 3, 1, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 2, 7, 0, 0, 0, 0, 0, 0, 1, 0, 2, 1, 1, 0, 1, 0, 0, 0, 4, 1, 2, 0, 53, 0, 0, 18, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 3, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 2, 0, 2, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 2, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 2, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 2, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 4, 0, 0, 0, 2, 0, 0, 1, 0, 0, 0, 4, 1, 0, 12, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 12, 3, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 18, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 4, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 5, 1, 0, 2, 0, 1, 0, 1, 0, 0, 0, 6, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 5, 1, 0, 3, 0, 0, 0, 2, 0, 0, 1, 0, 0, 0, 1, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 5, 0, 2, 0, 0, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 3, 0, 0, 0, 0, 0, 6, 0, 0, 0, 1, 0, 0, 0, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 3, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 3, 0, 0, 0, 1, 0, 0, 2, 1, 0, 0, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 11, 0, 0, 0, 0, 0, 0, 0, 0, 15, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 58, 13, 0, 0, 17, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 1, 0, 0, 4, 0, 0, 0, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 3, 5, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 4, 0, 0, 0, 0, 0, 4, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 3, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 6, 4, 0, 0, 0, 1, 0, 0, 0, 2, 5, 0, 4, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 6, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 15, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 17, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 2, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 26, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 3, 1, 1, 0, 0, 0, 0, 0, 4, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 1, 1, 0, 0, 1, 0, 0, 3, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 2, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 2, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 4, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 10, 0, 1, 3, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 2, 0, 0, 0, 1, 0, 0, 0, 1, 3, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 10, 0, 0, 0, 0, 0, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 10, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 12, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 21, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 16, 0, 0, 0, 4, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 91, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0, 11, 0, 0, 0, 0, 3, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 2, 1, 0, 0, 33, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 1, 35, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 3, 0, 0, 0, 0, 1, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 8, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 19, 10, 0, 23, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 32, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 2, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 1, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 13, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 3, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 18, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 2, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0, 0, 6, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 3, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 17, 0, 0, 0, 0, 18, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 12, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 2, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 6, 5, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 13, 0, 2, 74, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 2, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 3, 6, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 4, 4, 4, 3, 3, 3, 1, 1, 2, 2, 2, 2, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 20, 21, 19, 7, 8, 8, 3, 1, 1, 2, 1, 1, 1, 1, 3, 1, 1, 2, 1, 1, 4, 4, 1, 1, 2, 6, 1, 2, 1, 1, 1, 1, 3, 4, 2, 2, 1, 1, 1, 4, 12, 6, 8, 8, 7, 6, 1, 4, 21, 2, 1, 2, 1, 5, 1, 1, 3, 3, 1, 1, 5, 4, 3, 8, 5, 4, 4, 4, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 22, 5, 3, 4, 1, 1, 1, 2, 1, 1, 1, 3, 7, 1, 1, 1, 2, 1, 1, 1, 2, 1, 3, 3, 2, 1, 2, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 33, 1, 1, 2, 1, 1, 1, 4, 5, 7, 3, 6, 4, 6, 6, 1, 4, 8, 1, 1, 1, 1, 1, 1, 4, 3, 1, 1, 1, 1, 3, 1, 1, 1, 2, 4, 1, 4, 1, 4, 1, 1, 2, 1, 2, 1, 1, 1, 6, 2, 2, 2, 1, 1, 2, 1, 1, 1, 3, 1, 11, 1, 2, 6, 4, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 2, 3, 2, 2, 1, 1, 3, 4, 2, 1, 3, 1, 2, 1, 1, 2, 1, 2, 2, 2, 1, 1, 2, 2, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 5, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "----\n",
      "vectors[9] = [9, 3, 370, 20, 201, 104, 10, 758, 2, 7, 1, 5, 1, 4, 412, 4, 605, 1, 3, 1, 1, 1, 13, 1, 98, 3, 2, 23, 5, 496, 1, 2, 348, 3, 18, 3, 2, 16, 8, 480, 5, 3, 7, 10, 6, 39, 4, 77, 19, 2, 19, 7, 22, 7, 380, 8, 108, 1, 2, 2, 128, 4, 17, 6, 1, 1, 11, 62, 26, 48, 70, 1, 2, 2, 32, 5, 2, 1, 5, 1, 1, 1, 4, 67, 1, 2, 18, 1, 1, 129, 78, 551, 182, 2, 1, 1, 2, 16, 1, 2, 1, 3, 1, 1, 1, 1, 1, 2, 123, 35, 3, 12, 2, 7, 1, 86, 1, 19, 9, 2, 2, 14, 1, 5, 11, 3, 1, 2, 3, 4, 1, 12, 236, 1, 2, 1, 7, 13, 2, 3, 2, 2, 8, 5, 2, 1, 64, 2, 1, 3, 1, 36, 61, 1, 87, 1, 1, 1, 1, 4, 7, 9, 3, 3, 7, 2, 3, 3, 4, 2, 3, 19, 1, 1, 1, 2, 4, 7, 13, 2, 1, 3, 6, 1, 3, 1, 1, 48, 52, 2, 4, 1, 1, 1, 0, 2, 3, 2, 1, 1, 2, 4, 20, 6, 3, 206, 1, 1, 23, 70, 5, 1, 2, 8, 48, 49, 49, 3, 7, 8, 1, 2, 111, 1, 1, 1, 37, 8, 2, 1, 2, 12, 75, 1, 3, 4, 10, 3, 6, 1, 7, 1, 1, 11, 24, 2, 29, 9, 4, 3, 52, 1, 1, 6, 6, 5, 53, 5, 6, 4, 179, 1, 1, 1, 1, 1, 1, 5, 14, 2, 3, 1, 1, 2, 10, 1, 22, 1, 34, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 13, 6, 24, 59, 13, 27, 3, 1, 3, 2, 2, 29, 4, 14, 10, 11, 25, 2, 2, 5, 17, 12, 1, 1, 1, 1, 2, 7, 1, 1, 11, 1, 3, 3, 4, 5, 4, 1, 5, 11, 2, 8, 3, 1, 1, 2, 2, 16, 8, 1, 4, 1, 2, 507, 4, 1, 4, 9, 7, 1, 1, 1, 1, 32, 18, 1, 3, 1, 1, 6, 2, 2, 1, 12, 1, 1, 1, 1, 1, 1, 1, 1, 2, 3, 1, 15, 4, 3, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 11, 15, 18, 8, 109, 168, 46, 1, 4, 3, 1, 12, 150, 3, 14, 5, 5, 3, 136, 1, 0, 0, 0, 60, 9, 0, 23, 0, 7, 0, 26, 0, 0, 5, 34, 0, 0, 19, 0, 1, 0, 2, 3, 27, 8, 10, 0, 2, 2, 4, 2, 15, 9, 267, 0, 0, 15, 19, 0, 6, 9, 0, 10, 0, 0, 9, 0, 9, 1, 0, 1, 0, 0, 0, 3, 0, 5, 2, 0, 11, 0, 17, 37, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 27, 0, 22, 0, 0, 0, 3, 10, 0, 0, 0, 1, 1, 6, 0, 0, 0, 0, 0, 1, 1, 0, 1, 2, 1, 0, 0, 2, 0, 0, 3, 0, 72, 0, 0, 0, 1, 1, 3, 0, 0, 0, 0, 0, 0, 3, 0, 1, 0, 0, 1, 4, 36, 1, 0, 13, 1, 28, 1, 2, 0, 0, 12, 0, 0, 4, 4, 1, 1, 0, 0, 0, 0, 0, 0, 47, 0, 1, 6, 3, 10, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 6, 0, 3, 2, 0, 0, 1, 7, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 1, 4, 18, 0, 2, 0, 11, 6, 0, 15, 3, 0, 1, 0, 3, 0, 50, 2, 10, 0, 1, 9, 0, 18, 1, 0, 26, 0, 0, 1, 6, 0, 0, 0, 15, 6, 16, 0, 0, 0, 0, 0, 17, 4, 0, 1, 0, 0, 0, 0, 0, 2, 1, 0, 4, 1, 0, 0, 0, 21, 3, 4, 0, 1, 0, 4, 0, 0, 0, 95, 4, 1, 0, 0, 0, 0, 0, 0, 13, 0, 26, 1, 3, 0, 0, 0, 0, 1, 7, 0, 0, 0, 0, 10, 0, 0, 0, 3, 1, 3, 0, 0, 0, 4, 0, 22, 2, 7, 0, 0, 2, 0, 1, 1, 0, 0, 0, 0, 0, 3, 0, 1, 2, 3, 1, 0, 0, 0, 0, 12, 0, 4, 0, 25, 2, 0, 98, 3, 0, 0, 0, 2, 2, 0, 2, 4, 0, 0, 2, 1, 0, 0, 0, 1, 1, 1, 0, 0, 13, 0, 0, 5, 0, 10, 0, 1, 0, 0, 1, 8, 1, 1, 3, 11, 2, 0, 1, 104, 3, 0, 0, 4, 0, 2, 1, 3, 0, 1, 0, 0, 2, 3, 0, 0, 0, 3, 2, 5, 20, 14, 7, 0, 5, 0, 4, 1, 2, 0, 0, 0, 2, 0, 0, 2, 0, 7, 0, 3, 0, 1, 0, 3, 0, 5, 0, 7, 2, 2, 0, 4, 1, 15, 0, 0, 2, 0, 2, 0, 0, 0, 2, 7, 2, 0, 3, 0, 0, 0, 1, 16, 5, 0, 3, 0, 0, 0, 0, 0, 7, 2, 1, 0, 0, 6, 0, 0, 0, 0, 4, 0, 6, 0, 8, 5, 0, 0, 10, 18, 1, 1, 2, 0, 0, 4, 0, 0, 2, 5, 0, 26, 0, 0, 162, 0, 21, 7, 0, 0, 10, 0, 7, 15, 3, 0, 0, 5, 0, 1, 0, 0, 0, 4, 3, 14, 1, 0, 1, 4, 9, 11, 0, 6, 0, 0, 0, 13, 1, 3, 6, 2, 1, 0, 15, 0, 2, 0, 3, 6, 0, 1, 1, 0, 0, 5, 2, 4, 0, 1, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 6, 0, 0, 3, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 2, 0, 5, 2, 5, 6, 1, 9, 3, 3, 0, 0, 5, 0, 3, 1, 1, 0, 9, 1, 1, 1, 10, 0, 3, 0, 0, 5, 1, 0, 6, 31, 0, 2, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 5, 0, 0, 9, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0, 0, 2, 11, 6, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 9, 20, 3, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 12, 2, 0, 0, 0, 0, 0, 9, 10, 2, 2, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 28, 0, 0, 0, 0, 0, 5, 0, 0, 3, 2, 0, 20, 0, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 20, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 7, 1, 1, 0, 0, 0, 0, 2, 2, 0, 0, 0, 1, 0, 0, 0, 7, 0, 13, 0, 0, 1, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 40, 0, 11, 3, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 15, 0, 0, 0, 2, 9, 2, 16, 1, 1, 0, 0, 0, 0, 3, 1, 0, 26, 0, 25, 1, 0, 0, 1, 0, 0, 1, 2, 4, 0, 10, 0, 0, 5, 2, 0, 0, 0, 1, 5, 0, 0, 0, 0, 0, 0, 4, 1, 0, 1, 6, 3, 0, 1, 0, 0, 1, 0, 16, 0, 0, 2, 0, 0, 2, 6, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 6, 0, 0, 0, 0, 1, 0, 0, 0, 2, 4, 0, 0, 0, 0, 1, 0, 0, 26, 0, 6, 0, 0, 1, 0, 0, 6, 0, 1, 0, 0, 81, 17, 0, 1, 0, 83, 0, 0, 3, 2, 3, 0, 0, 0, 1, 0, 1, 0, 0, 1, 3, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 15, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 47, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 16, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 9, 0, 0, 18, 0, 0, 0, 1, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 3, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 1, 1, 2, 1, 420, 1, 1, 2, 1, 1, 1, 2, 3, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 0, 0, 0, 22, 0, 0, 1, 0, 0, 0, 0, 0, 16, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 5, 1, 0, 0, 0, 0, 0, 3, 0, 0, 0, 2, 0, 0, 21, 0, 0, 0, 15, 0, 3, 0, 5, 3, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 6, 0, 0, 1, 1, 0, 12, 0, 0, 0, 7, 2, 0, 0, 0, 1, 0, 3, 1, 1, 0, 1, 0, 3, 14, 0, 0, 4, 0, 0, 0, 0, 0, 3, 0, 0, 1, 0, 3, 0, 0, 0, 0, 0, 0, 20, 2, 2, 0, 0, 0, 1, 0, 3, 1, 0, 0, 12, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 8, 0, 0, 0, 1, 1, 3, 0, 20, 0, 0, 0, 10, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 20, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 9, 0, 0, 0, 0, 3, 0, 0, 1, 0, 1, 0, 2, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 1, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 9, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 2, 2, 0, 1, 0, 4, 0, 0, 17, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 4, 2, 0, 2, 0, 0, 3, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 2, 0, 0, 0, 1, 0, 0, 1, 4, 0, 1, 1, 0, 0, 2, 0, 1, 0, 0, 7, 0, 2, 0, 16, 0, 3, 17, 0, 2, 0, 0, 0, 0, 0, 67, 0, 0, 0, 0, 0, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 2, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 2, 5, 0, 0, 0, 0, 0, 0, 0, 2, 13, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 2, 0, 0, 0, 5, 78, 0, 14, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 2, 0, 3, 1, 0, 1, 55, 1, 25, 0, 7, 0, 0, 0, 0, 2, 0, 1, 2, 2, 3, 0, 52, 0, 0, 0, 0, 2, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 6, 3, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 319, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 24, 0, 2, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 4, 0, 0, 0, 1, 17, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 3, 1, 1, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 3, 0, 0, 0, 2, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 2, 0, 1, 0, 0, 0, 4, 0, 0, 2, 4, 26, 0, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 5, 0, 0, 0, 0, 0, 1, 3, 0, 0, 1, 1, 0, 2, 0, 0, 0, 13, 3, 0, 0, 69, 0, 0, 46, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 8, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 1, 0, 1, 0, 0, 3, 1, 0, 0, 0, 0, 0, 0, 0, 4, 0, 1, 0, 0, 0, 0, 1, 0, 0, 3, 4, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 10, 0, 0, 3, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 10, 8, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 105, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0, 0, 0, 5, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 14, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 12, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 2, 0, 7, 0, 0, 0, 0, 0, 0, 14, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 2, 0, 0, 1, 0, 2, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 1, 1, 12, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 44, 1, 0, 2, 1, 0, 1, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 10, 0, 0, 0, 1, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 2, 3, 2, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 12, 0, 0, 0, 0, 0, 3, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 18, 0, 2, 3, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 10, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 18, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 68, 0, 0, 0, 0, 0, 16, 0, 0, 5, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 198, 0, 0, 14, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 13, 12, 0, 0, 1, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 14, 0, 2, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 54, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 65, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 21, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 10, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 9, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 3, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 3, 1, 2, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 3, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 14, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 4, 0, 0, 10, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 14, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 2, 6, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 19, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 4, 3, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 1, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 13, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 16, 0, 0, 1, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 16, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 23, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 197, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 32, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 5, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 17, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 19, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 33, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 3, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 18, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 56, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 29, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 2, 0, 0, 0, 0, 11, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 10, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 4, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 11, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 11, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 2, 3, 67, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 2, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 1, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 4, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 5, 3, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 3, 1, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 2, 33, 0, 1, 0, 2, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 2, 0, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 12, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 5, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 4, 0, 0, 0, 0, 0, 13, 2, 1, 1, 6, 1, 3, 1, 2, 6, 2, 1, 1, 49, 1, 1, 2, 4, 9, 1, 1, 1, 1, 1, 21, 1, 1, 4, 1, 1, 2, 2, 2, 2, 2, 2, 2, 4, 1, 15, 1, 8, 1, 1, 5, 4, 4, 2, 7, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 4, 1, 1, 82, 10, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 1, 1, 1, 1, 1, 1, 1, 2, 113, 1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 2, 4, 11, 3, 7, 8, 1, 6, 29, 2, 2, 1, 1, 8, 20, 1, 21, 2, 3, 2, 3, 1, 8, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 32, 1, 1, 1, 1, 1, 2, 109, 30, 2, 1, 1, 1, 1, 1, 1, 2, 28, 41, 19, 23, 16, 1, 3, 1, 1, 1, 2, 18, 18, 10, 6, 3, 3, 1, 1, 1, 2, 2, 3, 1, 1, 7, 2, 2, 3, 1, 20, 6, 3, 2, 1, 1, 1, 1, 1, 1, 4, 1, 1, 2, 8, 1, 1, 3, 1, 1, 2, 4, 1, 3, 3, 1, 1, 1, 1, 2, 7, 1, 2, 1, 2, 1, 3, 1, 1, 1, 1, 4, 3, 1, 4, 1, 3, 2, 1, 1, 6, 1, 1, 22, 9, 1, 3, 1, 1, 1, 5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 1, 1, 1, 2, 4, 1, 14, 1, 1, 21, 1, 1, 3, 1, 1, 1, 1, 1, 17, 2, 1, 7, 3, 7, 1, 1, 1, 1, 1, 1, 2, 2, 1, 2, 1, 1, 3, 6, 1, 1, 1, 2, 1, 2, 2, 1, 2, 1, 1, 2, 1, 1, 1, 3, 5, 2, 1, 3, 1, 1, 1, 3, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 4, 2, 2, 4, 2, 2, 3, 2, 2, 1, 1, 1, 6, 6, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "----\n",
      "vectors[10] = [0, 0, 4, 0, 3, 0, 0, 19, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 12, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 5, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 3, 5, 1, 2, 4, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 5, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 2, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 2, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 3, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "----\n",
      "vectors[11] = [19, 3, 224, 14, 84, 104, 15, 997, 2, 8, 1, 5, 1, 4, 618, 120, 686, 1, 283, 1, 1, 1, 13, 1, 97, 3, 2, 130, 5, 629, 1, 2, 496, 6, 17, 4, 2, 16, 8, 108, 20, 3, 13, 10, 6, 41, 4, 101, 19, 5, 21, 7, 21, 7, 485, 8, 211, 1, 3, 2, 83, 4, 18, 6, 1, 1, 11, 62, 26, 48, 49, 1, 2, 2, 32, 2, 2, 1, 5, 1, 1, 1, 10, 78, 1, 2, 19, 27, 1, 636, 130, 763, 286, 2, 1, 1, 2, 24, 1, 4, 1, 3, 1, 1, 1, 1, 1, 2, 18, 37, 3, 9, 2, 15, 1, 61, 1, 15, 14, 2, 2, 5, 1, 6, 25, 3, 1, 6, 5, 5, 1, 12, 318, 1, 2, 1, 6, 16, 2, 11, 2, 2, 78, 4, 2, 19, 72, 2, 1, 4, 1, 19, 103, 1, 84, 1, 1, 1, 1, 4, 8, 9, 3, 3, 28, 2, 3, 3, 12, 2, 3, 5, 1, 1, 1, 3, 5, 30, 13, 2, 4, 7, 15, 1, 3, 1, 2, 1, 45, 2, 2, 1, 1, 1, 0, 4, 2, 2, 1, 1, 2, 5, 28, 22, 1, 225, 2, 6, 31, 67, 5, 3, 4, 18, 57, 42, 98, 7, 15, 4, 1, 8, 99, 3, 1, 1, 46, 5, 4, 1, 4, 9, 72, 2, 3, 4, 10, 7, 13, 1, 19, 4, 1, 6, 36, 1, 32, 9, 4, 3, 48, 1, 1, 6, 7, 7, 6, 4, 10, 4, 6, 1, 1, 1, 1, 2, 1, 5, 13, 2, 4, 1, 9, 2, 10, 1, 22, 1, 20, 1, 1, 1, 1, 1, 1, 4, 2, 14, 1, 1, 1, 1, 13, 7, 24, 59, 13, 11, 4, 1, 3, 18, 2, 96, 4, 14, 2, 44, 48, 2, 4, 4, 17, 12, 1, 3, 1, 1, 2, 7, 1, 1, 11, 1, 3, 2, 4, 4, 4, 1, 5, 13, 2, 22, 3, 1, 1, 2, 2, 14, 38, 2, 12, 1, 2, 211, 7, 1, 4, 40, 11, 1, 1, 1, 1, 30, 18, 6, 3, 1, 1, 1, 2, 2, 1, 12, 1, 1, 1, 1, 1, 1, 2, 1, 2, 3, 1, 31, 4, 3, 1, 6, 2, 3, 2, 1, 1, 1, 1, 1, 1, 2, 2, 17, 18, 4, 250, 4, 1, 2, 1, 1, 2, 153, 1, 9, 3, 71, 6, 129, 0, 0, 0, 5, 8, 1, 0, 57, 0, 2, 0, 45, 2, 0, 2, 47, 0, 0, 55, 0, 1, 0, 6, 2, 2, 4, 8, 8, 2, 2, 14, 2, 23, 5, 401, 2, 0, 54, 23, 0, 19, 10, 2, 64, 4, 0, 12, 2, 24, 3, 0, 4, 2, 2, 0, 2, 0, 4, 5, 3, 9, 4, 2, 45, 0, 1, 0, 4, 0, 0, 0, 2, 15, 11, 2, 17, 16, 0, 13, 0, 0, 0, 2, 9, 1, 0, 0, 12, 0, 6, 0, 1, 0, 0, 0, 0, 2, 4, 4, 13, 0, 1, 0, 10, 1, 2, 0, 55, 75, 6, 6, 0, 5, 3, 4, 0, 0, 0, 104, 0, 2, 7, 1, 2, 0, 0, 1, 4, 65, 17, 5, 4, 1, 37, 7, 2, 17, 4, 19, 2, 0, 26, 26, 9, 5, 8, 0, 0, 0, 0, 0, 2, 0, 0, 12, 3, 17, 0, 0, 0, 1, 3, 1, 0, 0, 0, 0, 24, 0, 2, 5, 5, 4, 3, 3, 0, 1, 0, 2, 0, 2, 0, 2, 6, 4, 23, 3, 11, 0, 1, 0, 6, 5, 0, 14, 29, 34, 1, 2, 1, 1, 10, 2, 8, 2, 2, 11, 0, 19, 3, 0, 6, 0, 0, 1, 5, 0, 2, 0, 18, 5, 24, 36, 0, 1, 0, 3, 47, 5, 0, 1, 2, 0, 1, 0, 0, 5, 13, 1, 0, 3, 0, 0, 1, 13, 2, 12, 0, 0, 0, 6, 0, 1, 0, 7, 1, 0, 0, 0, 0, 2, 3, 1, 7, 0, 15, 0, 0, 0, 2, 0, 2, 1, 39, 0, 0, 0, 5, 14, 2, 0, 0, 7, 0, 0, 0, 0, 0, 5, 0, 32, 8, 15, 8, 0, 0, 0, 0, 0, 1, 1, 0, 10, 0, 22, 0, 0, 1, 1, 0, 1, 0, 2, 0, 7, 0, 10, 2, 18, 3, 0, 51, 51, 1, 1, 3, 0, 1, 1, 0, 16, 0, 0, 1, 3, 1, 0, 1, 9, 0, 8, 2, 0, 0, 2, 3, 3, 0, 6, 3, 15, 0, 2, 2, 10, 0, 1, 0, 3, 0, 11, 0, 54, 3, 8, 0, 1, 0, 7, 0, 6, 0, 1, 0, 4, 0, 4, 3, 0, 0, 6, 8, 5, 5, 25, 6, 4, 4, 1, 14, 6, 0, 0, 0, 0, 12, 0, 0, 2, 0, 2, 0, 47, 0, 0, 0, 11, 0, 0, 0, 13, 6, 2, 0, 4, 1, 9, 0, 0, 3, 1, 14, 0, 0, 0, 8, 10, 14, 0, 3, 0, 1, 0, 1, 9, 0, 1, 4, 0, 0, 0, 4, 0, 13, 0, 1, 1, 7, 2, 5, 0, 3, 0, 0, 5, 8, 0, 4, 5, 1, 3, 16, 19, 1, 0, 1, 0, 1, 5, 0, 2, 8, 58, 0, 13, 0, 0, 338, 1, 35, 7, 0, 0, 49, 1, 2, 22, 0, 3, 1, 5, 1, 4, 0, 0, 0, 2, 1, 2, 3, 2, 3, 2, 8, 11, 4, 0, 3, 1, 1, 22, 3, 3, 1, 1, 1, 6, 22, 3, 2, 0, 1, 17, 0, 0, 5, 0, 0, 6, 9, 1, 0, 3, 1, 0, 1, 0, 3, 0, 0, 0, 0, 1, 13, 0, 5, 0, 0, 3, 0, 0, 0, 8, 0, 0, 0, 0, 10, 0, 0, 5, 0, 0, 10, 0, 12, 1, 0, 1, 3, 3, 2, 3, 2, 23, 53, 2, 0, 1, 3, 5, 4, 2, 0, 6, 4, 1, 0, 1, 3, 6, 0, 3, 0, 4, 0, 5, 4, 1, 0, 2, 0, 4, 7, 2, 0, 0, 0, 20, 1, 0, 4, 4, 1, 0, 1, 0, 2, 0, 0, 0, 0, 8, 0, 1, 3, 0, 0, 0, 4, 0, 0, 0, 0, 1, 0, 0, 0, 0, 3, 1, 0, 0, 5, 6, 7, 1, 1, 0, 0, 12, 2, 1, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 3, 2, 0, 0, 0, 20, 0, 2, 5, 0, 0, 6, 0, 3, 2, 1, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 18, 1, 6, 2, 0, 0, 0, 6, 23, 3, 0, 2, 3, 0, 1, 2, 4, 0, 5, 1, 66, 0, 0, 0, 0, 0, 0, 0, 1, 0, 19, 0, 0, 0, 0, 0, 0, 1, 0, 0, 18, 0, 0, 0, 0, 0, 6, 1, 4, 2, 3, 0, 0, 0, 3, 0, 0, 1, 0, 2, 1, 1, 1, 2, 16, 1, 0, 0, 0, 0, 0, 2, 0, 0, 2, 42, 3, 3, 3, 2, 5, 0, 0, 2, 4, 1, 0, 0, 2, 1, 6, 2, 14, 5, 1, 1, 0, 1, 0, 1, 0, 9, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 2, 14, 2, 0, 21, 3, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 1, 11, 0, 9, 17, 3, 0, 0, 0, 0, 3, 2, 3, 10, 0, 1, 2, 1, 1, 6, 0, 0, 0, 3, 1, 0, 14, 2, 2, 9, 1, 5, 1, 0, 6, 10, 1, 0, 0, 1, 0, 0, 3, 0, 3, 1, 2, 1, 0, 3, 0, 0, 10, 0, 40, 1, 1, 1, 0, 6, 2, 8, 1, 0, 0, 5, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 7, 0, 0, 0, 4, 2, 3, 1, 6, 0, 3, 0, 0, 18, 0, 2, 1, 0, 0, 0, 3, 9, 5, 0, 0, 0, 5, 22, 0, 0, 1, 19, 4, 1, 1, 0, 3, 0, 0, 1, 3, 1, 0, 0, 0, 7, 51, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 2, 0, 0, 3, 29, 1, 0, 0, 1, 3, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 18, 3, 2, 0, 1, 5, 1, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 5, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 2, 0, 8, 0, 0, 0, 5, 0, 3, 0, 0, 0, 0, 0, 10, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 6, 2, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 13, 5, 0, 7, 0, 0, 0, 0, 0, 0, 0, 1, 4, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 13, 0, 15, 6, 10, 12, 4, 6, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 2, 1, 37, 0, 0, 1, 0, 0, 0, 0, 5, 1, 0, 0, 0, 0, 1, 0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 1, 2, 3, 29, 1, 1, 16, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 4, 1, 2, 1, 0, 0, 1, 0, 5, 0, 0, 1, 0, 0, 1, 0, 7, 2, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 1, 1, 1, 1, 0, 5, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 13, 0, 0, 2, 0, 0, 0, 13, 0, 4, 0, 0, 1, 1, 0, 0, 0, 0, 0, 2, 0, 1, 0, 1, 1, 0, 0, 1, 0, 6, 0, 3, 1, 1, 0, 21, 0, 12, 1, 8, 0, 0, 0, 0, 0, 0, 6, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 1, 0, 0, 0, 2, 4, 0, 2, 0, 1, 9, 0, 1, 0, 2, 0, 3, 8, 0, 0, 0, 1, 0, 1, 2, 3, 6, 1, 0, 0, 0, 2, 0, 3, 1, 0, 3, 0, 0, 1, 1, 0, 0, 0, 1, 2, 4, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 2, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 2, 0, 1, 0, 6, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 16, 0, 0, 5, 0, 2, 0, 0, 0, 0, 0, 0, 1, 29, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 9, 0, 0, 0, 0, 4, 1, 2, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 8, 0, 4, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 1, 2, 3, 1, 0, 0, 0, 0, 1, 0, 0, 1, 3, 5, 0, 0, 0, 0, 0, 0, 0, 7, 0, 14, 1, 2, 1, 0, 3, 0, 0, 0, 1, 5, 2, 8, 4, 0, 1, 0, 4, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 2, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 2, 0, 0, 1, 0, 1, 0, 3, 0, 1, 2, 0, 4, 2, 1, 3, 0, 0, 3, 2, 0, 0, 1, 3, 0, 0, 3, 0, 0, 0, 0, 2, 0, 0, 0, 8, 0, 0, 6, 0, 1, 2, 0, 1, 4, 0, 147, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 9, 3, 2, 1, 0, 0, 6, 0, 2, 1, 0, 2, 0, 0, 1, 0, 0, 1, 0, 0, 0, 3, 7, 0, 0, 1, 0, 0, 0, 1, 5, 0, 1, 0, 3, 3, 1, 0, 6, 0, 3, 1, 3, 0, 10, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 3, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0, 0, 9, 17, 2, 14, 1, 0, 0, 0, 0, 3, 0, 0, 2, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 3, 1, 0, 13, 0, 2, 0, 1, 0, 0, 0, 0, 1, 0, 4, 0, 1, 2, 2, 19, 0, 0, 0, 0, 3, 1, 0, 0, 2, 1, 3, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 24, 0, 0, 0, 1, 5, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 3, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 1, 0, 2, 1, 1, 0, 0, 0, 3, 0, 1, 0, 3, 0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 0, 8, 2, 3, 0, 0, 0, 0, 0, 10, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0, 0, 0, 2, 20, 0, 0, 0, 2, 0, 2, 1, 0, 0, 2, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 6, 1, 1, 2, 0, 1, 2, 4, 1, 0, 1, 0, 1, 1, 0, 8, 1, 0, 0, 3, 3, 0, 1, 0, 0, 0, 4, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 5, 2, 0, 0, 1, 0, 0, 2, 2, 1, 0, 5, 1, 2, 0, 14, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 5, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 2, 0, 4, 0, 0, 0, 0, 4, 4, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 9, 0, 0, 0, 0, 0, 0, 0, 6, 4, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 2, 0, 4, 0, 2, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 6, 0, 3, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 2, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 2, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 3, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 2, 1, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 1, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 18, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 4, 0, 0, 1, 0, 2, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 4, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 2, 0, 0, 1, 0, 0, 0, 0, 0, 13, 1, 0, 0, 2, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 0, 0, 1, 2, 1, 8, 0, 0, 2, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 4, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 3, 1, 1, 0, 4, 1, 1, 0, 6, 1, 7, 2, 0, 0, 0, 0, 0, 0, 1, 0, 2, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 4, 0, 0, 1, 0, 0, 0, 0, 0, 4, 4, 1, 0, 1, 2, 2, 0, 0, 0, 0, 0, 4, 0, 1, 3, 3, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 3, 0, 0, 2, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 1, 1, 22, 0, 1, 1, 0, 0, 2, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 2, 0, 0, 3, 4, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 22, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 3, 11, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 1, 4, 0, 0, 4, 0, 0, 0, 0, 0, 2, 0, 4, 3, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 3, 0, 0, 1, 23, 9, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 5, 0, 0, 1, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 6, 2, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 1, 0, 0, 2, 0, 1, 0, 3, 0, 0, 8, 0, 0, 0, 0, 64, 0, 1, 1, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 2, 0, 3, 1, 1, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 2, 42, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 74, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 1, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 5, 2, 1, 1, 4, 0, 0, 0, 0, 0, 0, 0, 24, 2, 0, 31, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 10, 0, 0, 0, 0, 0, 15, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 7, 0, 2, 0, 3, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 7, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 3, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 28, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 9, 0, 0, 0, 0, 2, 0, 0, 3, 0, 15, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 3, 0, 0, 0, 0, 2, 1, 0, 2, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 8, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 8, 0, 5, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 0, 4, 6, 6, 0, 0, 4, 28, 19, 0, 0, 1, 2, 1, 0, 19, 3, 27, 0, 0, 0, 0, 0, 0, 27, 0, 2, 0, 1, 0, 28, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 9, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 1, 0, 2, 2, 2, 0, 0, 0, 0, 0, 2, 1, 4, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 5, 0, 9, 1, 2, 1, 0, 0, 0, 32, 20, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0, 4, 0, 0, 1, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 2, 11, 0, 0, 0, 0, 0, 0, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 2, 0, 0, 0, 0, 3, 0, 0, 1, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 4, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 2, 0, 0, 0, 0, 1, 1, 3, 1, 3, 2, 1, 1, 0, 1, 0, 5, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 1, 5, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 3, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 4, 4, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 5, 2, 0, 5, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 15, 0, 0, 0, 0, 0, 0, 0, 1, 1, 5, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 4, 11, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 14, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 18, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 1, 0, 0, 3, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 10, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 4, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 3, 0, 1, 0, 0, 0, 9, 1, 0, 0, 0, 3, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 1, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 17, 3, 1, 1, 4, 3, 3, 1, 2, 3, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 2, 2, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 3, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 3, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 3, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 1, 0, 4, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 9, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 1, 0, 5, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 34, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 4, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 2, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 3, 0, 0, 0, 0, 3, 0, 21, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 16, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 3, 2, 4, 3, 1, 1, 1, 1, 1, 1, 3, 3, 1, 1, 1, 3, 1, 1, 1, 6, 1, 3, 3, 2, 1, 6, 1, 1, 2, 5, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 2, 1, 4, 1, 2, 1, 2, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 6, 1, 2, 1, 1, 1, 3, 1, 1, 1, 1, 3, 1, 2, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 6, 5, 3, 6, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 3, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 3, 1, 1, 1, 2, 5, 1, 2, 3, 3, 1, 2, 2, 2, 5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 2, 1, 3, 7, 4, 4, 2, 1, 1, 2, 2, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 10, 2, 1, 1, 3, 1, 6, 1, 1, 1, 2, 6, 2, 2, 1, 1, 1, 1, 1, 1, 3, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 13, 2, 1, 1, 2, 1, 2, 1, 2, 1, 2, 2, 2, 2, 2, 2, 2, 4, 3, 1, 1, 1, 1, 1, 1, 1, 4, 4, 4, 4, 4, 2, 1, 1, 2, 1, 1, 1, 1, 1, 18, 3, 1, 3, 3, 1, 1, 5, 2, 2, 1, 1, 1, 1, 2, 1, 3, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 5, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 3, 2, 3, 1, 2, 1, 1, 1, 4, 2, 2, 2, 2, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 2, 1, 2, 1, 5, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 2, 2, 2, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 2, 1, 3, 2, 1, 1, 3, 2, 1, 7, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 7, 1, 1, 1, 2, 4, 2, 2, 4, 1, 2, 4, 4, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 4, 2, 1, 2, 2, 1, 3, 5, 7, 2, 1, 2, 12, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 3, 2, 2, 2, 7, 1, 1, 1, 4, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "----\n",
      "vectors[12] = [7, 3, 64, 13, 28, 104, 7, 337, 2, 5, 1, 5, 1, 4, 1, 4, 188, 1, 1, 1, 1, 1, 2, 1, 97, 3, 2, 29, 5, 137, 1, 2, 217, 3, 11, 3, 2, 16, 8, 37, 6, 3, 7, 10, 6, 35, 8, 21, 19, 2, 16, 7, 21, 7, 106, 8, 36, 1, 2, 2, 12, 4, 17, 6, 1, 1, 11, 62, 26, 48, 46, 1, 2, 2, 32, 1, 2, 1, 5, 1, 1, 1, 1, 55, 1, 2, 18, 1, 1, 12, 10, 210, 72, 2, 1, 1, 2, 2, 1, 2, 1, 3, 1, 1, 1, 1, 1, 2, 4, 13, 3, 2, 2, 6, 1, 14, 1, 11, 9, 2, 2, 3, 1, 5, 5, 2, 1, 2, 3, 4, 1, 12, 180, 1, 2, 1, 1, 1, 2, 3, 2, 2, 12, 4, 2, 1, 18, 2, 1, 1, 2, 6, 18, 1, 33, 1, 1, 1, 1, 4, 10, 6, 3, 3, 10, 2, 3, 3, 6, 2, 3, 5, 1, 1, 1, 2, 4, 5, 2, 2, 1, 3, 4, 1, 3, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 3, 2, 2, 2, 1, 2, 4, 9, 3, 1, 66, 1, 1, 7, 20, 5, 1, 2, 5, 20, 26, 18, 1, 13, 5, 1, 2, 41, 1, 1, 1, 9, 3, 1, 1, 2, 4, 20, 1, 2, 1, 10, 2, 1, 1, 1, 1, 1, 4, 24, 1, 29, 9, 5, 3, 14, 1, 1, 6, 6, 3, 3, 4, 4, 4, 4, 1, 1, 1, 1, 2, 1, 5, 11, 2, 3, 1, 2, 2, 10, 1, 22, 1, 13, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 13, 5, 24, 45, 13, 10, 2, 2, 3, 5, 2, 29, 4, 14, 2, 8, 25, 2, 2, 4, 17, 12, 1, 1, 1, 1, 2, 7, 1, 1, 11, 1, 5, 2, 4, 3, 4, 1, 5, 2, 3, 7, 3, 1, 1, 2, 2, 6, 1, 1, 2, 1, 2, 10, 2, 1, 4, 2, 1, 1, 1, 1, 1, 30, 18, 1, 3, 1, 1, 1, 2, 2, 1, 12, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 5, 4, 3, 1, 1, 1, 3, 2, 1, 1, 1, 1, 1, 1, 2, 2, 5, 3, 1, 88, 2, 3, 1, 1, 1, 2, 31, 5, 7, 14, 55, 13, 16, 1, 0, 0, 9, 6, 6, 0, 8, 0, 3, 0, 4, 7, 1, 0, 9, 0, 0, 9, 1, 3, 0, 0, 0, 5, 2, 4, 24, 0, 0, 1, 0, 6, 1, 109, 4, 0, 2, 2, 0, 5, 0, 0, 3, 0, 0, 1, 1, 6, 0, 3, 0, 0, 0, 1, 1, 0, 0, 3, 0, 6, 3, 8, 11, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 5, 0, 3, 0, 2, 1, 0, 2, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 2, 0, 0, 1, 2, 1, 1, 0, 2, 7, 1, 5, 0, 1, 6, 3, 1, 0, 0, 5, 0, 2, 0, 0, 0, 1, 0, 0, 1, 4, 2, 1, 0, 8, 4, 0, 0, 4, 0, 6, 1, 0, 6, 6, 1, 0, 5, 0, 0, 0, 0, 0, 3, 0, 0, 2, 1, 5, 1, 1, 0, 0, 0, 4, 0, 0, 0, 4, 9, 0, 1, 1, 1, 0, 3, 1, 0, 0, 0, 6, 0, 6, 0, 1, 0, 3, 11, 2, 0, 0, 0, 0, 1, 1, 0, 4, 3, 4, 1, 0, 0, 7, 1, 3, 2, 0, 1, 2, 0, 1, 0, 0, 1, 0, 0, 0, 2, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 10, 1, 0, 2, 0, 0, 0, 0, 0, 2, 1, 0, 3, 0, 0, 0, 0, 4, 2, 6, 0, 0, 0, 0, 0, 2, 0, 2, 1, 1, 1, 0, 0, 0, 1, 0, 5, 0, 0, 0, 2, 1, 0, 0, 0, 1, 8, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 12, 0, 9, 0, 0, 0, 0, 1, 0, 1, 2, 0, 1, 0, 1, 0, 1, 0, 2, 2, 0, 0, 0, 1, 1, 0, 1, 2, 6, 0, 0, 4, 4, 0, 4, 2, 0, 0, 0, 0, 4, 0, 0, 0, 2, 0, 2, 2, 3, 0, 11, 0, 0, 0, 0, 1, 2, 2, 6, 1, 2, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 4, 0, 0, 0, 3, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 1, 3, 1, 0, 1, 0, 0, 3, 2, 3, 4, 8, 0, 1, 0, 0, 0, 0, 1, 0, 2, 0, 1, 0, 0, 1, 0, 1, 0, 3, 0, 0, 0, 2, 0, 2, 0, 0, 0, 1, 5, 0, 5, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 6, 0, 0, 1, 1, 0, 2, 1, 1, 0, 1, 4, 1, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 1, 0, 0, 3, 0, 0, 1, 0, 0, 1, 1, 2, 0, 5, 0, 1, 0, 0, 0, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 2, 0, 0, 2, 3, 1, 0, 0, 1, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 2, 0, 0, 0, 6, 0, 0, 0, 0, 3, 0, 0, 1, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 1, 0, 3, 6, 1, 0, 0, 2, 6, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 3, 0, 1, 3, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 2, 3, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 6, 3, 12, 0, 1, 0, 1, 0, 0, 1, 0, 2, 2, 0, 0, 1, 0, 1, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 3, 0, 0, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 2, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 2, 1, 0, 5, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 5, 1, 0, 0, 0, 0, 0, 1, 5, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 4, 3, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 4, 0, 0, 0, 2, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 3, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 3, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 3, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 4, 1, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 3, 2, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 3, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 1, 1, 3, 1, 1, 1, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 5, 0, 0, 1, 0, 0, 0, 0, 0, 2, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 3, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 6, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 2, 0, 0, 0, 0, 0, 1, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 3, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 2, 0, 3, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 2, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 2, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 6, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 6, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 3, 3, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "vectors = make_vectors_eng(sentence, codebook)\n",
    "for index in range(len(sentence)):\n",
    "    print('vectors[{}] = {}'.format(index,vectors[index]))\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(len(codebook),vectors[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ユークリッド距離を求める"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# euclidean_distance\n",
      "[0.0, 1611.9857319467812, 2211.5716131294507, 2678.0039208335747, 2249.7193158258656, 1997.7191994872553, 1775.0504218190536, 1007.4790320398732, 1313.3392554858017, 1403.9907407102085, 795.6418792396489, 1610.5294160616875, 248.18944377229263]\n",
      "[1611.9857319467812, 0.0, 1079.8273010069713, 1238.4655828887617, 847.4426234265067, 769.4114633926375, 942.8700864912408, 1013.7869598687882, 930.4719232733463, 1157.6579805797564, 2189.460207448402, 815.6512735231889, 1749.6713977201548]\n",
      "[2211.5716131294507, 1079.8273010069713, 0.0, 940.6784785462033, 1134.5377913494112, 898.9755280317702, 1556.1834724736027, 1622.224706999619, 1581.9352072698805, 1640.3581925908743, 2713.5266720634977, 1422.728364797722, 2323.9477188611622]\n",
      "[2678.0039208335747, 1238.4655828887617, 940.6784785462033, 0.0, 934.0706611386528, 1063.5229193581115, 1644.6163686404195, 1974.9706326930534, 1822.5358707032353, 1892.6597686853281, 3212.9349511000064, 1560.6473016027676, 2803.7241305092766]\n",
      "[2249.7193158258656, 847.4426234265067, 1134.5377913494112, 934.0706611386528, 0.0, 841.3994295220315, 1065.8466118536944, 1509.8135646496225, 1307.3320159775787, 1483.0539437255813, 2841.5782586443047, 1138.972343825784, 2396.005634384026]\n",
      "[1997.7191994872553, 769.4114633926375, 898.9755280317702, 1063.5229193581115, 841.3994295220315, 0.0, 1123.1758544413249, 1262.7010730968752, 1214.591289282119, 1388.3457782555467, 2552.1148093297056, 1078.0848760649599, 2125.901220659135]\n",
      "[1775.0504218190536, 942.8700864912408, 1556.1834724736027, 1644.6163686404195, 1065.8466118536944, 1123.1758544413249, 0.0, 1102.4227864118195, 900.2966177877156, 1259.428441794134, 2395.0761992053613, 1071.5330139571063, 1922.8120032910133]\n",
      "[1007.4790320398732, 1013.7869598687882, 1622.224706999619, 1974.9706326930534, 1509.8135646496225, 1262.7010730968752, 1102.4227864118195, 0.0, 707.3895673530958, 993.7202825745281, 1631.0677484396533, 1001.346593343184, 1136.3045366449965]\n",
      "[1313.3392554858017, 930.4719232733463, 1581.9352072698805, 1822.5358707032353, 1307.3320159775787, 1214.591289282119, 900.2966177877156, 707.3895673530958, 0.0, 982.3105415295104, 1945.35138214154, 1025.374565707576, 1454.5129769101409]\n",
      "[1403.9907407102085, 1157.6579805797564, 1640.3581925908743, 1892.6597686853281, 1483.0539437255813, 1388.3457782555467, 1259.428441794134, 993.7202825745281, 982.3105415295104, 0.0, 1909.3433426180845, 1248.2063130748859, 1498.1602050515157]\n",
      "[795.6418792396489, 2189.460207448402, 2713.5266720634977, 3212.9349511000064, 2841.5782586443047, 2552.1148093297056, 2395.0761992053613, 1631.0677484396533, 1945.35138214154, 1909.3433426180845, 0.0, 2211.09271628306, 619.1801030394953]\n",
      "[1610.5294160616875, 815.6512735231889, 1422.728364797722, 1560.6473016027676, 1138.972343825784, 1078.0848760649599, 1071.5330139571063, 1001.346593343184, 1025.374565707576, 1248.2063130748859, 2211.09271628306, 0.0, 1737.4035225013215]\n",
      "[248.18944377229263, 1749.6713977201548, 2323.9477188611622, 2803.7241305092766, 2396.005634384026, 2125.901220659135, 1922.8120032910133, 1136.3045366449965, 1454.5129769101409, 1498.1602050515157, 619.1801030394953, 1737.4035225013215, 0.0]\n"
     ]
    }
   ],
   "source": [
    "distances = euclidean_distance(vectors)\n",
    "print('# euclidean_distance')\n",
    "for index in range(len(distances)):\n",
    "    print(distances[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### コサイン類似度を求める"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# cosine_similarity\n",
      "[1.    0.817 0.726 0.745 0.811 0.782 0.85  0.882 0.875 0.765 0.699 0.836\n",
      " 0.971]\n",
      "[0.817 1.    0.926 0.966 0.977 0.959 0.921 0.902 0.907 0.852 0.627 0.932\n",
      " 0.789]\n",
      "[0.726 0.926 1.    0.964 0.919 0.944 0.824 0.838 0.822 0.806 0.593 0.855\n",
      " 0.708]\n",
      "[0.745 0.966 0.964 1.    0.96  0.958 0.869 0.868 0.864 0.849 0.58  0.901\n",
      " 0.723]\n",
      "[0.811 0.977 0.919 0.96  1.    0.958 0.932 0.914 0.919 0.88  0.628 0.93\n",
      " 0.78 ]\n",
      "[0.782 0.959 0.944 0.958 0.958 1.    0.9   0.912 0.89  0.847 0.606 0.909\n",
      " 0.761]\n",
      "[0.85  0.921 0.824 0.869 0.932 0.9   1.    0.921 0.936 0.855 0.68  0.897\n",
      " 0.826]\n",
      "[0.882 0.902 0.838 0.868 0.914 0.912 0.921 1.    0.938 0.857 0.657 0.909\n",
      " 0.874]\n",
      "[0.875 0.907 0.822 0.864 0.919 0.89  0.936 0.938 1.    0.873 0.702 0.888\n",
      " 0.86 ]\n",
      "[0.765 0.852 0.806 0.849 0.88  0.847 0.855 0.857 0.873 1.    0.552 0.829\n",
      " 0.762]\n",
      "[0.699 0.627 0.593 0.58  0.628 0.606 0.68  0.657 0.702 0.552 1.    0.596\n",
      " 0.638]\n",
      "[0.836 0.932 0.855 0.901 0.93  0.909 0.897 0.909 0.888 0.829 0.596 1.\n",
      " 0.829]\n",
      "[0.971 0.789 0.708 0.723 0.78  0.761 0.826 0.874 0.86  0.762 0.638 0.829\n",
      " 1.   ]\n"
     ]
    }
   ],
   "source": [
    "cosin = cos_sim(vectors)\n",
    "print('# cosine_similarity')\n",
    "for index in range(len(cosin)):\n",
    "    print(np.round(cosin[index],3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x11ce23198>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(cosin,vmax=1, vmin=-1, center=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000</td>\n",
       "      <td>0.817</td>\n",
       "      <td>0.726</td>\n",
       "      <td>0.745</td>\n",
       "      <td>0.811</td>\n",
       "      <td>0.782</td>\n",
       "      <td>0.850</td>\n",
       "      <td>0.882</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.765</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.836</td>\n",
       "      <td>0.971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.817</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.926</td>\n",
       "      <td>0.966</td>\n",
       "      <td>0.977</td>\n",
       "      <td>0.959</td>\n",
       "      <td>0.921</td>\n",
       "      <td>0.902</td>\n",
       "      <td>0.907</td>\n",
       "      <td>0.852</td>\n",
       "      <td>0.627</td>\n",
       "      <td>0.932</td>\n",
       "      <td>0.789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.726</td>\n",
       "      <td>0.926</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.964</td>\n",
       "      <td>0.919</td>\n",
       "      <td>0.944</td>\n",
       "      <td>0.824</td>\n",
       "      <td>0.838</td>\n",
       "      <td>0.822</td>\n",
       "      <td>0.806</td>\n",
       "      <td>0.593</td>\n",
       "      <td>0.855</td>\n",
       "      <td>0.708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.745</td>\n",
       "      <td>0.966</td>\n",
       "      <td>0.964</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.960</td>\n",
       "      <td>0.958</td>\n",
       "      <td>0.869</td>\n",
       "      <td>0.868</td>\n",
       "      <td>0.864</td>\n",
       "      <td>0.849</td>\n",
       "      <td>0.580</td>\n",
       "      <td>0.901</td>\n",
       "      <td>0.723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.811</td>\n",
       "      <td>0.977</td>\n",
       "      <td>0.919</td>\n",
       "      <td>0.960</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.958</td>\n",
       "      <td>0.932</td>\n",
       "      <td>0.914</td>\n",
       "      <td>0.919</td>\n",
       "      <td>0.880</td>\n",
       "      <td>0.628</td>\n",
       "      <td>0.930</td>\n",
       "      <td>0.780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.782</td>\n",
       "      <td>0.959</td>\n",
       "      <td>0.944</td>\n",
       "      <td>0.958</td>\n",
       "      <td>0.958</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.900</td>\n",
       "      <td>0.912</td>\n",
       "      <td>0.890</td>\n",
       "      <td>0.847</td>\n",
       "      <td>0.606</td>\n",
       "      <td>0.909</td>\n",
       "      <td>0.761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.850</td>\n",
       "      <td>0.921</td>\n",
       "      <td>0.824</td>\n",
       "      <td>0.869</td>\n",
       "      <td>0.932</td>\n",
       "      <td>0.900</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.921</td>\n",
       "      <td>0.936</td>\n",
       "      <td>0.855</td>\n",
       "      <td>0.680</td>\n",
       "      <td>0.897</td>\n",
       "      <td>0.826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.882</td>\n",
       "      <td>0.902</td>\n",
       "      <td>0.838</td>\n",
       "      <td>0.868</td>\n",
       "      <td>0.914</td>\n",
       "      <td>0.912</td>\n",
       "      <td>0.921</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.938</td>\n",
       "      <td>0.857</td>\n",
       "      <td>0.657</td>\n",
       "      <td>0.909</td>\n",
       "      <td>0.874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.875</td>\n",
       "      <td>0.907</td>\n",
       "      <td>0.822</td>\n",
       "      <td>0.864</td>\n",
       "      <td>0.919</td>\n",
       "      <td>0.890</td>\n",
       "      <td>0.936</td>\n",
       "      <td>0.938</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.873</td>\n",
       "      <td>0.702</td>\n",
       "      <td>0.888</td>\n",
       "      <td>0.860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.765</td>\n",
       "      <td>0.852</td>\n",
       "      <td>0.806</td>\n",
       "      <td>0.849</td>\n",
       "      <td>0.880</td>\n",
       "      <td>0.847</td>\n",
       "      <td>0.855</td>\n",
       "      <td>0.857</td>\n",
       "      <td>0.873</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.552</td>\n",
       "      <td>0.829</td>\n",
       "      <td>0.762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.699</td>\n",
       "      <td>0.627</td>\n",
       "      <td>0.593</td>\n",
       "      <td>0.580</td>\n",
       "      <td>0.628</td>\n",
       "      <td>0.606</td>\n",
       "      <td>0.680</td>\n",
       "      <td>0.657</td>\n",
       "      <td>0.702</td>\n",
       "      <td>0.552</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.596</td>\n",
       "      <td>0.638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.836</td>\n",
       "      <td>0.932</td>\n",
       "      <td>0.855</td>\n",
       "      <td>0.901</td>\n",
       "      <td>0.930</td>\n",
       "      <td>0.909</td>\n",
       "      <td>0.897</td>\n",
       "      <td>0.909</td>\n",
       "      <td>0.888</td>\n",
       "      <td>0.829</td>\n",
       "      <td>0.596</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.971</td>\n",
       "      <td>0.789</td>\n",
       "      <td>0.708</td>\n",
       "      <td>0.723</td>\n",
       "      <td>0.780</td>\n",
       "      <td>0.761</td>\n",
       "      <td>0.826</td>\n",
       "      <td>0.874</td>\n",
       "      <td>0.860</td>\n",
       "      <td>0.762</td>\n",
       "      <td>0.638</td>\n",
       "      <td>0.829</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0      1      2      3      4      5      6      7      8      9   \\\n",
       "0   1.000  0.817  0.726  0.745  0.811  0.782  0.850  0.882  0.875  0.765   \n",
       "1   0.817  1.000  0.926  0.966  0.977  0.959  0.921  0.902  0.907  0.852   \n",
       "2   0.726  0.926  1.000  0.964  0.919  0.944  0.824  0.838  0.822  0.806   \n",
       "3   0.745  0.966  0.964  1.000  0.960  0.958  0.869  0.868  0.864  0.849   \n",
       "4   0.811  0.977  0.919  0.960  1.000  0.958  0.932  0.914  0.919  0.880   \n",
       "5   0.782  0.959  0.944  0.958  0.958  1.000  0.900  0.912  0.890  0.847   \n",
       "6   0.850  0.921  0.824  0.869  0.932  0.900  1.000  0.921  0.936  0.855   \n",
       "7   0.882  0.902  0.838  0.868  0.914  0.912  0.921  1.000  0.938  0.857   \n",
       "8   0.875  0.907  0.822  0.864  0.919  0.890  0.936  0.938  1.000  0.873   \n",
       "9   0.765  0.852  0.806  0.849  0.880  0.847  0.855  0.857  0.873  1.000   \n",
       "10  0.699  0.627  0.593  0.580  0.628  0.606  0.680  0.657  0.702  0.552   \n",
       "11  0.836  0.932  0.855  0.901  0.930  0.909  0.897  0.909  0.888  0.829   \n",
       "12  0.971  0.789  0.708  0.723  0.780  0.761  0.826  0.874  0.860  0.762   \n",
       "\n",
       "       10     11     12  \n",
       "0   0.699  0.836  0.971  \n",
       "1   0.627  0.932  0.789  \n",
       "2   0.593  0.855  0.708  \n",
       "3   0.580  0.901  0.723  \n",
       "4   0.628  0.930  0.780  \n",
       "5   0.606  0.909  0.761  \n",
       "6   0.680  0.897  0.826  \n",
       "7   0.657  0.909  0.874  \n",
       "8   0.702  0.888  0.860  \n",
       "9   0.552  0.829  0.762  \n",
       "10  1.000  0.596  0.638  \n",
       "11  0.596  1.000  0.829  \n",
       "12  0.638  0.829  1.000  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_label=[0,1,2,3,4,5,6,7,8,9,10,11,12]\n",
    "cosin = np.round(cosin,3)\n",
    "df = pd.DataFrame(cosin,list_label)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000</td>\n",
       "      <td>0.817</td>\n",
       "      <td>0.726</td>\n",
       "      <td>0.745</td>\n",
       "      <td>0.811</td>\n",
       "      <td>0.782</td>\n",
       "      <td>0.850</td>\n",
       "      <td>0.882</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.765</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.836</td>\n",
       "      <td>0.971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.817</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.926</td>\n",
       "      <td>0.966</td>\n",
       "      <td>0.977</td>\n",
       "      <td>0.959</td>\n",
       "      <td>0.921</td>\n",
       "      <td>0.902</td>\n",
       "      <td>0.907</td>\n",
       "      <td>0.852</td>\n",
       "      <td>0.627</td>\n",
       "      <td>0.932</td>\n",
       "      <td>0.789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.726</td>\n",
       "      <td>0.926</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.964</td>\n",
       "      <td>0.919</td>\n",
       "      <td>0.944</td>\n",
       "      <td>0.824</td>\n",
       "      <td>0.838</td>\n",
       "      <td>0.822</td>\n",
       "      <td>0.806</td>\n",
       "      <td>0.593</td>\n",
       "      <td>0.855</td>\n",
       "      <td>0.708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.745</td>\n",
       "      <td>0.966</td>\n",
       "      <td>0.964</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.960</td>\n",
       "      <td>0.958</td>\n",
       "      <td>0.869</td>\n",
       "      <td>0.868</td>\n",
       "      <td>0.864</td>\n",
       "      <td>0.849</td>\n",
       "      <td>0.580</td>\n",
       "      <td>0.901</td>\n",
       "      <td>0.723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.811</td>\n",
       "      <td>0.977</td>\n",
       "      <td>0.919</td>\n",
       "      <td>0.960</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.958</td>\n",
       "      <td>0.932</td>\n",
       "      <td>0.914</td>\n",
       "      <td>0.919</td>\n",
       "      <td>0.880</td>\n",
       "      <td>0.628</td>\n",
       "      <td>0.930</td>\n",
       "      <td>0.780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.782</td>\n",
       "      <td>0.959</td>\n",
       "      <td>0.944</td>\n",
       "      <td>0.958</td>\n",
       "      <td>0.958</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.900</td>\n",
       "      <td>0.912</td>\n",
       "      <td>0.890</td>\n",
       "      <td>0.847</td>\n",
       "      <td>0.606</td>\n",
       "      <td>0.909</td>\n",
       "      <td>0.761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.850</td>\n",
       "      <td>0.921</td>\n",
       "      <td>0.824</td>\n",
       "      <td>0.869</td>\n",
       "      <td>0.932</td>\n",
       "      <td>0.900</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.921</td>\n",
       "      <td>0.936</td>\n",
       "      <td>0.855</td>\n",
       "      <td>0.680</td>\n",
       "      <td>0.897</td>\n",
       "      <td>0.826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.882</td>\n",
       "      <td>0.902</td>\n",
       "      <td>0.838</td>\n",
       "      <td>0.868</td>\n",
       "      <td>0.914</td>\n",
       "      <td>0.912</td>\n",
       "      <td>0.921</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.938</td>\n",
       "      <td>0.857</td>\n",
       "      <td>0.657</td>\n",
       "      <td>0.909</td>\n",
       "      <td>0.874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.875</td>\n",
       "      <td>0.907</td>\n",
       "      <td>0.822</td>\n",
       "      <td>0.864</td>\n",
       "      <td>0.919</td>\n",
       "      <td>0.890</td>\n",
       "      <td>0.936</td>\n",
       "      <td>0.938</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.873</td>\n",
       "      <td>0.702</td>\n",
       "      <td>0.888</td>\n",
       "      <td>0.860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.765</td>\n",
       "      <td>0.852</td>\n",
       "      <td>0.806</td>\n",
       "      <td>0.849</td>\n",
       "      <td>0.880</td>\n",
       "      <td>0.847</td>\n",
       "      <td>0.855</td>\n",
       "      <td>0.857</td>\n",
       "      <td>0.873</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.552</td>\n",
       "      <td>0.829</td>\n",
       "      <td>0.762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.699</td>\n",
       "      <td>0.627</td>\n",
       "      <td>0.593</td>\n",
       "      <td>0.580</td>\n",
       "      <td>0.628</td>\n",
       "      <td>0.606</td>\n",
       "      <td>0.680</td>\n",
       "      <td>0.657</td>\n",
       "      <td>0.702</td>\n",
       "      <td>0.552</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.596</td>\n",
       "      <td>0.638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.836</td>\n",
       "      <td>0.932</td>\n",
       "      <td>0.855</td>\n",
       "      <td>0.901</td>\n",
       "      <td>0.930</td>\n",
       "      <td>0.909</td>\n",
       "      <td>0.897</td>\n",
       "      <td>0.909</td>\n",
       "      <td>0.888</td>\n",
       "      <td>0.829</td>\n",
       "      <td>0.596</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.971</td>\n",
       "      <td>0.789</td>\n",
       "      <td>0.708</td>\n",
       "      <td>0.723</td>\n",
       "      <td>0.780</td>\n",
       "      <td>0.761</td>\n",
       "      <td>0.826</td>\n",
       "      <td>0.874</td>\n",
       "      <td>0.860</td>\n",
       "      <td>0.762</td>\n",
       "      <td>0.638</td>\n",
       "      <td>0.829</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0      1      2      3      4      5      6      7      8      9   \\\n",
       "0   1.000  0.817  0.726  0.745  0.811  0.782  0.850  0.882  0.875  0.765   \n",
       "1   0.817  1.000  0.926  0.966  0.977  0.959  0.921  0.902  0.907  0.852   \n",
       "2   0.726  0.926  1.000  0.964  0.919  0.944  0.824  0.838  0.822  0.806   \n",
       "3   0.745  0.966  0.964  1.000  0.960  0.958  0.869  0.868  0.864  0.849   \n",
       "4   0.811  0.977  0.919  0.960  1.000  0.958  0.932  0.914  0.919  0.880   \n",
       "5   0.782  0.959  0.944  0.958  0.958  1.000  0.900  0.912  0.890  0.847   \n",
       "6   0.850  0.921  0.824  0.869  0.932  0.900  1.000  0.921  0.936  0.855   \n",
       "7   0.882  0.902  0.838  0.868  0.914  0.912  0.921  1.000  0.938  0.857   \n",
       "8   0.875  0.907  0.822  0.864  0.919  0.890  0.936  0.938  1.000  0.873   \n",
       "9   0.765  0.852  0.806  0.849  0.880  0.847  0.855  0.857  0.873  1.000   \n",
       "10  0.699  0.627  0.593  0.580  0.628  0.606  0.680  0.657  0.702  0.552   \n",
       "11  0.836  0.932  0.855  0.901  0.930  0.909  0.897  0.909  0.888  0.829   \n",
       "12  0.971  0.789  0.708  0.723  0.780  0.761  0.826  0.874  0.860  0.762   \n",
       "\n",
       "       10     11     12  \n",
       "0   0.699  0.836  0.971  \n",
       "1   0.627  0.932  0.789  \n",
       "2   0.593  0.855  0.708  \n",
       "3   0.580  0.901  0.723  \n",
       "4   0.628  0.930  0.780  \n",
       "5   0.606  0.909  0.761  \n",
       "6   0.680  0.897  0.826  \n",
       "7   0.657  0.909  0.874  \n",
       "8   0.702  0.888  0.860  \n",
       "9   0.552  0.829  0.762  \n",
       "10  1.000  0.596  0.638  \n",
       "11  0.596  1.000  0.829  \n",
       "12  0.638  0.829  1.000  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "codebook = collect_words_eng1(sentence)\n",
    "distances = euclidean_distance(vectors)\n",
    "cosin = cos_sim(vectors)\n",
    "cosin = np.round(cosin,3)\n",
    "df = pd.DataFrame(cosin,list_label)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## それぞれのFiIeの関係性をコサイン類似度で確認する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1\n",
      "# cosine_similarity\n",
      "[1.    0.811]\n",
      "[0.811 1.   ]\n",
      "0 2\n",
      "# cosine_similarity\n",
      "[1.    0.813]\n",
      "[0.813 1.   ]\n",
      "0 3\n",
      "# cosine_similarity\n",
      "[1.    0.781]\n",
      "[0.781 1.   ]\n",
      "0 4\n",
      "# cosine_similarity\n",
      "[1.  0.8]\n",
      "[0.8 1. ]\n",
      "0 5\n",
      "# cosine_similarity\n",
      "[1.   0.82]\n",
      "[0.82 1.  ]\n",
      "0 6\n",
      "# cosine_similarity\n",
      "[1.    0.876]\n",
      "[0.876 1.   ]\n",
      "0 7\n",
      "# cosine_similarity\n",
      "[1.    0.828]\n",
      "[0.828 1.   ]\n",
      "0 8\n",
      "# cosine_similarity\n",
      "[1.    0.858]\n",
      "[0.858 1.   ]\n",
      "0 9\n",
      "# cosine_similarity\n",
      "[1.    0.793]\n",
      "[0.793 1.   ]\n",
      "0 10\n",
      "# cosine_similarity\n",
      "[1.    0.651]\n",
      "[0.651 1.   ]\n",
      "0 11\n",
      "# cosine_similarity\n",
      "[1.    0.871]\n",
      "[0.871 1.   ]\n",
      "0 12\n",
      "# cosine_similarity\n",
      "[1.    0.907]\n",
      "[0.907 1.   ]\n",
      "1 2\n",
      "# cosine_similarity\n",
      "[1.    0.987]\n",
      "[0.987 1.   ]\n",
      "1 3\n",
      "# cosine_similarity\n",
      "[1.    0.994]\n",
      "[0.994 1.   ]\n",
      "1 4\n",
      "# cosine_similarity\n",
      "[1.    0.993]\n",
      "[0.993 1.   ]\n",
      "1 5\n",
      "# cosine_similarity\n",
      "[1.    0.991]\n",
      "[0.991 1.   ]\n",
      "1 6\n",
      "# cosine_similarity\n",
      "[1.    0.963]\n",
      "[0.963 1.   ]\n",
      "1 7\n",
      "# cosine_similarity\n",
      "[1.    0.978]\n",
      "[0.978 1.   ]\n",
      "1 8\n",
      "# cosine_similarity\n",
      "[1.   0.98]\n",
      "[0.98 1.  ]\n",
      "1 9\n",
      "# cosine_similarity\n",
      "[1.    0.972]\n",
      "[0.972 1.   ]\n",
      "1 10\n",
      "# cosine_similarity\n",
      "[1.    0.519]\n",
      "[0.519 1.   ]\n",
      "1 11\n",
      "# cosine_similarity\n",
      "[1.    0.954]\n",
      "[0.954 1.   ]\n",
      "1 12\n",
      "# cosine_similarity\n",
      "[1.   0.72]\n",
      "[0.72 1.  ]\n",
      "2 3\n",
      "# cosine_similarity\n",
      "[1.    0.989]\n",
      "[0.989 1.   ]\n",
      "2 4\n",
      "# cosine_similarity\n",
      "[1.    0.987]\n",
      "[0.987 1.   ]\n",
      "2 5\n",
      "# cosine_similarity\n",
      "[1.    0.992]\n",
      "[0.992 1.   ]\n",
      "2 6\n",
      "# cosine_similarity\n",
      "[1.    0.954]\n",
      "[0.954 1.   ]\n",
      "2 7\n",
      "# cosine_similarity\n",
      "[1.   0.98]\n",
      "[0.98 1.  ]\n",
      "2 8\n",
      "# cosine_similarity\n",
      "[1.    0.966]\n",
      "[0.966 1.   ]\n",
      "2 9\n",
      "# cosine_similarity\n",
      "[1.    0.957]\n",
      "[0.957 1.   ]\n",
      "2 10\n",
      "# cosine_similarity\n",
      "[1.    0.554]\n",
      "[0.554 1.   ]\n",
      "2 11\n",
      "# cosine_similarity\n",
      "[1.    0.954]\n",
      "[0.954 1.   ]\n",
      "2 12\n",
      "# cosine_similarity\n",
      "[1.    0.715]\n",
      "[0.715 1.   ]\n",
      "3 4\n",
      "# cosine_similarity\n",
      "[1.    0.993]\n",
      "[0.993 1.   ]\n",
      "3 5\n",
      "# cosine_similarity\n",
      "[1.   0.99]\n",
      "[0.99 1.  ]\n",
      "3 6\n",
      "# cosine_similarity\n",
      "[1.    0.946]\n",
      "[0.946 1.   ]\n",
      "3 7\n",
      "# cosine_similarity\n",
      "[1.    0.974]\n",
      "[0.974 1.   ]\n",
      "3 8\n",
      "# cosine_similarity\n",
      "[1.    0.966]\n",
      "[0.966 1.   ]\n",
      "3 9\n",
      "# cosine_similarity\n",
      "[1.    0.966]\n",
      "[0.966 1.   ]\n",
      "3 10\n",
      "# cosine_similarity\n",
      "[1.    0.492]\n",
      "[0.492 1.   ]\n",
      "3 11\n",
      "# cosine_similarity\n",
      "[1.    0.944]\n",
      "[0.944 1.   ]\n",
      "3 12\n",
      "# cosine_similarity\n",
      "[1.    0.684]\n",
      "[0.684 1.   ]\n",
      "4 5\n",
      "# cosine_similarity\n",
      "[1.    0.991]\n",
      "[0.991 1.   ]\n",
      "4 6\n",
      "# cosine_similarity\n",
      "[1.    0.959]\n",
      "[0.959 1.   ]\n",
      "4 7\n",
      "# cosine_similarity\n",
      "[1.    0.985]\n",
      "[0.985 1.   ]\n",
      "4 8\n",
      "# cosine_similarity\n",
      "[1.    0.973]\n",
      "[0.973 1.   ]\n",
      "4 9\n",
      "# cosine_similarity\n",
      "[1.    0.962]\n",
      "[0.962 1.   ]\n",
      "4 10\n",
      "# cosine_similarity\n",
      "[1.    0.526]\n",
      "[0.526 1.   ]\n",
      "4 11\n",
      "# cosine_similarity\n",
      "[1.    0.958]\n",
      "[0.958 1.   ]\n",
      "4 12\n",
      "# cosine_similarity\n",
      "[1.    0.713]\n",
      "[0.713 1.   ]\n",
      "5 6\n",
      "# cosine_similarity\n",
      "[1.    0.965]\n",
      "[0.965 1.   ]\n",
      "5 7\n",
      "# cosine_similarity\n",
      "[1.    0.982]\n",
      "[0.982 1.   ]\n",
      "5 8\n",
      "# cosine_similarity\n",
      "[1.    0.975]\n",
      "[0.975 1.   ]\n",
      "5 9\n",
      "# cosine_similarity\n",
      "[1.    0.962]\n",
      "[0.962 1.   ]\n",
      "5 10\n",
      "# cosine_similarity\n",
      "[1.    0.538]\n",
      "[0.538 1.   ]\n",
      "5 11\n",
      "# cosine_similarity\n",
      "[1.    0.958]\n",
      "[0.958 1.   ]\n",
      "5 12\n",
      "# cosine_similarity\n",
      "[1.   0.72]\n",
      "[0.72 1.  ]\n",
      "6 7\n",
      "# cosine_similarity\n",
      "[1.    0.962]\n",
      "[0.962 1.   ]\n",
      "6 8\n",
      "# cosine_similarity\n",
      "[1.    0.979]\n",
      "[0.979 1.   ]\n",
      "6 9\n",
      "# cosine_similarity\n",
      "[1.   0.95]\n",
      "[0.95 1.  ]\n",
      "6 10\n",
      "# cosine_similarity\n",
      "[1.    0.603]\n",
      "[0.603 1.   ]\n",
      "6 11\n",
      "# cosine_similarity\n",
      "[1.    0.963]\n",
      "[0.963 1.   ]\n",
      "6 12\n",
      "# cosine_similarity\n",
      "[1.    0.814]\n",
      "[0.814 1.   ]\n",
      "7 8\n",
      "# cosine_similarity\n",
      "[1.    0.973]\n",
      "[0.973 1.   ]\n",
      "7 9\n",
      "# cosine_similarity\n",
      "[1.    0.956]\n",
      "[0.956 1.   ]\n",
      "7 10\n",
      "# cosine_similarity\n",
      "[1.    0.574]\n",
      "[0.574 1.   ]\n",
      "7 11\n",
      "# cosine_similarity\n",
      "[1.    0.964]\n",
      "[0.964 1.   ]\n",
      "7 12\n",
      "# cosine_similarity\n",
      "[1.    0.758]\n",
      "[0.758 1.   ]\n",
      "8 9\n",
      "# cosine_similarity\n",
      "[1.    0.974]\n",
      "[0.974 1.   ]\n",
      "8 10\n",
      "# cosine_similarity\n",
      "[1.    0.591]\n",
      "[0.591 1.   ]\n",
      "8 11\n",
      "# cosine_similarity\n",
      "[1.    0.956]\n",
      "[0.956 1.   ]\n",
      "8 12\n",
      "# cosine_similarity\n",
      "[1.    0.783]\n",
      "[0.783 1.   ]\n",
      "9 10\n",
      "# cosine_similarity\n",
      "[1.    0.516]\n",
      "[0.516 1.   ]\n",
      "9 11\n",
      "# cosine_similarity\n",
      "[1.    0.926]\n",
      "[0.926 1.   ]\n",
      "9 12\n",
      "# cosine_similarity\n",
      "[1.    0.728]\n",
      "[0.728 1.   ]\n",
      "10 11\n",
      "# cosine_similarity\n",
      "[1.    0.588]\n",
      "[0.588 1.   ]\n",
      "10 12\n",
      "# cosine_similarity\n",
      "[1.    0.576]\n",
      "[0.576 1.   ]\n",
      "11 12\n",
      "# cosine_similarity\n",
      "[1.    0.825]\n",
      "[0.825 1.   ]\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,len(sentence)):\n",
    "    for j in range(0,len(sentence)):\n",
    "        list=[]\n",
    "        if i < j:\n",
    "            print(i,j)\n",
    "            list.append(sentence[i])\n",
    "            list.append(sentence[j])\n",
    "            \n",
    "        else:\n",
    "            continue\n",
    "        codebook = collect_words_eng(list)\n",
    "        vectors = make_vectors_eng(list, codebook)\n",
    "        similarities = cos_sim(vectors)\n",
    "        print('# cosine_similarity')\n",
    "        for index in range(len(similarities)):\n",
    "            print(np.round(similarities[index],3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここで一度別の**前処理**として、文章ベクトルを**標準化**を行う"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文章ベクトルを標準化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn import preprocessing as pp\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = make_vectors_eng(sentence, codebook)\n",
    "ppSS = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_std = ppSS.fit_transform(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "10500\n"
     ]
    }
   ],
   "source": [
    "print(type(data_std))\n",
    "print(len(data_std[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vectors[0] = [-0.44622066  0.28867513 -1.52352009 ... -0.28867513 -0.28867513\n",
      " -0.28867513]\n",
      "-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "vectors[1] = [ 0.05354648  0.28867513  0.05836863 ... -0.28867513 -0.28867513\n",
      " -0.28867513]\n",
      "-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "vectors[2] = [ 0.51761596  0.28867513  0.1162426  ... -0.28867513 -0.28867513\n",
      " -0.28867513]\n",
      "-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "vectors[3] = [ 0.01784883  0.28867513  1.07437845 ... -0.28867513 -0.28867513\n",
      " -0.28867513]\n",
      "-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "vectors[4] = [ 3.30203287  0.28867513  1.24800038 ... -0.28867513 -0.28867513\n",
      " -0.28867513]\n",
      "-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "vectors[5] = [-0.41052301  0.28867513  0.38632117 ... -0.28867513 -0.28867513\n",
      " -0.28867513]\n",
      "-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "vectors[6] = [-0.35697653  0.28867513  1.15154376 ... -0.28867513 -0.28867513\n",
      " -0.28867513]\n",
      "-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "vectors[7] = [-0.35697653  0.28867513  0.22556012 ... -0.28867513 -0.28867513\n",
      " -0.28867513]\n",
      "-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "vectors[8] = [-0.23203474  0.28867513  0.56637354 ... -0.28867513 -0.28867513\n",
      " -0.28867513]\n",
      "-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "vectors[9] = [-0.51761596  0.28867513  0.48920824 ... -0.28867513 -0.28867513\n",
      " -0.28867513]\n",
      "-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "vectors[10] = [-0.6782554  -3.46410162 -1.86433351 ... -0.28867513 -0.28867513\n",
      " -0.28867513]\n",
      "-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "vectors[11] = [-0.3391277   0.28867513 -0.44963629 ... -0.28867513 -0.28867513\n",
      " -0.28867513]\n",
      "-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "vectors[12] = [-0.55331362  0.28867513 -1.478507   ...  3.46410162  3.46410162\n",
      "  3.46410162]\n",
      "-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for index in range(len(data_std)):\n",
    "    print('vectors[{}] = {}'.format(index,data_std[index]))\n",
    "    print('-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# cosine_similarity\n",
      "[ 1.    -0.082 -0.126 -0.175 -0.118 -0.109 -0.096 -0.024 -0.064 -0.03\n",
      "  0.189 -0.064  0.174]\n",
      "[-0.082  1.    -0.086 -0.084 -0.063 -0.076 -0.081 -0.105 -0.098 -0.117\n",
      " -0.113 -0.112 -0.106]\n",
      "[-0.126 -0.086  1.    -0.101 -0.1   -0.096 -0.12  -0.133 -0.127 -0.128\n",
      " -0.128 -0.104 -0.139]\n",
      "[-0.175 -0.084 -0.101  1.    -0.055 -0.1   -0.107 -0.12  -0.119 -0.136\n",
      " -0.21  -0.104 -0.199]\n",
      "[-0.118 -0.063 -0.1   -0.055  1.    -0.066 -0.065 -0.12  -0.088 -0.106\n",
      " -0.176 -0.079 -0.152]\n",
      "[-0.109 -0.076 -0.096 -0.1   -0.066  1.    -0.058 -0.06  -0.071 -0.091\n",
      " -0.104 -0.101 -0.105]\n",
      "[-0.096 -0.081 -0.12  -0.107 -0.065 -0.058  1.    -0.04  -0.068 -0.087\n",
      " -0.097 -0.094 -0.076]\n",
      "[-0.024 -0.105 -0.133 -0.12  -0.12  -0.06  -0.04   1.    -0.045 -0.047\n",
      "  0.041 -0.093  0.001]\n",
      "[-0.064 -0.098 -0.127 -0.119 -0.088 -0.071 -0.068 -0.045  1.     0.003\n",
      " -0.039 -0.097 -0.041]\n",
      "[-0.03  -0.117 -0.128 -0.136 -0.106 -0.091 -0.087 -0.047  0.003  1.\n",
      "  0.028 -0.096  0.001]\n",
      "[ 0.189 -0.113 -0.128 -0.21  -0.176 -0.104 -0.097  0.041 -0.039  0.028\n",
      "  1.    -0.098  0.286]\n",
      "[-0.064 -0.112 -0.104 -0.104 -0.079 -0.101 -0.094 -0.093 -0.097 -0.096\n",
      " -0.098  1.    -0.075]\n",
      "[ 0.174 -0.106 -0.139 -0.199 -0.152 -0.105 -0.076  0.001 -0.041  0.001\n",
      "  0.286 -0.075  1.   ]\n"
     ]
    }
   ],
   "source": [
    "hoge = cos_sim(data_std)\n",
    "print('# cosine_similarity')\n",
    "for index in range(len(hoge)):\n",
    "    print(np.round(hoge[index],3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 正規化してデータ分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.05829596 1.         0.10950413 ... 0.         0.         0.        ]\n",
      " [0.1838565  1.         0.6177686  ... 0.         0.         0.        ]\n",
      " [0.30044843 1.         0.63636364 ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.08520179 1.         0.45454545 ... 0.         0.         0.        ]\n",
      " [0.03139013 1.         0.12396694 ... 1.         1.         1.        ]]\n"
     ]
    }
   ],
   "source": [
    "ms = MinMaxScaler()\n",
    "mms = ms.fit_transform(vectors)\n",
    "print(mms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05829596412556054 13\n",
      "1.0 3\n",
      "0.10950413223140495 57\n",
      "0.5185185185185185 14\n",
      "0.08368200836820083 23\n",
      "0.8188976377952756 104\n",
      "0.14666666666666667 11\n",
      "0.3102678571428571 436\n",
      "1.0 2\n",
      "0.38461538461538464 5\n",
      "1.0 1\n",
      "1.0 5\n",
      "1.0 1\n",
      "1.0 4\n",
      "0.00404040404040404 4\n",
      "0.03333333333333333 4\n",
      "0.3073727933541017 300\n",
      "1.0 1\n",
      "0.0035335689045936395 1\n",
      "1.0 1\n",
      "1.0 1\n",
      "1.0 1\n",
      "0.15384615384615385 2\n",
      "1.0 1\n",
      "0.9150943396226415 97\n",
      "1.0 3\n",
      "1.0 2\n",
      "0.3023255813953488 40\n",
      "0.5555555555555556 5\n",
      "0.1886993603411514 189\n",
      "1.0 1\n",
      "0.4 2\n",
      "0.44646464646464645 222\n",
      "0.42857142857142855 3\n",
      "0.19298245614035087 11\n",
      "0.75 3\n",
      "1.0 2\n",
      "0.9411764705882353 16\n",
      "1.0 8\n",
      "0.07708333333333334 37\n",
      "0.09803921568627451 5\n",
      "1.0 3\n",
      "0.17391304347826086 8\n",
      "1.0 10\n",
      "1.0 6\n",
      "0.8163265306122448 40\n",
      "0.4 4\n",
      "0.13986013986013987 20\n",
      "0.9500000000000001 19\n",
      "0.2 2\n",
      "0.48484848484848486 16\n",
      "1.0 7\n",
      "0.4117647058823529 21\n",
      "1.0 7\n",
      "0.16483516483516486 121\n",
      "1.0 8\n",
      "0.31906614785992216 84\n",
      "0.3333333333333333 1\n",
      "0.6666666666666666 2\n",
      "1.0 2\n",
      "0.1937984496124031 26\n",
      "1.0 4\n",
      "1.0 21\n",
      "1.0 6\n",
      "1.0 1\n",
      "1.0 1\n",
      "1.0 11\n",
      "1.0 62\n",
      "1.0 26\n",
      "1.0 48\n",
      "0.6571428571428571 46\n",
      "1.0 1\n",
      "1.0 2\n",
      "0.25 2\n",
      "0.9696969696969697 32\n",
      "0.1111111111111111 1\n",
      "1.0 2\n",
      "1.0 1\n",
      "0.8333333333333333 5\n",
      "1.0 1\n",
      "1.0 1\n",
      "1.0 1\n",
      "0.022727272727272728 1\n",
      "0.3529411764705882 62\n",
      "1.0 1\n",
      "1.0 2\n",
      "0.8260869565217391 19\n",
      "0.037037037037037035 1\n",
      "0.5 1\n",
      "0.015723270440251572 10\n",
      "0.03414634146341464 7\n",
      "0.2651331719128329 219\n",
      "0.3039443155452436 133\n",
      "1.0 2\n",
      "1.0 1\n",
      "1.0 1\n",
      "1.0 2\n",
      "0.05208333333333333 5\n",
      "1.0 1\n",
      "0.1 2\n",
      "0.25 1\n",
      "1.0 3\n",
      "1.0 1\n",
      "1.0 1\n",
      "1.0 1\n",
      "0.3333333333333333 1\n",
      "1.0 1\n",
      "1.0 2\n",
      "0.04878048780487805 6\n",
      "0.1559633027522936 17\n",
      "1.0 3\n",
      "0.07692307692307693 2\n",
      "1.0 2\n",
      "0.08771929824561403 5\n",
      "1.0 1\n",
      "0.08045977011494253 14\n",
      "1.0 1\n",
      "0.48148148148148145 13\n",
      "0.23809523809523808 10\n",
      "1.0 2\n",
      "1.0 2\n",
      "0.16666666666666666 3\n",
      "1.0 1\n",
      "0.2272727272727273 5\n",
      "0.1724137931034483 21\n",
      "0.07142857142857142 2\n",
      "1.0 1\n",
      "0.13333333333333333 2\n",
      "0.23076923076923078 3\n",
      "0.3333333333333333 4\n",
      "1.0 1\n",
      "1.0 12\n",
      "0.5736040609137055 229\n",
      "1.0 1\n",
      "1.0 2\n",
      "1.0 1\n",
      "0.13333333333333333 2\n",
      "0.15942028985507248 11\n",
      "1.0 2\n",
      "0.36363636363636365 4\n",
      "1.0 2\n",
      "1.0 2\n",
      "0.10344827586206898 26\n",
      "0.06451612903225806 2\n",
      "1.0 2\n",
      "0.01098901098901099 1\n",
      "0.11194029850746269 31\n",
      "0.05405405405405406 2\n",
      "0.3333333333333333 1\n",
      "0.25 1\n",
      "1.0 8\n",
      "0.2542372881355932 15\n",
      "0.18705035971223022 26\n",
      "1.0 1\n",
      "0.36538461538461536 59\n",
      "1.0 1\n",
      "1.0 1\n",
      "1.0 1\n",
      "1.0 1\n",
      "1.0 4\n",
      "0.9090909090909092 10\n",
      "0.34782608695652173 8\n",
      "1.0 3\n",
      "1.0 3\n",
      "0.6296296296296295 18\n",
      "1.0 2\n",
      "0.2727272727272727 3\n",
      "1.0 3\n",
      "0.9166666666666666 11\n",
      "1.0 2\n",
      "1.0 3\n",
      "0.2631578947368421 5\n",
      "1.0 1\n",
      "1.0 1\n",
      "1.0 1\n",
      "0.07894736842105263 3\n",
      "0.17391304347826086 4\n",
      "0.34375 11\n",
      "0.12903225806451613 8\n",
      "1.0 3\n",
      "0.01818181818181818 1\n",
      "0.8333333333333333 10\n",
      "0.04433497536945813 9\n",
      "1.0 1\n",
      "1.0 3\n",
      "1.0 1\n",
      "0.030303030303030304 1\n",
      "0.020833333333333332 1\n",
      "0.3246753246753247 26\n",
      "1.0 2\n",
      "0.09523809523809523 2\n",
      "1.0 1\n",
      "1.0 1\n",
      "1.0 1\n",
      "1.0 2\n",
      "0.5555555555555556 5\n",
      "0.9999999999999999 7\n",
      "1.0 5\n",
      "1.0 2\n",
      "1.0 2\n",
      "0.5 2\n",
      "0.8 4\n",
      "0.42424242424242425 14\n",
      "0.3181818181818182 7\n",
      "0.3333333333333333 1\n",
      "0.25 92\n",
      "0.3333333333333333 1\n",
      "0.3333333333333333 2\n",
      "0.08235294117647059 7\n",
      "0.2975206611570248 38\n",
      "0.8333333333333333 5\n",
      "0.6666666666666666 2\n",
      "0.5 2\n",
      "0.5 25\n",
      "0.24418604651162792 22\n",
      "0.39814814814814814 48\n",
      "0.15422885572139303 31\n",
      "0.14285714285714285 2\n",
      "0.424 56\n",
      "0.3333333333333333 3\n",
      "0.5 1\n",
      "0.375 3\n",
      "0.14590747330960854 41\n",
      "0.023255813953488372 1\n",
      "1.0 1\n",
      "0.08333333333333333 1\n",
      "0.08450704225352113 6\n",
      "0.125 3\n",
      "0.16666666666666666 1\n",
      "0.5 1\n",
      "0.0625 1\n",
      "0.16129032258064516 5\n",
      "0.23357664233576642 33\n",
      "0.07142857142857142 1\n",
      "0.1 1\n",
      "0.125 1\n",
      "1.0 10\n",
      "0.3333333333333333 3\n",
      "0.46153846153846156 6\n",
      "0.4 2\n",
      "0.02702702702702703 1\n",
      "0.1111111111111111 2\n",
      "1.0 1\n",
      "0.36363636363636365 4\n",
      "0.6944444444444444 25\n",
      "0.25 1\n",
      "0.90625 29\n",
      "1.0 11\n",
      "0.6666666666666666 4\n",
      "1.0 3\n",
      "0.43478260869565216 30\n",
      "0.4 2\n",
      "1.0 1\n",
      "0.6000000000000001 6\n",
      "0.8888888888888888 8\n",
      "0.16666666666666666 4\n",
      "0.05660377358490566 3\n",
      "0.8 4\n",
      "0.2 4\n",
      "1.0 4\n",
      "0.0132013201320132 5\n",
      "1.0 1\n",
      "0.2 1\n",
      "1.0 1\n",
      "1.0 1\n",
      "0.25 1\n",
      "1.0 1\n",
      "1.0 5\n",
      "0.4074074074074074 11\n",
      "0.1764705882352941 3\n",
      "0.42857142857142855 3\n",
      "1.0 1\n",
      "0.265625 17\n",
      "0.6000000000000001 3\n",
      "1.0 10\n",
      "0.09090909090909091 1\n",
      "0.7857142857142857 22\n",
      "1.0 1\n",
      "0.2641509433962264 14\n",
      "1.0 1\n",
      "0.16666666666666666 1\n",
      "1.0 1\n",
      "1.0 1\n",
      "1.0 1\n",
      "1.0 1\n",
      "0.5 2\n",
      "1.0 2\n",
      "0.037037037037037035 1\n",
      "1.0 1\n",
      "0.25 1\n",
      "1.0 1\n",
      "1.0 1\n",
      "0.7368421052631579 14\n",
      "0.7142857142857142 5\n",
      "1.0 24\n",
      "0.75 48\n",
      "1.0 13\n",
      "0.37037037037037035 10\n",
      "0.23076923076923078 3\n",
      "0.5 1\n",
      "1.0 3\n",
      "0.1111111111111111 2\n",
      "1.0 2\n",
      "0.3020833333333333 29\n",
      "1.0 4\n",
      "0.9333333333333333 14\n",
      "0.0045871559633027525 1\n",
      "0.09090909090909091 5\n",
      "0.5208333333333333 25\n",
      "1.0 2\n",
      "0.16666666666666666 2\n",
      "0.23529411764705882 4\n",
      "1.0 17\n",
      "1.0 12\n",
      "1.0 1\n",
      "0.058823529411764705 1\n",
      "0.5 1\n",
      "1.0 1\n",
      "1.0 2\n",
      "1.0 7\n",
      "0.3333333333333333 1\n",
      "1.0 1\n",
      "0.3783783783783784 14\n",
      "1.0 1\n",
      "0.6000000000000001 3\n",
      "0.5 2\n",
      "0.8 4\n",
      "0.06818181818181818 3\n",
      "1.0 4\n",
      "1.0 1\n",
      "1.0 5\n",
      "0.13636363636363635 3\n",
      "0.6666666666666666 2\n",
      "0.15555555555555556 8\n",
      "1.0 3\n",
      "1.0 1\n",
      "1.0 1\n",
      "1.0 2\n",
      "0.6666666666666666 2\n",
      "0.07594936708860758 13\n",
      "0.07692307692307693 3\n",
      "0.25 1\n",
      "0.06060606060606061 2\n",
      "1.0 1\n",
      "0.5 2\n",
      "0.016873889875666074 20\n",
      "0.2 6\n",
      "0.5 1\n",
      "1.0 4\n",
      "0.04545454545454545 3\n",
      "0.008771929824561403 1\n",
      "1.0 1\n",
      "1.0 1\n",
      "1.0 1\n",
      "1.0 1\n",
      "0.9375 30\n",
      "1.0 18\n",
      "0.03125 2\n",
      "1.0 3\n",
      "1.0 1\n",
      "0.3333333333333333 1\n",
      "0.037037037037037035 1\n",
      "1.0 2\n",
      "0.6666666666666666 2\n",
      "1.0 1\n",
      "1.0 12\n",
      "1.0 1\n",
      "1.0 1\n",
      "1.0 1\n",
      "1.0 1\n",
      "1.0 1\n",
      "1.0 1\n",
      "0.2 1\n",
      "0.3333333333333333 1\n",
      "1.0 3\n",
      "1.0 3\n",
      "1.0 1\n",
      "0.16129032258064516 5\n",
      "0.4444444444444444 4\n",
      "1.0 3\n",
      "1.0 1\n",
      "0.01818181818181818 2\n",
      "0.4 4\n",
      "0.05970149253731343 4\n",
      "0.4 2\n",
      "1.0 1\n",
      "1.0 1\n",
      "1.0 1\n",
      "0.2 1\n",
      "1.0 1\n",
      "1.0 1\n",
      "0.030303030303030304 1\n",
      "0.05555555555555555 1\n",
      "0.0273972602739726 2\n",
      "0.023255813953488372 1\n",
      "0.004975124378109453 1\n",
      "0.46627565982404695 164\n",
      "0.01818181818181818 1\n",
      "0.3333333333333333 1\n",
      "0.3333333333333333 2\n",
      "0.3333333333333333 1\n",
      "1.0 1\n",
      "0.027777777777777776 1\n",
      "0.25296442687747034 67\n",
      "0.9318181818181819 42\n",
      "0.12195121951219515 6\n",
      "1.0 20\n",
      "0.8656716417910448 62\n",
      "0.8275862068965517 26\n",
      "0.11764705882352941 32\n",
      "0.16666666666666666 2\n",
      "1.0 1\n",
      "1.0 1\n",
      "0.5555555555555556 5\n",
      "0.03333333333333333 2\n",
      "0.03571428571428571 1\n",
      "0.5 1\n",
      "0.2588235294117647 22\n",
      "0.5 1\n",
      "0.14285714285714285 1\n",
      "0.5 3\n",
      "0.22222222222222224 10\n",
      "0.40384615384615385 21\n",
      "1.0 1\n",
      "0.125 1\n",
      "0.2153846153846154 14\n",
      "1.0 2\n",
      "0.2 1\n",
      "0.22115384615384617 24\n",
      "1.0 2\n",
      "0.2 1\n",
      "1.0 1\n",
      "0.14285714285714285 2\n",
      "0.2 1\n",
      "0.037037037037037035 1\n",
      "0.03225806451612903 1\n",
      "0.44 11\n",
      "1.0 27\n",
      "0.3333333333333333 2\n",
      "0.6000000000000001 3\n",
      "0.058823529411764705 1\n",
      "0.6666666666666666 8\n",
      "0.0392156862745098 4\n",
      "0.16666666666666666 2\n",
      "0.26595744680851063 130\n",
      "0.8888888888888888 8\n",
      "1.0 1\n",
      "0.4716981132075472 26\n",
      "0.13043478260869565 6\n",
      "1.0 2\n",
      "0.15789473684210525 3\n",
      "0.21052631578947367 4\n",
      "0.1111111111111111 1\n",
      "0.03114186851211073 10\n",
      "0.04081632653061224 2\n",
      "0.16666666666666666 1\n",
      "0.12 3\n",
      "0.6666666666666666 4\n",
      "0.225 9\n",
      "0.3333333333333333 1\n",
      "0.13333333333333333 2\n",
      "0.125 1\n",
      "0.5 1\n",
      "0.3333333333333333 1\n",
      "0.5 1\n",
      "0.5 4\n",
      "0.375 3\n",
      "0.07407407407407407 2\n",
      "0.29411764705882354 5\n",
      "0.3333333333333333 1\n",
      "0.2272727272727273 5\n",
      "1.0 7\n",
      "0.19047619047619047 5\n",
      "0.2277227722772277 24\n",
      "1.0 1\n",
      "1.0 1\n",
      "0.5 1\n",
      "0.2 1\n",
      "1.0 1\n",
      "1.0 1\n",
      "1.0 1\n",
      "0.09090909090909091 1\n",
      "0.15789473684210525 3\n",
      "0.018867924528301886 1\n",
      "0.09090909090909091 1\n",
      "0.10526315789473684 2\n",
      "0.21212121212121213 8\n",
      "1.0 1\n",
      "0.08571428571428572 3\n",
      "0.5 1\n",
      "0.23529411764705882 4\n",
      "0.1 1\n",
      "0.25 1\n",
      "0.10909090909090907 7\n",
      "1.0 1\n",
      "0.42857142857142855 3\n",
      "0.25 1\n",
      "0.02857142857142857 1\n",
      "0.2 1\n",
      "0.8571428571428571 6\n",
      "1.0 1\n",
      "0.16666666666666666 1\n",
      "0.5 1\n",
      "1.0 1\n",
      "0.1111111111111111 1\n",
      "1.0 2\n",
      "0.3333333333333333 1\n",
      "0.2222222222222222 2\n",
      "0.5 4\n",
      "0.15384615384615385 2\n",
      "0.5 1\n",
      "1.0 1\n",
      "0.5 1\n",
      "0.5 5\n",
      "0.5 3\n",
      "0.5 1\n",
      "1.0 5\n",
      "0.05454545454545454 3\n",
      "0.136 17\n",
      "0.07142857142857142 1\n",
      "0.16666666666666666 1\n",
      "1.0 2\n",
      "1.0 12\n",
      "1.0 8\n",
      "1.0 11\n",
      "1.0 1\n",
      "0.5 2\n",
      "1.0 2\n",
      "0.047619047619047616 9\n",
      "1.0 1\n",
      "1.0 7\n",
      "0.08333333333333333 1\n",
      "1.0 3\n",
      "0.5 1\n",
      "1.0 2\n",
      "1.0 4\n",
      "1.0 12\n",
      "0.2222222222222222 2\n",
      "0.2876712328767123 21\n",
      "0.09090909090909091 3\n",
      "0.4117647058823529 7\n",
      "0.2727272727272727 6\n",
      "1.0 10\n",
      "0.15254237288135594 9\n",
      "0.6363636363636364 7\n",
      "1.0 4\n",
      "0.6470588235294118 11\n",
      "0.4545454545454546 5\n",
      "0.15384615384615385 8\n",
      "0.6666666666666666 2\n",
      "0.3333333333333333 1\n",
      "0.5357142857142857 16\n",
      "0.5357142857142857 16\n",
      "0.6666666666666666 6\n",
      "0.07692307692307693 3\n",
      "0.125 1\n",
      "1.0 1\n",
      "1.0 1\n",
      "1.0 2\n",
      "1.0 1\n",
      "1.0 1\n",
      "0.18571428571428572 26\n",
      "0.6666666666666666 2\n",
      "0.08333333333333333 2\n",
      "0.10714285714285714 3\n",
      "0.5 2\n",
      "0.16666666666666666 6\n",
      "1.0 3\n",
      "1.0 3\n",
      "1.0 1\n",
      "1.0 1\n",
      "0.3333333333333333 1\n",
      "0.4 2\n",
      "1.0 1\n",
      "1.0 2\n",
      "1.0 1\n",
      "1.0 4\n",
      "0.3333333333333333 10\n",
      "1.0 1\n",
      "0.125 1\n",
      "0.25 4\n",
      "0.7000000000000001 7\n",
      "0.5 2\n",
      "0.6666666666666666 6\n",
      "1.0 8\n",
      "1.0 2\n",
      "0.25 1\n",
      "0.25 1\n",
      "0.4 4\n",
      "1.0 2\n",
      "0.5 3\n",
      "1.0 4\n",
      "0.75 3\n",
      "0.5 4\n",
      "1.0 9\n",
      "0.3913043478260869 9\n",
      "0.08333333333333333 1\n",
      "0.09999999999999999 7\n",
      "1.0 1\n",
      "0.3333333333333333 1\n",
      "1.0 1\n",
      "0.2727272727272727 3\n",
      "0.05714285714285714 2\n",
      "1.0 8\n",
      "0.22448979591836732 11\n",
      "0.13333333333333333 4\n",
      "0.25609756097560976 21\n",
      "1.0 6\n",
      "0.5 1\n",
      "0.25 2\n",
      "0.42857142857142855 3\n",
      "0.0625 4\n",
      "0.6666666666666666 2\n",
      "1.0 26\n",
      "0.25 1\n",
      "0.625 5\n",
      "0.08695652173913043 4\n",
      "0.6000000000000001 3\n",
      "0.21875 7\n",
      "0.045454545454545456 1\n",
      "1.0 1\n",
      "0.0641025641025641 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 1\n",
      "1.0 1\n",
      "0.38461538461538464 5\n",
      "0.3333333333333333 2\n",
      "1.0 4\n",
      "0.2 1\n",
      "0.2 1\n",
      "0.07142857142857142 2\n",
      "0.3333333333333333 3\n",
      "0.15555555555555556 7\n",
      "0.1111111111111111 4\n",
      "1.0 1\n",
      "0.16666666666666666 1\n",
      "1.0 2\n",
      "0.25 2\n",
      "0.11235955056179775 10\n",
      "0.19047619047619047 4\n",
      "1.0 1\n",
      "0.1875 3\n",
      "0.3333333333333333 1\n",
      "0.3333333333333333 1\n",
      "1.0 1\n",
      "1.0 2\n",
      "1.0 1\n",
      "0.625 5\n",
      "0.20833333333333331 5\n",
      "0.5 1\n",
      "0.3333333333333333 2\n",
      "0.3333333333333333 2\n",
      "1.0 1\n",
      "1.0 1\n",
      "1.0 1\n",
      "0.15555555555555556 7\n",
      "0.8 4\n",
      "0.9999999999999999 13\n",
      "0.14285714285714285 2\n",
      "0.5 1\n",
      "0.5 1\n",
      "0.23255813953488372 10\n",
      "1.0 1\n",
      "0.5 1\n",
      "1.0 1\n",
      "0.042105263157894736 4\n",
      "0.043478260869565216 1\n",
      "1.0 7\n",
      "0.3333333333333333 1\n",
      "1.0 2\n",
      "1.0 2\n",
      "0.5 1\n",
      "0.5555555555555556 5\n",
      "0.3333333333333333 1\n",
      "0.1764705882352941 3\n",
      "0.16666666666666666 1\n",
      "0.3 9\n",
      "1.0 1\n",
      "1.0 3\n",
      "1.0 1\n",
      "0.07692307692307693 1\n",
      "1.0 1\n",
      "0.5 1\n",
      "0.6666666666666666 2\n",
      "0.42105263157894735 17\n",
      "1.0 3\n",
      "1.0 1\n",
      "1.0 1\n",
      "0.2 1\n",
      "0.11764705882352941 2\n",
      "0.16666666666666666 1\n",
      "0.5 1\n",
      "1.0 1\n",
      "0.42857142857142855 3\n",
      "0.6666666666666666 2\n",
      "0.14285714285714285 1\n",
      "1.0 2\n",
      "1.0 1\n",
      "1.0 1\n",
      "0.16666666666666666 1\n",
      "1.0 1\n",
      "0.22580645161290322 14\n",
      "0.0625 2\n",
      "0.21739130434782608 6\n",
      "0.1111111111111111 1\n",
      "1.0 1\n",
      "1.0 3\n",
      "1.0 1\n",
      "1.0 4\n",
      "0.5 2\n",
      "1.0 3\n",
      "0.3333333333333333 1\n",
      "1.0 1\n",
      "0.5 5\n",
      "1.0 5\n",
      "0.058823529411764705 2\n",
      "0.5 1\n",
      "1.0 2\n",
      "0.5 1\n",
      "0.5 8\n",
      "0.5 1\n",
      "0.25 1\n",
      "0.2 1\n",
      "1.0 2\n",
      "1.0 1\n",
      "0.10526315789473684 4\n",
      "1.0 1\n",
      "0.12 3\n",
      "1.0 2\n",
      "0.1764705882352941 9\n",
      "0.3333333333333333 1\n",
      "1.0 1\n",
      "0.09278350515463916 10\n",
      "0.058823529411764705 3\n",
      "0.05263157894736842 1\n",
      "0.06666666666666667 2\n",
      "0.1111111111111111 2\n",
      "1.0 2\n",
      "1.0 2\n",
      "0.5 2\n",
      "0.14285714285714285 1\n",
      "0.014285714285714285 2\n",
      "0.6666666666666666 2\n",
      "1.0 1\n",
      "0.2 2\n",
      "0.5714285714285714 4\n",
      "1.0 3\n",
      "1.0 4\n",
      "1.0 2\n",
      "0.1875 3\n",
      "0.5 1\n",
      "0.02459016393442623 3\n",
      "0.25 2\n",
      "1.0 2\n",
      "0.07692307692307693 1\n",
      "0.25 1\n",
      "0.6666666666666666 2\n",
      "0.5714285714285714 4\n",
      "1.0 2\n",
      "0.06451612903225806 2\n",
      "0.09090909090909091 1\n",
      "0.2777777777777778 10\n",
      "1.0 1\n",
      "0.08333333333333333 1\n",
      "0.3333333333333333 1\n",
      "0.21739130434782608 5\n",
      "1.0 1\n",
      "0.25 1\n",
      "0.42857142857142855 3\n",
      "0.2727272727272727 3\n",
      "0.8333333333333333 5\n",
      "0.09090909090909091 1\n",
      "0.5 1\n",
      "0.15841584158415842 20\n",
      "0.25 3\n",
      "0.2 2\n",
      "1.0 1\n",
      "0.2 2\n",
      "1.0 1\n",
      "0.2857142857142857 2\n",
      "0.1 1\n",
      "0.3333333333333333 3\n",
      "1.0 1\n",
      "0.25 1\n",
      "0.5 1\n",
      "0.3333333333333333 2\n",
      "0.5 1\n",
      "0.5 2\n",
      "0.0819672131147541 5\n",
      "0.08 4\n",
      "0.0625 1\n",
      "0.1 4\n",
      "0.10526315789473684 2\n",
      "0.05 1\n",
      "0.047619047619047616 2\n",
      "0.016260162601626018 4\n",
      "0.125 2\n",
      "0.25 2\n",
      "0.0625 1\n",
      "0.16666666666666666 2\n",
      "0.14285714285714285 3\n",
      "0.1176470588235294 3\n",
      "0.16666666666666666 1\n",
      "1.0 1\n",
      "1.0 1\n",
      "0.5 1\n",
      "0.2727272727272727 4\n",
      "1.0 1\n",
      "0.5 1\n",
      "0.5 1\n",
      "1.0 1\n",
      "0.18181818181818182 2\n",
      "1.0 1\n",
      "0.08695652173913043 6\n",
      "1.0 1\n",
      "1.0 4\n",
      "0.3333333333333333 2\n",
      "0.3333333333333333 4\n",
      "0.2 1\n",
      "0.2 1\n",
      "0.2 1\n",
      "0.23809523809523808 5\n",
      "0.125 1\n",
      "0.09090909090909091 2\n",
      "1.0 1\n",
      "0.3333333333333333 2\n",
      "0.3333333333333333 2\n",
      "0.034482758620689655 1\n",
      "0.20689655172413793 6\n",
      "1.0 1\n",
      "0.021052631578947368 2\n",
      "1.0 1\n",
      "0.029411764705882353 2\n",
      "0.5 1\n",
      "0.5 1\n",
      "1.0 1\n",
      "0.46153846153846156 6\n",
      "0.25925925925925924 8\n",
      "0.07142857142857142 1\n",
      "1.0 1\n",
      "0.14285714285714285 3\n",
      "1.0 1\n",
      "1.0 2\n",
      "1.0 1\n",
      "0.14285714285714285 1\n",
      "0.10344827586206896 3\n",
      "0.058823529411764705 1\n",
      "0.2857142857142857 2\n",
      "0.75 3\n",
      "0.3333333333333333 1\n",
      "1.0 1\n",
      "1.0 3\n",
      "0.09090909090909091 2\n",
      "1.0 1\n",
      "0.05 1\n",
      "0.25 1\n",
      "0.14285714285714285 1\n",
      "1.0 3\n",
      "0.125 1\n",
      "0.1 1\n",
      "0.6000000000000001 3\n",
      "1.0 1\n",
      "0.25 2\n",
      "0.2 1\n",
      "0.14285714285714285 1\n",
      "0.4 2\n",
      "0.13333333333333333 4\n",
      "0.5 1\n",
      "0.06666666666666667 1\n",
      "0.125 2\n",
      "1.0 3\n",
      "0.2 1\n",
      "0.26666666666666666 5\n",
      "0.07999999999999999 3\n",
      "0.5 2\n",
      "0.2 1\n",
      "0.5 1\n",
      "0.8 4\n",
      "1.0 1\n",
      "0.21428571428571427 3\n",
      "1.0 1\n",
      "0.6666666666666666 2\n",
      "0.10526315789473684 2\n",
      "0.04597701149425287 4\n",
      "0.028985507246376812 2\n",
      "0.04918032786885246 3\n",
      "0.3333333333333333 2\n",
      "0.46153846153846156 6\n",
      "0.0030959752321981426 2\n",
      "0.5 1\n",
      "0.02962962962962963 4\n",
      "0.0967741935483871 3\n",
      "0.2 1\n",
      "0.09090909090909091 2\n",
      "0.007905138339920948 2\n",
      "0.3333333333333333 1\n",
      "0.14285714285714285 1\n",
      "0.16666666666666666 8\n",
      "0.1111111111111111 1\n",
      "0.05 1\n",
      "1.0 1\n",
      "0.1111111111111111 1\n",
      "0.6000000000000001 3\n",
      "0.125 2\n",
      "0.1111111111111111 1\n",
      "0.5 1\n",
      "0.25 1\n",
      "0.13333333333333336 3\n",
      "0.6666666666666666 2\n",
      "0.08163265306122448 4\n",
      "0.03773584905660377 2\n",
      "1.0 2\n",
      "0.07142857142857142 1\n",
      "0.5 2\n",
      "0.03225806451612903 1\n",
      "0.025 1\n",
      "0.008064516129032258 1\n",
      "0.0625 1\n",
      "0.1111111111111111 1\n",
      "0.2 1\n",
      "0.25 1\n",
      "0.0125 1\n",
      "0.05555555555555555 1\n",
      "0.4 2\n",
      "0.06451612903225806 2\n",
      "0.5 1\n",
      "0.14285714285714285 2\n",
      "0.058823529411764705 1\n",
      "0.017857142857142856 1\n",
      "0.14285714285714285 1\n",
      "0.16666666666666666 1\n",
      "0.5 1\n",
      "0.14285714285714285 1\n",
      "0.1111111111111111 4\n",
      "1.0 1\n",
      "0.16666666666666666 2\n",
      "0.06666666666666667 1\n",
      "0.25 1\n",
      "0.07692307692307693 1\n",
      "0.2631578947368421 5\n",
      "0.23076923076923078 3\n",
      "0.25 1\n",
      "0.3333333333333333 1\n",
      "0.11764705882352941 2\n",
      "1.0 1\n",
      "0.3333333333333333 1\n",
      "1.0 3\n",
      "0.5 1\n",
      "0.09090909090909091 1\n",
      "1.0 1\n",
      "0.25 1\n",
      "1.0 2\n",
      "0.6000000000000001 3\n",
      "1.0 1\n",
      "0.3076923076923077 4\n",
      "1.0 1\n",
      "0.6000000000000001 3\n",
      "0.07692307692307693 1\n",
      "0.2 1\n",
      "0.16666666666666666 1\n",
      "0.3333333333333333 2\n",
      "1.0 1\n",
      "1.0 1\n",
      "0.16666666666666666 3\n",
      "0.6000000000000001 3\n",
      "0.2 1\n",
      "1.0 1\n",
      "1.0 1\n",
      "0.7000000000000001 7\n",
      "1.0 1\n",
      "0.2 1\n",
      "0.2 1\n",
      "1.0 1\n",
      "1.0 1\n",
      "0.2 2\n",
      "1.0 1\n",
      "0.045454545454545456 1\n",
      "1.0 2\n",
      "0.2 1\n",
      "1.0 1\n",
      "0.3333333333333333 1\n",
      "0.03571428571428571 1\n",
      "0.2222222222222222 2\n",
      "0.3333333333333333 1\n",
      "0.3333333333333333 1\n",
      "0.05555555555555555 3\n",
      "0.05660377358490566 3\n",
      "0.05405405405405406 2\n",
      "0.16666666666666666 1\n",
      "0.16666666666666666 1\n",
      "0.2222222222222222 2\n",
      "0.05 2\n",
      "0.5 2\n",
      "1.0 3\n",
      "0.3333333333333333 1\n",
      "0.07692307692307693 1\n",
      "0.09803921568627451 5\n",
      "0.25 1\n",
      "0.021739130434782608 1\n",
      "1.0 3\n",
      "0.3333333333333333 2\n",
      "0.125 2\n",
      "1.0 5\n",
      "0.8333333333333333 5\n",
      "0.07142857142857142 2\n",
      "0.32 8\n",
      "1.0 2\n",
      "0.16666666666666666 2\n",
      "0.125 1\n",
      "0.125 1\n",
      "0.08333333333333333 1\n",
      "0.25 1\n",
      "0.5 1\n",
      "0.18181818181818182 2\n",
      "0.0625 2\n",
      "0.5 1\n",
      "0.6666666666666666 2\n",
      "0.11764705882352941 2\n",
      "1.0 1\n",
      "0.06896551724137931 2\n",
      "0.5 1\n",
      "1.0 2\n",
      "0.0625 1\n",
      "0.07142857142857142 1\n",
      "1.0 1\n",
      "1.0 1\n",
      "0.125 1\n",
      "1.0 1\n",
      "0.07142857142857142 1\n",
      "1.0 1\n",
      "0.16666666666666666 1\n",
      "1.0 1\n",
      "1.0 1\n",
      "0.25 2\n",
      "1.0 1\n",
      "0.00510204081632653 1\n",
      "0.02 2\n",
      "0.005988023952095809 1\n",
      "0.2 1\n",
      "1.0 1\n",
      "0.25 1\n",
      "0.30000000000000004 3\n",
      "1.0 1\n",
      "1.0 1\n",
      "1.0 1\n",
      "0.25 2\n",
      "0.25 1\n",
      "1.0 1\n",
      "1.0 2\n",
      "1.0 3\n",
      "0.30000000000000004 3\n",
      "0.1111111111111111 1\n",
      "0.125 1\n",
      "1.0 1\n",
      "0.16666666666666666 2\n",
      "0.06666666666666668 3\n",
      "0.09090909090909091 1\n",
      "0.5 1\n",
      "1.0 1\n",
      "1.0 1\n",
      "0.5 1\n",
      "0.08 2\n",
      "0.5 1\n",
      "0.5 1\n",
      "0.125 1\n",
      "1.0 4\n",
      "1.0 3\n",
      "1.0 1\n",
      "0.05263157894736842 1\n",
      "0.5 1\n",
      "1.0 1\n",
      "0.125 1\n",
      "0.0975609756097561 4\n",
      "0.25 1\n",
      "0.08333333333333333 1\n",
      "1.0 1\n",
      "0.3333333333333333 1\n",
      "0.14285714285714285 1\n",
      "0.06666666666666667 1\n",
      "0.0625 1\n",
      "0.045454545454545456 1\n",
      "0.125 2\n",
      "1.0 4\n",
      "0.2 4\n",
      "0.16666666666666666 1\n",
      "0.5 1\n",
      "0.2 1\n",
      "1.0 1\n",
      "0.1111111111111111 1\n",
      "0.3333333333333333 2\n",
      "0.2 1\n",
      "0.75 3\n",
      "0.16666666666666666 1\n",
      "0.16666666666666666 1\n",
      "0.3333333333333333 1\n",
      "0.4 2\n",
      "1.0 1\n",
      "1.0 1\n",
      "1.0 1\n",
      "1.0 1\n",
      "1.0 1\n",
      "1.0 1\n",
      "0.3333333333333333 2\n",
      "0.6666666666666666 2\n",
      "0.34782608695652173 8\n",
      "1.0 2\n",
      "0.14285714285714285 1\n",
      "0.3333333333333333 1\n",
      "1.0 2\n",
      "1.0 12\n",
      "1.0 4\n",
      "0.1875 3\n",
      "0.04838709677419355 3\n",
      "0.6666666666666666 2\n",
      "0.5 1\n",
      "1.0 3\n",
      "0.3333333333333333 1\n",
      "0.3333333333333333 1\n",
      "0.5 1\n",
      "0.4 2\n",
      "0.2 2\n",
      "1.0 2\n",
      "0.4 2\n",
      "1.0 1\n",
      "0.015151515151515152 1\n",
      "0.2 1\n",
      "0.07692307692307693 1\n",
      "1.0 2\n",
      "0.0625 1\n",
      "0.15384615384615385 2\n",
      "1.0 1\n",
      "1.0 1\n",
      "0.5 1\n",
      "1.0 1\n",
      "0.004405286343612335 1\n",
      "0.06060606060606061 2\n",
      "0.038461538461538464 1\n",
      "0.06666666666666667 1\n",
      "1.0 1\n",
      "1.0 1\n",
      "0.08333333333333333 2\n",
      "0.5 1\n",
      "1.0 1\n",
      "0.013513513513513514 1\n",
      "0.009708737864077669 1\n",
      "0.043478260869565216 1\n",
      "0.01818181818181818 1\n",
      "0.03125 1\n",
      "0.03225806451612903 1\n",
      "1.0 1\n",
      "0.16666666666666666 1\n",
      "0.03125 1\n",
      "0.012738853503184714 2\n",
      "0.013888888888888888 1\n",
      "0.07407407407407407 2\n",
      "1.0 1\n",
      "0.05 1\n",
      "0.25 1\n",
      "0.03571428571428571 1\n",
      "0.2 1\n",
      "0.25 1\n",
      "0.16666666666666666 1\n",
      "0.3333333333333333 1\n",
      "0.14285714285714285 1\n",
      "0.045454545454545456 1\n",
      "0.2857142857142857 2\n",
      "0.08333333333333333 1\n",
      "0.09090909090909091 1\n",
      "0.05 1\n",
      "1.0 1\n",
      "0.5 1\n",
      "1.0 1\n",
      "0.25 1\n",
      "0.14285714285714285 1\n",
      "1.0 1\n",
      "0.03225806451612903 1\n",
      "1.0 1\n",
      "0.5 1\n",
      "1.0 2\n",
      "0.047619047619047616 2\n",
      "0.3333333333333333 1\n",
      "0.6666666666666666 2\n",
      "0.09090909090909091 1\n",
      "0.5 1\n",
      "0.2 1\n",
      "1.0 1\n",
      "1.0 1\n",
      "0.4444444444444444 4\n",
      "0.41666666666666663 5\n",
      "0.3333333333333333 1\n",
      "1.0 1\n",
      "1.0 1\n",
      "0.5 1\n",
      "1.0 1\n",
      "0.25 2\n",
      "0.6666666666666666 2\n",
      "0.14285714285714285 2\n",
      "0.2 1\n",
      "0.3333333333333333 1\n",
      "0.2 1\n",
      "1.0 1\n",
      "1.0 1\n",
      "1.0 1\n",
      "0.14285714285714285 1\n",
      "1.0 1\n",
      "0.05555555555555555 2\n",
      "1.0 1\n",
      "1.0 1\n",
      "0.1111111111111111 1\n",
      "0.5 2\n",
      "1.0 1\n",
      "0.13043478260869565 3\n",
      "0.5 1\n",
      "1.0 1\n",
      "0.5 1\n",
      "1.0 1\n",
      "0.3333333333333333 1\n",
      "1.0 1\n",
      "0.25 1\n",
      "0.3333333333333333 1\n",
      "1.0 1\n",
      "1.0 1\n",
      "0.14285714285714285 1\n",
      "0.07142857142857142 1\n",
      "0.3571428571428571 5\n",
      "0.05 2\n",
      "1.0 1\n",
      "0.09523809523809523 2\n",
      "0.1111111111111111 1\n",
      "0.2 1\n",
      "1.0 1\n",
      "1.0 1\n",
      "1.0 2\n",
      "0.3333333333333333 1\n",
      "1.0 1\n",
      "1.0 1\n",
      "1.0 1\n",
      "1.0 2\n",
      "0.6666666666666666 2\n",
      "1.0 1\n",
      "0.14285714285714285 3\n",
      "1.0 2\n",
      "1.0 2\n",
      "1.0 1\n",
      "0.06666666666666667 1\n",
      "0.09090909090909091 1\n",
      "0.043478260869565216 1\n",
      "0.0625 1\n",
      "0.1 3\n",
      "0.25 1\n",
      "1.0 1\n",
      "1.0 1\n",
      "1.0 8\n",
      "1.0 1\n",
      "0.1111111111111111 1\n",
      "0.09090909090909091 1\n",
      "0.2 1\n",
      "0.1282051282051282 5\n",
      "1.0 1\n",
      "0.04 1\n",
      "0.09090909090909091 1\n",
      "1.0 1\n",
      "0.21428571428571427 3\n",
      "0.16666666666666666 1\n",
      "0.3333333333333333 1\n",
      "1.0 1\n",
      "0.3333333333333333 1\n",
      "0.25 2\n",
      "0.125 1\n",
      "1.0 1\n",
      "0.043478260869565216 2\n",
      "0.5 1\n",
      "0.5 1\n",
      "0.1 2\n",
      "0.25 1\n",
      "0.14285714285714285 2\n",
      "0.25 1\n",
      "0.3333333333333333 1\n",
      "0.07142857142857142 1\n",
      "0.011363636363636364 1\n",
      "0.3333333333333333 1\n",
      "1.0 1\n",
      "0.5 1\n",
      "0.5 1\n",
      "1.0 1\n",
      "1.0 2\n",
      "0.2 1\n",
      "0.07142857142857142 1\n",
      "0.3333333333333333 1\n",
      "0.5 1\n",
      "0.125 1\n",
      "0.1 1\n",
      "1.0 1\n",
      "0.3333333333333333 1\n",
      "0.5 2\n",
      "1.0 2\n",
      "0.1 1\n",
      "0.16666666666666666 1\n",
      "0.03076923076923077 4\n",
      "1.0 1\n",
      "0.2 1\n",
      "0.5 1\n",
      "1.0 2\n",
      "0.16666666666666666 1\n",
      "1.0 2\n",
      "0.03636363636363636 2\n",
      "0.5 1\n",
      "1.0 1\n",
      "1.0 1\n",
      "0.05 1\n",
      "1.0 1\n",
      "0.25 1\n",
      "0.2 1\n",
      "0.5 1\n",
      "0.5 1\n",
      "1.0 1\n",
      "0.5 1\n",
      "0.25 1\n",
      "0.14285714285714285 1\n",
      "0.25 1\n",
      "0.14285714285714285 1\n",
      "0.6666666666666666 2\n",
      "0.5 1\n",
      "0.14285714285714285 1\n",
      "0.1 1\n",
      "0.5 1\n",
      "0.2 1\n",
      "0.08333333333333333 1\n",
      "0.1111111111111111 1\n",
      "0.3333333333333333 1\n",
      "0.08333333333333333 1\n",
      "0.16666666666666666 1\n",
      "0.5 1\n",
      "0.3333333333333333 1\n",
      "1.0 1\n",
      "0.5 1\n",
      "0.045454545454545456 2\n",
      "1.0 3\n",
      "0.16666666666666666 1\n",
      "1.0 2\n",
      "1.0 2\n",
      "0.3333333333333333 1\n",
      "1.0 1\n",
      "0.3333333333333333 1\n",
      "0.1111111111111111 3\n",
      "0.2 1\n",
      "1.0 1\n",
      "1.0 1\n",
      "1.0 1\n",
      "0.012345679012345678 1\n",
      "0.038461538461538464 1\n",
      "0.5 1\n",
      "0.25 1\n",
      "0.6666666666666666 2\n",
      "0.00641025641025641 1\n",
      "0.25 1\n",
      "0.07142857142857142 1\n",
      "0.6666666666666666 2\n",
      "1.0 2\n",
      "0.5 2\n",
      "1.0 1\n",
      "0.8333333333333333 5\n",
      "0.3333333333333333 2\n",
      "0.6666666666666666 2\n",
      "0.6666666666666666 2\n",
      "0.16666666666666666 1\n",
      "0.5 1\n",
      "1.0 3\n",
      "0.07142857142857142 1\n",
      "0.0196078431372549 1\n",
      "1.0 1\n",
      "0.16666666666666666 1\n",
      "0.3333333333333333 1\n",
      "0.16666666666666666 1\n",
      "1.0 1\n",
      "1.0 1\n",
      "1.0 2\n",
      "1.0 1\n",
      "1.0 1\n",
      "1.0 1\n",
      "0.4 2\n",
      "1.0 1\n",
      "0.5 1\n",
      "0.1111111111111111 1\n",
      "0.034482758620689655 1\n",
      "1.0 2\n",
      "1.0 1\n",
      "1.0 1\n",
      "0.5 1\n",
      "1.0 5\n",
      "1.0 1\n",
      "0.25 1\n",
      "0.1111111111111111 1\n",
      "1.0 1\n",
      "1.0 1\n",
      "0.3333333333333333 1\n",
      "1.0 1\n",
      "1.0 1\n",
      "1.0 1\n",
      "1.0 1\n",
      "1.0 3\n",
      "0.05555555555555555 1\n",
      "0.3333333333333333 1\n",
      "0.5 1\n",
      "1.0 1\n",
      "0.9999999999999999 7\n",
      "0.75 4\n",
      "0.9999999999999999 7\n",
      "1.0 5\n",
      "0.07894736842105263 3\n",
      "1.0 3\n",
      "1.0 1\n",
      "1.0 1\n",
      "1.0 1\n",
      "1.0 1\n",
      "0.5 1\n",
      "1.0 2\n",
      "0.2 1\n",
      "1.0 1\n",
      "0.034482758620689655 2\n",
      "0.05555555555555555 1\n",
      "1.0 1\n",
      "0.5 1\n",
      "1.0 1\n",
      "1.0 2\n",
      "0.125 1\n",
      "1.0 2\n",
      "1.0 2\n",
      "1.0 1\n",
      "1.0 1\n",
      "1.0 1\n",
      "1.0 1\n",
      "0.5 1\n",
      "1.0 1\n",
      "1.0 1\n",
      "1.0 1\n",
      "1.0 1\n",
      "1.0 1\n",
      "1.0 1\n",
      "1.0 1\n",
      "0.6666666666666666 2\n",
      "1.0 1\n",
      "1.0 1\n",
      "1.0 1\n",
      "1.0 1\n",
      "1.0 1\n",
      "1.0 1\n",
      "0.2222222222222222 2\n",
      "1.0 1\n",
      "1.0 1\n",
      "1.0 1\n",
      "0.5 1\n",
      "1.0 1\n",
      "1.0 1\n",
      "1.0 1\n",
      "1.0 1\n",
      "1.0 1\n",
      "1.0 1\n",
      "1.0 1\n",
      "1.0 1\n",
      "0.5 1\n",
      "1.0 1\n",
      "1.0 1\n",
      "1.0 1\n",
      "1.0 1\n",
      "0.5 1\n",
      "1.0 1\n",
      "1.0 1\n",
      "1.0 1\n",
      "1.0 1\n",
      "1.0 1\n",
      "1.0 1\n",
      "1.0 1\n",
      "0.3333333333333333 1\n",
      "1.0 1\n",
      "1.0 1\n",
      "1.0 1\n",
      "0.25 2\n",
      "1.0 2\n",
      "1.0 1\n",
      "1.0 1\n",
      "1.0 1\n",
      "0.25 1\n",
      "0.5 2\n",
      "1.0 1\n",
      "0.25 2\n",
      "1.0 1\n",
      "1.0 1\n",
      "1.0 2\n",
      "0.2 1\n",
      "0.5 1\n",
      "1.0 3\n",
      "1.0 1\n",
      "1.0 1\n",
      "1.0 1\n",
      "1.0 6\n",
      "1.0 3\n",
      "0.13793103448275862 4\n",
      "1.0 1\n",
      "1.0 1\n",
      "1.0 1\n",
      "1.0 1\n",
      "1.0 1\n",
      "1.0 1\n",
      "1.0 1\n",
      "1.0 2\n",
      "1.0 1\n",
      "0.125 1\n",
      "1.0 1\n",
      "1.0 1\n",
      "0.011764705882352941 1\n",
      "1.0 2\n",
      "1.0 1\n",
      "1.0 1\n",
      "1.0 1\n",
      "0.6666666666666666 2\n",
      "1.0 1\n",
      "1.0 1\n",
      "1.0 1\n",
      "0.16666666666666666 1\n",
      "0.5 1\n",
      "1.0 1\n",
      "1.0 1\n",
      "0.1111111111111111 1\n",
      "1.0 1\n",
      "1.0 1\n",
      "1.0 1\n",
      "1.0 1\n",
      "1.0 1\n",
      "1.0 1\n",
      "0.2 1\n",
      "1.0 1\n",
      "0.058823529411764705 1\n",
      "0.3333333333333333 3\n",
      "0.3333333333333333 1\n",
      "0.0625 1\n",
      "1.0 1\n",
      "1.0 1\n",
      "1.0 1\n",
      "1.0 1\n",
      "1.0 1\n",
      "1.0 1\n",
      "1.0 1\n",
      "0.25 1\n",
      "0.25 1\n",
      "1.0 3\n",
      "1.0 2\n",
      "1.0 1\n",
      "0.9 9\n",
      "1.0 3\n",
      "1.0 3\n",
      "1.0 1\n",
      "1.0 1\n",
      "0.07142857142857142 1\n",
      "0.3333333333333333 1\n",
      "1.0 1\n",
      "1.0 1\n",
      "1.0 1\n",
      "1.0 1\n",
      "0.3333333333333333 2\n",
      "1.0 1\n",
      "1.0 1\n",
      "1.0 2\n",
      "1.0 1\n",
      "0.5 1\n",
      "0.05 1\n",
      "0.5 1\n",
      "0.022222222222222223 1\n",
      "0.05555555555555555 1\n",
      "0.04 1\n",
      "0.08333333333333333 1\n",
      "0.25 1\n",
      "0.5 3\n",
      "1.0 1\n",
      "0.25 1\n",
      "0.5 2\n",
      "1.0 2\n",
      "1.0 1\n",
      "0.1 1\n",
      "1.0 1\n",
      "1.0 1\n",
      "1.0 1\n",
      "1.0 1\n",
      "0.5 1\n",
      "1.0 1\n",
      "0.6666666666666666 2\n",
      "1.0 1\n",
      "1.0 1\n",
      "1.0 1\n",
      "0.3333333333333333 1\n",
      "0.5 1\n",
      "0.3333333333333333 2\n",
      "1.0 1\n",
      "0.3333333333333333 1\n",
      "0.5 1\n",
      "0.02702702702702703 1\n",
      "1.0 1\n",
      "1.0 1\n",
      "1.0 1\n",
      "1.0 1\n",
      "1.0 1\n",
      "0.5 1\n",
      "0.25 1\n",
      "0.125 1\n",
      "1.0 2\n",
      "1.0 1\n",
      "1.0 1\n",
      "1.0 1\n",
      "1.0 1\n",
      "0.25 1\n",
      "1.0 1\n",
      "0.16666666666666666 1\n",
      "1.0 1\n",
      "0.5 1\n",
      "1.0 1\n",
      "1.0 2\n",
      "1.0 1\n",
      "1.0 1\n",
      "0.5 1\n",
      "0.3333333333333333 1\n",
      "1.0 1\n",
      "0.6666666666666666 2\n",
      "0.25 1\n",
      "0.004761904761904762 2\n",
      "1.0 1\n",
      "0.0 1\n",
      "0.0 1\n",
      "0.0 1\n",
      "0.0 1\n",
      "0.3333333333333333 1\n",
      "0.1 1\n",
      "0.25 2\n",
      "1.0 1\n",
      "0.25 1\n",
      "0.3333333333333333 1\n",
      "1.0 1\n",
      "0.07692307692307693 1\n",
      "1.0 1\n",
      "0.25 1\n",
      "0.2 1\n",
      "0.3333333333333333 1\n",
      "1.0 1\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n",
      "0.0 0\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(mms[0])):\n",
    "    print(mms[0][i],vectors[0][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# cosine_similarity\n",
      "[1.    0.315 0.261 0.263 0.314 0.298 0.306 0.314 0.313 0.307 0.063 0.317\n",
      " 0.409]\n",
      "[0.315 1.    0.284 0.308 0.343 0.332 0.323 0.3   0.314 0.276 0.036 0.284\n",
      " 0.291]\n",
      "[0.261 0.284 1.    0.262 0.285 0.285 0.266 0.252 0.263 0.243 0.023 0.26\n",
      " 0.243]\n",
      "[0.263 0.308 0.262 1.    0.339 0.305 0.298 0.297 0.296 0.267 0.018 0.282\n",
      " 0.244]\n",
      "[0.314 0.343 0.285 0.339 1.    0.349 0.348 0.306 0.338 0.305 0.02  0.319\n",
      " 0.285]\n",
      "[0.298 0.332 0.285 0.305 0.349 1.    0.349 0.338 0.342 0.302 0.022 0.298\n",
      " 0.293]\n",
      "[0.306 0.323 0.266 0.298 0.348 0.349 1.    0.353 0.338 0.306 0.016 0.302\n",
      " 0.316]\n",
      "[0.314 0.3   0.252 0.297 0.306 0.338 0.353 1.    0.341 0.311 0.011 0.298\n",
      " 0.318]\n",
      "[0.313 0.314 0.263 0.296 0.338 0.342 0.338 0.341 1.    0.372 0.027 0.303\n",
      " 0.319]\n",
      "[0.307 0.276 0.243 0.267 0.305 0.302 0.306 0.311 0.372 1.    0.023 0.28\n",
      " 0.313]\n",
      "[0.063 0.036 0.023 0.018 0.02  0.022 0.016 0.011 0.027 0.023 1.    0.023\n",
      " 0.031]\n",
      "[0.317 0.284 0.26  0.282 0.319 0.298 0.302 0.298 0.303 0.28  0.023 1.\n",
      " 0.305]\n",
      "[0.409 0.291 0.243 0.244 0.285 0.293 0.316 0.318 0.319 0.313 0.031 0.305\n",
      " 1.   ]\n"
     ]
    }
   ],
   "source": [
    "cosin = cos_sim(mms)\n",
    "print('# cosine_similarity')\n",
    "for index in range(len(cosin)):\n",
    "    print(np.round(cosin[index],3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000</td>\n",
       "      <td>0.315</td>\n",
       "      <td>0.261</td>\n",
       "      <td>0.263</td>\n",
       "      <td>0.314</td>\n",
       "      <td>0.298</td>\n",
       "      <td>0.306</td>\n",
       "      <td>0.314</td>\n",
       "      <td>0.313</td>\n",
       "      <td>0.307</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.317</td>\n",
       "      <td>0.409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.315</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.284</td>\n",
       "      <td>0.308</td>\n",
       "      <td>0.343</td>\n",
       "      <td>0.332</td>\n",
       "      <td>0.323</td>\n",
       "      <td>0.300</td>\n",
       "      <td>0.314</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.284</td>\n",
       "      <td>0.291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.261</td>\n",
       "      <td>0.284</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.262</td>\n",
       "      <td>0.285</td>\n",
       "      <td>0.285</td>\n",
       "      <td>0.266</td>\n",
       "      <td>0.252</td>\n",
       "      <td>0.263</td>\n",
       "      <td>0.243</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.260</td>\n",
       "      <td>0.243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.263</td>\n",
       "      <td>0.308</td>\n",
       "      <td>0.262</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.339</td>\n",
       "      <td>0.305</td>\n",
       "      <td>0.298</td>\n",
       "      <td>0.297</td>\n",
       "      <td>0.296</td>\n",
       "      <td>0.267</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.282</td>\n",
       "      <td>0.244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.314</td>\n",
       "      <td>0.343</td>\n",
       "      <td>0.285</td>\n",
       "      <td>0.339</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.349</td>\n",
       "      <td>0.348</td>\n",
       "      <td>0.306</td>\n",
       "      <td>0.338</td>\n",
       "      <td>0.305</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.319</td>\n",
       "      <td>0.285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.298</td>\n",
       "      <td>0.332</td>\n",
       "      <td>0.285</td>\n",
       "      <td>0.305</td>\n",
       "      <td>0.349</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.349</td>\n",
       "      <td>0.338</td>\n",
       "      <td>0.342</td>\n",
       "      <td>0.302</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.298</td>\n",
       "      <td>0.293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.306</td>\n",
       "      <td>0.323</td>\n",
       "      <td>0.266</td>\n",
       "      <td>0.298</td>\n",
       "      <td>0.348</td>\n",
       "      <td>0.349</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.353</td>\n",
       "      <td>0.338</td>\n",
       "      <td>0.306</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.302</td>\n",
       "      <td>0.316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.314</td>\n",
       "      <td>0.300</td>\n",
       "      <td>0.252</td>\n",
       "      <td>0.297</td>\n",
       "      <td>0.306</td>\n",
       "      <td>0.338</td>\n",
       "      <td>0.353</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.341</td>\n",
       "      <td>0.311</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.298</td>\n",
       "      <td>0.318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.313</td>\n",
       "      <td>0.314</td>\n",
       "      <td>0.263</td>\n",
       "      <td>0.296</td>\n",
       "      <td>0.338</td>\n",
       "      <td>0.342</td>\n",
       "      <td>0.338</td>\n",
       "      <td>0.341</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.303</td>\n",
       "      <td>0.319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.307</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.243</td>\n",
       "      <td>0.267</td>\n",
       "      <td>0.305</td>\n",
       "      <td>0.302</td>\n",
       "      <td>0.306</td>\n",
       "      <td>0.311</td>\n",
       "      <td>0.372</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.063</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.023</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.317</td>\n",
       "      <td>0.284</td>\n",
       "      <td>0.260</td>\n",
       "      <td>0.282</td>\n",
       "      <td>0.319</td>\n",
       "      <td>0.298</td>\n",
       "      <td>0.302</td>\n",
       "      <td>0.298</td>\n",
       "      <td>0.303</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.023</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.409</td>\n",
       "      <td>0.291</td>\n",
       "      <td>0.243</td>\n",
       "      <td>0.244</td>\n",
       "      <td>0.285</td>\n",
       "      <td>0.293</td>\n",
       "      <td>0.316</td>\n",
       "      <td>0.318</td>\n",
       "      <td>0.319</td>\n",
       "      <td>0.313</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.305</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0      1      2      3      4      5      6      7      8      9   \\\n",
       "0   1.000  0.315  0.261  0.263  0.314  0.298  0.306  0.314  0.313  0.307   \n",
       "1   0.315  1.000  0.284  0.308  0.343  0.332  0.323  0.300  0.314  0.276   \n",
       "2   0.261  0.284  1.000  0.262  0.285  0.285  0.266  0.252  0.263  0.243   \n",
       "3   0.263  0.308  0.262  1.000  0.339  0.305  0.298  0.297  0.296  0.267   \n",
       "4   0.314  0.343  0.285  0.339  1.000  0.349  0.348  0.306  0.338  0.305   \n",
       "5   0.298  0.332  0.285  0.305  0.349  1.000  0.349  0.338  0.342  0.302   \n",
       "6   0.306  0.323  0.266  0.298  0.348  0.349  1.000  0.353  0.338  0.306   \n",
       "7   0.314  0.300  0.252  0.297  0.306  0.338  0.353  1.000  0.341  0.311   \n",
       "8   0.313  0.314  0.263  0.296  0.338  0.342  0.338  0.341  1.000  0.372   \n",
       "9   0.307  0.276  0.243  0.267  0.305  0.302  0.306  0.311  0.372  1.000   \n",
       "10  0.063  0.036  0.023  0.018  0.020  0.022  0.016  0.011  0.027  0.023   \n",
       "11  0.317  0.284  0.260  0.282  0.319  0.298  0.302  0.298  0.303  0.280   \n",
       "12  0.409  0.291  0.243  0.244  0.285  0.293  0.316  0.318  0.319  0.313   \n",
       "\n",
       "       10     11     12  \n",
       "0   0.063  0.317  0.409  \n",
       "1   0.036  0.284  0.291  \n",
       "2   0.023  0.260  0.243  \n",
       "3   0.018  0.282  0.244  \n",
       "4   0.020  0.319  0.285  \n",
       "5   0.022  0.298  0.293  \n",
       "6   0.016  0.302  0.316  \n",
       "7   0.011  0.298  0.318  \n",
       "8   0.027  0.303  0.319  \n",
       "9   0.023  0.280  0.313  \n",
       "10  1.000  0.023  0.031  \n",
       "11  0.023  1.000  0.305  \n",
       "12  0.031  0.305  1.000  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosin = np.round(cosin,3)\n",
    "df = pd.DataFrame(cosin,list_label)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 主成分分析によるデータの圧縮化(標準化)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.30571528 0.16943287 0.0849565  0.06835538 0.06490385 0.0574546\n",
      " 0.05544767 0.04645715 0.04230637 0.03883215]\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "[116.34629908  86.61494477  61.33273226  55.01490413  53.60795238\n",
      "  50.43783141  49.54908631  45.35448967  43.28095887  41.46576691]\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "[[-1.18160964e+01  2.06532198e+01 -1.58291485e+00 -1.34507865e+00\n",
      "   7.34272066e-01 -4.21848692e+00 -1.17986159e+00 -9.20080918e+00\n",
      "   1.21000796e+01 -5.14824092e+00]\n",
      " [-7.39059816e+00 -1.25558051e+01  2.03533271e-01 -2.39158208e+00\n",
      "  -5.42101998e+00 -3.57650560e+00 -1.49981726e+01 -1.07674155e+01\n",
      "   3.16270821e+01 -6.61631430e+00]\n",
      " [-4.30205424e+00 -1.19505126e+01 -4.11783831e+00 -5.28660507e+00\n",
      "  -1.17443400e+01 -1.31779064e+01 -2.47759010e+01 -2.11026515e+01\n",
      "  -2.23001219e+01  8.35076134e+00]\n",
      " [-1.21302665e+00 -3.44299242e+01  2.26231028e+00 -2.59849771e+01\n",
      "  -2.72844568e+01  2.01657705e+01  1.49882116e+01  4.16740936e+00\n",
      "  -2.69635556e+00 -3.96773543e+00]\n",
      " [-9.56158069e-01 -3.48007571e+01  1.01068833e+01 -1.80913909e+01\n",
      "   4.16691013e+01 -4.08727019e+00  4.88262055e+00 -1.21681608e+00\n",
      "  -2.41079634e+00  8.03557812e-01]\n",
      " [-6.57856198e+00 -1.45157084e+01 -2.64502888e+00  7.15948472e+00\n",
      "  -4.11478316e+00 -1.45314062e+01 -1.70784460e+01  3.53083558e+01\n",
      "  -2.82797528e+00 -5.43605521e+00]\n",
      " [-6.64942064e+00 -1.36931780e+01  2.08504843e+00  3.39630373e+01\n",
      "   8.18360428e+00  3.27415509e+01 -6.50577987e+00 -4.33743781e+00\n",
      "  -5.36501450e+00 -1.84059001e+00]\n",
      " [-9.73525089e+00  2.29055722e+00 -6.97779187e+00  8.51560121e+00\n",
      "  -1.91534297e+00  2.21634302e-01  1.28556391e+00  4.55441073e+00\n",
      "   2.87901616e+00  2.36963515e+00]\n",
      " [-8.61906865e+00 -4.23018635e+00 -4.11198774e+00  1.16330375e+01\n",
      "  -4.67614767e+00 -9.81317852e+00  1.93796667e+01  1.80725072e+00\n",
      "   6.78626769e+00  3.20222354e+01]\n",
      " [-1.06853815e+01  1.89276820e+00 -7.94162243e+00  1.48886891e+01\n",
      "  -2.71978593e+00 -1.90747980e+01  2.55130820e+01 -8.16213320e+00\n",
      "  -9.33630976e+00 -2.22910067e+01]\n",
      " [-2.17186926e+01  4.47285817e+01 -3.49108777e+01 -1.81330787e+01\n",
      "   9.99320735e+00  1.30029464e+01 -2.02925813e+00  5.00768115e+00\n",
      "  -4.15824846e+00  1.26766046e+00]\n",
      " [ 1.09866945e+02  1.50646336e+01 -7.86927181e-02  1.28344677e-01\n",
      "   2.04291527e-01  4.09710832e-01  4.18460134e-01  6.86942234e-01\n",
      "   2.34878117e-01 -2.12308862e-01]\n",
      " [-2.02026356e+01  4.15463112e+01  4.77089792e+01 -5.05548198e+00\n",
      "  -2.90860003e+00  1.93793890e+00  9.98142261e-02  3.25521326e+00\n",
      "  -4.53250187e+00  6.98401265e-01]]\n"
     ]
    }
   ],
   "source": [
    "pca = PCA(n_components=10)\n",
    "pca.fit(data_std)\n",
    "print(pca.explained_variance_ratio_)\n",
    "print(\"------------------------------------------------------------------------------------------------------------------------------------------------------\")\n",
    "print(pca.singular_values_)\n",
    "print(\"------------------------------------------------------------------------------------------------------------------------------------------------------\")\n",
    "pca_X = pca.transform(data_std)\n",
    "print(pca_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 最終的なコサイン類似度の計算(標準化)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# cosine_similarity\n",
      "[ 1.     0.318 -0.167 -0.503 -0.398 -0.358 -0.264  0.295 -0.064  0.213\n",
      "  0.58  -0.307  0.477]\n",
      "[ 0.318  1.     0.076  0.108  0.057  0.059 -0.017  0.058 -0.092 -0.185\n",
      " -0.235 -0.224 -0.194]\n",
      "[-0.167  0.076  1.     0.063  0.001  0.052 -0.07  -0.207 -0.089 -0.084\n",
      " -0.145 -0.135 -0.173]\n",
      "[-0.503  0.108  0.063  1.     0.162  0.021 -0.019 -0.244 -0.03  -0.117\n",
      " -0.301 -0.101 -0.267]\n",
      "[-0.398  0.057  0.001  0.162  1.     0.044  0.027 -0.402 -0.06  -0.113\n",
      " -0.312 -0.096 -0.249]\n",
      "[-0.358  0.059  0.052  0.021  0.044  1.    -0.02   0.313 -0.044 -0.056\n",
      " -0.184 -0.182 -0.171]\n",
      "[-0.264 -0.017 -0.07  -0.019  0.027 -0.02   1.     0.304 -0.04  -0.065\n",
      " -0.189 -0.163 -0.134]\n",
      "[ 0.295  0.058 -0.207 -0.244 -0.402  0.313  0.304  1.     0.49   0.282\n",
      "  0.374 -0.575 -0.069]\n",
      "[-0.064 -0.092 -0.089 -0.03  -0.06  -0.044 -0.04   0.49   1.     0.099\n",
      " -0.093 -0.214 -0.092]\n",
      "[ 0.213 -0.185 -0.084 -0.117 -0.113 -0.056 -0.065  0.282  0.099  1.\n",
      " -0.011 -0.226 -0.061]\n",
      "[ 0.58  -0.235 -0.145 -0.301 -0.312 -0.184 -0.189  0.374 -0.093 -0.011\n",
      "  1.    -0.233  0.171]\n",
      "[-0.307 -0.224 -0.135 -0.101 -0.096 -0.182 -0.163 -0.575 -0.214 -0.226\n",
      " -0.233  1.    -0.215]\n",
      "[ 0.477 -0.194 -0.173 -0.267 -0.249 -0.171 -0.134 -0.069 -0.092 -0.061\n",
      "  0.171 -0.215  1.   ]\n"
     ]
    }
   ],
   "source": [
    "hoge = cos_sim(pca_X)\n",
    "print('# cosine_similarity')\n",
    "for index in range(len(hoge)):\n",
    "    print(np.round(hoge[index],3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 主成分分析だけしたデータでのコサイン類似度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.fit(vectors)\n",
    "pca_y = pca.transform(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# cosine_similarity\n",
      "[ 1.    -0.944 -0.935 -0.983 -0.981 -0.946  0.147  0.508  0.351 -0.516\n",
      "  0.995  0.779  0.998]\n",
      "[-0.944  1.     0.862  0.954  0.929  0.888 -0.21  -0.614 -0.324  0.48\n",
      " -0.932 -0.805 -0.943]\n",
      "[-0.935  0.862  1.     0.935  0.912  0.935 -0.232 -0.479 -0.471  0.359\n",
      " -0.933 -0.744 -0.942]\n",
      "[-0.983  0.954  0.935  1.     0.969  0.937 -0.262 -0.56  -0.42   0.475\n",
      " -0.969 -0.804 -0.98 ]\n",
      "[-0.981  0.929  0.912  0.969  1.     0.927 -0.185 -0.468 -0.396  0.413\n",
      " -0.971 -0.754 -0.978]\n",
      "[-0.946  0.888  0.935  0.937  0.927  1.    -0.158 -0.503 -0.405  0.363\n",
      " -0.946 -0.759 -0.955]\n",
      "[ 0.147 -0.21  -0.232 -0.262 -0.185 -0.158  1.    -0.019  0.466 -0.058\n",
      "  0.077  0.275  0.128]\n",
      "[ 0.508 -0.614 -0.479 -0.56  -0.468 -0.503 -0.019  1.     0.096 -0.434\n",
      "  0.521  0.484  0.524]\n",
      "[ 0.351 -0.324 -0.471 -0.42  -0.396 -0.405  0.466  0.096  1.     0.148\n",
      "  0.296  0.205  0.329]\n",
      "[-0.516  0.48   0.359  0.475  0.413  0.363 -0.058 -0.434  0.148  1.\n",
      " -0.497 -0.576 -0.496]\n",
      "[ 0.995 -0.932 -0.933 -0.969 -0.971 -0.946  0.077  0.521  0.296 -0.497\n",
      "  1.     0.742  0.997]\n",
      "[ 0.779 -0.805 -0.744 -0.804 -0.754 -0.759  0.275  0.484  0.205 -0.576\n",
      "  0.742  1.     0.778]\n",
      "[ 0.998 -0.943 -0.942 -0.98  -0.978 -0.955  0.128  0.524  0.329 -0.496\n",
      "  0.997  0.778  1.   ]\n"
     ]
    }
   ],
   "source": [
    "hoge = cos_sim(pca_y)\n",
    "print('# cosine_similarity')\n",
    "for index in range(len(hoge)):\n",
    "    print(np.round(hoge[index],3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 正規化の主成分分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.fit(mms)\n",
    "pca_z = pca.transform(mms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# cosine_similarity\n",
      "[ 1.     0.16  -0.134 -0.502 -0.392 -0.374 -0.293  0.229 -0.155  0.186\n",
      "  0.738 -0.291  0.534]\n",
      "[ 0.16   1.     0.036  0.068  0.019  0.045 -0.035 -0.066 -0.088 -0.167\n",
      " -0.229 -0.181 -0.172]\n",
      "[-0.134  0.036  1.     0.054 -0.011  0.039 -0.083 -0.235 -0.102 -0.095\n",
      " -0.152 -0.108 -0.184]\n",
      "[-0.502  0.068  0.054  1.     0.154  0.01  -0.03  -0.26  -0.04  -0.138\n",
      " -0.314 -0.069 -0.272]\n",
      "[-0.392  0.019 -0.011  0.154  1.     0.036  0.009 -0.413 -0.064 -0.125\n",
      " -0.31  -0.072 -0.251]\n",
      "[-0.374  0.045  0.039  0.01   0.036  1.    -0.049  0.355 -0.067 -0.079\n",
      " -0.194 -0.166 -0.169]\n",
      "[-0.293 -0.035 -0.083 -0.03   0.009 -0.049  1.     0.276 -0.053 -0.05\n",
      " -0.185 -0.147 -0.119]\n",
      "[ 0.229 -0.066 -0.235 -0.26  -0.413  0.355  0.276  1.     0.358  0.144\n",
      "  0.53  -0.496 -0.082]\n",
      "[-0.155 -0.088 -0.102 -0.04  -0.064 -0.067 -0.053  0.358  1.     0.092\n",
      " -0.092 -0.182 -0.088]\n",
      "[ 0.186 -0.167 -0.095 -0.138 -0.125 -0.079 -0.05   0.144  0.092  1.\n",
      "  0.01  -0.219 -0.049]\n",
      "[ 0.738 -0.229 -0.152 -0.314 -0.31  -0.194 -0.185  0.53  -0.092  0.01\n",
      "  1.    -0.272  0.204]\n",
      "[-0.291 -0.181 -0.108 -0.069 -0.072 -0.166 -0.147 -0.496 -0.182 -0.219\n",
      " -0.272  1.    -0.231]\n",
      "[ 0.534 -0.172 -0.184 -0.272 -0.251 -0.169 -0.119 -0.082 -0.088 -0.049\n",
      "  0.204 -0.231  1.   ]\n"
     ]
    }
   ],
   "source": [
    "hoge = cos_sim(pca_z)\n",
    "print('# cosine_similarity')\n",
    "for index in range(len(hoge)):\n",
    "    print(np.round(hoge[index],3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
