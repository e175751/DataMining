{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データマイニング Report3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 全体の流れ\n",
    "    + NLTKの解説本の0章〜12章まで、計13個のHTMLファイルをダウンロードせよ。\n",
    "    + BoWベースの特徴ベクトル（Level 1 もしくは Level 2）を生成せよ。\n",
    "    + 共起行列ベースの特徴ベクトル（Level3）を生成せよ。\n",
    "    + ラベル付き文書に対して分類タスク（Level4）を実行せよ。\n",
    "+ Level 1: 文書ファイル毎に、``Bag-of-Words``で特徴ベクトルを生成せよ。\n",
    "+ Level 2: ``BoW``に``TF-IDF``で重み調整した特徴ベクトルを生成せよ。\n",
    "+ Level 3: 単語の``共起行列``から特徴ベクトルを生成せよ。\n",
    "+ Level 4: ``文書分類``せよ。\n",
    "+ オプション例\n",
    "    + 相互情報量から``特徴ベクトル``を生成してみよう。\n",
    "    + 共起行列に基づいた特徴ベクトル、もしくは相互特徴量に基づいた特徴ベクトルを``SVD``により``次元削減``してみよう。\n",
    "    + SVDによる次元削減時に``2次元``とせよ。気になる単語1つを選び、上位10件と下位10件を2次元空間にマッピングせよ。マッピング結果、どのように散らばっているか観察し、想定とどのぐらい似通っているか考察してみよう。\n",
    "    + ``日本語文書``について自然言語処理してみよう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import wordpunct_tokenize, sent_tokenize\n",
    "import numpy as np\n",
    "import glob\n",
    "import scipy.spatial.distance as distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ collect_words_eng(): 英文書集合から単語コードブック作成\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nltkのdownloadするべきmoudle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/e175751/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/e175751/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
      "[nltk_data] Downloading package punkt to /Users/e175751/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag-of-Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文書集合からターム素性集合（コードブック）を作る"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def collect_words_eng(docs):\n",
    "    '''\n",
    "    英文書集合から単語コードブック作成。\n",
    "    シンプルに文書集合を予め決めうちした方式で処理する。\n",
    "    必要に応じて指定できるようにしていた方が使い易いかも。\n",
    "\n",
    "    :param docs(list): 1文書1文字列で保存。複数文書をリストとして並べたもの。\n",
    "    :return (list): 文分割、単語分割、基本形、ストップワード除去した、ユニークな単語一覧。\n",
    "    '''\n",
    "    \n",
    "    codebook = []\n",
    "    stopwords = nltk.corpus.stopwords.words('english') \n",
    "    \n",
    "    #stopwords.append('.')   # ピリオドを追加。\n",
    "    #stopwords.append(',')   # カンマを追加。\n",
    "    #stopwords.append('')    # 空文字を追加。\n",
    "    \n",
    "    symbol = [\"'\", '\"', ':', ';', '.', ',', '-', '!', '?', \"'s\"]\n",
    "    clean_frequency = nltk.FreqDist(w.lower() for w in docs if w.lower() not in stopwords + symbol)\n",
    "    \n",
    "    wnl = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "    \n",
    "    for doc in docs:\n",
    "        for sent in sent_tokenize(doc):\n",
    "            for word in wordpunct_tokenize(sent):\n",
    "                this_word = wnl.lemmatize(word.lower())\n",
    "                if this_word not in codebook and this_word not in clean_frequency:\n",
    "                    codebook.append(this_word)\n",
    "    return codebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``clean_frequencya``を使った場合\n",
    "これにより、vector数が10個になる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs3 = []\n",
    "docs3.append(\"This is test.\")\n",
    "docs3.append(\"That is test too.\")\n",
    "docs3.append(\"There are so many many tests.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "codebook =  ['this', 'is', 'test', '.', 'that', 'too', 'there', 'are', 'so', 'many']\n"
     ]
    }
   ],
   "source": [
    "codebook = collect_words_eng(docs3)\n",
    "print('codebook = ',codebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``stopwords``のままの場合\n",
    "これにより、vector数が2個となる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "codebook =  ['test', 'many']\n"
     ]
    }
   ],
   "source": [
    "codebook = collect_words_eng(docs3)\n",
    "print('codebook = ',codebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## コードブックを素性とする文書ベクトルを作る (直接ベクトル生成)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_vectors_eng(docs, codebook):\n",
    "    '''コードブックを素性とする文書ベクトルを作る（直接ベクトル生成）\n",
    "\n",
    "    :param docs(list): 1文書1文字列で保存。複数文書をリストとして並べたもの。\n",
    "    :param codebook(list): ユニークな単語一覧。\n",
    "    :return (list): コードブックを元に、出現回数を特徴量とするベクトルを返す。\n",
    "    '''\n",
    "    vectors = []\n",
    "    wnl = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "    for doc in docs:\n",
    "        this_vector = []\n",
    "        fdist = nltk.FreqDist()\n",
    "        for sent in sent_tokenize(doc):\n",
    "            for word in wordpunct_tokenize(sent):\n",
    "                this_word = wnl.lemmatize(word.lower())\n",
    "                fdist[this_word] += 1\n",
    "        for word in codebook:\n",
    "            this_vector.append(fdist[word])\n",
    "        vectors.append(this_vector)\n",
    "    return vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docs[0] = This is test.\n",
      "vectors[0] = [1, 1, 1, 1, 0, 0, 0, 0, 0, 0]\n",
      "----\n",
      "docs[1] = That is test too.\n",
      "vectors[1] = [0, 1, 1, 1, 1, 1, 0, 0, 0, 0]\n",
      "----\n",
      "docs[2] = There are so many many tests.\n",
      "vectors[2] = [0, 0, 1, 1, 0, 0, 1, 1, 1, 2]\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "vectors = make_vectors_eng(docs3, codebook)\n",
    "for index in range(len(docs3)):\n",
    "    print('docs[{}] = {}'.format(index,docs3[index]))\n",
    "    print('vectors[{}] = {}'.format(index,vectors[index]))\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ユークリッド距離"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(vectors):\n",
    "    vectors = np.array(vectors)\n",
    "    distances = []\n",
    "    for i in range(len(vectors)):\n",
    "        temp = []\n",
    "        for j in range(len(vectors)):\n",
    "            temp.append(np.linalg.norm(vectors[i] - vectors[j]))\n",
    "        distances.append(temp)\n",
    "    return distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# euclidean_distance\n",
      "[0.0, 1.7320508075688772, 3.0]\n",
      "[1.7320508075688772, 0.0, 3.1622776601683795]\n",
      "[3.0, 3.1622776601683795, 0.0]\n"
     ]
    }
   ],
   "source": [
    "distances = euclidean_distance(vectors)\n",
    "print('# euclidean_distance')\n",
    "for index in range(len(distances)):\n",
    "    print(distances[index])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## コサイン類似度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vectors):\n",
    "    vectors = np.array(vectors)\n",
    "    distances = []\n",
    "    for i in range(len(vectors)):\n",
    "        temp = []\n",
    "        for j in range(len(vectors)):\n",
    "            temp.append(distance.cosine(vectors[i], vectors[j]))\n",
    "        distances.append(temp)\n",
    "    return distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# cosine_similarity\n",
      "[0.0, 0.3291796067500631, 0.6666666666666667]\n",
      "[0.3291796067500631, 0.0, 0.7018576030000281]\n",
      "[0.6666666666666667, 0.7018576030000281, 0.0]\n"
     ]
    }
   ],
   "source": [
    "similarities = cosine_similarity(vectors)\n",
    "print('# cosine_similarity')\n",
    "for index in range(len(similarities)):\n",
    "    print(similarities[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## それでは実際に文章を分類する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fileのpathを配列に格納する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "List_Data_NL=[]\n",
    "for i in range(1,14):\n",
    "    List_Data_NL = glob.glob( \"./data/*.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sentence = []\n",
    "for l in List_Data_NL:\n",
    "    with open(l) as f:\n",
    "        r = f.read()\n",
    "        sentence.append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### コードブック生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "codebook = collect_words_eng(sentence)\n",
    "print('codebook = ',codebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 文書ベクトル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = make_vectors_eng(docs, codebook)\n",
    "for index in range(len(docs3)):\n",
    "    print('docs[{}] = {}'.format(index,docs3[index]))\n",
    "    print('vectors[{}] = {}'.format(index,vectors[index]))\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ユークリッド距離を求める"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = euclidean_distance(vectors)\n",
    "print('# euclidean_distance')\n",
    "for index in range(len(distances)):\n",
    "    print(distances[index])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### コサイン類似度を求める"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarities = cosine_similarity(vectors)\n",
    "print('# cosine_similarity')\n",
    "for index in range(len(similarities)):\n",
    "    print(similarities[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
